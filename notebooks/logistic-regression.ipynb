{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "767fa17a",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "- Classification: targets are discrete, called \"labels\"\n",
    "- Binary Classification: only two classes, usually $y=0,1$\n",
    "- Decision boundary: Line where $\\hat y = 0.5$, separating the classes 0 and 1\n",
    "- Could try linear regression to fit binary problem, but it doesn't work all that well and allows values outside of the range $[0,1]$ since it's highly sensitive to outliers on either side of the decision boundary\n",
    "- Logistic Regression: Choose functions $\\hat y= f_\\theta(x) = \\sigma(x^\\top \\theta)$, or equivalently $\\hat y=\\sigma(x^\\top W + b)$\n",
    "- Model $y=0,1$ with $\\hat y = p(y=1|x,\\theta)$ for positive examples, so $1 - \\hat y = p(y=1|x,\\theta)$ for negative examples\n",
    "- Can re-write both of these equations simultaneously by using $p(y|x,\\theta) = \\hat y^y (1 - \\hat y)^{1-y}$, which looks like a Bernoulli random variable with $p_0=\\hat y$\n",
    "- Question: Why not use the simple estimators $\\hat y = \\frac{|y=1|}{m}$, which is the accuracy?\n",
    "- Loss for logistic regression is the NNL assuming the $y$ are IID $Ber(\\hat y)$, which is the binary cross entropy\n",
    "$$J(\\theta) = -\\frac{1}{m}\\sum_{i=1}^m \\hat y^{y^{(i)}} (1 - \\hat y)^{1-y^{(i)}} = -\\frac{1}{m}\\sum_{i=1}^m f_\\theta(x^{(i)})^{y^{(i)}} (1 - f_\\theta(x^{(i)}))^{1-y^{(i)}}$$\n",
    "- The binary cross entropy loss is convex for logistic regression, which ensures the only stationary point is the global min, hence optimizers will always converge\n",
    "- Logits (\"log odds\") $z = \\log \\frac{\\hat y}{1 - \\hat y}$. Note $\\hat y = \\sigma(z)$.\n",
    "- Cover the notation zoo. Using the Ng convention. Will always assume $X$ is $(m,n)$ and multiplies parameters on the left as $X\\theta$, equivalent to individual examples multiplying as $x^\\top \\theta$\n",
    "- Exponential family: Density function of the form $p(x|\\theta) = b(x) \\exp(T(x)^\\top \\theta - a(\\theta))$\n",
    "    - $x$: \"data\"\n",
    "    - $\\theta$: \"natural parameter\"\n",
    "    - $T(x)$: \"sufficient statistic\" (usually $T(x)=x$ in practice)\n",
    "    - $b(x)$: \"base measure\"\n",
    "    - $a(\\theta)$: \"log partition function\" (i.e. $a(\\theta) = \\log Z(\\theta)$)\n",
    "    - the NNL is always convex wrt $\\theta$, hence a unique global min\n",
    "    - mean is $\\langle x \\rangle = \\frac{d}{d\\theta} a(\\theta) = \\frac{d}{d\\theta} \\log Z(\\theta)$\n",
    "    - variance is $\\sigma^2 = \\frac{d^2}{d\\theta^2} a(\\theta) = \\frac{d^2}{d\\theta^2} \\log Z(\\theta)$\n",
    "- Generalized linear models (GLMs): Assume\n",
    "    - $y|X,\\theta$ is an exponential family in $y$,\n",
    "    - $X \\theta$ is the natural parameter.\n",
    "    - Predictions given by $\\hat y = f_\\theta(X) = \\langle y|X,\\theta \\rangle$\n",
    "    - The function $f_\\theta(X) = \\frac{d}{d(X\\theta)} \\log Z(X \\theta)$ specifies the form of a GLM, called the \"inverse link function\"\n",
    "    - Link function is given by $f^{-1}_\\theta(X)$, and satisfies $f^{-1}_\\theta(\\hat y) = X\\theta$\n",
    "    - The form of the gradient is the same for all GLMs, hence so is the learning update rule, $\\theta \\leftarrow \\theta - \\alpha \\big(y^{(i)} - f_\\theta(x^{(i)})\\big) x^{(i)}$\n",
    "- Logistic regression falls right out of the GLM, assuming $y$ is Bernoulli with $p_0=f_\\theta(X)$\n",
    "- When covering this material, maybe introduce logistic regression first informally (sigmoid is natural for binary probabilities), and then towards the end give the probabilistic interpretation as a GLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d385ab",
   "metadata": {},
   "source": [
    "- Multiclass Classification: Any of $K$ classes $0,1,\\cdots,K-1$\n",
    "- By convention, treat each $y_k$ as a one-hot vector, $y_k = e_k = (0,\\cdots,1,\\cdots,0)$, so data targets become a 2D array $y \\in \\{0,1\\}^{m \\times k}$\n",
    "- Decision boundaries are now logits $X \\theta_k$, where $\\boldsymbol{\\theta} = [\\theta_0 \\cdots \\theta_{K-1}]^\\top$ is now a $(K,n+1)$ matrix of parameters\n",
    "- Multiclass generalization of logistic regression is \"softmax regression\"\n",
    "- Instead of a sigmoid, uses the softmax function $\\hat y_k = f_\\theta(X) = \\text{softmax}(X\\theta_k) = \\boldsymbol{\\sigma}(X\\theta_k)$, which is just normalized, exponentiated logits (or normalized counts)\n",
    "- Want to make the parameter distribution $p(\\hat y|X,\\theta)$ match the true distribution of the data $p(y|X,\\theta)$ as close as possible, i.e. we want to minimize the cross entropy between the data $p$ distribution and model distribution $\\hat p$,\n",
    "$$\\text{CE}(p, \\hat p) = -\\sum_{k=0}^{K-1} p_k \\log(\\hat p_k) = -[y=c] \\log(\\hat p_c) = -\\log(\\text{softmax}(X\\theta_c))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fa30c5",
   "metadata": {},
   "source": [
    "- Generative vs Discriminative algorithms:\n",
    "    - Generative learns $p(X|y)$, i.e. what do the features look like for each given class, along with $p(y)$, which can then be put together via Bayes rule to get $p(y|X) \\propto p(X|y)p(y)$\n",
    "    - Discriminative directly learns $p(y|X)$, i.e. which class is likely given a set of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e917b504",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f645f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520c9144",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744075a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6868cce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebcb430",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da9d38c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1fafa8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce3fbdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd26cf4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
