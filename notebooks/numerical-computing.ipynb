{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00ec3d6f",
   "metadata": {},
   "source": [
    "# Numerical Computation\n",
    "\n",
    "In this lesson I'll discuss the basics of numerical computation, specifically what integers and floating point numbers are and how they're represented on a computer. This may seem too basic to mention, but it's actually very important. There's a lot of subtlety involved. Let's get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "122dd7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils.math_ml import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec5f9224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this makes every line of a cell print instead of just the last line\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f73479",
   "metadata": {},
   "source": [
    "## Integers\n",
    "\n",
    "### Basics\n",
    "\n",
    "Recall the **integers** are whole numbers that can be positive, negative, or zero. Examples are 5, 100151, 0, -72, etc. The set of all integers is commonly denoted by the symbol $\\mathbb{Z}$.\n",
    "\n",
    "In python, integers (ints for short) are builtin objects of type `int` that more or less follow the rules that integers in math follow.\n",
    "\n",
    "Among other things, the following operations can be performed with integers:\n",
    "- Addition: $2 + 2 = 4$.\n",
    "- Subtraction: $2 - 5 = -3$.\n",
    "- Multiplication: $3 \\times 3 = 9$ (in python this is the `*` operator, e.g. `3 * 3 = 9`)\n",
    "- Exponentiation: $2^3 = 2 \\times 2 \\times 2 = 8$ (in python this is the `**` operator, e.g. `2 ** 3 = 8`).\n",
    "- Remainder (or Modulo): the remainder of 10 when divided by 3 is 1, written $10 \\text{ mod } 3 = 1$ (in python this is the `%` operator, e.g. `10 % 3 = 1`).\n",
    "\n",
    "If any of these operations are applied to two integers, the output will itself always be an integer.\n",
    "\n",
    "Here are a few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ae73635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "-3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 + 2\n",
    "2 - 5\n",
    "3 * 3\n",
    "10 % 3\n",
    "2 ** 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbbd25d",
   "metadata": {},
   "source": [
    "What about division? You can't always divide two integers and get another integer. What you have to do instead is called integer division. Here you divide the two numbers and then round the answer down to the nearest whole number. Since $5 \\div 2 = 2.5$, the nearest rounded down integer is 2. \n",
    "\n",
    "In math, this \"nearest rounded down integer\" 2 is usually called the **floor** of 2.5, and represented with the funny symbol $\\lfloor 2.5 \\rfloor.$ Using this notation we can write the above integer division as \n",
    "$$\\big\\lfloor \\frac{5}{2} \\big\\rfloor = 2.$$\n",
    "\n",
    "In python, integer division is done using the `//` operator, e.g. `5 // 2 = 2`. I'll usually write $5 \\ // \\ 2$ instead of $\\big\\lfloor \\frac{5}{2} \\big\\rfloor$ when it makes sense,\n",
    "$$5 \\ // \\ 2 = \\big\\lfloor \\frac{5}{2} \\big\\rfloor = 2.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4b69b3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5 // 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41db816a",
   "metadata": {},
   "source": [
    "We can also do regular division `/` with ints, but the output will *not* be an integer even if the answer should be, e.g. `4 / 2`. Only integer division is guaranteed to return an integer. I'll get to this shortly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "683046fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4 / 2\n",
    "type(4 / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e637d622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4 // 2\n",
    "type (4 // 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0cee34",
   "metadata": {},
   "source": [
    "Division by zero is of course undefined for both division and integer division. In python it will always raise a `ZeroDivisionError` like so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52cd06fc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;241;43m4\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "4 / 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d793253",
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "integer division or modulo by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;241;43m4\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: integer division or modulo by zero"
     ]
    }
   ],
   "source": [
    "4 // 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917090ba",
   "metadata": {},
   "source": [
    "### Representing Integers\n",
    "\n",
    "Just like every other data type, on a computer integers are actually represented internally as a sequence of bits.\n",
    "A **bit** is a \"binary digit\", 0 or 1. A sequence of bits is just a sequence of zeros and ones, e.g. 0011001010 or 1001001.\n",
    "\n",
    "The number of bits used to represent a piece of data is called its **word size**. If we use a word size of $n$ bits to represent an integer, then there are $2^n$ possible integer values we can represent.\n",
    "\n",
    "If integers could only be positive or zero, representing them with bits would be easy. We could just convert them to binary and that's it. To convert a non-negative integer to binary, we just need to keep dividing it by 2 and recording its remainder (0 or 1) at each step. The binary form is then just the sequence of remainders, written right to left. More generally, the binary sequence of some arbitrary number $x$ is the sequence of coefficients $b_k=0,1$ in the sum \n",
    "\n",
    "$$x = \\sum_{k=-\\infty}^\\infty b_k 2^k = \\cdots + b_2 2^2 + b_1 2^1 + b_0 2^0 + b_{-1} 2^{-1} + b_{-2} 2^{-2} + \\cdots.$$\n",
    "\n",
    "Here's an example. Suppose we wanted to represent the number $12$ in binary.\n",
    "1. $12 \\ // \\ 2 = 6$ with a remainder of $0 = 12 \\text{ mod } 2$, so the first bit from the right is then $0$.\n",
    "2. $6 \\ // \\ 2 = 3$ with a remainder of $0 = 6 \\text{ mod } 2$, so the next bit is $0$.\n",
    "3. $3 \\ // \\ 2 = 1$ with a remainder of $1 = 3 \\text{ mod } 2$, so the next bit is $1$.\n",
    "4. $1 \\ // \\ 2 = 0$ with a remainder of $1 = 1 \\text{ mod } 2$, so the next bit is $1$.\n",
    "\n",
    "So the binary representation of $12$ is $1100$, which is the sequence of coefficients in the sum\n",
    "\n",
    "$$12 = 1 \\cdot 2^{3} + 1 \\cdot 2^{2} + 0 \\cdot 2^{1} + 0 \\cdot 2^{0}.$$\n",
    "\n",
    "Rather than keep doing these by hand, you can quickly convert a number to binary in python by using `bin`. It'll return a string representing the binary sequence of that number, prepended with the special prefix `0b`. To get back to the integer from, use `int`, passing in a base of `2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b2b6e66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0b1100'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bin(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f1beac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int('0b110', 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6841291",
   "metadata": {},
   "source": [
    "This representation works fine for non-negative integers, also called the **unsigned integers** in computer science. To represent an unsigned integer with $n$ bits, just get its binary form and prepend it with enough zeros on the left until all $n$ bits are used. For example, if we used 8-bit unsigned integers then $n=8$, hence representing the number $12$ would look like $00000110$. Simple, right?\n",
    "\n",
    "Unsigned ints work fine if we never have to worry about negative numbers. But in general we do. These are called the **signed integers** in computer science. To represent signed ints, we need to use one of the bits to represent the sign. What we can do is reserve the left-most bit for the sign, $0$ if the integers is positive or zero, $1$ if the integer is negative.\n",
    "\n",
    "For example, if we used 8-bit *signed* integers to represent $12$, we'd again write $00000110$, exactly as before. But this time it's understood that left-most $0$ is encoding the fact that $12$ is positive. If instead we wanted to represent the number $-12$ we'd need to flip that bit to a $1$, so we'd get $10000110$.\n",
    "\n",
    "Let's now do an example of a simple integer system. Consider the system of 4-bit signed ints. In this simple system, $n=4$ is the word size, and an integer $x$ is represented with the sequence of bits\n",
    "\n",
    "$$x \\equiv sb_1b_2b_3,$$ \n",
    "\n",
    "where $s$ is the sign bit and $b_1b_2b_3$ are the remaining 3 bits allowed to represent the numerical digits. This system can represent $2^4=16$ possible values in the range $[-2^3+1,2^3-1] = [-8,7]$, given in the following table:\n",
    "\n",
    "| Integer | Representation | Integer | Representation |\n",
    "|---------|----------------|---------|----------------|\n",
    "| -0      | 1000           | +0      | 0000           |\n",
    "| -1      | 1001           | 1       | 0001           |\n",
    "| -2      | 1010           | 2       | 0010           |\n",
    "| -3      | 1011           | 3       | 0011           |\n",
    "| -4      | 1100           | 4       | 0100           |\n",
    "| -5      | 1101           | 5       | 0101           |\n",
    "| -6      | 1110           | 6       | 0110           |\n",
    "| -7      | 1111           | 7       | 0111           |\n",
    "\n",
    "Note the presence of $-0 \\equiv 1110$ in the upper left. This is because the system as I've defined it leaves open the possibility of two zeros, $+0$ and $-0$, since for zero the sign bit is redundant. A way to get around this is to encode the negative numbers slightly differently, by not just setting the sign bit to one, but also inverting the remaining bits and subtracting one from them. This is called the **two's complement representation**. It's how most languages, including python, actually represent integers. I won't go into this representation in any depth, except to say that it gets rid of the need for $-0$ and replaces it with $-2^{n-1}$.\n",
    "\n",
    "Here's what that table looks like for 4-bit integers. It's almost the same, except there's no $-0$, instead a $-8$. Notice the positive integers look exactly the same. It's only the negative integers that look different. For them, the right three bits get inverted and added with a one.\n",
    "\n",
    "| Integer | Two's Complement | Integer | Two's Complement |\n",
    "|---------|----------------|---------|----------------|\n",
    "| -1      | 1111           | 0       | 0000           |\n",
    "| -2      | 1110           | 1       | 0001           |\n",
    "| -3      | 1101           | 2       | 0010           |\n",
    "| -4      | 1100           | 3       | 0011           |\n",
    "| -5      | 1011           | 4       | 0100           |\n",
    "| -6      | 1010           | 5       | 0101           |\n",
    "| -7      | 1001           | 6       | 0110           |\n",
    "| -8      | 1000           | 7       | 0111           |\n",
    "\n",
    "It's worth visualizing what integers look like on the number line, if for nothing else than to compare it with what floats look like later on. Below I'll plot what a 6-bit signed integer system would look like. Such a system should go from -32 to 31. I'll to use the helper function `plot_number_dist` to do the plotting. As you'd expect, you just see a bunch of equally spaced points from -32 to 31."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70352bb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA7oAAACPCAYAAADUW6fuAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/P9b71AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAgzklEQVR4nO3deVRV9frH8c/BGAQBcQBkEHFIM1G7lorYVRRFI4ey1O6txMomtSyvN9MKtcmllVOWDXeh2WBpA9pgzjagZlzSMrXIKSUkS4FyQOD7+8PL+XXkgEjAOW3fr7VYsr/7u/fzcM4D8rAnmzHGCAAAAAAAi/BwdQIAAAAAAFQnGl0AAAAAgKXQ6AIAAAAALIVGFwAAAABgKTS6AAAAAABLodEFAAAAAFgKjS4AAAAAwFJodAEAAAAAlkKjCwAAAACwFBpdAPiLmDJlimw2W63E6tmzp3r27Glf3rBhg2w2m5YtW1Yr8ZOTk9WsWbNaiVVVv/32m2677TaFhobKZrNp3LhxNRYrOTlZ9erVq9Rcm82mKVOm1Fgupfbt2yebzaaFCxfWeKw/Y+HChbLZbNq3b5+rUwEA1CIaXQBwgdJfvks/fHx8FBYWpsTERM2dO1cFBQXVEic7O1tTpkzRV199VS37q07unFtlPPHEE1q4cKHuuusuLV68WDfddFOF8wsLC/XEE0+oTZs28vHxUUhIiJKSknTw4MEazTM9PV1TpkzRsWPHKr3NihUr1KNHDwUHB8vX11fNmzfX0KFDtXLlyppL1MVK/5B05MiR8972r17LAGBFF7k6AQC4kE2bNk3R0dE6ffq0cnJytGHDBo0bN07PPPOMli9frvbt29vnPvTQQ5o4ceJ57T87O1tTp05Vs2bN1LFjx0pvt2rVqvOKUxUV5fbSSy+ppKSkxnP4M9atW6euXbsqJSXlnHNPnz6tpKQkpaena9SoUWrfvr2OHj2qLVu2KC8vTxEREdWW14kTJ3TRRf//33t6erqmTp2q5ORk1a9f/5zbP/XUU5owYYJ69OihBx98UL6+vsrKytKaNWu0ZMkS9evXT5IUFRWlEydOyNPTs9py/6uq6vcZAKDm0OgCgAv1799fl19+uX35wQcf1Lp163T11Vdr4MCB2rlzp+rWrStJuuiiixwamJpw/Phx+fr6ysvLq0bjnMtfoXnKzc1V27ZtKzV31qxZ2rhxoz777DN17ty5RvPy8fGp8rZFRUV69NFH1adPH6d/7MjNzbV/XnomAgAA7ohTlwHAzfTq1UsPP/yw9u/fr1dffdU+7uwa3dWrV6t79+6qX7++6tWrp9atW2vSpEmSzlxXe8UVV0iSRo4caT9NuvSayp49e6pdu3bKyMjQ3//+d/n6+tq3Pfsa3VLFxcWaNGmSQkND5efnp4EDB+rHH390mNOsWTMlJyeX2faP+zxXbs6u0f399981fvx4RUZGytvbW61bt9ZTTz0lY4zDPJvNpjFjxui9995Tu3bt5O3trUsvvbTSp93m5ubq1ltvVUhIiHx8fNShQwctWrTIvr70euW9e/fqgw8+sOde3jWgJSUlmjNnjq655hp17txZRUVFOn78eKVyOduePXuUmJgoPz8/hYWFadq0aU6//tJrdKdMmaIJEyZIkqKjo8+Z65EjR5Sfn6+4uDin64ODg+2fl3eN7tKlS9W2bVv5+PioXbt2evfdd8u8n6XbPvXUU3rxxRfVokULeXt764orrtDWrVvLxN21a5euu+46NWjQQD4+Prr88su1fPnyMvN27NihXr16qW7duoqIiNBjjz32p84MKP0e+fbbbxUfHy9fX1+Fh4drxowZ9jnnquXvv/9eQ4YMUWhoqHx8fBQREaHhw4crLy+vynkBAM6NI7oA4IZuuukmTZo0SatWrdKoUaOcztmxY4euvvpqtW/fXtOmTZO3t7eysrL0+eefS5IuueQSTZs2TY888ohuv/12XXnllZKkbt262ffxyy+/qH///ho+fLhuvPFGhYSEVJjX448/LpvNpgceeEC5ubmaPXu2EhIS9NVXX9mPPFdGZXL7I2OMBg4cqPXr1+vWW29Vx44d9fHHH2vChAk6dOiQZs2a5TD/s88+0zvvvKO7775b/v7+mjt3roYMGaIDBw6oYcOG5eZ14sQJ9ezZU1lZWRozZoyio6O1dOlSJScn69ixY7r33nt1ySWXaPHixbrvvvsUERGh8ePHS5IaN27sdJ/ffvutsrOz1b59e91+++1atGiRCgsLFRMTozlz5ig+Pr5Sr1lxcbH69eunrl27asaMGVq5cqVSUlJUVFSkadOmOd3m2muv1Xfffac33nhDs2bNUqNGjSrMNTg4WHXr1tWKFSs0duxYNWjQoFK5lfrggw80bNgwxcTE6Mknn9TRo0d16623Kjw83On8119/XQUFBbrjjjtks9k0Y8YMXXvttdqzZ4/9qP6OHTsUFxen8PBwTZw4UX5+fnrrrbc0ePBgvf3227rmmmskSTk5OYqPj1dRUZF93osvvnhedenM0aNH1a9fP1177bUaOnSoli1bpgceeEAxMTHq379/hbVcWFioxMREnTp1SmPHjlVoaKgOHTqk999/X8eOHVNgYOCfyg0AUAEDAKh1qampRpLZunVruXMCAwPNZZddZl9OSUkxf/yxPWvWLCPJ/Pzzz+XuY+vWrUaSSU1NLbOuR48eRpJZsGCB03U9evSwL69fv95IMuHh4SY/P98+/tZbbxlJZs6cOfaxqKgoM2LEiHPus6LcRowYYaKiouzL7733npFkHnvsMYd51113nbHZbCYrK8s+Jsl4eXk5jG3bts1IMvPmzSsT649mz55tJJlXX33VPlZYWGhiY2NNvXr1HL72qKgok5SUVOH+jDHmnXfeMZJMw4YNTatWrUxqaqpJTU01rVq1Ml5eXmbbtm3n3MeIESOMJDN27Fj7WElJiUlKSjJeXl4ONSDJpKSk2JdnzpxpJJm9e/eeM44xxjzyyCNGkvHz8zP9+/c3jz/+uMnIyCgzb+/evWXev5iYGBMREWEKCgrsYxs2bDCSHN7P0m0bNmxofv31V/t4WlqakWRWrFhhH+vdu7eJiYkxJ0+edPjau3XrZlq1amUfGzdunJFktmzZYh/Lzc01gYGBlfr6S7+//vhaln6PvPLKK/axU6dOmdDQUDNkyBD7WHm1nJmZaSSZpUuXVhgbAFD9OHUZANxUvXr1Krz7cumNhdLS0qp8eqa3t7dGjhxZ6fk333yz/P397cvXXXedmjRpog8//LBK8Svrww8/VJ06dXTPPfc4jI8fP17GGH300UcO4wkJCWrRooV9uX379goICNCePXvOGSc0NFQ33HCDfczT01P33HOPfvvtN23cuPG8c//tt98kSQUFBVq7dq2Sk5OVnJysNWvWyBjjcBrsuYwZM8b+eekp2oWFhVqzZs1551WeqVOn6vXXX9dll12mjz/+WJMnT1anTp30t7/9TTt37ix3u+zsbH399de6+eabHR6F1KNHD8XExDjdZtiwYQoKCrIvlx4NLX2ffv31V61bt05Dhw5VQUGBjhw5oiNHjuiXX35RYmKivv/+ex06dEjSmfeua9euDtdAN27cWP/85z+r/mLozPfhjTfeaF/28vJS586dz1lLkuxHbD/++OMqn64OAKgaGl0AcFO//fabQ1N5tmHDhikuLk633XabQkJCNHz4cL311lvn1fSGh4ef142nWrVq5bBss9nUsmXLGn9G6f79+xUWFlbm9bjkkkvs6/+oadOmZfYRFBSko0ePnjNOq1at5OHh+N9jeXEqo/TU2bi4OEVGRjrk2L17d6Wnp0s68/ihnJwch4/i4mL7fA8PDzVv3txh3xdffLEkVfvrf8MNN+jTTz/V0aNHtWrVKv3jH/9QZmamBgwYoJMnTzrdpvS1admyZZl1zsaksu9TadNb+j5lZWXJGKOHH35YjRs3dvgovdt16Q2ySt+7s7Vu3boyX3K5IiIiylwbX5laks5cF33//ffr5ZdfVqNGjZSYmKj58+dzfS4A1AKu0QUAN3Tw4EHl5eWV2yBIZxqoTz75ROvXr9cHH3yglStX6s0331SvXr20atUq1alT55xx/uz1i86c3RSUKi4urlRO1aG8OOasGzfVhrCwMElyev1zcHCwMjMzJZ15DNDZ1+vu3bu3zE25alNAQID69OmjPn36yNPTU4sWLdKWLVvUo0ePatn/ud6n0j/a/Otf/1JiYqLTuRV9j1SHP1tLTz/9tJKTk5WWlqZVq1bpnnvu0ZNPPqnNmzdX62OlAACOaHQBwA0tXrxYksr95b6Uh4eHevfurd69e+uZZ57RE088ocmTJ2v9+vVKSEgot+msqu+//95h2RijrKwsh+f9BgUF6dixY2W23b9/v8MRyfPJLSoqSmvWrFFBQYHDUd1du3bZ11eHqKgobd++XSUlJQ5Hdf9MnJiYGHl6etpPsf2j7Oxs+42hOnTooNWrVzusDw0NtX9eUlKiPXv22I/iStJ3330nSRU2w9VVA5dffrkWLVqkn376yen60tcmKyurzDpnY5VRWi+enp5KSEiocG5UVFSZ+pSk3bt3Vyn2+TjXaxwTE6OYmBg99NBDSk9PV1xcnBYsWKDHHnusxnMDgAsVpy4DgJtZt26dHn30UUVHR1d4feGvv/5aZqxjx46SpFOnTkmS/Pz8JMlp41kVr7zyisN1w8uWLdNPP/2k/v3728datGihzZs3q7Cw0D72/vvvl3kM0fnkdtVVV6m4uFjPPvusw/isWbNks9kc4v8ZV111lXJycvTmm2/ax4qKijRv3jzVq1evSkcy/f39ddVVVyk9Pd3eMEvSzp07lZ6erj59+kg68weChIQEh4+zn1P7x6/fGKNnn31Wnp6e6t27d7nxz+d1Pn78uDZt2uR0Xel10OWdChwWFqZ27drplVdesV+XLEkbN27U119/fc7YzgQHB6tnz5564YUXnDbYP//8s/3zq666Sps3b9YXX3zhsP61116rUuzzUd5rnJ+fr6KiIoexmJgYeXh42L9HAQA1gyO6AOBCH330kXbt2qWioiIdPnxY69at0+rVqxUVFaXly5eXaXT+aNq0afrkk0+UlJSkqKgo5ebm6rnnnlNERIS6d+8u6UzTWb9+fS1YsED+/v7y8/NTly5dFB0dXaV8GzRooO7du2vkyJE6fPiwZs+erZYtWzo8Aum2227TsmXL1K9fPw0dOlQ//PCDXn31VYebQ51vbgMGDFB8fLwmT56sffv2qUOHDlq1apXS0tI0bty4Mvuuqttvv10vvPCCkpOTlZGRoWbNmmnZsmX6/PPPNXv27Aqvma7IE088obVr16pXr172G2rNnTtXDRo0sD+7+Fx8fHy0cuVKjRgxQl26dNFHH32kDz74QJMmTSr3cUGS1KlTJ0nS5MmTNXz4cHl6emrAgAH25uyPjh8/rm7duqlr167q16+fIiMjdezYMb333nv69NNPNXjwYF122WUVfp2DBg1SXFycRo4cqaNHj+rZZ59Vu3btHJrf8zF//nx1795dMTExGjVqlJo3b67Dhw9r06ZNOnjwoLZt2yZJ+ve//63FixerX79+uvfee+2PFyo9Sl+Tyqvlbdu2acyYMbr++ut18cUXq6ioSIsXL1adOnU0ZMiQGs0JAC54rrvhMwBcuEofL1T64eXlZUJDQ02fPn3MnDlzHB5jU+rsxwutXbvWDBo0yISFhRkvLy8TFhZmbrjhBvPdd985bJeWlmbatm1rLrroIodHoPTo0cNceumlTvMr7/FCb7zxhnnwwQdNcHCwqVu3rklKSjL79+8vs/3TTz9twsPDjbe3t4mLizNffvllmX1WlNvZjxcyxpiCggJz3333mbCwMOPp6WlatWplZs6caUpKShzmSTKjR48uk1N5jz062+HDh83IkSNNo0aNjJeXl4mJiXH6CKTKPl6oVEZGhklISDB+fn7G39/fDBo0qMx7VZ4RI0YYPz8/88MPP5i+ffsaX19fExISYlJSUkxxcbHDXJ31eCFjjHn00UdNeHi48fDwqPBRO6dPnzYvvfSSGTx4sImKijLe3t7G19fXXHbZZWbmzJnm1KlT9rnOHi9kjDFLliwxbdq0Md7e3qZdu3Zm+fLlZsiQIaZNmzZltp05c2aZHJzl/8MPP5ibb77ZhIaGGk9PTxMeHm6uvvpqs2zZMod527dvNz169DA+Pj4mPDzcPProo+Y///nPn3q8kLPvEWf16ayW9+zZY2655RbTokUL4+PjYxo0aGDi4+PNmjVrKswFAPDn2YxxwZ05AADABaNjx45q3LhxmWuQAQCoKVyjCwAAqsXp06fLXJO6YcMGbdu2TT179nRNUgCACxJHdAEAQLXYt2+fEhISdOONNyosLEy7du3SggULFBgYqG+++UYNGzZ0dYoAgAsEN6MCAADVIigoSJ06ddLLL7+sn3/+WX5+fkpKStL06dNpcgEAtYojugAAAAAAS+EaXQAAAACApdDoAgAAAAAspcrX6JaUlCg7O1v+/v6y2WzVmRMAAAAAAGUYY1RQUKCwsDB5eJR/3LbKjW52drYiIyOrujkAAAAAAFXy448/KiIiotz1VW50/f397QECAgKquhsAAAAAAColPz9fkZGR9n60PFVudEtPVw4ICKDRBQAAAADUmnNdPsvNqAAAAAAAlkKjCwAAAACwFBpdAAAAAICl0OgCAAAAACyFRhcAAAAAYCk0ugAAAAAAS6HRBQAAAABYCo0uAAAAAMBSaHQBAAAAAJZCowsAAAAAsBQaXQAAAACApdDoAgAAAAAshUYXAAAAAGApNLoAAAAAAEuh0QUAAAAAWAqNLgAAAADAUmh0AQAAAACWQqMLAAAAALAUGl0AAAAAgKXQ6AIAAAAALIVGFwAAAABgKTS6AAAAAABLodEFAAAAAFgKjS4AAAAAwFJodAEAAAAAlkKjCwAAAACwFBpdAAAAAICl0OgCAAAAACyFRhcAAAAAYCk0ugAAAAAAS6HRBQAAAABYCo0uAAAAAMBSaHQBAAAAAJZCowsAAAAAsBTrN7rLl0v33Xfm35rerra2sWosd8/PqrHcPT+rxnL3/GozlrvnZ9VY7p6fVWO5e361Gcvd87NqLHfPrzZjuXt+f4VY7sxUUV5enpFk8vLyqrqLmpeWZoxkTJ06Z/5NS6u57WprG6vGcvf8rBrL3fOzaix3z682Y7l7flaN5e75WTWWu+dXm7HcPT+rxnL3/Gozlrvn91eI5SKV7UM9XN1o16j166U6daTi4jP/bthQc9vV1jZWjeXu+Vk1lrvnZ9VY7p5fbcZy9/ysGsvd87NqLHfPrzZjuXt+Vo3l7vnVZix3z++vEMvNWbvRjY///zeruFjq2bPmtqutbaway93zs2osd8/PqrHcPb/ajOXu+Vk1lrvnZ9VY7p5fbcZy9/ysGsvd86vNWO6e318hlpuzGWNMVTbMz89XYGCg8vLyFBAQUN15VZ/ly8/8RaJnT2ngwJrdrra2sWosd8/PqrHcPT+rxnL3/GozlrvnZ9VY7p6fVWO5e361Gcvd87NqLHfPrzZjuXt+f4VYLlDZPtT6jS4AAAAAwBIq24da+9RlAAAAAMAFh0YXAAAAAGApNLoAAAAAAEuh0QUAAAAAWAqNLgAAAADAUmh0AQAAAACWQqMLAAAAALAUGl0AAAAAgKXQ6AIAAAAALIVGFwAAAABgKTS6AAAAAABLodEFAAAAAFgKjS4AAAAAwFJodAEAAAAAlkKjCwAAAACwFBpdAAAAAICl0OgCAAAAACyFRhcAAAAAYCk0ugAAAAAAS6HRBQAAAABYCo0uAAAAAMBSaHQBAAAAAJZCowsAAAAAsBQaXQAAAACApdDoAgAAAAAshUYXAAAAAGApNLoAAAAAAEuh0QUAAAAAWAqNLgAAAADAUmh0AQAAAACWQqMLAAAAALAUGl0AAAAAgKXQ6AIAAAAALOWiqm5ojJEk5efnV1syAAAAAACUp7T/LO1Hy1PlRregoECSFBkZWdVdAAAAAABw3goKChQYGFjueps5VytcjpKSEmVnZ8vf3182m63KCVpJfn6+IiMj9eOPPyogIMDV6cBNUBdwhrpAeagNOENdwBnqAs5YvS6MMSooKFBYWJg8PMq/ErfKR3Q9PDwUERFR1c0tLSAgwJJFhT+HuoAz1AXKQ23AGeoCzlAXcMbKdVHRkdxS3IwKAAAAAGApNLoAAAAAAEuh0a1G3t7eSklJkbe3t6tTgRuhLuAMdYHyUBtwhrqAM9QFnKEuzqjyzagAAAAAAHBHHNEFAAAAAFgKjS4AAAAAwFJodAEAAAAAlkKjCwAAAACwFBrdajJw4EA1bdpUPj4+atKkiW666SZlZ2c7zNm+fbuuvPJK+fj4KDIyUjNmzHBRtqgN+/bt06233qro6GjVrVtXLVq0UEpKigoLCx3mURcXnscff1zdunWTr6+v6tev73TOgQMHlJSUJF9fXwUHB2vChAkqKiqq3URR6+bPn69mzZrJx8dHXbp00RdffOHqlFDLPvnkEw0YMEBhYWGy2Wx67733HNYbY/TII4+oSZMmqlu3rhISEvT999+7JlnUiieffFJXXHGF/P39FRwcrMGDB2v37t0Oc06ePKnRo0erYcOGqlevnoYMGaLDhw+7KGPUhueff17t27dXQECAAgICFBsbq48++si+npqg0a028fHxeuutt7R79269/fbb+uGHH3TdddfZ1+fn56tv376KiopSRkaGZs6cqSlTpujFF190YdaoSbt27VJJSYleeOEF7dixQ7NmzdKCBQs0adIk+xzq4sJUWFio66+/XnfddZfT9cXFxUpKSlJhYaHS09O1aNEiLVy4UI888kgtZ4ra9Oabb+r+++9XSkqK/vvf/6pDhw5KTExUbm6uq1NDLfr999/VoUMHzZ8/3+n6GTNmaO7cuVqwYIG2bNkiPz8/JSYm6uTJk7WcKWrLxo0bNXr0aG3evFmrV6/W6dOn1bdvX/3+++/2Offdd59WrFihpUuXauPGjcrOzta1117rwqxR0yIiIjR9+nRlZGToyy+/VK9evTRo0CDt2LFDEjUhSTKoEWlpacZms5nCwkJjjDHPPfecCQoKMqdOnbLPeeCBB0zr1q1dlSJcYMaMGSY6Otq+TF1c2FJTU01gYGCZ8Q8//NB4eHiYnJwc+9jzzz9vAgICHGoF1tK5c2czevRo+3JxcbEJCwszTz75pAuzgitJMu+++659uaSkxISGhpqZM2fax44dO2a8vb3NG2+84YIM4Qq5ublGktm4caMx5kwNeHp6mqVLl9rn7Ny500gymzZtclWacIGgoCDz8ssvUxP/wxHdGvDrr7/qtddeU7du3eTp6SlJ2rRpk/7+97/Ly8vLPi8xMVG7d+/W0aNHXZUqalleXp4aNGhgX6Yu4MymTZsUExOjkJAQ+1hiYqLy8/Ptf6mFtRQWFiojI0MJCQn2MQ8PDyUkJGjTpk0uzAzuZO/evcrJyXGok8DAQHXp0oU6uYDk5eVJkv33iYyMDJ0+fdqhLtq0aaOmTZtSFxeI4uJiLVmyRL///rtiY2Opif+h0a1GDzzwgPz8/NSwYUMdOHBAaWlp9nU5OTkOv7RKsi/n5OTUap5wjaysLM2bN0933HGHfYy6gDPUxYXnyJEjKi4udvq+856jVGktUCcXrpKSEo0bN05xcXFq166dpDN14eXlVeaeD9SF9X399deqV6+evL29deedd+rdd99V27ZtqYn/odGtwMSJE2Wz2Sr82LVrl33+hAkTlJmZqVWrVqlOnTq6+eabZYxx4VeAmnC+dSFJhw4dUr9+/XT99ddr1KhRLsocNakqdQEAwPkYPXq0vvnmGy1ZssTVqcANtG7dWl999ZW2bNmiu+66SyNGjNC3337r6rTcxkWuTsCdjR8/XsnJyRXOad68uf3zRo0aqVGjRrr44ot1ySWXKDIyUps3b1ZsbKxCQ0PL3OmsdDk0NLTac0fNOd+6yM7OVnx8vLp161bmJlPUhXWcb11UJDQ0tMzddqkLa2vUqJHq1Knj9OcB7zlKldbC4cOH1aRJE/v44cOH1bFjRxdlhdoyZswYvf/++/rkk08UERFhHw8NDVVhYaGOHTvmcASPnx/W5+XlpZYtW0qSOnXqpK1bt2rOnDkaNmwYNSEa3Qo1btxYjRs3rtK2JSUlkqRTp05JkmJjYzV58mSdPn3aft3u6tWr1bp1awUFBVVPwqgV51MXhw4dUnx8vDp16qTU1FR5eDieREFdWMef+XlxttjYWD3++OPKzc1VcHCwpDN1ERAQoLZt21ZLDLgXLy8vderUSWvXrtXgwYMlnfl/ZO3atRozZoxrk4PbiI6OVmhoqNauXWtvbPPz8+1Hc2BNxhiNHTtW7777rjZs2KDo6GiH9Z06dZKnp6fWrl2rIUOGSJJ2796tAwcOKDY21hUpw0VKSkp06tQpaqKUq++GZQWbN2828+bNM5mZmWbfvn1m7dq1plu3bqZFixbm5MmTxpgzd8QLCQkxN910k/nmm2/MkiVLjK+vr3nhhRdcnD1qysGDB03Lli1N7969zcGDB81PP/1k/yhFXVyY9u/fbzIzM83UqVNNvXr1TGZmpsnMzDQFBQXGGGOKiopMu3btTN++fc1XX31lVq5caRo3bmwefPBBF2eOmrRkyRLj7e1tFi5caL799ltz++23m/r16zvcfRvWV1BQYP+ZIMk888wzJjMz0+zfv98YY8z06dNN/fr1TVpamtm+fbsZNGiQiY6ONidOnHBx5qgpd911lwkMDDQbNmxw+F3i+PHj9jl33nmnadq0qVm3bp358ssvTWxsrImNjXVh1qhpEydONBs3bjR79+4127dvNxMnTjQ2m82sWrXKGENNGGMMjW412L59u4mPjzcNGjQw3t7eplmzZubOO+80Bw8edJi3bds20717d+Pt7W3Cw8PN9OnTXZQxakNqaqqR5PTjj6iLC8+IESOc1sX69evtc/bt22f69+9v6tataxo1amTGjx9vTp8+7bqkUSvmzZtnmjZtary8vEznzp3N5s2bXZ0Satn69eud/nwYMWKEMebMI4YefvhhExISYry9vU3v3r3N7t27XZs0alR5v0ukpqba55w4ccLcfffdJigoyPj6+pprrrnG4Q/rsJ5bbrnFREVFGS8vL9O4cWPTu3dve5NrDDVhjDE2Y7hbEgAAAADAOrjrMgAAAADAUmh0AQAAAACWQqMLAAAAALAUGl0AAAAAgKXQ6AIAAAAALIVGFwAAAABgKTS6AAAAAABLodEFAAAAAFgKjS4AAAAAwFJodAEAAAAAlkKjCwAAAACwFBpdAAAAAICl/B9YdV5zLiuVKQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1200x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n = 6\n",
    "six_bit_ints = range(-2**(n-1), 2**(n-1))\n",
    "plot_number_dist(six_bit_ints, title=f'Distribution of {n}-bit Signed Ints')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b56e07",
   "metadata": {},
   "source": [
    "In python, integers are represented by default using a much bigger word size of $n=64$ bits, called **long** integers, or **int64** for short. This means (using two's complement) we can represent $2^{64}=18446744073709551616$ possible integer values in the range $[-2^{63}, 2^{63}-1]$.\n",
    "\n",
    "You can see from this that 64-bit integers have a minimum integer allowed and a maximum integer allowed, which are\n",
    "\n",
    "$$\\text{min_int}=-2^{63}=-9223372036854775808, \\qquad \\text{max_int}=2^{63}-1=9223372036854775807.$$\n",
    "\n",
    "What I've said is technically only exactly true in older versions of pythons as well as other programming languages like C. It turns out newer versions of python have a few added tricks that allow you to represent essentially arbitrarily large integers. You can see this by comparing it to numpy's internal int64 representation, which uses the C version. A numpy int64 outside the valid range will throw an overflow error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6eed9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_int = -2 ** 63\n",
    "max_int = 2 ** 63 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97e19405",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-9223372036854775809"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "OverflowError",
     "evalue": "Python int too large to convert to C long",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m min_int \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint64\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmin_int\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOverflowError\u001b[0m: Python int too large to convert to C long"
     ]
    }
   ],
   "source": [
    "min_int - 1\n",
    "np.int64(min_int - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2528c5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9223372036854775808"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "OverflowError",
     "evalue": "Python int too large to convert to C long",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m max_int \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint64\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_int\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOverflowError\u001b[0m: Python int too large to convert to C long"
     ]
    }
   ],
   "source": [
    "max_int + 1\n",
    "np.int64(max_int + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6b9b59",
   "metadata": {},
   "source": [
    "## Floats\n",
    "\n",
    "### Basics\n",
    "\n",
    "What if we want to represent decimal numbers or fractions instead of whole numbers, like $1.2$ or $0.99999$, or even irrational numbers like $\\pi=3.1415926\\dots$? To do this we need a new system of numbers that I'll call floating point numbers, or **floats**, for reasons I'll explain soon. Floats will be a computer's best attempt to represent the real numbers $\\mathbb{R}$. They'll represent real numbers only approximately with some specified precision.\n",
    "\n",
    "In python, floats are builtin objects of type `float`. Floats obey pretty much the same operations that integers do with some minor exceptions:\n",
    "- Addition: $1.2 + 4.3 = 5.5$.\n",
    "- Subtraction: $1.2 - 4.3 = -3.1$.\n",
    "- Multiplication: $1.2 \\times 4.3 = 5.16$.\n",
    "- Exponentiation: $4.3^2 = 18.49$.\n",
    "- Remainder (or Modulo): $4.3 \\text{ mod } 1.2 = 0.7$.\n",
    "- Integer Division: $4.3 \\ // \\ 1.2 = 3.0$.\n",
    "- Division: $4.3 \\div 1.2$.\n",
    "\n",
    "I'll print these out in python to verify the answers are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7be491cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.5"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "-3.0999999999999996"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "5.159999999999999"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "18.49"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.7"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "3.5833333333333335"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.2 + 4.3\n",
    "1.2 - 4.3\n",
    "1.2 * 4.3\n",
    "4.3 ** 2\n",
    "4.3 % 1.2\n",
    "4.3 // 1.2\n",
    "4.3 / 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ed1212",
   "metadata": {},
   "source": [
    "Most of them look right. But what the heck is going on with $1.2 - 4.3$ and $1.2 \\times 4.3$? We're getting some weird trailing nines that shouldn't be there. This gets to how floats are actually represented on a computer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b43d2d",
   "metadata": {},
   "source": [
    "### Representing Floats\n",
    "\n",
    "Representing real numbers on a computer is a lot more subtle than representing integers. Since a computer can only have a finite number of bits, they can't represent infinitely many digits, e.g. in irrational numbers like $\\pi$. Using finite word sizes will necessarily have to truncate real numbers to some number of decimal places. This truncation will create an error in the calculation called **numerical roundoff**.\n",
    "\n",
    "So how should we represent a decimal number using $n$ bits? As an example, let's imagine we're trying to represent the number $x=157.208$. Perhaps the first thing you might think of is to use some number of those bits to represent the integer part, and some number to represent the fractional part. Suppose you have $n=16$ bits available to represent $x$. Then maybe you can use 8 bits for the integer part $157$, and 8 bits for the fractional part $0.208$. Converting both halves to binary, you'd get \n",
    "$$157 \\equiv 10011101, \\quad 0.208 \\equiv 0011010100111111.$$\n",
    "\n",
    "Truncating both sequences to 8 bits (from the left), you could thus adopt a convention that $157.208 \\equiv 10011101 \\ 00110101$.\n",
    "\n",
    "This system is an example of a **fixed point** representation. This has to do with the fact that we're always using a fixed number of bits for the integer part, and a fixed number for the fractional part. The decimal point isn't allowed to **float**, or move around to allocate more bits to the integer or fractional part depending which needs more precision. The decimal point is **fixed**.\n",
    "\n",
    "As I've suggested, the fixed point representation seems to be limited and not terribly useful. If you need really high precision in the fractional part, your only option is to use a larger word size. If you're dealing with really big numbers and don't care much about the fractional part, you also need a larger word size so you don't run out of numbers. A solution to this problem is to allow the decimal point to float. We won't allocate a fixed number of bits to represent the integer or fractional parts. We'll design it in such a way that larger numbers give the integer part more bits, and smaller numbers give the fractional part more bits.\n",
    "\n",
    "The trick to allowing the decimal point to float is to represent not just the digits of a number but also its exponent. Think about scientific notation, where if you have a number like say $x=1015.23$, you can write it as $1.01523 \\cdot 10^3$, or `1.01523e3`. That $3$ is the exponent. It says something about how big the number is. What we can do is convert a number to scientific notation. Then use some number of bits to represent the exponent $3$ and some to represent the remaining part $1.01523$. This is essentially the whole idea behind floating point.\n",
    "\n",
    "In floating point representation, instead of using scientific notation with powers of ten, it's more typical to use powers of two. When using powers of two, the decimal part can always be scaled to be between 1 and 2, so they look like $1.567$ or something like that. Since the $1.$ part is always there, we can agree it's always there, and only worry about representing the fractional part $0.567$. We'll call this term the **mantissa**. Denoting the sign bit as $s$, the exponent as $e$, and the mantissa as $m$, we can thus right any decimal number $x$ in a modified scientific notation of the form\n",
    "$$x = (-1)^s \\cdot (1+m) \\cdot 2^{e}.$$\n",
    "Once we've converted $x$ to this form, all we need to do is to figure out how to represent $s$, $m$, and $e$ using some number of bits of $n$, called the floating point **precision**. Assume the $n$ bits of precision allocate $1$ bit for the sign, $n_e$ bits for the exponent, and $n_m$ bits for the mantissa, so $n=1+n_e+n_m$.\n",
    "\n",
    "Here are the steps to convert a number $x$ into its $n$-bit floating point representation.\n",
    "- Given some number $x$, get its modified scientific notation form $x = (-1)^s \\cdot (1+m) \\cdot 2^e$.\n",
    "    - Determine the sign of $x$. If negative, set the sign bit to $s=1$, else default to $s=0$. Set $x = |x|$.\n",
    "    - Keep performing the operation $x = x \\ // \\ 2$ until $1 \\leq x \\leq 2$. Keep track of the number of times you're dividing, which is the **exponent** $e$.\n",
    "    - The remaining part will be some $1 \\leq x \\leq 2$. Write it in the form $x = 1 + m$, where $m$ is the mantissa.\n",
    "- Convert the scientific notation form into a sequence of $n$ bits, truncating where necessary.\n",
    "    - For reasons I'll describe in a second, it's good to add a **bias** term $b$ to the exponent $e$ before converting the exponent to binary. Let $e'=e+b$ be this modified exponent.\n",
    "    - Convert each of $e'$ and $m$ into binary sequences, truncated to sizes $n_e$, and $n_m$ respectively.\n",
    "    - Concatenate these binary sequences together to get a sequence of $n=1+n_e+n_m$ total bits. By convention, assume the order of bit concatenation is the sign bit, then exponent bits, then the mantissa bits.\n",
    "\n",
    "There are of course other ways you could do it, for example by storing the sequences in a different order. I'm just stating one common way it's done.\n",
    "\n",
    "Since all of this must seem like Greek, here's a quick example. Let's consider the number $x=15.25$. We'll represent it using $n=8$ bits of precision, where $n_e=4$ is the number of exponent bits, $n_m=3$ is the number of precision bits, and $b=10$ is the bias.\n",
    "- Convert $x=15.25$ to its modified scientific notation.\n",
    "    - Since $x \\geq 0$ the sign is positive, so $s=0$.\n",
    "    - Keep integer dividing $x$ by $2$ until it's less than $2$. It takes $e=3$ divisions before $x<2$.\n",
    "    - We now have $x = 1.90625 \\cdot 2^3$. The mantissa is then $m = (1.90625-1) = 0.90625$.\n",
    "    - In modified scientific notation form we now have $x=(-1)^0 \\cdot (1 + 0.90625) \\cdot 2^3$.\n",
    "- Convert everything to binary.\n",
    "    - Adding the bias to the exponent gives $e'=3+10=13$.\n",
    "    - Converting each piece to binary we get $e' = 13 \\equiv 1101$, $m = 0.90625 \\equiv 11101$.\n",
    "    - Since $m$ requires more than $n_m=3$ bits to represent, truncate off the two right bits to get $m \\equiv 111$. \n",
    "        - This truncation will cause numerical roundoff, since $0.90625$ truncates to $0.875$. That's an error of $0.03125$ that gets permanently lost.\n",
    "    - The final representation is thus $x \\equiv 0 \\ 1101 \\ 111$.\n",
    "    \n",
    "So you can experiment, I wrote a helper function `represent_as_float` that lets you visualize this for different values of $x$. Below I show the example I just calculated. I print out both the scientific notation form and its binary representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "570db3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scientific notation: (-1)^0 * (1 + 0.90625) * 2^3\n",
      "8-bit floating point representation: 0 1101 111\n"
     ]
    }
   ],
   "source": [
    "represent_as_float(15.25, n=8, n_exp=4, n_man=3, bias=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b0ff4e",
   "metadata": {},
   "source": [
    "So what's going on with the bias term $b$? Why do we need it? The easiest answer to give is that without it, we can't have negative exponents without having to use another sign bit for them. Consider a number like $x=0.5$. In modified scientific notation this would look like $x=(-1)^0 \\cdot (1+0) \\cdot 2^{-1} = 2^{-1}$, meaning its exponent would be $e=-1$. Rather than have to keep yet another sign bit for the exponent, it's easier to just add a bias term $b$ that ensures the exponent $e'=e+b$ is always non-negative. The higher the bias, the more precision we can show in the range $-1 < x < 1$. The trade-off is that we lose precision for large values of $x$.\n",
    "\n",
    "On top of floats defined the way I mentioned, we also have some special numbers that get defined in a floating point system. These are $\\pm 0$, $\\pm \\infty$, and $\\text{NaN}$ or \"not a number\". Each of these numbers is allocated its own special sequence of bits, depending on the precision.\n",
    "- $+0$ and $-0$: These numbers are typically represented using a biased exponent $e'=0$ (all zero bits) and a mantissa $m=0$ (all zero bits). The sign bit is used to distinguish between $+0$ and $-0$. In our example, these would be $+0 \\equiv 0 \\ 0000 \\ 000$ and $-0 \\equiv 1 \\ 0000 \\ 000$.\n",
    "- $+\\infty$ and $-\\infty$: These numbers are typically represented using the max allowed exponent (all one bits) and a mantissa $m=0$ (all zero bits). The sign bit is used to distinguish between $+\\infty$ and $-\\infty$. In our example, these would be $+\\infty \\equiv 0 \\ 1111 \\ 000$ and $-\\infty \\equiv 1 \\ 1111 \\ 000$.\n",
    "- $\\text{NaN}$: This value is typically represented using the max allowed exponent (all one bits) and a non-zero $m \\neq 0$. The sign bit is usually not used for $\\text{NaN}$ values. Note this means we can have many different sequences that all represent $\\text{NaN}$. In our example, any number of the form $\\text{NaN} \\equiv \\text{x} \\ 1111 \\ \\text{xxx}$ would work.\n",
    "\n",
    "So I can illustrate some points about how floating point numbers behave, I'm going to generate *all possible* $8$-bit floats (excluding the special numbers) and plot them on a number line, similar to what I did above with the $8$-bit signed integers. I'll generate the floats using the using the helper function `gen_all_floats`, passing in the number of mantissa bits `n_man=3`, the number of exponent bits `n_exp=4`, and a bias of `bias=10`.\n",
    "\n",
    "First, I'll use these numbers to print out some interesting statistics of this 8-bit floating point system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "830f3c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of 8-bit floats: 120\n",
      "Most negative float: -56.0\n",
      "Most positive float: 56.0\n",
      "Smallest nonzero float: 0.001953125\n",
      "Machine Epsilon: 0.25\n"
     ]
    }
   ],
   "source": [
    "eight_bit_floats = gen_all_floats(n=8, n_man=3, n_exp=4, bias=10)\n",
    "print(f'Total number of 8-bit floats: {len(eight_bit_floats)}')\n",
    "print(f'Most negative float: {min(eight_bit_floats)}')\n",
    "print(f'Most positive float: {max(eight_bit_floats)}')\n",
    "print(f'Smallest nonzero float: {min([x for x in eight_bit_floats if x > 0])}')\n",
    "print(f'Machine Epsilon: {min([x for x in eight_bit_floats if x > 1]) - 1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b32617",
   "metadata": {},
   "source": [
    "We can see that this 8-bit system only contains 120 unique floats. We could practically list them all out. Just like with the integers, we see there's a most negative float, $-56.0$, and a most positive float, $56.0$. The smallest float, i.e. the one closest to $0$, is $0.001953125$. Notice how much more precision the smallest float has than the largest ones do. The largest ones are basically whole numbers, while the smallest one has nine digits of precision. Evidently, floating point representations give much higher precision to numbers close to zero than to numbers far away from zero.\n",
    "\n",
    "What happens if you try to input a float larger than the max, in this case $56.0$? Typically it will **overflow**. This will result in either the system raising an error, or the number getting set to $+\\infty$, in a sense getting \"rounded up\". Similarly, for numbers more negative than the min, in this case $-56.0$, either an overflow error will be raised, or the number will get \"rounded down\" to $-\\infty$. \n",
    "\n",
    "You have to be careful in overflow situations like this, especially when you don't know for sure which of these your particular system will do. It's amusing to note that python will raise an overflow error, but numpy will round to $\\pm \\infty$. Two different conventions to worry about. Just as amusing, when dealing with signed integers, it's numpy that will raise an error if you overflow, while python won't care. One of those things...\n",
    "\n",
    "What happens when you try to input a float smaller than the smallest value, in this case $0.001953125$? In this case, the number is said to **undeflow**. Usually underflow won't raise an error. The number will pretty much always just get set to $+0$ (or $-0$). This is again something you have to worry about, especially if you're dealing with small numbers in denominators, where they can lead to division by zero errors which *do* get raised.\n",
    "\n",
    "Overflow and underflow errors are some of the most common numerical bugs that occur in deep learning, and usually result from not handling floats correctly to begin with.\n",
    "\n",
    "I also printed out a special value called the **machine epsilon**. The machine epsilon, denoted $\\varepsilon_m$, is defined as the smallest value in a floating point system that's larger than $1$. In some sense, $\\varepsilon_m$ is a proxy for how finely you can represent numbers in a given $n$-bit floating point system. The smaller $\\varepsilon_m$ the more precisely you can represent numbers, i.e. the more decimal places of precision you get access to. In our case, we get $\\varepsilon_m=0.25$. This means numbers in 8-bit floating point tend to be $0.25$ apart from each other on average, which means we can represent numbers in this system only with a measly 2-3 digits of precision.\n",
    "\n",
    "With these numbers in hand let's now plot their distribution on the number line. I'll use the helper function `plot_number_dist` function to do this. Compare with the plot of the signed integers I did above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "afd43456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_number_dist(eight_bit_floats, title='Distribution of 8-bit Floats')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f83a2c",
   "metadata": {},
   "source": [
    "Notice how different this plot is from the ones for the signed integers. With the integers, the points were equally spaced. Now points close to $0$ are getting represented much closer together than points far from $0$. There are $74$ of the $120$ total points showing up just in the range $[-1,1]$. That's over half!. Meanwhile, only $22$ points total show up in the combined ranges of $[-60,-10]$ and $[10,60]$. Very strange.\n",
    "\n",
    "Feel free to play around with different floating point systems by using different choices for `n`, `n_man`, `n_exp`, and `bias`. Be careful, however, not to make `n_exp` too large or you may crash the kernel...\n",
    "\n",
    "### Double Precision\n",
    "\n",
    "So how does python represent floats? Python by default uses what's called **double precision** to represent floats, also called **float64**. This means $n=64$ total bits of precision are used, with $n_e=11$, $n_m=52$, and bias $b=1023=2^{10}-1$. Double precision allows for a *much* larger range of numbers than 8-bit precision does: \n",
    "- The max value allowed is $2^{2^{n_e}-b} = 2^{1025} \\approx 10^{308}$.\n",
    "- The min value allowed is $-2^{2^{n_e}-b} = -2^{1025} \\approx -10^{308}$.\n",
    "- Numbers *outside* the range of about $[-10^{308}, 10^{308}]$ will *overflow*.\n",
    "- The smallest values allowed are (plus or minus) $2^{-b+1} = 2^{-1022} \\approx 10^{-308}$.\n",
    "    - Using subordinal numbers, the smallest values are (plus or minus) $2^{-b-n_m+1} = 2^{-1074} \\approx 10^{-324}$.\n",
    "- Numbers *inside* the range of about $[-10^{-308}, 10^{-308}]$ will *underflow*.\n",
    "    - Using subordinal numbers, this range is around $[-10^{-324}, 10^{-324}]$.\n",
    "- The machine epsilon is $\\varepsilon_m = 2^{-53} \\approx 10^{-16}$.\n",
    "- Numbers requiring more than about 15-16 digits of precision will get truncated, resulting in numerical roundoff.\n",
    "- The special numbers $\\pm \\infty$, $\\pm 0$, and $\\text{NaN}$ are represented similarly as before, except using 64 bits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c174c776",
   "metadata": {},
   "source": [
    "To illustrate the point regarding numerical roundoff, here's what happens if we try to use double precision floating point to define the constant $\\pi$ to its first [100 digits](https://www.wolframalpha.com/input?i=pi+to+100+digits)? Notice it just gets truncated to its first 15 digits. Double precision is unable to keep track of the other 85 digits. They just get lost to numerical roundoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8498ed80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.141592653589793"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi = 3.141592653589793238462643383279502884197169399375105820974944592307816406286208998628034825342117068\n",
    "pi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f2ba82",
   "metadata": {},
   "source": [
    "Another thing to worry about is adding small numbers to medium to large sized numbers, e.g. $10 + 10^{-16}$, which will just get rounded down to $10.0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fcbe2afe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10.0 + 1e-16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185c2295",
   "metadata": {},
   "source": [
    "Numerical roundoff is often an issue when subtracting two floats. Here's what happens when we try to subtract two numbers that should be equal, $x=0.1+0.2$ and $y=0.3$. Instead of $y-x=0$, we get $y-x \\approx -5.55 \\cdot 10^{-17}$. The problem comes from the calculation $x=0.1+0.2$, which caused a slight loss of precision in $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ffc41369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5.551115123125783e-17"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 0.1 + 0.2\n",
    "y = 0.3\n",
    "y - x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d47b83f",
   "metadata": {},
   "source": [
    "A major implication of these calculations is that you should *never* test floating points for exact equality because numerical roundoff can mess it up. If you'd tried to test something like `(y - x) == 0.0`, you'd have gotten the wrong answer. Instead, you want to test that `y - x` is less than some small number `tol`, called a *tolerance*, i.e. `abs(y - x) < tol`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9a075dd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y - x == 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21ee8af2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tol = 1e-5\n",
    "abs(y - x) < tol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7f634e",
   "metadata": {},
   "source": [
    "Numerical roundoff explains why we got the weird results above when subtracting $1.2 - 4.3$. The imperfect precision in the two numbers resulted in a numerical roundoff error, leading in the trailing $9$s that should've rounded up to $-3.1$ exactly. In general, subtracting floats is one of the most dangerous operations to do, as it tends to lead to the highest loss of precision in calculations. The closer two numbers are to being equal the worse this loss of precision tends to get.\n",
    "\n",
    "I mentioned that double precision has a smallest number of $2^{-1022} \\approx 10^{-308}$, but caveated that by saying that, by using a trick called **subordinal numbers**, we can get the smallest number down to about $10^{-324}$. What did I mean by this? It turns out that the bits where the biased exponent $e'=0$ (i.e. all exponent bits are zero) go mostly unused in the standard version of double precision. By using this zero exponent and allowing the mantissa $m$ to take on all its possible values, we can get about $2^{52}$ more values (since the mantissa has 52 bits). This lets us get all the way down to $2^{-1022} \\cdot 2^{-52} = 2^{-1074} \\approx 10^{-324}$.\n",
    "\n",
    "Python (and numpy) by default implements double precision with subordinal numbers, as we can see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad702b3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5e-324"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 ** (-1074)\n",
    "2 ** (-1075)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a46f8b0",
   "metadata": {},
   "source": [
    "The special numbers $\\pm \\infty$, $\\pm 0$, and $\\text{NaN}$ are also defined in double precision. In python (and numpy) they're given by the following commands,\n",
    "- $\\infty$: `float('inf')` or `np.inf`, \n",
    "- $-\\infty$: `float('-inf')` or `-np.inf`, \n",
    "- $\\pm 0$: `0`,\n",
    "- $\\text{NaN}$: `float('nan')` or `np.nan`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "26726865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float('inf')\n",
    "np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f86d9f9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-inf"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "-inf"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float('-inf')\n",
    "-np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "96bf474e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0\n",
    "-0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5a3ef855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float('nan')\n",
    "np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddf166a",
   "metadata": {},
   "source": [
    "You may be curious what exactly $\\text{NaN}$ (\"not a number\") is and where it might show up. Basically, NaNs are used wherever values are undefined. Anytime an operation doesn't return a sensible value it risks getting converted to NaN. One example is the operation $\\infty - \\infty = \\infty + (-\\infty)$, which mathematically doesn't make sense. No, it's not zero..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4b551ba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float('inf') + float('-inf')\n",
    "np.inf - np.inf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72e8d46",
   "metadata": {},
   "source": [
    "I'll finish this section by mentioning that there are two other floating point representations worth being aware of: **single precision** (or **float32**), and **half precision** (or **float16**). Single precision uses 32 bits to represent a floating point number. Half precision uses 16 bits. It may seem strange to even bother having these less-precise precisions lying around, but they do have their uses. For example, half precision shows up in deep learning as a more efficient way to represent the weights of a neural network. Since half precision floats only take up 25% as many bits as default double precision floats do, using them can yield a 4x reduction in model memory sizes. We'll see more on this later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544e622e",
   "metadata": {},
   "source": [
    "### Common Floating Point Pitfalls\n",
    "\n",
    "To cap this long section on floats, here's a list of common pitfalls people run into when working with floating point numbers, and some ways to avoid each one. This is probably the most important thing to take away from this section. You may find it helpful to reference later on. See this [post](https://www.codeproject.com/Articles/29637/Five-Tips-for-Floating-Point-Programming) for more information.\n",
    "\n",
    "1. Numerical overflow: Letting a number blow up to infinity (or negative infinity)\n",
    "    - Clip numbers from above to keep them from being too large\n",
    "    - Work with the log of the number instead\n",
    "    - Make sure you're not dividing by zero or a really small number\n",
    "    - Normalize numbers so they're all on the same scale\n",
    "2. Numerical underflow: Letting a number spiral down to zero\n",
    "    - Clip numbers from below to keep them from being too small\n",
    "    - Work with the exp of the number instead\n",
    "    - Normalize numbers so they're all on the same scale\n",
    "3. Subtracting floats: Avoid subtracting two numbers that are approximately equal\n",
    "    - Reorder operations so approximately equal numbers aren't nearby to each other\n",
    "    - Use some algebraic manipulation to recast the problem into a different form\n",
    "    - Avoid differencing squares (e.g. when calculating the standard deviation)\n",
    "4. Testing for equality: Trying to test exact equality of two floats\n",
    "    - Instead of testing `x == y`, test for approximate equality with something like `abs(x - y) <= tol`\n",
    "    - Use functions like `np.allclose(x, y)`, which will do this for you\n",
    "5. Unstable functions: Defining some functions in the naive way instead of in a stable way\n",
    "    - Examples: factorials, softmax, logsumexp\n",
    "    - Use a more stable library implementation of these functions\n",
    "    - Look for the same function but in log form, e.g. `log_factorial` or `log_softmax`\n",
    "6. Beware of NaNs: Once a number becomes NaN it'll always be a NaN from then on\n",
    "    - Prevent underflow and overflow\n",
    "    - Remove missing values or replace them with finite values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
