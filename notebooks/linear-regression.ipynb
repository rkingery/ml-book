{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d528eb0d",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "- Housing price prediction (sq ft -> price): $\\hat y = \\theta_0 + \\theta_1 x_1$\n",
    "- Housing price prediction (sq ft, # bedrooms, ... -> price): $\\hat y = \\theta_0 + \\theta_1 x_1 + \\cdots + \\theta_n x_n$\n",
    "- Definitions: \n",
    "    - inputs $x$ are \"features\"\n",
    "    - outputs $y$ are \"targets\"\n",
    "    - the $\\hat y$ are \"predictions\"\n",
    "    - the pairs $(x,y)$ are \"training examples\"\n",
    "    - coefficients $\\boldsymbol{\\theta} = \\theta_0,\\cdots,\\theta_n$ are \"parameters\"\n",
    "    - $\\theta_0$ is the \"bias\"\n",
    "    - $\\theta_1,\\cdots,\\theta_n$ are the \"weights\n",
    "- Notation zoo (Ng notation):\n",
    "    - $m$: # training examples\n",
    "    - $n$: # features\n",
    "    - i<sup>th</sup> training example: $\\mathbf{x}^{(i)}$\n",
    "    - j<sup>th</sup> feature vector: $\\mathbf{x}_j$\n",
    "    - data matrix: $\\mathbf{X}$ of shape $(m,n+1)$ if we take $\\mathbf{X}_0 = (1, \\cdots, 1)$ else shape $(m,n)$\n",
    "    - target vector: $\\mathbf{y}$ of shape $(m,1)$\n",
    "    - cost function $J(\\theta) = \\langle L(\\hat y, y) \\rangle$\n",
    "- Goal: Choose \"best\" function $f(\\mathbf{X})$ such that $y \\approx \\hat y = f(\\mathbf{X})$, in the sense that some loss function $J(\\boldsymbol{\\theta}) = L(\\hat y, y)$ is minimized, where in this case\n",
    "$$J(\\boldsymbol{\\theta}) = \\frac{1}{2}||f_{\\boldsymbol{\\theta}}(\\mathbf{X}) - \\mathbf{y}||_2^2$$\n",
    "- Least Squares: Goal is to solve for the optimal $\\boldsymbol{\\hat \\theta}$ solving the problem\n",
    "$$\\underset{\\boldsymbol{\\theta}}{\\text{minimize}} \\ \\frac{1}{2}\\big(f_{\\boldsymbol{\\theta}}(\\mathbf{X}) - \\mathbf{y}\\big)^\\top \\big(f_{\\boldsymbol{\\theta}}(\\mathbf{X}) - \\mathbf{y}\\big).$$\n",
    "- Supposing $f_{\\boldsymbol{\\theta}}(\\mathbf{X}) = \\mathbf{X}^{\\top}\\boldsymbol{\\theta}$, the gradient $\\frac{d}{d\\boldsymbol{\\theta}} J(\\boldsymbol{\\theta})$ is just\n",
    "$$\\frac{d}{d\\boldsymbol{\\theta}} J( \\boldsymbol{\\theta}) = \\big(f_{\\boldsymbol{\\theta}}(\\mathbf{X}) - \\mathbf{y}\\big)^{\\top} \\mathbf{X}.$$\n",
    "- Using gradient descent, solve for $\\boldsymbol{\\hat \\theta}$ by making updates\n",
    "$$\\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta} - \\alpha \\cdot \\big(f_{\\boldsymbol{\\theta}}(\\mathbf{X}) - \\mathbf{y}\\big)^{\\top} \\mathbf{X}.$$\n",
    "- Another way to solve is by solving for $\\boldsymbol{\\hat \\theta}$ analytically, which gives the normal equation\n",
    "$$\\mathbf{X}^{\\top}\\mathbf{y} = \\mathbf{X}^{\\top}\\mathbf{X}\\boldsymbol{\\hat \\theta},$$\n",
    "$$\\boldsymbol{\\hat \\theta} = \\big(\\mathbf{X}^{\\top}\\mathbf{X} \\big)^{-1} \\mathbf{X}^{\\top}\\mathbf{y}.$$\n",
    "- Linear regression can fit more than just lines. Any transformation $g(\\mathbf{X})$ would also work,\n",
    "$$f_{\\boldsymbol{\\theta}}(\\mathbf{X}) = g(\\mathbf{X})^{\\top}\\boldsymbol{\\theta}.$$\n",
    "- Common choices for $g(x)$ include $x^n$, $\\log(x)$, $\\sqrt{x}$\n",
    "- Consider showing gradient descent work interactively here, for both 2D and 3D situations.\n",
    "- Trying to model the random variable $\\mathbf{y} = f_{\\boldsymbol{\\theta}}(\\mathbf{X}) + \\boldsymbol{\\varepsilon}$, where $\\boldsymbol{\\varepsilon} \\sim p(\\mathbf{X})$ with mean zero.\n",
    "    - Common choice (Gauss-Markov): $\\boldsymbol{\\varepsilon} \\overset{iid}{\\sim} \\mathcal{N}(\\boldsymbol{0}, \\sigma^2\\boldsymbol{I})$\n",
    "    - Equivalently, $\\mathbf{y}|\\mathbf{X},\\boldsymbol{\\theta} \\overset{iid}{\\sim} \\mathcal{N}(f_{\\boldsymbol{\\theta}}(\\mathbf{X}), \\sigma^2\\boldsymbol{I})$\n",
    "    - By Central Limit Theorem, provided features are uncorrelated, errors will always be approximately Gaussian\n",
    "    - The MSE loss is the negative log likelihood when errors are Gauss-Markov, meaning $\\mathcal{L}(\\boldsymbol{\\theta}) = p(\\mathbf{y}|\\mathbf{X},\\boldsymbol{\\hat \\theta})$ is maximized\n",
    "- Parametric vs non-parametric models:\n",
    "    - Parametric: number of parameters is fixed ahead of time\n",
    "    - Non-parametric: number of parameters can grow with the size of the data\n",
    "        - need to keep all of the training data around just to make predictions (sklearn does this for you)\n",
    "        - don't need to feature engineer as much\n",
    "- Locally weighted regression: Instead of trying to fit the entire training set, when predicting a given point, just fit a line to the training points around that point in real time, then make a prediction based on that.\n",
    "    - Use some kind of weighting function in the loss to enforce this, $J(\\theta) = \\sum_{i=1}^m \\color{red}{w_i(x)}(f(x^{(i)}|\\theta) - y^{(i)})^2$\n",
    "    - Common choice is a Gaussian weighting function, $w_i(x) = \\exp\\bigg(-\\frac{(x-x^{(i)})^2}{2\\tau^2}\\bigg)$, which is ~1 near $x^{(i)}$ and ~0 otherwise. \n",
    "    - Parameter $\\tau$ is a \"bandwidth\" that determines how wide the window should be. Also controls over/underfitting.\n",
    "    - Common name: LO(W)ESS for locally estimated (weighted) scatterplot smoothing\n",
    "    - Shows up in the time series STL decomposition\n",
    "    - When to use: few features, lots of data, don't want to think about hand-engineering features\n",
    "- Regularized Least Squares: Minimize $J(\\theta) = ||X\\theta - y||^2 - \\lambda ||\\theta||^2$\n",
    "    - Least squares solution is given by the modified normal equation $X^\\top y = (X^\\top X - \\lambda^2 I) \\theta$,\n",
    "    $$\\hat \\theta = (X^\\top X - \\lambda^2 I)^{-1} X^\\top y$$\n",
    "    - Also called ridge regression, $\\lambda$ the \"ridge parameter\"\n",
    "- Start of with a simple example $y=\\theta x$, with one feature and no bias. Then $\\hat \\theta = \\frac{y}{x}=x^{-1}y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d7ed2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72d1d53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
