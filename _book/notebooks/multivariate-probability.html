<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Machine Learning For The 2020s - 9&nbsp; Multivariate Distributions</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../notebooks/probability.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Multivariate Distributions</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Machine Learning For The 2020s</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">Preface</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/basic-math.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Basic Math</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/numerical-computing.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Numerical Computation</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/vectorization.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Arrays and Vectorization</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/basic-calculus.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Basic Calculus</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/linear-systems.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Systems of Linear Equations</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/vector-spaces.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Vector Spaces</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/matrix-algebra.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Matrix Algebra</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/probability.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Probability Distributions</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/multivariate-probability.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Multivariate Distributions</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#multivariate-distributions" id="toc-multivariate-distributions" class="nav-link active" data-scroll-target="#multivariate-distributions">Multivariate Distributions</a>
  <ul class="collapse">
  <li><a href="#two-random-variables" id="toc-two-random-variables" class="nav-link" data-scroll-target="#two-random-variables">Two Random Variables</a></li>
  <li><a href="#conditional-probability" id="toc-conditional-probability" class="nav-link" data-scroll-target="#conditional-probability">Conditional Probability</a></li>
  <li><a href="#bayes-rule" id="toc-bayes-rule" class="nav-link" data-scroll-target="#bayes-rule">Bayes Rule</a></li>
  <li><a href="#n-random-variables" id="toc-n-random-variables" class="nav-link" data-scroll-target="#n-random-variables"><span class="math inline">\(n\)</span> Random Variables</a></li>
  <li><a href="#application-the-central-limit-theorem" id="toc-application-the-central-limit-theorem" class="nav-link" data-scroll-target="#application-the-central-limit-theorem">Application: The Central Limit Theorem</a></li>
  <li><a href="#continuous-multivariate-distributions" id="toc-continuous-multivariate-distributions" class="nav-link" data-scroll-target="#continuous-multivariate-distributions">Continuous Multivariate Distributions</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-body" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Multivariate Distributions</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>In this lesson I’ll continue on with the theory of probability by talking about multivariate probabilities, conditional probabilities, moments, and Bayes Rule. I’ll also touch on the two biggest theorems in probability, the Law of Large Numbers and the Central Limit Theorem. Let’s get started.</p>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sympy <span class="im">as</span> sp</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> utils.math_ml <span class="im">import</span> <span class="op">*</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">'figure.figsize'</span>] <span class="op">=</span> (<span class="dv">4</span>, <span class="dv">3</span>)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># derive expected value by starting with simple average and turning it into the right form &lt;x&gt; = sum p(i) x(i)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="multivariate-distributions" class="level1">
<h1>Multivariate Distributions</h1>
<section id="two-random-variables" class="level2">
<h2 class="anchored" data-anchor-id="two-random-variables">Two Random Variables</h2>
<p>In the last lesson we saw the problem of rolling a single die. Suppose now we roll <em>two</em> dice. What is the probability that any pair of dots will turn up? One way to think about this problem is to imagine two random variables <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, each representing the roll of each die. Now, let’s do just like we did with the single die. Assume you have no other information to go on. What would you guess the probability of rolling any <span class="math inline">\((x,y)\)</span> pair of values should be?</p>
<p>If you again employ the principle of indifference, you’d argue as follows: Since <span class="math inline">\(x\)</span> can take on 6 values, and <span class="math inline">\(y\)</span> can also take on 6 values, then the pair <span class="math inline">\((x,y)\)</span> together can take on <span class="math inline">\(6 \cdot 6 = 36\)</span> possible pairs of values,</p>
<p><span class="math display">\[(x,y) = (1,1), (1,2), \cdots, (1,6), \cdots, (6,1), (6,2), \cdots, (6,6).\]</span></p>
<p>Thus, each pair of rolls should take an equal probability of <span class="math inline">\(p_{x,y} = \frac{1}{36}\)</span>. Let’s try to unpack the assumptions here. For one thing, we’re assuming each die is fair. That is, the two dice rolled individually should each have equal probabilities <span class="math inline">\(p_x = \frac{1}{6}\)</span> and <span class="math inline">\(p_y = \frac{1}{6}\)</span>. But, observe that for each pair of values <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> we have</p>
<p><span class="math display">\[p_{x,y} = \frac{1}{36} = \frac{1}{6} \cdot \frac{1}{6} = p_x \cdot p_y.\]</span></p>
<p>This is a stronger assumption than assuming the dice are fair. We’re also assuming that they don’t affect each other at all. Rolling <span class="math inline">\(x\)</span> says nothing about what we’d get from rolling <span class="math inline">\(y\)</span>, and vice versa. If this fact weren’t true, then we couldn’t get a uniform value for all pairs. Some pair would have a different probability.</p>
<p>In probability, this pair of random variables <span class="math inline">\((x,y)\)</span> is called a <strong>joint random variable</strong>, or a <strong>random vector</strong>. The probability a random vector takes on any two particular particular values, say <span class="math inline">\(x=i\)</span> and <span class="math inline">\(y=j\)</span>, is called the <strong>joint probability</strong> of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, written <span class="math inline">\(\mathbb{Pr}(x=i, y=j)\)</span>. The probabilities of each variable acting alone, i.e.&nbsp;<span class="math inline">\(\mathbb{Pr}(x=i)\)</span> and <span class="math inline">\(\mathbb{Pr}(y=j)\)</span>, are called the <strong>marginal probabilities</strong>. Instead of saying <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> don’t affect each other, we say the two random variables <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> are statistically <strong>independent</strong>. Evidently, if they are independent their joint probability factors into a product of marginal probabilities,</p>
<p><span class="math display">\[\mathbb{Pr}\big(x=i, y=j\big) = \mathbb{Pr}\big(x=i\big) \cdot \mathbb{Pr}\big(y=j\big).\]</span></p>
<p>Here’s an experiment mimicking the rolling of two fair, independent dice. What I’ll do is simulate rolling each die 10,000 times by sampling from their “fair die” distributions <span class="math inline">\(x \sim DU(1,7)\)</span> and <span class="math inline">\(y \sim DU(1,7)\)</span> each separately 10,000 times. I’ll then use the helper function <code>plot_joint_histogram</code> to do the plotting. You can see from the histogram that things do look more or less uniform if we sample each die fairly and independently. Note since each bin should have an expected count of <span class="math inline">\(\frac{10000}{36} \approx 278\)</span>, each bin’s count can fluctuate within a range of about <span class="math inline">\(278 \pm 17\)</span> and still be uniform.</p>
<div class="cell" data-execution_count="80">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.random.randint(<span class="dv">1</span>, <span class="dv">7</span>, size<span class="op">=</span><span class="dv">10000</span>)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>y_indep <span class="op">=</span> np.random.randint(<span class="dv">1</span>, <span class="dv">7</span>, size<span class="op">=</span><span class="dv">10000</span>)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>plot_joint_histogram([x, y_indep], title<span class="op">=</span><span class="st">'Two Independent Fair Dice'</span>, figsize<span class="op">=</span>(<span class="dv">8</span>,<span class="dv">4</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="multivariate-probability_files/figure-html/cell-4-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Just as in the univariate case, we can model probabilities of two variables as a ratio of counts per number of trials. If we run <span class="math inline">\(N\)</span> trials (in the above example <span class="math inline">\(N=10000\)</span>), and count the number of times <span class="math inline">\(N_{i,j}\)</span> each pair <span class="math inline">\(i,j\)</span> occurs, then the joint probability <span class="math inline">\(\mathbb{Pr}(x=i,y=j)\)</span> is given by the ratio of counts per number of trials as <span class="math inline">\(N \rightarrow \infty\)</span>,</p>
<p><span class="math display">\[\mathbb{Pr}\big(x=i,y=j\big) = \lim_{N \rightarrow \infty} \frac{N_{i,j}}{N}.\]</span></p>
<p>We can then define a <strong>joint distribution</strong> by specifying a PMF of two variables <span class="math inline">\(p(x,y)\)</span>. If <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> are independent the joint distribution will factor,</p>
<p><span class="math display">\[p(x,y) = p(x) \cdot p(y).\]</span></p>
<p>In this example of rolling independent fair dice, since <span class="math inline">\(x, y \sim D(1, 7)\)</span> and <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> are independent, the joint distribution must be of the form</p>
<p><span class="math display">\[(x,y) \sim DU(1,7) \cdot DU(1,7).\]</span></p>
<p>This called a <strong>product distribution</strong> since it’s just the product of two univariate distributions, in this case each <span class="math inline">\(D(1,7)\)</span>.</p>
<p>What if the dice are fair but not independent? Then while each individual roll will be uniform, the pair of dice together won’t be. Here’s an example. Suppose that the dice were somehow rigged to where die <span class="math inline">\(x\)</span> could “communicate” information to die <span class="math inline">\(y\)</span>. Every time <span class="math inline">\(x\)</span> rolls, it will tell <span class="math inline">\(y\)</span> to roll one number higher (mod 6).</p>
<table class="table">
<thead>
<tr class="header">
<th>x</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>y</strong></td>
<td>2</td>
<td>3</td>
<td>4</td>
<td>5</td>
<td>6</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>Let’s run a simulation here by again rolling 10,000 times and plotting the histograms, first the marginal histograms, then the joint histogram. To create the given relationship in the table, we can use the formula <span class="math inline">\(y = x \text{ mod } 6 + 1\)</span>, i.e.&nbsp;<code>y = x % 6 + 1</code>.</p>
<div class="cell" data-execution_count="81">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>y_dep <span class="op">=</span> x <span class="op">%</span> <span class="dv">6</span> <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>plot_histogram(x, is_discrete<span class="op">=</span><span class="va">True</span>, title<span class="op">=</span><span class="st">'$x$'</span>)</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>plot_histogram(y_dep, is_discrete<span class="op">=</span><span class="va">True</span>, title<span class="op">=</span><span class="st">'$y$'</span>)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>plot_joint_histogram([x, y_dep], title<span class="op">=</span><span class="st">'$(x,y)$'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="multivariate-probability_files/figure-html/cell-5-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="multivariate-probability_files/figure-html/cell-5-output-2.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="multivariate-probability_files/figure-html/cell-5-output-3.png" class="img-fluid"></p>
</div>
</div>
<p>First, plotting the histograms of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> individually shows each one still looks quite uniform, hence fair. But when plotted together, we see only 6 of the 36 possible pairs ever show up! Why? Because <span class="math inline">\(y\)</span> depends on <span class="math inline">\(x\)</span>. Knowing <span class="math inline">\(x\)</span> gives 100% information to reproduce the values of <span class="math inline">\(y\)</span> (and vice versa). Because of this functional dependence, only 6 unique pairs (the size of <span class="math inline">\(x\)</span>) are possible. In this case, we say that <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> are <strong>perfectly correlated</strong>.</p>
<p>We can see this more starkly doing a 2D histogram plot, which will plot the counts as a heat map with <span class="math inline">\(x\)</span> on the x-axis and <span class="math inline">\(y\)</span> on the y-axis. The cells that are dark blue indicate a lot of counts occurring there, while the white cells indicate few to no counts occurring. You can see only 6 of the 36 cells are being darkened.</p>
<div class="cell" data-execution_count="82">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>plot_hist2d(x, y_indep, title<span class="op">=</span><span class="st">'Two Independent Fair Dice'</span>, bins<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">6</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="multivariate-probability_files/figure-html/cell-6-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="83">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>plot_hist2d(x, y_dep, title<span class="op">=</span><span class="st">'Two Perfectly Correlated Fair Dice'</span>, bins<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">6</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="multivariate-probability_files/figure-html/cell-7-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="conditional-probability" class="level2">
<h2 class="anchored" data-anchor-id="conditional-probability">Conditional Probability</h2>
<p>To understand correlation and independence better, it’s interesting to look at the ratio of the joint PMF <span class="math inline">\(p(x,y)\)</span> with the marginal PMF <span class="math inline">\(p(x)\)</span>. This tells us in a sense how much information is contained in the joint distribution <span class="math inline">\(p(x,y)\)</span> if we already knew what information was in the marginal distribution <span class="math inline">\(p(x)\)</span>.</p>
<p>In the case when both die were independent, <span class="math inline">\(p(x,y)=\frac{1}{36}\)</span> and <span class="math inline">\(p(x)=\frac{1}{6}\)</span> for all <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, so</p>
<p><span class="math display">\[\frac{p(x,y)}{p(x)} = \frac{1/36}{1/6} = \frac{1}{6}.\]</span></p>
<p>To make a brief aside, we can measure the <strong>information content</strong> of a value <span class="math inline">\(z \sim p(z)\)</span> by defining a function</p>
<p><span class="math display">\[I = \log_2 \frac{1}{p(z)} = -\log_2 p(z).\]</span></p>
<p>This functions tells us how many bits of information are contained in the value <span class="math inline">\(z\)</span> if it’s sampled from <span class="math inline">\(p(z)\)</span>. If we average this function over all <span class="math inline">\(z\)</span>, it tells us how many bits of information are contained in the entire distribution itself. This is called <strong>entropy</strong>, denoted <span class="math inline">\(H\)</span>. I’ll talk about entropy more generally in a future lesson. But, when the distribution is uniform (as in our case), the entropy is just <span class="math inline">\(I\)</span> itself, i.e.&nbsp;<span class="math inline">\(H=I\)</span>. If <span class="math inline">\(p(z)=\frac{1}{n}\)</span> for all <span class="math inline">\(z\)</span>, its entropy is <span class="math inline">\(H = \log_2 n\)</span>. That is, we’d need <span class="math inline">\(\log_2 n\)</span> bits of information to specify any given value <span class="math inline">\(x\)</span> sampled from <span class="math inline">\(p(z)\)</span>.</p>
<p>We can apply this idea of information to the ratio given above. In that case, it takes <span class="math inline">\(H=\log_2(6) \approx 2.6\)</span> bits of information to specify any given <span class="math inline">\((x,y)\)</span> pair, assuming we knew <span class="math inline">\(x\)</span>. It looks like even if we knew what <span class="math inline">\(x\)</span> is doing, we’d still need <span class="math inline">\(2.6\)</span> more bits to figure out what <span class="math inline">\(y\)</span> is doing. In fact, that’s the same number of bits contained in <span class="math inline">\(p(y)\)</span>, since <span class="math inline">\(I=\log_2(6) \approx 2.6\)</span>. It seems like knowing what <span class="math inline">\(x\)</span> is is telling us basically nothing about what <span class="math inline">\(y\)</span> is.</p>
<p>Let’s compare this now with the perfectly correlated case, where <span class="math inline">\(p(x)=\frac{1}{6}\)</span> for all <span class="math inline">\(x\)</span>, but</p>
<p><span class="math display">\[
p(x,y) =
\begin{cases}
\frac{1}{6}, &amp; (x,y) = (1,2), (2,3), (3,4), (4,5), (5,6), (6,1) \\
0, &amp; \text{otherwise}.
\end{cases}
\]</span></p>
<p>If we take the same ratio again, we’d get</p>
<p><span class="math display">\[
\frac{p(x,y)}{p(x)} =
\begin{cases}
1, &amp; (x,y) = (1,2), (2,3), (3,4), (4,5), (5,6), (6,1) \\
0, &amp; \text{otherwise}.
\end{cases}
\]</span></p>
<p>Looking at the entropy of this ratio, we’d need <span class="math inline">\(H=\log_2(1) = 0\)</span> bits of information to specify <span class="math inline">\((x,y)\)</span> if <span class="math inline">\(x\)</span> was known. That is, knowing what <span class="math inline">\(x\)</span> is is <em>completely sufficient</em> to tell us what <span class="math inline">\(y\)</span> is. We don’t need to know anything else. We have all the bits we need already by knowing <span class="math inline">\(p(x)\)</span>.</p>
<p>Clearly this ratio is important, so we give it a name. We call it the <strong>conditional probability</strong> of <span class="math inline">\(y\)</span>, given <span class="math inline">\(x\)</span>, written</p>
<p><span class="math display">\[p(y|x) = \frac{p(x,y)}{p(x)}.\]</span></p>
<p>It’s not obvious that this is a valid probability, but it is so long as <span class="math inline">\(p(x) \neq 0\)</span>. It’s always non-negative, and will sum to one over all <span class="math inline">\(y\)</span> values,</p>
<p><span class="math display">\[\sum_j p(y=j|x) = \sum_j \frac{p(x,y=j)}{p(x)} = \frac{p(x)}{p(x)} = 1.\]</span></p>
<p>The last equality follows from the fact that we can sum over one variable to get the marginal distribution of the other,</p>
<p><span class="math display">\[p(x) = \sum_j p(x,y=j).\]</span> <span class="math display">\[p(y) = \sum_i p(x=i,y).\]</span></p>
<p>This is why they’re called marginal probabilities, because you’re “marginalizing” out the other variables.</p>
<p>The random variable <span class="math inline">\(y|x\)</span> (pronounced “y given x”) is different than <span class="math inline">\(y\)</span> or <span class="math inline">\(x\)</span> or <span class="math inline">\((x,y)\)</span>. It’s a completely new thing. It’s the variable of <span class="math inline">\(y\)</span> values, <em>assuming</em> we know what <span class="math inline">\(x\)</span> is already. In our perfectly correlated example, if we know that <span class="math inline">\(x=1\)</span>, then <span class="math inline">\(p(y|x=1) = 1\)</span> since we then know for certain that <span class="math inline">\(y=2\)</span>.</p>
<p>In fact, generally <span class="math inline">\(y|x\)</span> won’t even have the same distribution as <span class="math inline">\(y\)</span>. The only time <span class="math inline">\(y\)</span> and <span class="math inline">\(y|x\)</span> will have the same distribution is if <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> are independent, since in that case <span class="math inline">\(p(x,y)=p(x)p(y)\)</span>, so</p>
<p><span class="math display">\[p(y|x) = \frac{p(x,y)}{p(x)} = \frac{p(x)p(y)}{p(x)} = p(y).\]</span></p>
<p>When <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> aren’t independent, if the come from a joint distribution <span class="math inline">\(p(x,y)\)</span> people often say they’re <strong>conditionally independent</strong>, since <span class="math inline">\(y|x\)</span> is <em>always</em> independent of <span class="math inline">\(x\)</span> due to the fact that <span class="math inline">\(p(x,y)=p(y|x)p(x)\)</span>.</p>
<p>Back to the dice roll example, here’s a plot of the histogram for <span class="math inline">\(y|x=1\)</span> in each case, first the independent dice rolls, then the perfectly correlated rolls. The first will contain counts in all 6 bins since <span class="math inline">\(p(y|x)=p(y)\)</span> is uniform. The second will only contain counts in bin 2, since <span class="math inline">\(y=2\)</span> with certainty if <span class="math inline">\(x=1\)</span>.</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>plot_histogram(y_indep[x <span class="op">==</span> <span class="dv">1</span>], is_discrete<span class="op">=</span><span class="va">True</span>, title<span class="op">=</span><span class="st">'$y|x=1$ (independent case)'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="multivariate-probability_files/figure-html/cell-8-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>plot_histogram(y_dep[x <span class="op">==</span> <span class="dv">1</span>], is_discrete<span class="op">=</span><span class="va">True</span>, title<span class="op">=</span><span class="st">'$y|x=1$ (perfectly correlated case)'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="multivariate-probability_files/figure-html/cell-9-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="bayes-rule" class="level2">
<h2 class="anchored" data-anchor-id="bayes-rule">Bayes Rule</h2>
<p>Suppose we condition <span class="math inline">\(y\)</span> on the random variable <span class="math inline">\(x\)</span>. We just saw the conditional random variable <span class="math inline">\(y|x\)</span> is given by</p>
<p><span class="math display">\[p(y|x) = \frac{p(x,y)}{p(x)}.\]</span></p>
<p>Notice that we could symmetrically condition <span class="math inline">\(x\)</span> on <span class="math inline">\(y\)</span> instead, and ask about the other conditional random variable <span class="math inline">\(x|y\)</span>. In that case, its probability would be given symmetrically by</p>
<p><span class="math display">\[p(x|y) = \frac{p(x,y)}{p(y)}.\]</span></p>
<p>Let’s put both of these together. Using the second equation to write the joint probability as <span class="math inline">\(p(x,y)=p(x|y)p(y)\)</span> and plugging this expression into the first equation, we get</p>
<p><span class="math display">\[p(y|x) = \frac{p(x|y)p(y)}{p(x)}.\]</span></p>
<p>This formula is called <strong>Bayes Rule</strong>. It looks quaint, but don’t let that fool you. This is perhaps the most important formula in probability and statistics. Entire machine learning algorithms derive practically from this formula alone.</p>
</section>
<section id="n-random-variables" class="level2">
<h2 class="anchored" data-anchor-id="n-random-variables"><span class="math inline">\(n\)</span> Random Variables</h2>
<p>What I just described for two random variables extends to any number of random variables as well. If we have <span class="math inline">\(n\)</span> random variables <span class="math inline">\(x_0,x_1,\cdots,x_{n-1}\)</span>, we can define a <strong>random vector</strong> out of them, <span class="math inline">\(\mathbf{x} = (x_0,x_1,\cdots,x_{n-1})\)</span>. As a running example, imagine rolling not just two dice, but <span class="math inline">\(n\)</span> dice. Each dice alone can take on one of 6 values, which means a whole random vector of dice can take on <span class="math inline">\(6^n\)</span> tuples of values. The possible outcomes add up fast!</p>
<p>The <strong>joint probability</strong> of these variables can be defined as the ratio of the number of times <span class="math inline">\(n_{i_0,i_1,\cdots,i_{n-1}}\)</span> each tuple of values <span class="math inline">\((i_0,i_1,\cdots,i_{n-1})\)</span> occurs divided by the total number of trials <span class="math inline">\(N\)</span>, as <span class="math inline">\(N \rightarrow \infty\)</span>,</p>
<p><span class="math display">\[\mathbb{Pr}(\mathbf{x}=\mathbf{i}) = \mathbb{Pr}(x_0=i_0,x_1=i_1,\cdots,x_{n-1}=i_{n-1}) = \lim_{N \rightarrow \infty} \frac{N_{i_0,i_1,\cdots,i_{n-1}}}{N}.\]</span></p>
<p>We can also define a <strong>joint distribution</strong> by specifying a probability mass function <span class="math inline">\(p(\mathbf{x}) = p(x_0,x_1,\cdots,x_{n-1})\)</span>. Then each probability is given by</p>
<p><span class="math display">\[\mathbb{Pr}(\mathbf{x}=\mathbf{i}) = p(\mathbf{i}) = p(i_0,i_1,\cdots,i_{n-1}).\]</span></p>
<p>Any distribution gotten by summing over one or more variables is called a <strong>marginal distribution</strong>. We can have <span class="math inline">\(n\)</span> marginal distributions <span class="math inline">\(p_0(x_0), p_1(x_1), \cdots, p_{n-1}(x_{n-1})\)</span>, as well as marginals of any <em>pairs</em> of random variables <span class="math inline">\(p_{i,j}(x_i,x_j)\)</span>, or triplets <span class="math inline">\(p_{i,j,k}(x_i,x_j,x_k)\)</span>, etc.</p>
<p>Independence extends as well. Two variables <span class="math inline">\(x_i\)</span> and <span class="math inline">\(x_j\)</span> are <strong>independent</strong> if their bivariate marginal distribution <span class="math inline">\(p_{i,j}(x_i,x_j) = p_i(x_i)p_j(x_j)\)</span>. If <em>all</em> the variables are independent of each other, we can factor the whole joint distribution into products too,</p>
<p><span class="math display">\[p(x_0,x_1,\cdots,x_{n-1}) = \prod_{i=0}^{n-1} p_i(x_i) = p_0(x_0)p_1(x_1) \cdots p_{n-1}(x_{n-1}).\]</span></p>
<p><strong>Notation:</strong> Trying to keep track of all these notations, where each distribution has its own indices, containing variables with the same indices can be a mess. For this reason it’s common in practice to omit the subscripts when writing down distributions. So instead of writing <span class="math inline">\(p_i(x_i)\)</span> we’d just write <span class="math inline">\(p(x_i)\)</span>, where it’s understood by the presence of <span class="math inline">\(x_i\)</span> that we’re referring to the distribution <span class="math inline">\(p_i(x_i)\)</span>. This usually works fine, but it can be confusing in some cases, for example when multiple random variables come from the same distribution. We’ll see this exception below. Outside of those exceptions, assume that each distribution need not be the same.</p>
<p>When dealing with <span class="math inline">\(n\)</span> random variables, the most important cases to know about are: - When all variables individually come from the same marginal distribution <span class="math inline">\(p(x)\)</span>, i.e.&nbsp;<span class="math inline">\(p(x)=p_1(x_1)=\cdots=p_{n-1}(x_{n-1})\)</span>. We then say the random variables are <strong>identically distributed</strong>, or <strong>ID</strong> for short. We’d write <span class="math inline">\(x_0,x_1,\cdots,x_{n-1} \sim p(x)\)</span>, to make it clear everything is sampled from the same distribution. - When all variables are independent of each other, in which case <span class="math inline">\(p(x_0,x_1,\cdots,x_{n-1}) = p_0(x_0)p_1(x_1) \cdots p_{n-1}(x_{n-1})\)</span>. A shorthand used sometimes to say random variables are all independent is to write <span class="math inline">\(x_0 \perp x_1 \perp \cdots \perp x_{n-1}\)</span>, where the symbol <span class="math inline">\(\perp\)</span> means “is independent of”. - When all variables are <em>both</em> identically distributed <em>and</em> independent. Lacking any creativity, we call such variables <strong>independent, identically distributed</strong>, or <strong>IID</strong> for short. As a short hand, if we sample <span class="math inline">\(n\)</span> variables IID from some distribution <span class="math inline">\(p(x)\)</span>, we’d write</p>
<p><span class="math display">\[x_0,x_1,\cdots,x_{n-1} \overset{iid}{\sim} p(x), \quad \text{or just} \quad \mathbf{x} \overset{iid}{\sim} p(x).\]</span></p>
<p>IID functions have the nice property that their joint distribution factors into a product of marginals that all follow the same distribution,</p>
<p><span class="math display">\[p(x_0,x_1,\cdots,x_{n-1}) = \prod_{i=0}^{n-1} p(x_i) = p(x_0) p(x_1) \cdots p(x_{n-1}),\]</span></p>
<p>where it’s understood each <span class="math inline">\(p(x_i)\)</span> on the right is the same marginal distribution, so <span class="math inline">\(p(x)=p_0(x)=\cdots=p_{n-1}(x)\)</span>. This property is really nice because it means all we need to do to find the joint distribution is to find a single marginal distribution. By studying one, we’re studying them all.</p>
<p>We can also condition any random variable on one or more other random variables. For example, if we condition <span class="math inline">\(x_{n-1}\)</span> on <span class="math inline">\(x_0,x_1,\cdots,x_{n-2}\)</span>, then its conditional distribution is given by</p>
<p><span class="math display">\[p(x_{n-1}|x_0,x_1,\cdots,x_{n-2}) = \frac{p(x_0,x_1,\cdots,x_{n-2},x_{n-1})}{p(x_0,x_1,\cdots,x_{n-2})}.\]</span></p>
<p>This conditional probability is equivalent to asking, how much information is contained in the joint distribution <span class="math inline">\(p(x_0,x_1,\cdots,x_{n-2},x_{n-1})\)</span> if we already knew the marginal distribution <span class="math inline">\(p(x_0,x_1,\cdots,x_{n-2})\)</span>. Equivalently, how much information do the random variables <span class="math inline">\(x_0,x_1,\cdots,x_{n-2}\)</span> give us about the variable <span class="math inline">\(x_{n-1}\)</span>.</p>
<p>We can derive the general form of Bayes Rule in a similar way to the bivariate case. For example, if we know what <span class="math inline">\(p(x_0|x_1,\cdots,x_{n-1})\)</span>, <span class="math inline">\(p(x_1,\cdots,x_{n-1})\)</span>, and <span class="math inline">\(p(x_0)\)</span> are, then we can get <span class="math inline">\(p(x_1,\cdots,x_{n-1}|x_0)\)</span> by taking</p>
<p><span class="math display">\[p(x_1,\cdots,x_{n-1}|x_0) = \frac{p(x_0|x_1,\cdots,x_{n-1}) p(x_1,\cdots,x_{n-1})}{p(x_0)}.\]</span></p>
<p>I realize this section looks confusing. If you take nothing else away from this section, take away the idea of what IID random variables are. Almost all of machine learning in some way depends on the assumption that data is IID in some way or another. We’ll see enough examples as we go that you should start getting somewhat comfortable with joint distributions of <span class="math inline">\(n\)</span> variables. Most of the time they’ll be IID.</p>
</section>
<section id="application-the-central-limit-theorem" class="level2">
<h2 class="anchored" data-anchor-id="application-the-central-limit-theorem">Application: The Central Limit Theorem</h2>
<p>An interesting application of the use of joint random variables is in considering the sum of <span class="math inline">\(n\)</span> IID Bernoulli random variables. Suppose we sample <span class="math inline">\(n\)</span> times IID from a Bernoulli distribution,</p>
<p><span class="math display">\[x_0,x_1,\cdots,x_{n-1} \overset{iid}{\sim} \text{Ber}(\text{p}).\]</span></p>
<p>Consider a new random variable <span class="math inline">\(x\)</span> gotten by summing up all these IID Bernoulli variables,</p>
<p><span class="math display">\[x = \sum_{j=0}^{n-1} x_j = x_0 + x_1 + \cdots x_{n-1}.\]</span></p>
<p>Let’s stare at this expression and see what we can already deduce about the sum <span class="math inline">\(x\)</span>. Since each <span class="math inline">\(x_i=0,1\)</span>, the smallest <span class="math inline">\(x\)</span> can ever be is <span class="math inline">\(0\)</span> (when all <span class="math inline">\(x_i=0\)</span>), and the largest it can be is <span class="math inline">\(n\)</span> (when all <span class="math inline">\(x_i=1\)</span>), so <span class="math inline">\(x\)</span> must have support <span class="math inline">\(0,1,2,\cdots,n\)</span>. Next, let’s use the fact that all the <span class="math inline">\(x_i\)</span> are IID and see what that gives us. In this case, the joint distribution is</p>
<p><span class="math display">\[p(x_0,x_1,\cdots,x_{n-1}) = p(x_0)p(x_1) \cdots p(x_{n-1}).\]</span></p>
<p>Since each <span class="math inline">\(x_i \sim \text{Ber}(\text{p})\)</span>, we must then have</p>
<p><span class="math display">\[
\begin{align*}
p(x_0,x_1,\cdots,x_{n-1}) &amp;= p(x_0)p(x_1) \cdots p(x_{n-1}) \\
&amp;= \text{p}^{x_0}(1-\text{p})^{1-x_0} \text{p}^{x_1}(1-\text{p})^{1-x_1} \cdots \text{p}^{x_{n-1}}(1-\text{p})^{1-x_{n-1}} \\
&amp;= \text{p}^{x_0+x_1+\cdots+x_{n-1}} (1-\text{p})^{n-(x_0+x_1+\cdots+x_{n-1})} \\
&amp;= \text{p}^{x} (1-\text{p})^{n-x}.
\end{align*}
\]</span></p>
<p>You may recall that this expression looks kind of like the expression for the binomial distribution,</p>
<p><span class="math display">\[p(x; n,\text{p}) = \binom{n}{x} \text{p}^{x} (1-\text{p})^{n-x}, \quad \text{where } x=0,1,\cdots,n.\]</span></p>
<p>Evidently, the joint distribution <span class="math inline">\(p(x_0,x_1,\cdots,x_{n-1})\)</span> pretty much <em>is</em> the binomial distribution <span class="math inline">\(p(x; n,\text{p})\)</span>. If we just re-normalize each term by summing over all <span class="math inline">\(x=1,2,\cdots,n\)</span> and setting the sum equal to one, we get the same normalizing constants from the binomial distribution, namely the binomial coefficients</p>
<p><span class="math display">\[\binom{n}{x} = \frac{n!}{n!(n-x)!}.\]</span></p>
<p>We’ve thus shown that the binomial distribution is just the sum of <span class="math inline">\(n\)</span> IID Bernoulli random variables. IID Bernoulli random variables are often called <strong>Bernoulli trials</strong>. You can think of Bernoulli trials as being equivalent to flipping a (not necessarily fair) coin <span class="math inline">\(n\)</span> times and asking which ones turn up heads. The total number of heads from <span class="math inline">\(n\)</span> coin flips would be given by a binomial distribution <span class="math inline">\(\text{Bin}(n,\text{p})\)</span>.</p>
<p>A curious fact is that the binomial distribution <em>looks like</em> a Gaussian distribution when <span class="math inline">\(n\)</span> is large and <span class="math inline">\(\text{p}\)</span> isn’t too close to 0 or 1. Then,</p>
<p><span class="math display">\[\text{Bin}(n,\text{p}) \approx \mathcal{N}(n\text{p}, n\text{p}(1-\text{p})).\]</span></p>
<p>That is, the binomial distribution in such cases has an approximate bell-curved shape that’s centered around a mean of <span class="math inline">\(n\text{p}\)</span> with a variance of <span class="math inline">\(n\text{p}(1-\text{p})\)</span>.</p>
<p>This is a consequence of the <strong>Central Limit Theorem</strong>. The CLT, which I won’t prove, says that if we have <span class="math inline">\(n\)</span> IID random variables all with the same mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>, then, provided <span class="math inline">\(n\)</span> is large, the distribution of their sum will be approximately a Gaussian,</p>
<p><span class="math display">\[x = \sum_{i=0}^{n-1} x_i \sim \mathcal{N}\bigg(n\mu, n\sigma^2\bigg).\]</span></p>
<p>In the case of Bernoulli trials, each <span class="math inline">\(x_i\)</span> has mean <span class="math inline">\(\mu=\text{p}\)</span> and variance <span class="math inline">\(\sigma^2=\text{p}(1-\text{p})\)</span>, so <span class="math inline">\(x \sim \mathcal{N}(n\text{p}, n\text{p}(1-\text{p}))\)</span>. It’s kind of weird when you think about it. The CLT is saying <em>no matter what</em> distribution we sample from, as long as we sample IID a whole bunch of times, then the sum of those samples will be roughly Gaussian. In fact, the samples need not even be IID, but I won’t go into that.</p>
<p>I’ll show a quick example of this fact below by sampling from a binomial distribution 10,000 times with parameters <span class="math inline">\(n=1000\)</span> and <span class="math inline">\(\text{p}=0.5\)</span>. Notice first how bell-shaped the histogram appears, similar to a Gaussian. Also notice how almost all of the values seem to lie in the range <span class="math inline">\(500 \pm 32\)</span>, as you’d expect for a Gaussian distribution with mean <span class="math inline">\(500\)</span> and variance <span class="math inline">\(250\)</span>.</p>
<p>Evidently, if you flip a fair coin 10,000 times, the total number of heads would be roughly Gaussian distributed just like this example. Curious, right?</p>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> <span class="fl">0.5</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.random.binomial(n, p, size<span class="op">=</span><span class="dv">10000</span>)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>plot_histogram(x, title<span class="op">=</span><span class="ss">f'$Bin(</span><span class="sc">{</span>n<span class="sc">}</span><span class="ss">,</span><span class="sc">{</span>p<span class="sc">}</span><span class="ss">) </span><span class="ch">\\</span><span class="ss">approx \mathcal</span><span class="ch">{{</span><span class="ss">N</span><span class="ch">}}</span><span class="ss">(</span><span class="sc">{</span><span class="bu">int</span>(n<span class="op">*</span>p)<span class="sc">}</span><span class="ss">, </span><span class="sc">{</span><span class="bu">int</span>(n<span class="op">*</span>p<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>p))<span class="sc">}</span><span class="ss">)$'</span>, is_discrete<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="multivariate-probability_files/figure-html/cell-10-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="continuous-multivariate-distributions" class="level2">
<h2 class="anchored" data-anchor-id="continuous-multivariate-distributions">Continuous Multivariate Distributions</h2>
<p>All of the common distributions we’ve covered so far can be extended into the multivariate case. The most useful multivariate extensions for machine learning purposes is the multivariate Gaussian distribution.</p>
<p><strong>Product Distributions:</strong> <span class="math inline">\(p(x_0,x_1,\cdots) = p_1(x_0)p_1(x_1)\cdots\)</span></p>
<p><strong>Multivariate Gaussian Distribution:</strong> <span class="math inline">\(\mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\Sigma})\)</span></p>
<p>The multivariate Gaussian distribution, denoted by the symbol <span class="math inline">\(\mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\Sigma})\)</span>, is defined on all of <span class="math inline">\(n\)</span> dimensional space <span class="math inline">\(\mathbb{R}^n\)</span> with density function</p>
<p><span class="math display">\[p(\mathbf{x}) = (2 \pi)^{-n/2} |\boldsymbol{\Sigma}|^{-1/2} \exp\bigg(-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^\top  \boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})\bigg),\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{\mu} \in \mathbb{R}^n\)</span> is an n-dimensional real vector and <span class="math inline">\(\boldsymbol{\Sigma} \in \mathbb{R}^{n \times n}\)</span> is a positive square matrix. The vector <span class="math inline">\(\boldsymbol{\mu}\)</span> is the <span class="math inline">\(n\)</span> dimensional generalization of the mean, which is just a vector of means. The mean of variable <span class="math inline">\(x_i\)</span> is <span class="math inline">\(\mu_i\)</span> for all <span class="math inline">\(i=0,\cdots,n-1\)</span>. The matrix <span class="math inline">\(\boldsymbol{\Sigma}\)</span> is called the <strong>covariance matrix</strong>. It’s a symmetric, positive matrix that generalizes the notion of variance to <span class="math inline">\(n\)</span> dimensions. Why a matrix? Because each <span class="math inline">\(x_i,x_j\)</span> can have a notion of (co)variance with each other. More on this soon.</p>
<p>Notice the special symbol <span class="math inline">\(|\boldsymbol{\Sigma}|\)</span> in the constant term outside the exponential. It’s not important to know what this is. Just consider it a function of <span class="math inline">\(\boldsymbol{\Sigma}\)</span> alone. If you must know, it’s the <em>determinant</em> of the matrix. The only thing worth knowing is when <span class="math inline">\(\boldsymbol{\Sigma}\)</span> is diagonal, <span class="math inline">\(|\boldsymbol{\Sigma}|\)</span> is the product of the diagonal elements, which I’ll use below.</p>
<p>Note the exponent term is really just a sum of a bunch of scalar terms when expanded out, <span class="math display">\[(\mathbf{x}-\boldsymbol{\mu})^\top  \boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu}) = \sum_{i=0}^{n-1} \sum_{j=0}^{n-1} (x_i - \mu_i) \cdot \Sigma_{i,j}^{-1} \cdot (x_j - \mu_j),\]</span> hence the exponent is really just the sum of a bunch of quadratic terms, reminiscent of the univariate Gaussian.</p>
<p>In the special case when the covariance matrix <span class="math inline">\(\boldsymbol{\Sigma}\)</span> is diagonal, we say the random variables <span class="math inline">\(x_0,x_1,\cdots,x_{n-1}\)</span> are <strong>uncorrelated</strong>. Suppose <span class="math display">\[\boldsymbol{\Sigma} = \text{diag}(\sigma_0^2, \sigma_2^2, \cdots, \sigma_{n-1}^2).\]</span> You can pretty easily verify that its inverse is simply the reciprocals of the diagonal terms, <span class="math display">\[\boldsymbol{\Sigma}^{-1} = \text{diag}\bigg(\frac{1}{\sigma_0^2}, \frac{1}{\sigma_1^2}, \cdots, \frac{1}{\sigma_{n-1}^2}\bigg).\]</span> This makes the complicated looking exponent term reduce to a simple sum of squares. Using the fact <span class="math inline">\(|\boldsymbol{\Sigma}|=\sigma_0\sigma_1\cdots\sigma_{n-1}\)</span> as well, we get</p>
<p><span class="math display">\[
\begin{align*}
p(\mathbf{x}) &amp;= \frac{1}{\sqrt{2 \pi \sigma_0 \sigma_1 \cdots \sigma_{n-1}}} \exp\bigg(-\frac{1}{2} \sum_{i=0}^{n-1} \frac{(x_i - \mu_i)^2}{\sigma_i^2} \bigg) \\
&amp;= \Bigg(\frac{1}{\sqrt{2 \pi \sigma_0^2}} \exp{\bigg(-\frac{(x_0 - \mu_0)^2}{2\sigma_0^2}\bigg)} \Bigg) \cdots \Bigg(\frac{1}{\sqrt{2 \pi \sigma_{n-1}^2}} \exp{\bigg(-\frac{(x_{n-1} - \mu_{n-1})^2}{2\sigma_{n-1}^2}\bigg)} \Bigg) \\
&amp;= p(x_0) p(x_1) \cdots p(x_{n-1}).
\end{align*}
\]</span></p>
<p>That is, the joint distribution <span class="math inline">\(p(\mathbf{x})\)</span> factors into a product of marginal Gaussian distributions <span class="math inline">\(p(x_0) p(x_1) \cdots p(x_{n-1})\)</span>. This means that the variables <span class="math inline">\(x_0,x_1,\cdots,x_{n-1}\)</span> must be independent if they’re uncorrelated. In the further special case where <span class="math inline">\(\mu_i=\mu\)</span> and <span class="math inline">\(\sigma_i^2=\sigma^2\)</span> for all <span class="math inline">\(i\)</span>, they’re also IID.</p>
<p>This distribution is so important to machine learning that it’s worth taking some time to visualize it, as a surface in 3D space, as a set of contours, as 3D histogram, and as a heat map. Let’s start with the first two. Suppose our random variable <span class="math inline">\(\mathbf{x}=(x,y)\)</span> is 2D. I’m going to plot the surface of the density function <span class="math inline">\(p(x,y)\)</span> as well as the contour plot side-by-side using a helper function <code>plot_multivariate_normal</code>. This function will take in the mean vector <span class="math inline">\(\boldsymbol{\mu}\)</span> and covariance matrix <span class="math inline">\(\boldsymbol{\Sigma}\)</span> and plot what the distribution looks like for that pair of parameters.</p>
<p>Let’s define <span class="math inline">\(\boldsymbol{\mu}=(\mu_x, \mu_y)\)</span> and <span class="math inline">\(\boldsymbol{\Sigma} = \begin{pmatrix} \sigma_x^2 &amp; \sigma_{x,y} \\ \sigma_{x,y} &amp; \sigma_y^2 \end{pmatrix}.\)</span> A few things to notice: - Changing <span class="math inline">\(\boldsymbol{\mu}\)</span> just changes where the density is centered. It doesn’t change the shape. - Changing <span class="math inline">\(\sigma_x^2\)</span> or <span class="math inline">\(\sigma_y^2\)</span> changes how much the density spreads in that direction. If they’re equal the density will be perfectly circular. If one is greater than the other the density will become elliptical. In the limit one becomes much much greater than the other, the density will essentially just become a univariate Gaussian. - Making <span class="math inline">\(\sigma_{x,y}\)</span> non-zero makes the density change its angle in the xy-plane. That’s what introducing correlation essentially does geometrically. - Making <span class="math inline">\(\sigma_{x,y}=0\)</span> means that <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> become independent as I showed above. Geometrically, independence essentially rotates until the density is aligned along the axes themselves.</p>
<p>Feel free to play around with different choices of parameters and see how it affects the densities.</p>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>mu <span class="op">=</span> np.array([<span class="dv">0</span>, <span class="dv">0</span>])</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>Sigma <span class="op">=</span> np.array([[<span class="dv">1</span>, <span class="fl">0.5</span>], [<span class="fl">0.5</span>, <span class="dv">2</span>]])</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>plot_multivariate_gaussian(mu, Sigma, elev<span class="op">=</span><span class="dv">10</span>, azim<span class="op">=</span><span class="dv">45</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="multivariate-probability_files/figure-html/cell-11-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>We can also of course plot the histograms. Suppose we sample a bunch of points <span class="math inline">\((x,y) \sim \mathcal{N}(\boldsymbol{\mu}, \boldsymbol{\Sigma})\)</span> a bunch of times and plot the histogram. We can do this in python by again using <code>np.random.randn</code>, but passing in a shape <span class="math inline">\((m,n)\)</span> instead of a single number.</p>
<p>When <span class="math inline">\(n=2\)</span> we can plot the histogram in 3D space. I’ll plot the standard 2D Gaussian <span class="math inline">\(\mathcal{N}(\mathbf{0}, \mathbf{I})\)</span>, which has mean zero and identity covariance. It’s the multivariate generalization of the standard <span class="math inline">\(\mathcal{N}(0,1)\)</span> Gaussian. I’m using another helper function called <code>plot_3d_hist</code> here to hide a lot of ugly code.</p>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.random.randn(<span class="dv">10000</span>, <span class="dv">2</span>)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>x, y <span class="op">=</span> X[:, <span class="dv">0</span>], X[:, <span class="dv">1</span>]</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>plot_3d_hist(x, y, bins<span class="op">=</span><span class="dv">30</span>, xlim<span class="op">=</span>(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>), ylim<span class="op">=</span>(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>), elev<span class="op">=</span><span class="dv">20</span>, azim<span class="op">=</span><span class="dv">30</span>, </span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>             title<span class="op">=</span><span class="st">'3D Histogram: $\mathcal</span><span class="sc">{N}</span><span class="st">(\mathbf</span><span class="sc">{0}</span><span class="st">, \mathbf</span><span class="sc">{I}</span><span class="st">)$'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="multivariate-probability_files/figure-html/cell-12-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Finally, we can view the same histogram contour-style using what’s called a <strong>heat map</strong>. A heat map is a way of plotting one variable that depends on 2 others, e.g.&nbsp;<span class="math inline">\(p(x,y)\)</span> vs <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>. In each grid of the plot the color indicates how “hot” the dependent variable is. The higher the counts in a given bin, the hotter the heat map will look at that bin. We can plot a histogram-style heatmap in matplotlib using <code>plt.hist2d</code>. Heat maps are very useful ways of visualizing data like this, usually more so than janky 3D plots like the one above.</p>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>plt.hist2d(x, y, bins<span class="op">=</span><span class="dv">30</span>)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'x'</span>)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'y'</span>)</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Heat Map: $\mathcal</span><span class="sc">{N}</span><span class="st">(\mathbf</span><span class="sc">{0}</span><span class="st">, \mathbf</span><span class="sc">{I}</span><span class="st">)$'</span>)</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="multivariate-probability_files/figure-html/cell-13-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># add product distributions, multivariate laplace, one-hot categorical, multinomial</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation column-body">
  <div class="nav-page nav-page-previous">
      <a href="../notebooks/probability.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Probability Distributions</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
  </div>
</nav>
</div> <!-- /content -->



</body></html>