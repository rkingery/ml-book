<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Math and Programming for Machine Learning - 11&nbsp; Basic Probability</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../notebooks/multivariate-probability.html" rel="next">
<link href="../notebooks/optimization.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Basic Probability</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Math and Programming for Machine Learning</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">Preface</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/programming.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Computer Programming</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/basic-math.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Basic Math</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/numerical-computing.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Numerical Computation</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/calculus.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Calculus</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/linear-systems.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Linear Systems</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/vectors.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Vector Spaces</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/matrices.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Matrix Algebra</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/tensors.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Tensor Algebra</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/multivariate-calculus.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Multivariate Calculus</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/optimization.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Optimization</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/probability.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Basic Probability</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/multivariate-probability.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Multivariate Distributions</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/statistics.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Statistics</span></a>
  </div>
</li>
    </ul>
    </div>
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#randomness" id="toc-randomness" class="nav-link active" data-scroll-target="#randomness"><span class="toc-section-number">11.1</span>  Randomness</a></li>
  <li><a href="#discrete-probability" id="toc-discrete-probability" class="nav-link" data-scroll-target="#discrete-probability"><span class="toc-section-number">11.2</span>  Discrete Probability</a>
  <ul class="collapse">
  <li><a href="#motivation-rolling-a-die" id="toc-motivation-rolling-a-die" class="nav-link" data-scroll-target="#motivation-rolling-a-die"><span class="toc-section-number">11.2.1</span>  Motivation: Rolling a Die</a></li>
  <li><a href="#general-case" id="toc-general-case" class="nav-link" data-scroll-target="#general-case"><span class="toc-section-number">11.2.2</span>  General Case</a></li>
  <li><a href="#discrete-distributions" id="toc-discrete-distributions" class="nav-link" data-scroll-target="#discrete-distributions"><span class="toc-section-number">11.2.3</span>  Discrete Distributions</a></li>
  <li><a href="#probabilities-of-multiple-outcomes" id="toc-probabilities-of-multiple-outcomes" class="nav-link" data-scroll-target="#probabilities-of-multiple-outcomes"><span class="toc-section-number">11.2.4</span>  Probabilities of Multiple Outcomes</a></li>
  </ul></li>
  <li><a href="#continuous-probability" id="toc-continuous-probability" class="nav-link" data-scroll-target="#continuous-probability"><span class="toc-section-number">11.3</span>  Continuous Probability</a>
  <ul class="collapse">
  <li><a href="#motivation-rand-again" id="toc-motivation-rand-again" class="nav-link" data-scroll-target="#motivation-rand-again"><span class="toc-section-number">11.3.1</span>  Motivation: Rand Again</a></li>
  <li><a href="#general-case-1" id="toc-general-case-1" class="nav-link" data-scroll-target="#general-case-1"><span class="toc-section-number">11.3.2</span>  General Case</a></li>
  <li><a href="#continuous-distributions" id="toc-continuous-distributions" class="nav-link" data-scroll-target="#continuous-distributions"><span class="toc-section-number">11.3.3</span>  Continuous Distributions</a></li>
  <li><a href="#cauchy-distribution" id="toc-cauchy-distribution" class="nav-link" data-scroll-target="#cauchy-distribution"><span class="toc-section-number">11.3.4</span>  Cauchy Distribution</a></li>
  </ul></li>
  </ul>
</nav>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content column-page-right" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Basic Probability</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>In this lesson I’ll cover the basic theory of probability, as well as univariate random variables and some of their common probability distributions. Let’s get started. I’ll start by loading the libraries we’ve been working with so far. New to this lesson is the seaborn library, a plotting library that extends matplotlib by adding a bunch of nice statistical plots.</p>
<div class="cell" data-execution_count="1">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sympy <span class="im">as</span> sp</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> utils.math_ml <span class="im">import</span> <span class="op">*</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">"figure.figsize"</span>] <span class="op">=</span> (<span class="dv">4</span>, <span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Probability is the study of randomness. When dealing with randomness, variables in code can often take on unpredictable values, which makes it hard to exactly replicate results. While not always necessary, when you want to ensure that your code is exactly reproducible, you have to remember to set a <strong>seed</strong> when working with random numbers. The seed can be any number you want, but you should pick it and put it at the top of your code. Setting the seed will ensure that every time your code is run the outputs will agree with the results somebody else gets from running your code.</p>
<p>Since I want to make sure my code in this book is reproducible, I will from now on always set a seed. To set a seed in numpy, you just need to pass in <code>np.random.seed(seed)</code> right after import numpy, where <code>seed</code> can be any positive integer you like. Different seeds will produce random numbers in different orders. Below I’ll choose a seed of zero, which is completely arbitrary.</p>
<div class="cell" data-execution_count="2">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<section id="randomness" class="level2" data-number="11.1">
<h2 data-number="11.1" class="anchored" data-anchor-id="randomness"><span class="header-section-number">11.1</span> Randomness</h2>
<p>Probability is a calculus for modeling random processes. There are things we just can’t predict with certainty given the information we have available. Stuff that we can’t predict with certainty we call <strong>random</strong>, or <strong>noise</strong>, or <strong>non-deterministic</strong>. Stuff we <em>can</em> predict with certainty we call <strong>deterministic</strong> or <strong>certain</strong>. Here are some examples of these two kinds of processes. The questions in the deterministic column have exact answers, while those in the random column do not.</p>
<table class="table">
<colgroup>
<col style="width: 65%">
<col style="width: 34%">
</colgroup>
<thead>
<tr class="header">
<th>Deterministic Process</th>
<th>Random Process</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Does <span class="math inline">\(2+2=4\)</span>?</td>
<td>Will it rain today?</td>
</tr>
<tr class="even">
<td>What is the capital of France?</td>
<td>What is the result of rolling a pair of dice?</td>
</tr>
<tr class="odd">
<td>How many sides does a square have?</td>
<td>What is the next card in a shuffled deck?</td>
</tr>
<tr class="even">
<td>What is the value of pi?</td>
<td>What is the stock price of Apple tomorrow?</td>
</tr>
<tr class="odd">
<td>What is the boiling point of water at sea level?</td>
<td>What is the winning number for next week’s lottery?</td>
</tr>
</tbody>
</table>
<p>Deterministic processes aren’t terribly interesting. They either will occur with certainty, or they won’t. Random processes <em>might</em> occur. To quantify what we mean by <em>might</em> we’ll introduce the notion of <strong>probability</strong>. You can think of probability as a function mapping questions like “Will it rain today?” to a number between <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span> that indicates our “degree of belief” in whether that question is true,</p>
<p><span class="math display">\[0 \leq \mathbb{Pr}(\text{Will it rain today?}) \leq 1.\]</span></p>
<p>The question inside this probability function is called an <strong>event</strong>. An event is anything that might occur. Mathematically speaking, an event is a <em>set</em> that lives in some abstract <em>sample space</em> of all possible outcomes.</p>
<p>When we’re <em>certain</em> an event will occur we say it has <strong>probability one</strong>, or a 100% chance of happening. When we’re certain an event <em>will not</em> occur we say it has <strong>probability zero</strong>, or a 0% chance of happening. These extremes are deterministic processes. Random processes are anything in between. For the question “Will it rain today?”, we might say there is a 20% chance of rain, in which case we believe <span class="math inline">\(\mathbb{Pr}(\text{Will it rain today?}) = 0.2\)</span>.</p>
<p>A common theme we’ll see in machine learning is that we’re interested in mapping arbitrary data structures like strings to numerical data structures that we can do mathematical calculations with, like floats or arrays. In this particular example, it’s convenient to map the question “Will it rain today?” to a binary variable I’ll call <span class="math inline">\(x\)</span>, <span class="math display">\[
x =
\begin{cases}
1, &amp; \text{It will rain today} \\
0, &amp; \text{It will not rain today}.
\end{cases}
\]</span></p>
<p>Then asking for <span class="math inline">\(\mathbb{Pr}(\text{Will it rain today?})\)</span> is the same thing as asking “what is the probability that <span class="math inline">\(x=1\)</span>”, or equivalently, what is <span class="math inline">\(\mathbb{Pr}(x=1)\)</span>? Saying we believe there’s a 20% chance of rain today is equivalent to saying we believe there is a 20% chance that <span class="math inline">\(x=1\)</span>, i.e.&nbsp;<span class="math inline">\(\mathbb{Pr}(x=1)=0.2\)</span>.</p>
<p>Variables like <span class="math inline">\(x\)</span> are called <strong>random variables</strong>. They’re a way of encoding random events numerically via some kind of encoding convention like I just used. It’s much more convenient to work with random variables than events or questions since we can now use all our usual mathematical tools like calculus and linear algebra to understand random processes.</p>
<p>To understand how random variables work, it’s often helpful to think of them as the outputs of <strong>random number generators</strong>. These are algorithms that generate, or <strong>sample</strong>, random numbers from some given distribution. Unlike regular functions, where a given input will <em>always</em> produce a definite output, a random number generator can (and usually will) produce different outputs every single time the same input is passed in.</p>
<p>The canonical example of a random number generator is called <strong>rand</strong>. It’s an algorithm for uniformly generating (pseudo) random real numbers <span class="math inline">\(0 \leq x \leq 1\)</span>. Every time we call rand we’ll get a different number with no clear pattern.</p>
<p>Here’s an example. I’ll call rand via the numpy function <code>np.random.rand</code> a bunch of times and print the first 10 outputs. Notice how all over the place they seem to be. The only thing we know is they’re between zero and one.</p>
<div class="cell" data-execution_count="3">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.random.rand(<span class="dv">100</span>)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>x[:<span class="dv">12</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="3">
<pre><code>array([0.5488135 , 0.71518937, 0.60276338, 0.54488318, 0.4236548 ,
       0.64589411, 0.43758721, 0.891773  , 0.96366276, 0.38344152,
       0.79172504, 0.52889492])</code></pre>
</div>
</div>
<p>Think of a random variable informally as being some variable <span class="math inline">\(x\)</span> whose values are determined by a function <span class="math inline">\(x=f(n)\)</span>, except the function can’t make up its mind or follow a pattern. On one sampling we might get <span class="math inline">\(x=f(0)=0.548\)</span>. Next, <span class="math inline">\(x=f(1)=0.715\)</span>. Next, <span class="math inline">\(x=f(2)=0.603\)</span>. Etc. We can’t force <span class="math inline">\(x\)</span> to take on a definite value. It jumps around with no clear pattern.</p>
<div class="cell" data-execution_count="4">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>plt.scatter(<span class="bu">range</span>(<span class="bu">len</span>(x)), x)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'n'</span>)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'x'</span>)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'$x = f(n)$'</span>)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>plt.show()<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="probability_files/figure-html/cell-5-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Since random variable outputs jump around like this we need a different way to visualize them than just thinking of them as points on the number line. The most useful way to visualize random variables is using a <strong>histogram</strong>. To create a histogram, we sample a random variable a whole bunch of times, and plot a count of how many times the variable takes on each given value. We then show these counts in a bar chart with the heights indicating the counts for each value.</p>
<p>In matplotlib we can plot histograms of an array of samples <code>x</code> using the function <code>plt.hist(x)</code>. Here’s an example. I’ll sample 100 values from rand and put them in an array <code>x</code>, then plot the histogram.</p>
<div class="cell" data-execution_count="5">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.random.rand(<span class="dv">100</span>)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>plt.hist(x)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>plt.show()<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="probability_files/figure-html/cell-6-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Notice that we just sampled <span class="math inline">\(100\)</span> different values, but we don’t see <span class="math inline">\(100\)</span> different bars. That’s because histograms don’t plot bars for <em>all values</em>. First, the values get <em>binned</em> into some number of equally spaced subintervals, called <strong>bins</strong>, then the counts that get plotted are the counts of values inside each bin. In this case, the histogram divides the samples into <span class="math inline">\(10\)</span> equally spaced bins. If you look carefully you should see <span class="math inline">\(10\)</span> bars in the plot. We can change the number of bins by passing in a keyword <code>bins</code> specifying how many bints to take.</p>
<p>Since I’ll be using histograms a lot in this lesson I’m going to write a helper function <code>plot_histogram</code> to bundle up the code to plot them nicely. Instead of using <code>plt.hist</code>, however, I’ll use the seaborn library’s <code>sns.histplot</code>, which creates much nicer looking histograms. Seaborn is an extension library of matplotlib made specifically for making nicer plots of data. Ignore the <code>is_discrete</code> argument for now. I’ll use it in the next section.</p>
<div class="cell" data-execution_count="6">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_histogram(x, is_discrete<span class="op">=</span><span class="va">False</span>, title<span class="op">=</span><span class="st">''</span>, <span class="op">**</span>kwargs):</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> is_discrete:</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>        sns.histplot(x, discrete<span class="op">=</span><span class="va">True</span>, shrink<span class="op">=</span><span class="fl">0.8</span>, <span class="op">**</span>kwargs)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>        unique <span class="op">=</span> np.unique(x)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">len</span>(unique) <span class="op">&lt;</span> <span class="dv">15</span>:</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>            plt.xticks(unique)</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>        sns.histplot(x, <span class="op">**</span>kwargs)</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>    plt.title(title)</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>It’s still kind of hard to see if the <span class="math inline">\(100\)</span> rand samples have any kind of pattern in the above histogram plot. Let’s now sample 10,000 numbers from rand and see if we can find one.</p>
<div class="cell" data-execution_count="7">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.random.rand(<span class="dv">10000</span>)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>plot_histogram(x, bins<span class="op">=</span><span class="dv">10</span>, title<span class="op">=</span><span class="ss">f'rand(</span><span class="sc">{</span><span class="dv">10000</span><span class="sc">}</span><span class="ss">)'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="probability_files/figure-html/cell-8-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>It should be increasingly clear now that what’s going on is that rand is sampling numbers between 0 and 1 with equal probability. Each bin should contain roughly <span class="math inline">\(\frac{10000}{10}=1000\)</span> counts, since there are <span class="math inline">\(10000\)</span> samples and <span class="math inline">\(10\)</span> bins. Said differently, the <em>values</em> in each bin should have a <span class="math inline">\(\frac{1}{10}=0.1\)</span> probability of being sampled. For example, the values in the left-most bin, call it <span class="math inline">\(I_0 = [0, 0.1]\)</span> should have</p>
<p><span class="math display">\[\mathbb{Pr}(x \in I_0) = \mathbb{Pr}(0 \leq x \leq 0.1) = 0.1.\]</span></p>
<p>This type of “flat”, equal probability sampling is called <strong>uniform random sampling</strong>.</p>
<p>You may be questioning that it’s indeed the case that each bin is truly getting sampled as much as the other bins. After all, the plot still clearly shows their heights vary a bit. Some bins have slightly more values than others do. We can look at how many counts are in the bin using <code>np.histogram</code>, which also defaults to <span class="math inline">\(10\)</span> bins. You can see some bins have as many as <span class="math inline">\(1037\)</span> values, some as few as <span class="math inline">\(960\)</span> values.</p>
<div class="cell" data-execution_count="8">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>bin_counts, _ <span class="op">=</span> np.histogram(x)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>bin_counts</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="8">
<pre><code>array([1025, 1036,  999,  981, 1037,  989,  956,  996,  976, 1005])</code></pre>
</div>
</div>
<section id="aside-estimating-the-fluctuation-in-bin-counts" class="level4" data-number="11.1.0.1">
<h4 data-number="11.1.0.1" class="anchored" data-anchor-id="aside-estimating-the-fluctuation-in-bin-counts"><span class="header-section-number">11.1.0.1</span> Aside: Estimating the Fluctuation in Bin Counts</h4>
<p>This variation in the bin counts is really due to the fact that we’re only sampling a finite number of values. To get <em>true</em> uniform sampling, where all bins have the same counts, we’d have to sample an infinitely large number of times.</p>
<p>Here’s a rule of thumb for how much the bin counts should be expected to fluctuate as a function of the sample size. If <span class="math inline">\(N\)</span> is the number of samples, and each bin <span class="math inline">\(k\)</span> contains <span class="math inline">\(N_k\)</span> counts (i.e.&nbsp;its bar height is <span class="math inline">\(N_k\)</span>), then you can expect the counts to fluctuate above and below <span class="math inline">\(N_k\)</span> by about</p>
<p><span class="math display">\[\sigma_k = \sqrt{N_k\bigg(1 - \frac{N_k}{N}\bigg)}.\]</span></p>
<p>Said differently, the counts should be expected to roughly lie in a range <span class="math inline">\(N_k \pm \sigma_k\)</span>. This notation means the same thing as saying the counts should roughly speaking lie in the range <span class="math inline">\([N_k - \sigma_k, N_k + \sigma_k]\)</span>. By “roughly”, I mean sometimes bins can have counts outside this range, but it’s uncommon.</p>
<p>In the above example, there are <span class="math inline">\(N=10000\)</span> samples, and each bin has about <span class="math inline">\(N_k=1000\)</span> counts, so you should expect the counts to fluctuate by about</p>
<p><span class="math display">\[\sigma_k = \sqrt{1000\bigg(1 - \frac{1000}{10000}\bigg)} = 30,\]</span></p>
<p>which means the counts should rougly lie in the range <span class="math inline">\(1000 \pm 30\)</span>. This seems to be in line with what we’re seeing experimentally. Notice as the sample size <span class="math inline">\(N \rightarrow \infty\)</span>, the fluctuations <span class="math inline">\(\sigma_k \rightarrow 0\)</span>. We’ll see where this rule comes from later (hint: the binomial distribution).</p>
<p>Back to random variables. Broadly speaking we can divide random variables into two classes of distributions:</p>
<ul>
<li>discrete distributions: random variables that can only take on a discrete set of values.</li>
<li>continuous distributions: random variables that can take on any continuum of real values.</li>
</ul>
<p>I’ll start by talking about the discrete case since it’s easier to understand.</p>
</section>
</section>
<section id="discrete-probability" class="level2" data-number="11.2">
<h2 data-number="11.2" class="anchored" data-anchor-id="discrete-probability"><span class="header-section-number">11.2</span> Discrete Probability</h2>
<p>Discrete random variables are variables that can only take on a discrete range of values. Usually this range is a finite set like <span class="math inline">\(\{0,1\}\)</span> or <span class="math inline">\(\{1,2,3,4,5,6\}\)</span> or something like that. But they could have an infinite range too, for example the set <span class="math inline">\(\mathbb{N}\)</span> of all non-negative integers. Rand is not an example of a discrete random variable, since there the range is all of the interval <span class="math inline">\([0,1]\)</span>.</p>
<p>Here are some examples of real life things that can be modeled by a discrete random variable:</p>
<ul>
<li>Modeling the rolls of a die with faces <span class="math inline">\(1,2,3,4,5,6\)</span>.</li>
<li>Modeling values from flipping a coin taking on a value of heads or tails.</li>
<li>Modeling a hand of poker, where there are 5 cards each drawn from the same deck of 52 cards.</li>
<li>Modeling the outputs of data used to train a machine learning classification model.</li>
<li>Modeling the number of heads gotten from flipping a coin a whole bunch of times.</li>
<li>Modeling the number of people entering a building per hour.</li>
</ul>
<section id="motivation-rolling-a-die" class="level3" data-number="11.2.1">
<h3 data-number="11.2.1" class="anchored" data-anchor-id="motivation-rolling-a-die"><span class="header-section-number">11.2.1</span> Motivation: Rolling a Die</h3>
<p>Consider a very simple toy problem: rolling a die (singular of dice). If you’ve never seen dice before, they’re white cubes with black dots on each face of the cube. Each face gets some number of black dots on it between 1 and 6. People like to “roll” these dice in games by shaking and tossing them onto the ground. The person with the highest score, i.e.&nbsp;the most number of dots facing upward, wins that round.</p>
<br><br>
<center>
<span style="font-size: 80px;">🎲</span>
</center>
<p><br></p>
<p>Let’s think a little bit about a single die. Suppose I want to roll a single die. Having not rolled the die yet, what should I “expect” the value to be when I roll the die? Call this score <span class="math inline">\(x\)</span>. The possible values I can have are just the number of dots on each face of the die, i.e.&nbsp;<span class="math inline">\(1,2,3,4,5,6\)</span>. This alone doesn’t tell me what the chance is that any given <span class="math inline">\(x\)</span> turns up in a roll. We need some other information.</p>
<p>Perhaps your common sense kicks in and you think, “Well clearly each number has an equal chance of showing up if you roll the die”. This is called the <strong>principle of indifference</strong>. In practice you’d usually be right. You’re saying that, since we don’t have any other information to go on, each number should have an equal chance of showing up on each roll. That is, on any given roll, the random variable <span class="math inline">\(x\)</span> should take on each value <span class="math inline">\(k=1,2,\cdots,6\)</span> with probability,</p>
<p><span class="math display">\[p_k = \mathbb{Pr}(x=k) = \frac{1}{6}.\]</span></p>
<p>This just says that the probability of rolling <span class="math inline">\(x=1\)</span> is <span class="math inline">\(p_1 = \frac{1}{6}\)</span>, the probability of rolling <span class="math inline">\(x=2\)</span> is also <span class="math inline">\(p_2 = \frac{1}{6}\)</span>, etc. Notice that these probabilities satisfy two properties that all probabilities must satisfy: 1. Each probability is non-negative: <span class="math inline">\(p_k = \frac{1}{6} \geq 0\)</span>, 2. The sum of all the possible probabilities is one: <span class="math inline">\(\sum_{k=1}^6 p_k = p_1 + p_2 + p_3 + p_4 + p_5 + p_6 = 6 \cdot \frac{1}{6} = 1\)</span>.</p>
<p>These two properties are the defining characteristics of a probability. The second condition is just a mathematical way of saying that rolling the die <em>must</em> return <em>some</em> value <span class="math inline">\(x \in \{1,2,3,4,5,6\}\)</span>. It can’t just make up some new value, or refuse to answer.</p>
<p>Anyway, suppose I rolled the die <span class="math inline">\(N=36\)</span> times and got the following values:</p>
<table class="table">
<colgroup>
<col style="width: 4%">
<col style="width: 2%">
<col style="width: 2%">
<col style="width: 2%">
<col style="width: 2%">
<col style="width: 2%">
<col style="width: 2%">
<col style="width: 2%">
<col style="width: 2%">
<col style="width: 2%">
<col style="width: 2%">
<col style="width: 2%">
<col style="width: 2%">
<col style="width: 2%">
<col style="width: 2%">
<col style="width: 2%">
<col style="width: 2%">
<col style="width: 2%">
<col style="width: 2%">
<col style="width: 2%">
<col style="width: 2%">
<col style="width: 2%">
<col style="width: 2%">
<col style="width: 2%">
<col style="width: 2%">
<col style="width: 2%">
<col style="width: 2%">
<col style="width: 2%">
<col style="width: 2%">
<col style="width: 2%">
<col style="width: 2%">
<col style="width: 2%">
<col style="width: 2%">
<col style="width: 2%">
<col style="width: 2%">
<col style="width: 2%">
<col style="width: 2%">
</colgroup>
<thead>
<tr class="header">
<th>Roll</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
<th>7</th>
<th>8</th>
<th>9</th>
<th>10</th>
<th>11</th>
<th>12</th>
<th>13</th>
<th>14</th>
<th>15</th>
<th>16</th>
<th>17</th>
<th>18</th>
<th>19</th>
<th>20</th>
<th>21</th>
<th>22</th>
<th>23</th>
<th>24</th>
<th>25</th>
<th>26</th>
<th>27</th>
<th>28</th>
<th>29</th>
<th>30</th>
<th>31</th>
<th>32</th>
<th>33</th>
<th>34</th>
<th>35</th>
<th>36</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Value</strong></td>
<td>3</td>
<td>4</td>
<td>5</td>
<td>4</td>
<td>3</td>
<td>1</td>
<td>3</td>
<td>6</td>
<td>5</td>
<td>2</td>
<td>1</td>
<td>5</td>
<td>4</td>
<td>2</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>6</td>
<td>5</td>
<td>6</td>
<td>3</td>
<td>5</td>
<td>5</td>
<td>3</td>
<td>3</td>
<td>6</td>
<td>6</td>
<td>1</td>
<td>5</td>
<td>4</td>
<td>2</td>
<td>2</td>
<td>4</td>
<td>6</td>
<td>2</td>
<td>4</td>
</tr>
</tbody>
</table>
<p>We can make a histogram out of these and check the principle of indifference by verifying the bins are all of about the same height (at least as close to the same as only 30 rolls will allow). Note that I’m now using <code>is_discrete=True</code> here, which tells the helper function to give each unique <span class="math inline">\(k\)</span> its own bin.</p>
<div class="cell" data-execution_count="9">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> [<span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">4</span>, <span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">6</span>, <span class="dv">5</span>, <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">5</span>, <span class="dv">4</span>, <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">6</span>, <span class="dv">5</span>, <span class="dv">6</span>, <span class="dv">3</span>, <span class="dv">5</span>, <span class="dv">5</span>, <span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">6</span>, <span class="dv">6</span>, <span class="dv">1</span>, <span class="dv">5</span>, <span class="dv">4</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">6</span>, <span class="dv">2</span>, <span class="dv">4</span>]</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>plot_histogram(x, is_discrete<span class="op">=</span><span class="va">True</span>, title<span class="op">=</span><span class="st">'36 Die Rolls'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="probability_files/figure-html/cell-10-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Given the fact that I only rolled <span class="math inline">\(36\)</span> times, this histogram looks very uniform, giving a pretty strong hint that each value has an equal probability of being rolled. Since most bars have height <span class="math inline">\(6\)</span>, they correspond to probabilities of <span class="math inline">\(\frac{6}{36}=\frac{1}{6}\)</span>, which is what our common sense expected. Note the counts can fluctuate in this case in a range of about <span class="math inline">\(6 \pm 2\)</span>. This is an example of a <em>fair die</em>.</p>
<p>What if our common sense was incorrect? What if I rolled the die a bunch of times and found out some numbers occurred a lot more often than others? This would happen if the die were weighted unevenly, or <em>loaded</em>. In this case we’re left to assign some <em>weight</em> <span class="math inline">\(N\)</span> to each number <span class="math inline">\(k\)</span>.</p>
<p>To determine what the right weights should be empirically, probably the easiest way would again be to roll the die a bunch of times and count how many times each value <span class="math inline">\(k\)</span> occurs. Those counts will be your weights <span class="math inline">\(N_k\)</span>. These are just the heights of each bin in the histogram. To turn them into probabilities <span class="math inline">\(p_k\)</span>, divide by the total number of rolls, call it <span class="math inline">\(N\)</span>. The probabilities would then be given approximately by</p>
<p><span class="math display">\[p_k = \mathbb{Pr}(x=k) \approx \frac{N_k}{N}.\]</span></p>
<p>That is, the probability <span class="math inline">\(p_k\)</span> is just a ratio of counts, the fraction of times <span class="math inline">\(x=k\)</span> occurred in <span class="math inline">\(N\)</span> counts. As <span class="math inline">\(N \rightarrow \infty\)</span> this equality goes from approximate to exact. In fact, we could <em>define</em> the probability <span class="math inline">\(p_k = \mathbb{Pr}(x=k)\)</span> as the limit</p>
<p><span class="math display">\[p_k = \mathbb{Pr}(x=k) = \lim_{N \rightarrow \infty} \frac{N_k}{N}.\]</span></p>
<p>This is an alternate way of defining a probability, different from the “degree of belief” approach I used above. This is usually called the <strong>frequentist</strong> or objective approach. In this approach, probability is the frequency of the number of times an outcome occurs in an experiment, i.e.&nbsp;<span class="math inline">\(\frac{N_k}{N}\)</span>. In contrast, the “degree of belief” perspective is called the <strong>Bayesian</strong> or subjective approach. Both approaches have their uses, so we’ll go back and forth between the two as it suits us.</p>
<p>To test if your die is loaded, what you can do is roll the die <span class="math inline">\(N\)</span> trials and calculate the probabilities. If they’re all roughly equal to <span class="math inline">\(1/6\)</span> like the example above then the die is fair. Otherwise it’s loaded. Suppose when I’d rolled the die I’d instead gotten the following outcomes:</p>
<table class="table">
<colgroup>
<col style="width: 4%">
<col style="width: 2%">
<col style="width: 2%">
<col style="width: 2%">
<col style="width: 2%">
<col style="width: 2%">
<col style="width: 2%">
<col style="width: 2%">
<col style="width: 2%">
<col style="width: 2%">
<col style="width: 2%">
<col style="width: 2%">
<col style="width: 2%">
<col style="width: 2%">
<col style="width: 2%">
<col style="width: 2%">
<col style="width: 2%">
<col style="width: 2%">
<col style="width: 2%">
<col style="width: 2%">
<col style="width: 2%">
<col style="width: 2%">
<col style="width: 2%">
<col style="width: 2%">
<col style="width: 2%">
<col style="width: 2%">
<col style="width: 2%">
<col style="width: 2%">
<col style="width: 2%">
<col style="width: 2%">
<col style="width: 2%">
<col style="width: 2%">
<col style="width: 2%">
<col style="width: 2%">
<col style="width: 2%">
<col style="width: 2%">
<col style="width: 2%">
</colgroup>
<thead>
<tr class="header">
<th>Roll</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
<th>7</th>
<th>8</th>
<th>9</th>
<th>10</th>
<th>11</th>
<th>12</th>
<th>13</th>
<th>14</th>
<th>15</th>
<th>16</th>
<th>17</th>
<th>18</th>
<th>19</th>
<th>20</th>
<th>21</th>
<th>22</th>
<th>23</th>
<th>24</th>
<th>25</th>
<th>26</th>
<th>27</th>
<th>28</th>
<th>29</th>
<th>30</th>
<th>31</th>
<th>32</th>
<th>33</th>
<th>34</th>
<th>35</th>
<th>36</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Value</strong></td>
<td>4</td>
<td>4</td>
<td>5</td>
<td>4</td>
<td>3</td>
<td>5</td>
<td>3</td>
<td>6</td>
<td>5</td>
<td>6</td>
<td>1</td>
<td>5</td>
<td>4</td>
<td>5</td>
<td>6</td>
<td>5</td>
<td>1</td>
<td>6</td>
<td>5</td>
<td>6</td>
<td>3</td>
<td>5</td>
<td>5</td>
<td>4</td>
<td>3</td>
<td>6</td>
<td>6</td>
<td>4</td>
<td>5</td>
<td>4</td>
<td>2</td>
<td>5</td>
<td>4</td>
<td>6</td>
<td>2</td>
<td>4</td>
</tr>
</tbody>
</table>
<p>Let’s plot the histogram of these outcomes and compare to the fair die case.</p>
<div class="cell" data-execution_count="10">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> [<span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">4</span>, <span class="dv">3</span>, <span class="dv">5</span>, <span class="dv">3</span>, <span class="dv">6</span>, <span class="dv">5</span>, <span class="dv">6</span>, <span class="dv">1</span>, <span class="dv">5</span>, <span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">6</span>, <span class="dv">5</span>, <span class="dv">1</span>, <span class="dv">6</span>, <span class="dv">5</span>, <span class="dv">6</span>, <span class="dv">3</span>, <span class="dv">5</span>, <span class="dv">5</span>, <span class="dv">4</span>, <span class="dv">3</span>, <span class="dv">6</span>, <span class="dv">6</span>, <span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">4</span>, <span class="dv">2</span>, <span class="dv">5</span>, <span class="dv">4</span>, <span class="dv">6</span>, <span class="dv">2</span>, <span class="dv">4</span>]</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>plot_histogram(x, is_discrete<span class="op">=</span><span class="va">True</span>, title<span class="op">=</span><span class="st">'36 Die Rolls (Round 2)'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="probability_files/figure-html/cell-11-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Notice how now the outcomes are skewed towards higher values. This clearly doesn’t look uniform anymore since most of the counts aren’t in the expected range of <span class="math inline">\(6 \pm 2\)</span>. The die has been “loaded to roll high”.</p>
<p>Using the experimental approach we can estimate what the probability of rolling each value is. To do that, we can just take each value <span class="math inline">\(k\)</span> and sum up the number of times <span class="math inline">\(x=k\)</span> and divide it by the total counts <span class="math inline">\(N\)</span>. This will return an array of probabilities, where each index <span class="math inline">\(k\)</span> contains the entry <span class="math inline">\(p_{k+1} = \frac{N_k}{N}\)</span>.</p>
<div class="cell" data-execution_count="11">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>support <span class="op">=</span> np.unique(x)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> <span class="bu">len</span>(x)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>Nk <span class="op">=</span> [<span class="bu">sum</span>([x <span class="op">==</span> k]) <span class="cf">for</span> k <span class="kw">in</span> support]</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> Nk <span class="op">/</span> N</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>[<span class="ss">f"Pr(x=</span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">) = </span><span class="sc">{</span><span class="bu">round</span>(p[i], <span class="dv">3</span>)<span class="sc">}</span><span class="ss">"</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(p))]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="11">
<pre><code>['Pr(x=1) = 0.056',
 'Pr(x=2) = 0.056',
 'Pr(x=3) = 0.111',
 'Pr(x=4) = 0.25',
 'Pr(x=5) = 0.306',
 'Pr(x=6) = 0.222']</code></pre>
</div>
</div>
</section>
<section id="general-case" class="level3" data-number="11.2.2">
<h3 data-number="11.2.2" class="anchored" data-anchor-id="general-case"><span class="header-section-number">11.2.2</span> General Case</h3>
<p>Of course, there’s nothing special about a die. We can define probabilities in exactly the same way for any discrete random variable. A random variable <span class="math inline">\(x\)</span> is called <strong>discrete</strong> if it can take on one of <span class="math inline">\(n\)</span> countable values <span class="math inline">\(x_0,x_1,\cdots,x_{n-1}\)</span>. Suppose we run an experiment <span class="math inline">\(n\)</span> times and observe the outcomes of <span class="math inline">\(x\)</span> at each trial. If <span class="math inline">\(x=x_k\)</span> for some number of counts <span class="math inline">\(n_j\)</span>, then the probability <span class="math inline">\(x=x_k\)</span> is given by the limit of running the experiment infinitely many times,</p>
<p><span class="math display">\[p_k = \mathbb{Pr}(x=k) = \lim_{N \rightarrow \infty} \frac{N_k}{N}.\]</span></p>
<p>The set of values that <span class="math inline">\(x\)</span> can take on are called the <strong>support</strong> of the random variable. For values outside the support, it’s assumed the probability is zero. As will always be true with probabilities, it’s still the case that each probability must be non-negative, and they must all sum to one,</p>
<p><span class="math display">\[p_k \geq 0, \quad \sum_{k=0}^{n-1} p_k = 1.\]</span></p>
<p>While we have an experimental way to calculate probabilities now, it would be useful to define probabilities as functions of random variables so we can study them mathematically. These functions are called <strong>probability distributions</strong>. Suppose the probabilities <span class="math inline">\(p_k\)</span> are given by some function <span class="math inline">\(p(x)\)</span> mapping outcomes to probabilities. When this is true, we say <span class="math inline">\(x\)</span> is distributed as <span class="math inline">\(p(x)\)</span>, written in short-hand as <span class="math inline">\(x \sim p(x)\)</span>. If <span class="math inline">\(x\)</span> is discrete, we call the function <span class="math inline">\(p(x)\)</span> a <strong>probability mass function</strong>, or <strong>PMF</strong> for short.</p>
<p>In the simple case of the fair die, since each <span class="math inline">\(p_k = \frac{1}{6}\)</span>, its PMF is just the simple constant function <span class="math inline">\(p(x) = \frac{1}{6}\)</span>. This distribution is an example of the <strong>discrete uniform distribution</strong>. If <span class="math inline">\(x\)</span> is a discrete random variable taking on one of <span class="math inline">\(k\)</span> outcomes, and <span class="math inline">\(x\)</span> is distributed as discrete uniform, then its probabilities are given by <span class="math inline">\(p_k = \frac{1}{n}\)</span> for all <span class="math inline">\(k\)</span>. In histogram language, all bins have approximately the same number of counts.</p>
<p>In the less simple case of the loaded die we had to estimate each probability empirically. Supposing we could calculate those probabilities exactly, the PMF for that particular loaded die would look like</p>
<p><span class="math display">\[
p(x) =
\begin{cases}
0.056, &amp; x = 1, \\
0.056, &amp; x = 2, \\
0.111, &amp; x = 3, \\
0.250, &amp; x = 4, \\
0.306, &amp; x = 5, \\
0.220, &amp; x = 6.
\end{cases}
\]</span></p>
<p>This is an example of a <strong>categorical distribution</strong>. Their histograms can look completely arbitrary. Each bin can contain as many counts as it likes. All that matters is that <span class="math inline">\(k\)</span> is finite and all the probabilities sum to one. Any time you take a discrete uniform random variable and weigh the outcomes (e.g.&nbsp;by loading a die) you’ll create a categorical distribution.</p>
<p>Typically each distribution will have one or more parameters <span class="math inline">\(\theta\)</span> that can be adjusted to change the shape or support of the distribution. Instead of writing <span class="math inline">\(p(x)\)</span> for the PMF, when we want to be explicit about the parameters we’ll sometimes write <span class="math inline">\(p(x; \theta)\)</span>. The semi-colon is used to say that any arguments listed after it are understood to be parameters, not function inputs. In this notation, parameters of a distribution are assumed to be known, non-random values. We’ll relax this requirement below, but assume parameters are non-random for now.</p>
<p>For example, the discrete uniform distribution has two parameters indicating the lowest and highest values in the support, called <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>. We could thus express its PMF as <span class="math inline">\(p(x;a,b)\)</span>, which means “the probability of <span class="math inline">\(x\)</span> given <em>known</em> parameters <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>”.</p>
<p>Using these parameters, it’s also common to use special symbols as a short-hand for common distributions. For example, the discrete uniform distribution with parameters <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> is often shortened to something like <span class="math inline">\(DU(a,b)\)</span>. If we want to say <span class="math inline">\(x\)</span> is a discrete uniform random variable, we’d write <span class="math inline">\(x \sim DU(a,b)\)</span>. You’ll also sometimes see people use the symbol to write the PMF as well, for example <span class="math inline">\(DU(x;a,b)\)</span>.</p>
</section>
<section id="discrete-distributions" class="level3" data-number="11.2.3">
<h3 data-number="11.2.3" class="anchored" data-anchor-id="discrete-distributions"><span class="header-section-number">11.2.3</span> Discrete Distributions</h3>
<p>Some discrete probability distributions occur so frequently that they get a special name. Each one tends to occur when modeling certain kinds of phenomena. Here are a few of the most common discrete distributions. I’ll just state them and summarize their properties for future reference.</p>
<section id="discrete-uniform-distribution" class="level4" data-number="11.2.3.1">
<h4 data-number="11.2.3.1" class="anchored" data-anchor-id="discrete-uniform-distribution"><span class="header-section-number">11.2.3.1</span> Discrete Uniform Distribution</h4>
<ul>
<li>Symbol: <span class="math inline">\(DU(a,b)\)</span></li>
<li>Parameters: Integers <span class="math inline">\(a, b\)</span>, where <span class="math inline">\(a\)</span> is the minimum and <span class="math inline">\(b-1\)</span> is the maximum value in the support</li>
<li>Support: <span class="math inline">\(x=a,a+1,\cdots,b-1\)</span></li>
<li>Probability mass function: <span class="math display">\[p(x; a,b) = \frac{1}{b-a}, \ \text{ for } x = a, a+1, \cdots, b-1.\]</span></li>
<li>Cumulative distribution function: <span class="math display">\[
P(x; a,b) =
\begin{cases}
0 &amp; x &lt; a, \\
\frac{\text{int}(x) - a}{b-a}, &amp; a \leq x \leq b, \\
1 &amp; x \geq 1.
\end{cases}
\]</span></li>
<li>Random number generator: <code>np.random.randint(a, b)</code></li>
<li>Notes:
<ul>
<li>Used to model discrete processes that occur with equal weight, or are suspected to (the principle of indifference)</li>
<li>Example: The fair die, taking <span class="math inline">\(a=1, b=7\)</span> gives <span class="math inline">\(x \sim D(1,7)\)</span> with <span class="math inline">\(p(x) = \frac{1}{7-1} = \frac{1}{6}\)</span></li>
</ul></li>
</ul>
<div class="cell" data-execution_count="12">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> <span class="dv">7</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.random.randint(a, b, size<span class="op">=</span><span class="dv">100000</span>)</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>plot_histogram(x, is_discrete<span class="op">=</span><span class="va">True</span>, stat<span class="op">=</span><span class="st">'probability'</span>, title<span class="op">=</span><span class="ss">f'$DU(</span><span class="sc">{</span>a<span class="sc">}</span><span class="ss">,</span><span class="sc">{</span>b<span class="sc">}</span><span class="ss">)$ PMF'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="probability_files/figure-html/cell-13-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="bernoulli-distribution" class="level4" data-number="11.2.3.2">
<h4 data-number="11.2.3.2" class="anchored" data-anchor-id="bernoulli-distribution"><span class="header-section-number">11.2.3.2</span> Bernoulli Distribution</h4>
<ul>
<li>Symbol: <span class="math inline">\(\text{Ber}(\text{p})\)</span></li>
<li>Parameters: The probability of success <span class="math inline">\(0 \leq \text{p} \leq 1\)</span></li>
<li>Support: <span class="math inline">\(x=0,1\)</span></li>
<li>Probability mass function: <span class="math display">\[
p(x; \text{p}) = \text{p}^x (1-\text{p})^{1-x} =
\begin{cases}
1-\text{p} &amp; x = 0, \\
\text{p} &amp; x = 1.
\end{cases}
\]</span></li>
<li>Cumulative distribution function: <span class="math display">\[
P(x; \text{p}) =
\begin{cases}
0 &amp; \text{if } x &lt; 0 \\
1-p &amp; \text{if } 0 \leq x &lt; 1 \\
1 &amp; \text{if } x \geq 1.
\end{cases}.
\]</span></li>
<li>Random number generator: <code>np.random.choice([0, 1], p=[1 - p, p])</code></li>
<li>Notes:
<ul>
<li>Used to model binary processes where the probability of success can be estimated</li>
<li>Example: Flipping a fair coin, where <span class="math inline">\(\text{tails} = 0\)</span>, <span class="math inline">\(\text{heads} = 1\)</span>, and <span class="math inline">\(\text{p}=\frac{1}{2}\)</span></li>
<li>Used for binary classification. Given an input <span class="math inline">\(\mathbf{x}\)</span> with some binary output <span class="math inline">\(y=0,1\)</span>. If <span class="math inline">\(\text{p}=\hat y\)</span>, then <span class="math inline">\(y \sim \text{Ber}(\hat y)\)</span>.</li>
<li>Special case of the binomial distribution where <span class="math inline">\(n=1\)</span>: <span class="math inline">\(\text{Ber}(\text{p}) = \text{Bin}(1, \text{p})\)</span>.</li>
</ul></li>
</ul>
<div class="cell" data-execution_count="13">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> <span class="fl">0.7</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.random.choice([<span class="dv">0</span>, <span class="dv">1</span>], p<span class="op">=</span>[<span class="dv">1</span> <span class="op">-</span> p, p], size<span class="op">=</span><span class="dv">1000</span>)</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>plot_histogram(x, is_discrete<span class="op">=</span><span class="va">True</span>, stat<span class="op">=</span><span class="st">'probability'</span>, title<span class="op">=</span><span class="ss">f'$Ber(</span><span class="sc">{</span>p<span class="sc">}</span><span class="ss">)$ PMF'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="probability_files/figure-html/cell-14-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="categorical-distribution" class="level4" data-number="11.2.3.3">
<h4 data-number="11.2.3.3" class="anchored" data-anchor-id="categorical-distribution"><span class="header-section-number">11.2.3.3</span> Categorical Distribution</h4>
<ul>
<li><p>Symbol: <span class="math inline">\(\text{Cat}(p_0,p_1,\cdots,p_{k-1})\)</span> or <span class="math inline">\(\text{Cat}(\mathbf{p})\)</span></p></li>
<li><p>Parameters: <span class="math inline">\(k\)</span> non-negative real numbers <span class="math inline">\(p_j\)</span> that sum to one, each representing the probability of getting <span class="math inline">\(x_j\)</span></p>
<ul>
<li>Commonly written as a vector <span class="math inline">\(\mathbf{p} = (p_0,p_1,\cdots,p_{k-1})\)</span></li>
</ul></li>
<li><p>Support: <span class="math inline">\(x = 0, 1, \cdots, k-1\)</span></p></li>
<li><p>Probability mass function: <span class="math display">\[
p(x; \mathbf{p}) = \begin{cases}
p_0 &amp; x = 0, \\
p_1 &amp; x = 1, \\
\vdots &amp; \vdots \\
p_{k-1} &amp; x = k-1.
\end{cases}
\]</span></p></li>
<li><p>Cumulative distribution function: <span class="math display">\[
P(x; \mathbf{p}) =
\begin{cases}
0 &amp; \text{if } x \leq x_0 \\
p_0 &amp; \text{if } x_0 \leq x \leq x_1 \\
p_0 + p_1 &amp; \text{if } x_1 \leq x \leq x_2 \\
p_0 + p_1 + p_2 &amp; \text{if } x_2 \leq x \leq x_3 \\
\vdots &amp; \vdots \\
1 &amp; \text{if } x \geq x_{n-1}.
\end{cases}
\]</span></p></li>
<li><p>Random number generator: <code>np.random.choice(np.arange(k), p=p)</code></p></li>
<li><p>Notes:</p>
<ul>
<li>Used to model categorical processes where a finite number of classes can occur with arbitrary probabilities</li>
<li>Used for multiclass classification. Given an input <span class="math inline">\(\mathbf{x}\)</span> with outputs in one of <span class="math inline">\(k\)</span> classes <span class="math inline">\(y=0,1,\cdots,k-1\)</span>. If <span class="math inline">\(\mathbf{p}=\mathbf{\hat y}\)</span>, then <span class="math inline">\(\mathbf{y} \sim \text{Cat}(\mathbf{\hat y})\)</span>.</li>
<li>Generalization of the Bernoulli distribution, allowing for <span class="math inline">\(k\)</span> distinct outcomes instead of just <span class="math inline">\(2\)</span>.</li>
<li>Models the values rolled from a die when <span class="math inline">\(k=6\)</span>.</li>
</ul></li>
</ul>
<div class="cell" data-execution_count="14">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> [<span class="fl">0.2</span>, <span class="fl">0.5</span>, <span class="fl">0.3</span>]</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.random.choice(np.arange(<span class="bu">len</span>(p)), p<span class="op">=</span>p, size<span class="op">=</span><span class="dv">1000</span>)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>plot_histogram(x, is_discrete<span class="op">=</span><span class="va">True</span>, stat<span class="op">=</span><span class="st">'probability'</span>, title<span class="op">=</span><span class="ss">f'$Cat</span><span class="sc">{</span><span class="bu">tuple</span>(p)<span class="sc">}</span><span class="ss">$ PMF'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="probability_files/figure-html/cell-15-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="binomial-distribution" class="level4" data-number="11.2.3.4">
<h4 data-number="11.2.3.4" class="anchored" data-anchor-id="binomial-distribution"><span class="header-section-number">11.2.3.4</span> Binomial Distribution</h4>
<ul>
<li>Symbol: <span class="math inline">\(\text{Bin}(n, \text{p})\)</span></li>
<li>Parameters: The number of trials <span class="math inline">\(n=1,2,3,\cdots\)</span> and probability <span class="math inline">\(0 \leq \text{p} \leq 1\)</span> of success of each trial</li>
<li>Support: <span class="math inline">\(x = 0, 1, \cdots, n\)</span></li>
<li>Probability mass function: <span class="math display">\[p(x; n,\text{p}) = \binom{n}{x} \text{p}^{x} (1-\text{p})^{n-x}, \ \text{for} \ x=0,1,\cdots,n, \ \text{where} \ \binom{n}{x} = \frac{n!}{x!(n-x)!}.\]</span></li>
<li>Cumulative distribution function: <span class="math display">\[P(x; n,\text{p}) = \sum_{k=0}^{\text{int}(x)} {n \choose k} p^k(1-p)^{n-k}.\]</span></li>
<li>Random number generator: <code>np.random.binomial(n, p)</code></li>
<li>Notes:
<ul>
<li>Used to model the number of successes from <span class="math inline">\(n\)</span> independent binary processes (analogous to coin flips)</li>
<li>Example: Flipping a fair coin <span class="math inline">\(n\)</span> times and counting the number of heads</li>
<li>Generalization of the Bernoulli distribution. The sum of <span class="math inline">\(n\)</span> independent Bernoulli variables is <span class="math inline">\(\text{Bin}(n, \text{p})\)</span>.</li>
<li>The number of counts in each bin of a histogram of independent samples can be modeled as a binomial random variable</li>
</ul></li>
</ul>
<div class="cell" data-execution_count="15">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> <span class="fl">0.7</span></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.random.binomial(n, p, size<span class="op">=</span><span class="dv">1000</span>)</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>plot_histogram(x, is_discrete<span class="op">=</span><span class="va">True</span>, stat<span class="op">=</span><span class="st">'probability'</span>, title<span class="op">=</span><span class="ss">f'$Bin</span><span class="sc">{</span>(n,p)<span class="sc">}</span><span class="ss">$ PMF'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="probability_files/figure-html/cell-16-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="poisson-distribution" class="level4" data-number="11.2.3.5">
<h4 data-number="11.2.3.5" class="anchored" data-anchor-id="poisson-distribution"><span class="header-section-number">11.2.3.5</span> Poisson Distribution</h4>
<ul>
<li>Symbol: <span class="math inline">\(\text{Poisson}(\lambda)\)</span></li>
<li>Parameters: A rate parameter <span class="math inline">\(\lambda \geq 0\)</span></li>
<li>Support: <span class="math inline">\(x = 0, 1, 2, 3, \cdots\)</span></li>
<li>Probability mass function: <span class="math display">\[p(x; \lambda) = \frac{\lambda^x e^{-\lambda}}{x!}, \quad \text{for} \ x=0,1,2,3,\cdots.\]</span></li>
<li>Cumulative distribution function: <span class="math display">\[P(x; \lambda) = e^{-\lambda}\sum_{k=0}^{\text{int}(x)}\frac{\lambda^k}{k!}.\]</span></li>
<li>Random number generator: <code>np.random.poisson(lambda)</code></li>
<li>Notes:
<ul>
<li>Used to model counting processes, like the number of calls coming into a call center, or the number of times a Geiger counter registers a click</li>
<li>Example: The number of people walking through the door of a coffee shop per hour can be modeled as a Poisson distribution</li>
</ul></li>
</ul>
<div class="cell" data-execution_count="16">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>lambda_ <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.random.poisson(lambda_, size<span class="op">=</span><span class="dv">1000</span>)</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>plot_histogram(x, is_discrete<span class="op">=</span><span class="va">True</span>, stat<span class="op">=</span><span class="st">'probability'</span>, title<span class="op">=</span><span class="ss">f'$Poisson(</span><span class="sc">{</span>lambda_<span class="sc">}</span><span class="ss">)$ PMF'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="probability_files/figure-html/cell-17-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
</section>
<section id="probabilities-of-multiple-outcomes" class="level3" data-number="11.2.4">
<h3 data-number="11.2.4" class="anchored" data-anchor-id="probabilities-of-multiple-outcomes"><span class="header-section-number">11.2.4</span> Probabilities of Multiple Outcomes</h3>
<p>We’ve seen how to calculate the probabilities of any one outcome. The probability that <span class="math inline">\(x=k\)</span> is given by <span class="math inline">\(\text{Pr}(x=k) = p(k)\)</span>, where <span class="math inline">\(p(k)\)</span> is the PMF. It’s natural to then ask how we can think about probabilities of multiple outcomes. For example, consider again the situation of rolling a fair die. Suppose we were interested in knowing what the probability was of rolling an even number, i.e.&nbsp;<span class="math inline">\(x=2,4,6\)</span>. How would we approach this? Your intuition suggests the right idea. We can just sum the probabilities of each outcome together,</p>
<p><span class="math display">\[\mathbb{Pr}(x\text{ is even}) = \mathbb{Pr}(x=2,4,6) = p(2) + p(4) + p(6) = \frac{1}{6} + \frac{1}{6} + \frac{1}{6} = \frac{1}{2}.\]</span></p>
<p>This same idea extends to any discrete set. Suppose we’re interested in the probability that some discrete random variable <span class="math inline">\(x\)</span> takes on values in some set <span class="math inline">\(E = \{x_0, x_1, \cdots, x_{m-1}\}\)</span>. Then all we need to do is some over the probabilities of all the outcomes in <span class="math inline">\(E\)</span>, i.e.</p>
<p><span class="math display">\[\mathbb{Pr}(x \in E) = \sum_{k \in E} p(k) = \sum_{i=0}^{m-1} p(x_i) = p(x_0) + p(x_1) + \cdots p(x_{m-1}).\]</span></p>
<p>When the set of interest is the entire support of <span class="math inline">\(x\)</span>, the right-hand side is just the sum the probability of all possible outcome, which is just one. Thus, we’ll always have <span class="math inline">\(0 \leq \mathbb{Pr}(x \in E) \leq 1\)</span> for any set <span class="math inline">\(E\)</span>.</p>
<p>Though we don’t really have to for discrete variables, it’s conventional to define another function <span class="math inline">\(P(x)\)</span> called the <strong>cumulative distribution function</strong>, or <strong>CDF</strong>. It’s the probability <span class="math inline">\(x \in (-\infty, x_0]\)</span> for some fixed value <span class="math inline">\(x_0 \in \mathbb{R}\)</span>,</p>
<p><span class="math display">\[P(x_0) = \mathbb{Pr}(x \leq x_0) = \sum_{k \leq x_0} p(k) = \sum_{k=-\infty}^{\text{int}(x_0)} p(k),\]</span></p>
<p>where it’s understood that <span class="math inline">\(p(k)=0\)</span> whenever <span class="math inline">\(k\)</span> isn’t in the support of <span class="math inline">\(x\)</span>. Note the CDF is a real-valued function. We can ask about <span class="math inline">\(P(x_0)\)</span> for <em>any</em> <span class="math inline">\(x_0 \in \mathbb{R}\)</span>, not just discrete values of <span class="math inline">\(x_0\)</span>.</p>
<p>But why should we care? It turns out if we know the CDF in some simple form, we can use it to calculate the probability <span class="math inline">\(x\)</span> is in any other interval by differencing the CDF at the endpoints. Suppose we’re interested in the probability <span class="math inline">\(a \leq x \leq b\)</span>. If we know the CDF for a particular distribution in some simple form, we can just difference it to get the probability of being in the interval, i.e.</p>
<p><span class="math display">\[\mathbb{Pr}(a \leq x \leq b) = \mathbb{Pr}(x \leq b) - \mathbb{Pr}(x \leq a) = P(b) - P(a).\]</span></p>
<p>This fact is more useful for continuous distributions than discrete ones, since in the discrete case we can always just sum over the values, which is usually pretty quick to do.</p>
<section id="application-getting-a-job" class="level4" data-number="11.2.4.1">
<h4 data-number="11.2.4.1" class="anchored" data-anchor-id="application-getting-a-job"><span class="header-section-number">11.2.4.1</span> Application: Getting a Job</h4>
<p>Here’s a useful application where probabilities of multiple outcomes can sometimes come in handy. Suppose you’re applying to a bunch of jobs, and you want to know what is the probability that you’ll get <em>at least one</em> offer. Suppose you’ve applied to <span class="math inline">\(n\)</span> jobs. For simplicity, assume each job has roughly the same probability <span class="math inline">\(\text{p}\)</span> of giving you an offer. Then each job application looks kind of like the situation of flipping a coin. If <span class="math inline">\(x_i=1\)</span> you get an offer, if <span class="math inline">\(x_i=0\)</span> you get rejected. We can thus think of each job application as a Bernoulli random variable <span class="math inline">\(x_i \sim \text{Ber}(\text{p})\)</span>.</p>
<p>Now, assume that the job applications are all independent of each other, so one company’s decision whether to give you an offer doesn’t affect another company’s decision to give you an offer. This isn’t perfectly true, but it’s reasonably true. In this scenario, the <em>total</em> number of offers <span class="math inline">\(x\)</span> you get out of <span class="math inline">\(n\)</span> job applications will then be binomially distributed, <span class="math inline">\(x \sim \text{Bin}(n, \text{p})\)</span>.</p>
<p>We can use this fact to answer the question we started out with: What is the probability that you receive at least one offer? It’s equivalent to asking, if <span class="math inline">\(x\)</span> is binomial, what is the probability that <span class="math inline">\(x \geq 1\)</span>? Now, since <span class="math inline">\(x\)</span> is only supported on non-negative values, we have</p>
<p><span class="math display">\[\begin{align*}
\mathbb{Pr}(x \geq 1) &amp;= \mathbb{Pr}(x \geq 0) - \mathbb{Pr}(x=0) \\
&amp;= 1 - \mathbb{Pr}(x=0) \\
&amp;= 1 - p(0;n,\text{p}) \\
&amp;= 1 - \binom{n}{0} \text{p}^0 (1-\text{p})^{n-0} \\
&amp;= 1 - \frac{n!}{0!(n-0)!} (1-\text{p})^n \\
&amp;= 1 - (1-\text{p})^n.
\end{align*}\]</span></p>
<p>We thus have a formula. The probability of receiving at least one job offer from applying to <span class="math inline">\(n\)</span> jobs, assuming each gives an offer with probability <span class="math inline">\(\text{p}\)</span>, and applications are independent of each other, is</p>
<p><span class="math display">\[\mathbb{Pr}(\text{at least one offer}) = 1 - (1-\text{p})^n.\]</span></p>
<p>Here’s an example of how this formula can be useful. Suppose you believe you have a 10% chance of getting an offer from any one company you apply to, so <span class="math inline">\(\text{p}=0.1\)</span>. If you apply to <span class="math inline">\(n=10\)</span> jobs, you’ll have about a 34.86% chance of receiving at least one offer.</p>
<div class="cell" data-execution_count="17">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>prob_offer <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> (<span class="dv">1</span> <span class="op">-</span> p) <span class="op">**</span> n</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>prob_offer</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="17">
<pre><code>0.6513215599</code></pre>
</div>
</div>
<p>Let’s now ask how many jobs you’d have to apply to to give yourself at least a 90% chance of getting at least one job offer? Here’s what you can do. Let <span class="math inline">\(O = \mathbb{Pr}(\text{at least one offer})\)</span>, so <span class="math inline">\(O = (1-p)^n\)</span>. Set <span class="math inline">\(O=0.9\)</span> and solve for <span class="math inline">\(n\)</span>. Then you’d have</p>
<p><span class="math display">\[\begin{align*}
O &amp;= 1 - (1-p)^n \\
(1-p)^n &amp;= 1 - O \\
n \log(1-p) &amp;= \log(1 - O) \\
n &amp;= \frac{\log(1 - O)}{\log(1 - p)}.
\end{align*}\]</span></p>
<p>Plugging in <span class="math inline">\(p=0.1\)</span> and <span class="math inline">\(O=0.9\)</span> gives <span class="math inline">\(n \approx 21.85\)</span>. Thus, you’d need to apply to at least <span class="math inline">\(n=22\)</span> jobs to have a decent chance of getting at least one offer. Here’s a plot of this idea. Each curve is a plot of <span class="math inline">\(n=n(p)\)</span> for different choices of <span class="math inline">\(O\)</span>, in this case, 50%, 75%, 90%, and 99%.</p>
<div class="cell" data-execution_count="18">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> np.linspace(<span class="fl">0.01</span>, <span class="fl">0.999</span>, <span class="dv">100</span>)</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>O <span class="op">=</span> [<span class="fl">0.5</span>, <span class="fl">0.75</span>, <span class="fl">0.9</span>, <span class="fl">0.99</span>]</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> o <span class="kw">in</span> O:</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> np.log(<span class="dv">1</span> <span class="op">-</span> o) <span class="op">/</span> np.log(<span class="dv">1</span> <span class="op">-</span> p)</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>    plt.plot(p, n, label<span class="op">=</span><span class="ss">f'$O=</span><span class="sc">{</span><span class="bu">round</span>(o<span class="op">*</span><span class="dv">100</span>)<span class="sc">}</span><span class="ss">$%'</span>)</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>plt.xticks(<span class="fl">0.1</span> <span class="op">*</span> np.arange(<span class="dv">11</span>))</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>plt.ylim(<span class="dv">0</span>, <span class="dv">70</span>)</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>plt.title(</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">"""How many jobs would you have to </span></span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a><span class="st">    apply to to get at least one job offer</span></span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a><span class="st">    with confidence $O$?"""</span>.title(), fontsize<span class="op">=</span><span class="dv">11</span>)</span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'$p$'</span>)</span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'$n$'</span>)</span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="probability_files/figure-html/cell-19-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>The moral of this story is that you have two ways to up your chances of getting a job offer: Up your chances of getting any one job (i.e.&nbsp;increase <span class="math inline">\(p\)</span>), or apply to a lot more jobs (i.e.&nbsp;increase <span class="math inline">\(n\)</span>). The more confident you want to be of getting an offer (i.e.&nbsp;<span class="math inline">\(O\)</span>), the more jobs you’ll need to apply to. This same idea can be used to model the probability of at least one occurrence for any binary event similar to this.</p>
</section>
</section>
</section>
<section id="continuous-probability" class="level2" data-number="11.3">
<h2 data-number="11.3" class="anchored" data-anchor-id="continuous-probability"><span class="header-section-number">11.3</span> Continuous Probability</h2>
<p>So far we’ve covered discrete random variables, ones that take on a finite (or countably infinite) set of values. We can also consider random variables that take on a continuous range of values. For example, a continuous random variable <span class="math inline">\(x\)</span> can take on values in the entire interval <span class="math inline">\([0,1]\)</span>, or the whole real line <span class="math inline">\(\mathbb{R} = (-\infty, \infty)\)</span>. The key difference between continuous variables and discrete variables is that we have to think in terms of calculus now. Instead of points we’ll have infinitesimal areas. Instead of sums we’ll have integrals.</p>
<p>It may not be obvious to you that there are practical examples where continuous random variables would be useful. Here are some examples:</p>
<ul>
<li>Modeling the behavior of random number generators like rand.</li>
<li>Modeling the total sales a business will do next quarter.</li>
<li>Modeling the time it takes for a customer to complete a purchase in an online store.</li>
<li>Modeling the amount of fuel consumed by a vehicle on a given day.</li>
<li>Modeling the height of waves in the ocean at a given time.</li>
<li>Modeling the length of a stay in a hospital by a typical patient.</li>
<li>Modeling the amount of rainfall in a specific region over a period of time.</li>
<li>Modeling the measured voltage of a car battery at any point in time.</li>
</ul>
<p>In fact, any continuous variable you can think of could be treated as random depending on the situation. Even if a variable is completely deterministic, there may be situations where it’s helpful to think of it as random. The whole idea of <em>Monte Carlo methods</em> is based on this idea, in fact.</p>
<section id="motivation-rand-again" class="level3" data-number="11.3.1">
<h3 data-number="11.3.1" class="anchored" data-anchor-id="motivation-rand-again"><span class="header-section-number">11.3.1</span> Motivation: Rand Again</h3>
<p>I showed example of a continuous random variable already at the beginning of this lesson, when I introduced the idea of random number generators like rand. Rand is an example of a function that can (approximately) generate samples of a continuous random variable. In particular, it samples uniformly from the interval <span class="math inline">\([0,1]\)</span>. I already showed what its histogram looks like for a large number of samples. Here it is again.</p>
<div class="cell" data-execution_count="19">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.random.rand(<span class="dv">10000</span>)</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>plot_histogram(x, bins<span class="op">=</span><span class="dv">10</span>, title<span class="op">=</span><span class="ss">f'rand(</span><span class="sc">{</span><span class="dv">10000</span><span class="sc">}</span><span class="ss">)'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="probability_files/figure-html/cell-20-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Let’s now try to figure out how we should define the probability of values sampled from rand. In the discrete case, we were able to define probabilities by running an experiment (e.g.&nbsp;rolling a die a bunch of times). We could look at the ratio of the number of times <span class="math inline">\(N_k\)</span> an outcome <span class="math inline">\(k\)</span> occurred over the number of total trials <span class="math inline">\(N\)</span>. This made sense in the discrete case since we could reasonably well rely on each outcome <span class="math inline">\(x=k\)</span> occurring enough times to get a meaningful count.</p>
<p>This approach doesn’t work well for continuous random variables. Suppose <span class="math inline">\(x\)</span> is the random variable resulting from rand, uniform on the interval <span class="math inline">\([0,1]\)</span>. If I sample a single value from rand, there’s no reason to assume I’ll ever see that <em>exact</em> value again. There are uncountably infinitely many values to choose from in <span class="math inline">\([0,1]\)</span>, so I’m pretty much guaranteed to never see the same value twice. Instead of counting how many times each value occurs, what I can do is use the binning trick we saw with histograms. For example, I can divide <span class="math inline">\([0,1]\)</span> up into ten subintervals (or bins)</p>
<p><span class="math display">\[I_0=[0,0.1], \quad I_1=[0.1,0.2], \quad I_3=[0.2,0.3], \quad \cdots, \quad I_9=[0.9,1].\]</span></p>
<p>If I sample one value from rand it’s guaranteed to be in one of these subintervals <span class="math inline">\(I_k\)</span>. If I sample a whole bunch of values from rand, say <span class="math inline">\(N=1000\)</span>, I should expect each <span class="math inline">\(I_k\)</span> to contain about <span class="math inline">\(N_k=100\)</span> counts (10% of the total since there are 10 bins). It thus seems to make perfect sense to define a probability on each <span class="math inline">\(I_k\)</span>,</p>
<p><span class="math display">\[\mathbb{Pr}(x \in I_k) = \frac{N_k}{N} = \frac{100}{1000} = \frac{1}{10} = 0.1.\]</span></p>
<div class="cell" data-execution_count="20">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> <span class="dv">10000</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>M <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>dx <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> M</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.random.rand(N)</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>plot_histogram(x, bins<span class="op">=</span>M, title<span class="op">=</span><span class="ss">f'M=$</span><span class="sc">{</span>M<span class="sc">}</span><span class="ss">$ subintervals of length $dx=</span><span class="sc">{</span>dx<span class="sc">}</span><span class="ss">$'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="probability_files/figure-html/cell-21-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>We still want to approximate the discrete idea of having a probability <span class="math inline">\(\mathbb{Pr}(x=k)\)</span>. How can we do it using this idea of subintervals? Enter calculus. What we can imagine doing is allowing each subinterval <span class="math inline">\(I_k\)</span> to become infinitesimally small. Suppose we subdivide <span class="math inline">\([0,1]\)</span> into <span class="math inline">\(M\)</span> total subintervals each of infinitesimal length <span class="math inline">\(dx\)</span>, satisfying <span class="math inline">\(M=\frac{1}{dx}\)</span>, i.e.</p>
<p><span class="math display">\[I_0=[0,dx], \quad I_1=[dx, 2dx], \quad I_2=[2dx, 3dx], \quad \cdots, \quad I_{M-1}=[(M-1)dx, 1].\]</span></p>
<p>Suppose <span class="math inline">\(x_0\)</span> is some point in one of these tiny intervals <span class="math inline">\(I_k=[kdx, (k+1)dx]\)</span>. Since each <span class="math inline">\(I_k\)</span> is a <em>very</em> tiny interval, the probability that <span class="math inline">\(x \approx x_0\)</span> is pretty much exactly the same thing as the probability that <span class="math inline">\(x \in I_k\)</span>. Let’s thus <em>define</em> the probability that <span class="math inline">\(x \approx x_0\)</span> as the probability that <span class="math inline">\(x \in I_k\)</span>,</p>
<p><span class="math display">\[\mathbb{Pr}(x \approx x_0) = \mathbb{Pr}(x \in I_k) = \lim_{N \rightarrow \infty} \frac{N_k}{N}.\]</span></p>
<p>Here’s an approximate representation of this idea. I won’t be able to make <span class="math inline">\(M=10^{300}\)</span> bins like I’d like, but I can at least make bins so you can see the point. I’ll need to generate a huge number of samples <span class="math inline">\(N\)</span> so the histogram will populate. Notice each <span class="math inline">\(N_k \approx \frac{N}{M} = 1000\)</span>. That is,</p>
<p><span class="math display">\[\mathbb{Pr}(x \approx x_0) \approx \frac{N_k}{N} \approx \frac{N/M}{N} = \frac{1}{M} = dx.\]</span></p>
<p>Evidently, the probability <span class="math inline">\(x \approx x_0\)</span> is infinitesimal, so very very tiny. This is why you’ll basically never sample the same value twice.</p>
<div class="cell" data-execution_count="21">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> <span class="dv">1000000</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>M <span class="op">=</span> N <span class="op">//</span> <span class="dv">1000</span></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>dx <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> M</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.random.rand(N)</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>plot_histogram(x, bins<span class="op">=</span>M, title<span class="op">=</span><span class="ss">f'M=$</span><span class="sc">{</span>M<span class="sc">}</span><span class="ss">$ subintervals of length $dx=</span><span class="sc">{</span>dx<span class="sc">}</span><span class="ss">$'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="probability_files/figure-html/cell-22-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="general-case-1" class="level3" data-number="11.3.2">
<h3 data-number="11.3.2" class="anchored" data-anchor-id="general-case-1"><span class="header-section-number">11.3.2</span> General Case</h3>
<p>The facts I’ve shown about rand extend to more general continuous random variables as well. Suppose <span class="math inline">\(x\)</span> is supported on some interval <span class="math inline">\([a,b]\)</span>. It could even be infinite. Let’s divide this interval up into <span class="math inline">\(M\)</span> tiny sub-intervals of length <span class="math inline">\(dx\)</span>, where <span class="math inline">\(M\)</span> must satisfy <span class="math inline">\(M = \frac{b-a}{dx}\)</span>,</p>
<p><span class="math display">\[I_0=[a,a+dx], \quad I_1=[a+dx, a+2dx], \quad I_2=[a+2dx, a+3dx], \quad \cdots, \quad I_{M-1}=[a+(M-1)dx, b].\]</span></p>
<p>Now, run an experiment <span class="math inline">\(N\)</span> times and count how many times outcomes occur, not for each <span class="math inline">\(x\)</span>, but for each <em>subinterval</em> <span class="math inline">\(I_k=[a+kdx, a+(k+1)dx]\)</span>. If <span class="math inline">\(x_0 \in I_k\)</span>, that is, if <span class="math inline">\(a+kdx \leq x_0 \leq a+(k+1)dx\)</span>, then the probability that <span class="math inline">\(x \approx x_0\)</span> is defined by,</p>
<p><span class="math display">\[\mathbb{Pr}(x \approx x_0) = \mathbb{Pr}(x \in I_k) = \lim_{N \rightarrow \infty} \frac{N_k}{N}.\]</span></p>
<p>Just as with the uniform case before, it’s useful to think of the probability <span class="math inline">\(\mathbb{Pr}(x \approx x_0)\)</span> as explicitly being proportional to the subinterval length <span class="math inline">\(dx\)</span>. In the uniform case it was just <span class="math inline">\(\mathbb{Pr}(x \approx x_0)=dx\)</span> exactly. In the more general case, <span class="math inline">\(\mathbb{Pr}(x \approx x_0)\)</span> may depend on the value of <span class="math inline">\(x_0\)</span>, so we need to weight the right-hand side by some non-negative weighting function <span class="math inline">\(p(x) \geq 0\)</span>, so</p>
<p><span class="math display">\[\mathbb{Pr}(x \approx  x_0) = \mathbb{Pr}(x \in I_k) = p(x_0)dx.\]</span></p>
<p>This weighting function <span class="math inline">\(p(x)\)</span> is called the <strong>probability density function</strong>, or <strong>PDF</strong> for short. It’s the continuous analogue of the probability mass function from the discrete case (hence why I use the same notation). Unlike the discrete PMF, the PDF is <em>not</em> a probability all by itself. It’s a probability per infinitesimal unit <span class="math inline">\(dx\)</span>. That is, it’s a <em>density</em>. For this reason, the PDF need not sum to one. It only needs to be non-negative, i.e.&nbsp;all outputs <span class="math inline">\(p(x_0)\)</span> should lie on or above the x-axis, never below it. But any one output <span class="math inline">\(p(x_0)\)</span> can be arbitrarily large, even <span class="math inline">\(\infty\)</span>!</p>
<p>What <em>must</em> be true is that all probabilities sum to one. Since each <span class="math inline">\(\mathbb{Pr}(x \approx x_0)\)</span> is infinitesimal now, this means all probablities must <em>integrate</em> to one over the support of <span class="math inline">\(x\)</span>. If <span class="math inline">\(x\)</span> is supported on <span class="math inline">\([a,b]\)</span>, then</p>
<p><span class="math display">\[\mathbb{Pr}(a \leq x \leq b) = \sum_{k=0}^{M-1} \mathbb{Pr}(x \in I_k) = \int_a^b p(x)dx = 1.\]</span></p>
<p>This means we can think of a PDF as being any non-negative function that integrates to one. In fact, <em>any</em> function that satisfies this property is a valid PDF for <em>some</em> continuous random variable.</p>
<p>Specifying the functional form of the PDF <span class="math inline">\(p(x)\)</span> creates a <strong>continuous probability distribution</strong>. By specifying <span class="math inline">\(p(x)\)</span>, we’ve uniquely specified what the probabilities have to be for the variable <span class="math inline">\(x\)</span>. In the next section I’ll define some of the most common continuous distributions.</p>
<p>Just as with discrete probabilities, we can get the probability that <span class="math inline">\(x\)</span> is in any set by summing over all the values in that set. The only difference is we replace the sum with an integral over the set. For example, the probability that <span class="math inline">\(c \leq x \leq d\)</span> is given by</p>
<p><span class="math display">\[\mathbb{Pr}(c \leq x \leq d) = \int_c^d p(x)dx.\]</span></p>
<p>We can also define a cumulative distribution function <span class="math inline">\(P(x)\)</span> for continuous probabilities in exactly the same way, except again replacing sums with integrals,</p>
<p><span class="math display">\[P(x_0) = \mathbb{Pr}(x \leq x_0) = \int_{-\infty}^{x_0} p(x')dx',\]</span></p>
<p>where it’s understood that <span class="math inline">\(p(x')=0\)</span> whenever <span class="math inline">\(x'\)</span> is outside the support of <span class="math inline">\(x\)</span>.</p>
<p>If we can obtain the CDF for a distribution, we can calculate the probability <span class="math inline">\(x\)</span> is in any set without having to evaluate an integral. For example, if the set is again the interval <span class="math inline">\([c,d]\)</span>, then</p>
<p><span class="math display">\[\mathbb{Pr}(c \leq x \leq d) = P(d) - P(a).\]</span></p>
<p>This is just a restatement of the rule for definite integrals from the calculus lesson, if <span class="math inline">\(f(x)=\frac{d}{dx}F(x)\)</span>, then</p>
<p><span class="math display">\[\int_c^d f(x) dx = F(d) - F(c).\]</span></p>
<p>To show a brief example, I’ll calculate the CDF of the rand distribution shown already, where <span class="math inline">\(x\)</span> is uniform on <span class="math inline">\([0,1]\)</span>. I already showed that its PDF is just <span class="math inline">\(p(x)=1\)</span> for all <span class="math inline">\(0 \leq x \leq 1\)</span>. Outside this interval <span class="math inline">\(p(x)=0\)</span> everywhere. Using the PDF I can calculate the CDF by integrating. There are three cases to consider. If <span class="math inline">\(x &lt; 0\)</span>, the CDF will just be <span class="math inline">\(P(x)=0\)</span> since <span class="math inline">\(p(x)=0\)</span>. If <span class="math inline">\(x &gt; 1\)</span>, <span class="math inline">\(P(x) = 1\)</span> since we’re integrating over the whole support <span class="math inline">\([0,1]\)</span>. Otherwise, we’re integrating over some subinterval <span class="math inline">\([0,x]\)</span>, in which case <span class="math inline">\(P(x)=x\)</span>. That is,</p>
<p><span class="math display">\[
P(x) = \int_{-\infty}^x p(x') dx' =
\begin{cases}
0, &amp; x &lt; 0 \\
x, &amp; 0 \leq x \leq 1 \\
1, &amp; x &gt; 1.
\end{cases}\]</span></p>
<p>Here’s a plot of both the PDF and CDF of rand. Notice the PDF is just the constant <span class="math inline">\(p(x)=1\)</span> on <span class="math inline">\([0,1]\)</span>, whose area under the curve is just one, since the total probability must integrate to one. Also, notice how this same area is the exact same thing that the histogram tries to approximate. In fact, a histogram is just a discrete approximation to the area under a continuous PDF.</p>
<p>For the CDF, notice how the function starts at <span class="math inline">\(P(x)=0\)</span> on the far left, and ramps up monotonically to <span class="math inline">\(P(x)=1\)</span> as <span class="math inline">\(x\)</span> increases. Every CDF will have this property. The only difference is what the ramp looks like. It’ll always be the case that <span class="math inline">\(P(-\infty)=0\)</span>, <span class="math inline">\(P(\infty)=1\)</span>, and some monotonic increasing curve connects these two extremes.</p>
<div class="cell" data-execution_count="22">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">100</span>)</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> <span class="kw">lambda</span> x: np.ones(<span class="bu">len</span>(x))</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>plot_function(x, p, xlim<span class="op">=</span>(<span class="op">-</span><span class="fl">0.5</span>, <span class="fl">1.5</span>), ylim<span class="op">=</span>(<span class="op">-</span><span class="fl">0.5</span>, <span class="fl">1.5</span>), set_ticks<span class="op">=</span><span class="va">True</span>, title<span class="op">=</span><span class="st">'Rand PDF'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="probability_files/figure-html/cell-23-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="23">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">100</span>)</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>P <span class="op">=</span> <span class="kw">lambda</span> x: np.clip(x, <span class="dv">0</span>, <span class="dv">1</span>) <span class="co">## quick way to define the piecewise CDF shown above</span></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>plot_function(x, P, xlim<span class="op">=</span>(<span class="op">-</span><span class="dv">1</span>, <span class="dv">2</span>), ylim<span class="op">=</span>(<span class="op">-</span><span class="fl">0.5</span>, <span class="fl">1.5</span>), title<span class="op">=</span><span class="st">'Rand CDF'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="probability_files/figure-html/cell-24-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="continuous-distributions" class="level3" data-number="11.3.3">
<h3 data-number="11.3.3" class="anchored" data-anchor-id="continuous-distributions"><span class="header-section-number">11.3.3</span> Continuous Distributions</h3>
<p>As with discrete distributions, some continuous distributions occur so frequently that they get a special name. Here are a few of the most common continuous distributions. I’ll just state them and summarize their properties for future reference.</p>
<section id="uniform-distribution" class="level4" data-number="11.3.3.1">
<h4 data-number="11.3.3.1" class="anchored" data-anchor-id="uniform-distribution"><span class="header-section-number">11.3.3.1</span> Uniform Distribution</h4>
<ul>
<li>Symbol: <span class="math inline">\(U(a,b)\)</span></li>
<li>Parameters: The minimum <span class="math inline">\(a\)</span> and maximum <span class="math inline">\(b\)</span> values in the support</li>
<li>Support: <span class="math inline">\(x \in [a,b]\)</span></li>
<li>Probability density function: <span class="math display">\[p(x; a,b) = \frac{1}{b-a}, \ \text{ for } a \leq x \leq b.\]</span></li>
<li>Cumulative distribution function: <span class="math display">\[
P(x; a, b) =
\begin{cases}
0 &amp; x &lt; a, \\
\frac{x - a}{b-a}, &amp; a \leq x \leq b, \\
1 &amp; x \geq 1.
\end{cases}
\]</span></li>
<li>Random number generator: <code>np.random.uniform(a, b)</code></li>
<li>Notes:
<ul>
<li>Used to model continuous processes that occur with equal weight, or are suspected to (the principle of indifference)</li>
<li>Example: The values sampled from rand, where <span class="math inline">\(a=0\)</span> and <span class="math inline">\(b=1\)</span>, so <span class="math inline">\(x \sim U(0,1)\)</span>.</li>
<li>The rand example <span class="math inline">\(U(0,1)\)</span> is called the <strong>standard uniform distribution</strong>.</li>
</ul></li>
</ul>
<div class="cell" data-execution_count="32">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>a, b <span class="op">=</span> <span class="op">-</span><span class="dv">2</span>, <span class="dv">5</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(a, b, <span class="dv">1000</span>)</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> <span class="kw">lambda</span> x: <span class="dv">1</span> <span class="op">/</span> (b <span class="op">-</span> a) <span class="op">*</span> np.ones(<span class="bu">len</span>(x))</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>plot_function(x, p, xlim<span class="op">=</span>(a <span class="op">-</span> <span class="fl">0.5</span>, b <span class="op">+</span> <span class="fl">0.5</span>), ylim<span class="op">=</span>(<span class="op">-</span><span class="fl">0.5</span> <span class="op">/</span> (b <span class="op">-</span> a), <span class="fl">1.5</span> <span class="op">/</span> (b <span class="op">-</span> a)), set_ticks<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>              title<span class="op">=</span><span class="ss">f'$U(</span><span class="sc">{</span>a<span class="sc">}</span><span class="ss">,</span><span class="sc">{</span>b<span class="sc">}</span><span class="ss">)$ PDF'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="probability_files/figure-html/cell-25-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="gaussian-distribution-normal-distribution" class="level4" data-number="11.3.3.2">
<h4 data-number="11.3.3.2" class="anchored" data-anchor-id="gaussian-distribution-normal-distribution"><span class="header-section-number">11.3.3.2</span> Gaussian Distribution (Normal Distribution)</h4>
<ul>
<li>Symbol: <span class="math inline">\(\mathcal{N}(\mu, \sigma^2)\)</span></li>
<li>Parameters: The mean <span class="math inline">\(\mu \in \mathbb{R}\)</span> and variance <span class="math inline">\(\sigma^2 \geq 0\)</span> of the distribution</li>
<li>Support: <span class="math inline">\(x \in \mathbb{R}\)</span></li>
<li>Probability density function: <span class="math display">\[p(x; \mu , \sigma^2) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp{\bigg(-\frac{(x - \mu)^2}{2\sigma^2}\bigg)}.\]</span></li>
<li>Cumulative distribution function: <span class="math display">\[P(x; \mu , \sigma^2) = \frac{1}{\sqrt{2 \pi \sigma^2}} \int_{-\infty}^x \exp{\bigg(-\frac{(x' - \mu)^2}{2\sigma^2}\bigg)} dx'.\]</span></li>
<li>Random number generator: <code>np.random.normal(mu, sigma)</code> (note it <code>sigma</code> is the <em>square root</em> of the variance <span class="math inline">\(\sigma^2\)</span>)</li>
<li>Notes:
<ul>
<li>Used to model the sum or mean of many continuous random variables, e.g.&nbsp;the distribution of unbiased measurements of some continuous quantity</li>
<li>Example: The distribution of heights in a given population of people.</li>
<li>Used in machine learning to model the outputs of an L2 regression model. Given a input <span class="math inline">\(\mathbf{x}\)</span> with a continuous output <span class="math inline">\(y\)</span>, model <span class="math inline">\(y = f(\mathbf{x}) + \varepsilon\)</span>, where <span class="math inline">\(\varepsilon \sim \mathcal{N}(0,\sigma^2)\)</span> is some random error term and <span class="math inline">\(f(\mathbf{x})\)</span> is some deterministic function to be learned. Then <span class="math inline">\(y \sim \mathcal{N}(f(\mathbf{x}),\sigma^2)\)</span>.</li>
<li>The special case when <span class="math inline">\(\mu=0, \sigma^2=1\)</span> is called the <strong>standard Gaussian distribution</strong>, written <span class="math inline">\(\mathcal{N}(0,1)\)</span>. Values sampled from a standard Gaussian are commonly denoted by <span class="math inline">\(z\)</span>. By convention, its PDF is denoted by <span class="math inline">\(\phi(z)\)</span> and its CDF by <span class="math inline">\(\Phi(z)\)</span>.</li>
<li>Can turn any Gaussian random variable <span class="math inline">\(x\)</span> into a standard Gaussian or vice versa via the transformations <span class="math display">\[z = \frac{x-\mu}{\sigma}, \qquad x = \sigma z + \mu.\]</span></li>
<li>The CDF of a Gaussian can’t be written in closed form since the Gaussian integral can’t be written in terms of elementary functions. Since the standard Gaussian CDF <span class="math inline">\(\Phi(z)\)</span> has a library implementation it’s most common to transform other Gaussian CDFs into standard form and then calculate that way. Use the function <code>norm.cdf</code> from <code>scipy.stats</code> to get the standard CDF function <span class="math inline">\(\Phi(z)\)</span>.</li>
</ul></li>
</ul>
<div class="cell" data-execution_count="25">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">1000</span>)</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>p_gaussian <span class="op">=</span> <span class="kw">lambda</span> x: <span class="dv">1</span> <span class="op">/</span> np.sqrt(<span class="dv">2</span> <span class="op">*</span> np.pi) <span class="op">*</span> np.exp(<span class="op">-</span><span class="dv">1</span><span class="op">/</span><span class="dv">2</span> <span class="op">*</span> x<span class="op">**</span><span class="dv">2</span>)</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>plot_function(x, p_gaussian, xlim<span class="op">=</span>(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>), ylim<span class="op">=</span>(<span class="dv">0</span>, <span class="fl">0.5</span>), set_ticks<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>              title<span class="op">=</span><span class="ss">f'Standard Gaussian PDF'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="probability_files/figure-html/cell-26-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="26">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> norm</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, num<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>Phi <span class="op">=</span> <span class="kw">lambda</span> x: norm.cdf(x)</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>plot_function(x, Phi, xlim<span class="op">=</span>(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>), ylim<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">1</span>), set_ticks<span class="op">=</span><span class="va">False</span>, title<span class="op">=</span><span class="st">'Standard Gaussian CDF'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="probability_files/figure-html/cell-27-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="laplace-distribution" class="level4" data-number="11.3.3.3">
<h4 data-number="11.3.3.3" class="anchored" data-anchor-id="laplace-distribution"><span class="header-section-number">11.3.3.3</span> Laplace Distribution</h4>
<ul>
<li>Symbol: <span class="math inline">\(\text{Laplace}(\mu, s)\)</span></li>
<li>Parameters: The mean <span class="math inline">\(\mu \in \mathbb{R}\)</span> and scale <span class="math inline">\(s \geq 0\)</span> of the distribution</li>
<li>Support: <span class="math inline">\(x \in \mathbb{R}\)</span></li>
<li>Probability density function: <span class="math display">\[p(x; \mu , s) = \frac{1}{2s} \exp\bigg(-\frac{|x-\mu|}{s}\bigg).\]</span></li>
<li>Cumulative distribution function: <span class="math display">\[
P(x; \mu , s) =
\begin{cases}
\frac{1}{2} \exp\bigg(-\frac{|x-\mu|}{s}\bigg), &amp; x \leq \mu \\
1 - \frac{1}{2} \exp\bigg(-\frac{|x-\mu|}{s}\bigg), &amp; x &gt; \mu.
\end{cases}
\]</span></li>
<li>Random number generator: <code>np.random.laplace(mu, s)</code></li>
<li>Notes:
<ul>
<li>Used to model Gaussian-like situations where extreme values are somewhat more likely to occur than in a Gaussian. These are called <em>outliers</em>.</li>
<li>Example: The distribution of finanical stock returns, where extreme returns are more likely than expected under a Gaussian distribution.</li>
<li>Used in machine learning to model the outputs of an L1 regression model. Given an input <span class="math inline">\(\mathbf{x}\)</span> with a continuous output <span class="math inline">\(y\)</span>, model <span class="math inline">\(y = f(\mathbf{x}) + \varepsilon\)</span>, where <span class="math inline">\(\varepsilon \sim \text{Laplace}(0, s)\)</span> is some random error term (that can be extreme-valued) and <span class="math inline">\(f(\mathbf{x})\)</span> is some deterministic function to be learned. Then the outputs are also Laplace distributed, with <span class="math inline">\(y \sim \text{Laplace}(f(\mathbf{x}), s)\)</span>.</li>
<li>The special case when <span class="math inline">\(\mu=0, s=1\)</span> is called the <strong>standard Laplace distribution</strong>, written <span class="math inline">\(\text{Laplace}(0, 1)\)</span>.</li>
</ul></li>
</ul>
<div class="cell" data-execution_count="27">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">1000</span>)</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>p_laplace <span class="op">=</span> <span class="kw">lambda</span> x: <span class="dv">1</span> <span class="op">/</span> (<span class="dv">2</span> <span class="op">*</span> np.pi) <span class="op">*</span> np.exp(<span class="op">-</span>np.<span class="bu">abs</span>(x))</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>ps <span class="op">=</span> [p_gaussian, p_laplace]</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>plot_function(x, ps, xlim<span class="op">=</span>(<span class="op">-</span><span class="dv">4</span>, <span class="dv">4</span>), ylim<span class="op">=</span>(<span class="dv">0</span>, <span class="fl">0.5</span>), set_ticks<span class="op">=</span><span class="va">False</span>, labels<span class="op">=</span>[<span class="st">'Gaussian'</span>, <span class="st">'Laplace'</span>],</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>             title<span class="op">=</span><span class="st">'Gaussian vs Laplace PDFs'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="probability_files/figure-html/cell-28-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
</section>
<section id="cauchy-distribution" class="level3" data-number="11.3.4">
<h3 data-number="11.3.4" class="anchored" data-anchor-id="cauchy-distribution"><span class="header-section-number">11.3.4</span> Cauchy Distribution</h3>
<ul>
<li>Symbol: <span class="math inline">\(\text{Cauchy}(m, s)\)</span></li>
<li>Parameters: The median <span class="math inline">\(m \in \mathbb{R}\)</span> and scale <span class="math inline">\(s &gt; 0\)</span> of the distribution.</li>
<li>Support: <span class="math inline">\(x \in \mathbb{R}\)</span></li>
<li>Probability density function: <span class="math display">\[p(x; m, s) = \frac{1}{\pi s} \frac{1}{1 + \big(\frac{x-m}{s}\big)^2}.\]</span></li>
<li>Cumulative distribution function: <span class="math display">\[P(x; m, s) = \frac{1}{\pi} \arctan \bigg(\frac{x-m}{s}\bigg) + \frac{1}{2}.\]</span></li>
<li>Random number generator: <code>s * np.random.standard_cauchy() + m</code></li>
<li>Notes:
<ul>
<li>Used to model Gaussian-like situations where extreme values are highly likely to occur frequently.</li>
<li>Such a distribution is said to exhibit <em>heavy-tailed</em> behavior, since there’s a “heavy” amount of probability in the tails of the distribution, making extreme values likely to occur.</li>
<li>Example: The distribution of computer program runtimes often exhibits heavy-tailed behavior.</li>
<li>The case when <span class="math inline">\(m=0\)</span> and <span class="math inline">\(s=1\)</span> is called the <strong>standard Cauchy distribution</strong>, denoted <span class="math inline">\(\text{Cauchy}(0, 1)\)</span>.</li>
<li>Technically speaking, the mean of the Cauchy distribution doesn’t exist, so you have to use the median instead.</li>
</ul></li>
</ul>
<div class="cell" data-execution_count="28">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">1000</span>)</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>p_cauchy <span class="op">=</span> <span class="kw">lambda</span> x: <span class="dv">1</span> <span class="op">/</span> np.pi <span class="op">*</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> x <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>ps <span class="op">=</span> [p_gaussian, p_laplace, p_cauchy]</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>plot_function(x, ps, xlim<span class="op">=</span>(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>), ylim<span class="op">=</span>(<span class="dv">0</span>, <span class="fl">0.5</span>), set_ticks<span class="op">=</span><span class="va">False</span>, labels<span class="op">=</span>[<span class="st">'Gaussian'</span>, <span class="st">'Laplace'</span>, <span class="st">'Cauchy'</span>],</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>             title<span class="op">=</span><span class="st">'Gaussian vs Laplace vs Caucy PDFs'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="probability_files/figure-html/cell-29-output-1.png" class="img-fluid"></p>
</div>
</div>
<section id="exponential-distribution" class="level4" data-number="11.3.4.1">
<h4 data-number="11.3.4.1" class="anchored" data-anchor-id="exponential-distribution"><span class="header-section-number">11.3.4.1</span> Exponential Distribution</h4>
<ul>
<li>Symbol: <span class="math inline">\(\text{Exp}(\lambda)\)</span></li>
<li>Parameters: A rate parameter <span class="math inline">\(\lambda &gt; 0\)</span></li>
<li>Support: <span class="math inline">\(x \in [0,\infty)\)</span></li>
<li>Probability density function: <span class="math display">\[p(x; \lambda) = \lambda e^{-\lambda x}.\]</span></li>
<li>Cumulative distribution function: <span class="math display">\[P(x; \lambda) = 1 - e^{-\lambda x}.\]</span></li>
<li>Random number generator: <code>np.random.exponential(lambda)</code></li>
<li>Notes:
<ul>
<li>Used to model the time between two independent discrete events, assuming those events occur at a roughly constant rate.</li>
<li>Example: The time between earthquakes in a given region, assuming earthquakes are rare and independent events.</li>
<li>Frequently used to model the time between two Poisson distributed events. If the events are Poisson distributed and independent, then the time between any two events will be exponentially distributed.</li>
</ul></li>
</ul>
<div class="cell" data-execution_count="29">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>lambda_ <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="dv">20</span>, <span class="dv">100</span>)</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> <span class="kw">lambda</span> x: lambda_ <span class="op">*</span> np.exp(<span class="op">-</span>lambda_ <span class="op">*</span> x)</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>plot_function(x, p, xlim<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">20</span>), ylim<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">1</span>), set_ticks<span class="op">=</span><span class="va">False</span>, title<span class="op">=</span><span class="ss">f'$Exp(</span><span class="sc">{</span>lambda_<span class="sc">}</span><span class="ss">)$ PDF'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="probability_files/figure-html/cell-30-output-1.png" class="img-fluid"></p>
</div>
</div>


</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation column-page-right">
  <div class="nav-page nav-page-previous">
      <a href="../notebooks/optimization.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Optimization</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../notebooks/multivariate-probability.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Multivariate Distributions</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>