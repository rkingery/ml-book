<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Mathematics for Machine Learning - 7&nbsp; Multivariate Calculus</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../notebooks/probability.html" rel="next">
<link href="../notebooks/matrix-algebra.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Multivariate Calculus</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Mathematics for Machine Learning</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">Preface</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/basic-math.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Basic Math</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/numerical-computing.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Numerical Computation</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/calculus.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Calculus</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/linear-systems.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Linear Systems</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/vector-spaces.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Vector Spaces</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/matrix-algebra.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Matrix Algebra</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/multivariate-calculus.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Multivariate Calculus</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/probability.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Basic Probability</span></a>
  </div>
</li>
    </ul>
    </div>
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#multivariate-differentiation" id="toc-multivariate-differentiation" class="nav-link active" data-scroll-target="#multivariate-differentiation"><span class="toc-section-number">7.1</span>  Multivariate Differentiation</a>
  <ul class="collapse">
  <li><a href="#the-gradient" id="toc-the-gradient" class="nav-link" data-scroll-target="#the-gradient"><span class="toc-section-number">7.1.1</span>  The Gradient</a></li>
  <li><a href="#visualizing-gradients" id="toc-visualizing-gradients" class="nav-link" data-scroll-target="#visualizing-gradients"><span class="toc-section-number">7.1.2</span>  Visualizing Gradients</a></li>
  <li><a href="#the-hessian" id="toc-the-hessian" class="nav-link" data-scroll-target="#the-hessian"><span class="toc-section-number">7.1.3</span>  The Hessian</a></li>
  <li><a href="#differentiation-in-n-dimensions" id="toc-differentiation-in-n-dimensions" class="nav-link" data-scroll-target="#differentiation-in-n-dimensions"><span class="toc-section-number">7.1.4</span>  Differentiation in <span class="math inline">\(n\)</span> Dimensions</a></li>
  <li><a href="#the-jacobian" id="toc-the-jacobian" class="nav-link" data-scroll-target="#the-jacobian"><span class="toc-section-number">7.1.5</span>  The Jacobian</a></li>
  <li><a href="#application-the-softmax-function" id="toc-application-the-softmax-function" class="nav-link" data-scroll-target="#application-the-softmax-function"><span class="toc-section-number">7.1.6</span>  Application: The Softmax Function</a></li>
  <li><a href="#gradient-and-jacobian-rules" id="toc-gradient-and-jacobian-rules" class="nav-link" data-scroll-target="#gradient-and-jacobian-rules"><span class="toc-section-number">7.1.7</span>  Gradient and Jacobian Rules</a></li>
  </ul></li>
  <li><a href="#multivariate-integration" id="toc-multivariate-integration" class="nav-link" data-scroll-target="#multivariate-integration"><span class="toc-section-number">7.2</span>  Multivariate Integration</a>
  <ul class="collapse">
  <li><a href="#integration-in-2-dimensions" id="toc-integration-in-2-dimensions" class="nav-link" data-scroll-target="#integration-in-2-dimensions"><span class="toc-section-number">7.2.1</span>  Integration in 2 Dimensions</a></li>
  <li><a href="#application-integrating-the-gaussian" id="toc-application-integrating-the-gaussian" class="nav-link" data-scroll-target="#application-integrating-the-gaussian"><span class="toc-section-number">7.2.2</span>  Application: Integrating the Gaussian</a></li>
  <li><a href="#integration-in-n-dimensions" id="toc-integration-in-n-dimensions" class="nav-link" data-scroll-target="#integration-in-n-dimensions"><span class="toc-section-number">7.2.3</span>  Integration in <span class="math inline">\(n\)</span> Dimensions</a></li>
  </ul></li>
  </ul>
</nav>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content column-page-right" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Multivariate Calculus</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>In this section of Math for ML I’ll carry on with the topic of calculus by discussing the calculus of multivariate functions. It’s in multivariate calculus when we start to see the <em>real</em> utility of linear algebra. Just like we could linearize a univariate function <span class="math inline">\(y=f(x)\)</span> by approximating it with its tangent line, we can linearize a multivariate function <span class="math inline">\(z=f(x,y)\)</span> by approximating it with its tangent plane. In fact, we can linearize any arbitrary (differentiable) vector function <span class="math inline">\(\mathbf{y} = \mathbf{F}(\mathbf{x})\)</span> by approximating it with a bunch of tangent hyperplanes. Once we’ve done this linear approximation, we’re just doing linear algebra again on vector spaces, except the vector spaces are now these tangent hyperplanes. In a sense, then, multivariate calculus is just a natural extension of linear algebra to nonlinear functions! Let’s get started.</p>
<div class="cell" data-execution_count="1">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sympy <span class="im">as</span> sp</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> utils.math_ml <span class="im">import</span> <span class="op">*</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<section id="multivariate-differentiation" class="level2" data-number="7.1">
<h2 data-number="7.1" class="anchored" data-anchor-id="multivariate-differentiation"><span class="header-section-number">7.1</span> Multivariate Differentiation</h2>
<p>Just as we can differentiate <em>univariate</em> functions like <span class="math inline">\(y=f(x)\)</span>, we can also differentiate <em>multivariate</em> functions like <span class="math inline">\(z=f(x,y)\)</span>. The main difference is that we can take derivatives of many inputs variables, not just one.</p>
<section id="the-gradient" class="level3" data-number="7.1.1">
<h3 data-number="7.1.1" class="anchored" data-anchor-id="the-gradient"><span class="header-section-number">7.1.1</span> The Gradient</h3>
<p>Suppose <span class="math inline">\(z=f(x,y)\)</span> and we want to ask the question, how does <span class="math inline">\(z\)</span> change if we change <span class="math inline">\(x\)</span> by an infinitesimal amount <span class="math inline">\(dx\)</span>, holding <span class="math inline">\(y\)</span> constant? Evidently it would be <span class="math inline">\(z + dz = f(x+dx, y)\)</span>. If we pretend <span class="math inline">\(y\)</span> is constant, this would mean</p>
<p><span class="math display">\[dz = f(x+dx, y) - f(x, y).\]</span></p>
<p>Dividing both sides by <span class="math inline">\(dx\)</span> we’d get <em>something</em> like a derivative. But it’s not <em>the</em> derivative since we’re only changing <span class="math inline">\(x\)</span> and fixing <span class="math inline">\(y\)</span>. For this reason it’s called the <strong>partial derivative</strong> of <span class="math inline">\(z\)</span> with respect to <span class="math inline">\(x\)</span>, and typically written with funny <span class="math inline">\(\partial\)</span> symbols instead of <span class="math inline">\(d\)</span> symbols,</p>
<p><span class="math display">\[\frac{\partial z}{\partial x} = \frac{f(x+dx, y) - f(x, y)}{dx}.\]</span></p>
<p>Similarly, we can ask the dual question, how does <span class="math inline">\(z\)</span> change if we change <span class="math inline">\(y\)</span> by an infinitesimal amount <span class="math inline">\(dy\)</span>, holding <span class="math inline">\(x\)</span> constant? By the same logic, we’d get</p>
<p><span class="math display">\[dz = f(x, y + dy) - f(x, y),\]</span></p>
<p>and dividing by <span class="math inline">\(dy\)</span> would give the partial derivative of <span class="math inline">\(z\)</span> with respect to <span class="math inline">\(y\)</span>,</p>
<p><span class="math display">\[\frac{\partial z}{\partial y} = \frac{f(x, y + dy) - f(x, y)}{dy}.\]</span></p>
<p>But these don’t tell us everything. We want to know how <span class="math inline">\(z\)</span> changes if we change <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> arbitrarily, not if we hold one of them constant. That is, we want the full <span class="math inline">\(dz\)</span>. In the case when <span class="math inline">\(y=f(x)\)</span>, we saw that <span class="math inline">\(dy=\frac{dy}{dx}dx\)</span>. If we only change <span class="math inline">\(x\)</span>, evidently <span class="math inline">\(dz = \frac{\partial z}{\partial x} dx\)</span>. Similarly if we only change <span class="math inline">\(y\)</span>, then <span class="math inline">\(dz = \frac{\partial z}{\partial y} dy\)</span>. It seems like if we want to change <em>both</em>, we should add these two effects together,</p>
<p><span class="math display">\[dz = \frac{\partial z}{\partial x} dx + \frac{\partial z}{\partial y} dy.\]</span></p>
<p>This equation is called the bivariate <strong>chain rule</strong>. Since it depends on changes in both <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, <span class="math inline">\(dz\)</span> is called the <strong>total differential</strong>. The chain rule tells us everything we need to know about how <span class="math inline">\(z\)</span> changes when either <span class="math inline">\(x\)</span> or <span class="math inline">\(y\)</span> are perturbed by some small amount. The amount that <span class="math inline">\(z\)</span> gets perturbed is <span class="math inline">\(dz\)</span>.</p>
<p>If we have a composite function like, say, <span class="math inline">\(z=f(x,y)\)</span>, <span class="math inline">\(x=g(u, v)\)</span>, <span class="math inline">\(y=h(u, v)\)</span>, we can do just like in the univariate chain rule and divide the total differential by <span class="math inline">\(du\)</span> or <span class="math inline">\(dv\)</span> to get the chain rule in partial derivative form,</p>
<p><span class="math display">\[\frac{\partial z}{\partial u} = \frac{\partial z}{\partial x}\frac{\partial x}{\partial u} + \frac{\partial z}{\partial y}\frac{\partial y}{\partial u},\]</span></p>
<p><span class="math display">\[\frac{\partial z}{\partial v} = \frac{\partial z}{\partial x}\frac{\partial x}{\partial v} + \frac{\partial z}{\partial y}\frac{\partial y}{\partial v}.\]</span></p>
<p>This is the form in which the bivariate chain rule usually appears in deep learning, but with many more variables.</p>
<p>It’s interesting to write this formula as a dot product of two vectors. If we define two vectors as follows,</p>
<p><span class="math display">\[\frac{dz}{d\mathbf{x}}=\big(\frac{\partial z}{\partial x}, \frac{\partial z}{\partial y}\big),\]</span></p>
<p><span class="math display">\[d\mathbf{x} = (dx, dy),\]</span></p>
<p>then the chain rule would say</p>
<p><span class="math display">\[dz = \frac{dz}{d\mathbf{x}} \cdot d\mathbf{x}.\]</span></p>
<p>This looks just like the equation for the ordinary derivative, <span class="math inline">\(dy=\frac{dy}{dx}dx\)</span>,except there’s a dot product of vectors here.</p>
<p>The vector <span class="math inline">\(\frac{dz}{d\mathbf{x}}\)</span> looks like the ordinary derivative, but it’s now a vector of partial derivatives. It’s called the <strong>gradient</strong> of <span class="math inline">\(z=f(x,y)\)</span>.</p>
<p>In many texts, the gradient is often written with the funny symbol <span class="math inline">\(\nabla f(x,y)\)</span>. Other notations used are <span class="math inline">\(\frac{d}{d\mathbf{x}}f(\mathbf{x})\)</span>, or <span class="math inline">\(\mathbf{f}'(\mathbf{x})\)</span>. I’ll often instead use the simpler notation of <span class="math inline">\(\mathbf{g}\)</span> or <span class="math inline">\(\mathbf{g}(x,y)\)</span> for the gradient when it’s clear what the function is we’re differentiating. All of these notations can represent the <em>same</em> gradient vector,</p>
<p><span class="math display">\[\mathbf{g} = \nabla f(\mathbf{x}) = \mathbf{f}'(\mathbf{x}) = \frac{d}{d\mathbf{x}}f(\mathbf{x}) = \frac{dz}{d\mathbf{x}}.\]</span></p>
<p>Note it’s very common in calculus and applications to abuse the difference between points and vectors. We might write <span class="math inline">\(f(x, y)\)</span>, or just <span class="math inline">\(f(\mathbf{x})\)</span>, where it’s understood <span class="math inline">\(\mathbf{x}\)</span> is the vector <span class="math inline">\(\mathbf{x}=(x,y)\)</span>. We’ll do this a lot. There’s no real difference between them.</p>
<p>Let’s do an example. Consider the function <span class="math inline">\(z=x^2+y^2\)</span>. This function has a surface that looks like a bowl.</p>
<div class="cell" data-execution_count="2">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">100</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">100</span>)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>f <span class="op">=</span> <span class="kw">lambda</span> x, y: x<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> y<span class="op">**</span><span class="dv">2</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>plot_function_3d(x, y, f, title<span class="op">=</span><span class="st">'$z=x^2+y^2$'</span>, titlepad<span class="op">=</span><span class="dv">10</span>, labelpad<span class="op">=</span><span class="dv">5</span>, ticks_every<span class="op">=</span>[<span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">50</span>], dist<span class="op">=</span><span class="dv">12</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="multivariate-calculus_files/figure-html/cell-3-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Suppose we treat <span class="math inline">\(y\)</span> as constant, say <span class="math inline">\(y=2\)</span>. If we nudge <span class="math inline">\(x\)</span> to <span class="math inline">\(x+dx\)</span>, then <span class="math inline">\(z\)</span> would get nudged to</p>
<p><span class="math display">\[z+dz = f(x+dx,y) = (x+dx)^2 + y^2 = (x^2 + 2xdx + dx^2) + y^2 \approx z + 2xdx.\]</span></p>
<p>That is, <span class="math display">\[\frac{\partial z}{\partial x} = 2x.\]</span></p>
<p>This is exactly what we got before in the univariate case with <span class="math inline">\(f(x)=x^2\)</span>. This makes since. By treating <span class="math inline">\(y\)</span> as constant we’re effectively pretending it’s not there in the calculation, which makes it act like we’re taking the 1D derivative <span class="math inline">\(z=x^2\)</span>.</p>
<p>Since <span class="math inline">\(z=x^2+y^2\)</span> is symmetric in <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, the exact same argument above would show</p>
<p><span class="math display">\[\frac{\partial z}{\partial y} = 2y.\]</span></p>
<p>The gradient vector would thus be <span class="math display">\[\frac{dz}{d\mathbf{x}} = (2x, 2y) = 2\mathbf{x}, \quad \text{where }\mathbf{x} = (x,y).\]</span></p>
<p>The gradient looks exactly like the 1D version where <span class="math inline">\(y=x^2\)</span> and <span class="math inline">\(\frac{dy}{dx}=2x\)</span>, except there’s a vector <span class="math inline">\(\mathbf{x}\)</span> instead.</p>
<p>Just as with the ordinary derivative, we can see that the gradient is a function of its inputs. The difference though is the gradient is a <em>vector-valued function</em>. Its output is a vector, not a scalar.</p>
<p>Numerical differentiation extends naturally to the bivariate case as well. We can calculate partial derivatives numerically straight from their definitions, using reasonably small values like <span class="math inline">\(dx=dy=10^{-5}\)</span>. To get the gradient, just calculate the partials numerically and put them into an array.</p>
<p>Here’s an example. I’ll calculate the partials <span class="math inline">\(\frac{\partial z}{\partial x}, \frac{\partial z}{\partial y}\)</span> at the point <span class="math inline">\(x_0=1, y_0=1\)</span>. The partials are given by <code>dzdx</code> and <code>dzdy</code> respectively, and the gradient vector by <code>grad</code>. Notice the error is again on the order of <span class="math inline">\(dx\)</span> and <span class="math inline">\(dy\)</span>, hence we get good agreement with the above equation when <span class="math inline">\(x_0=1, y_0=1\)</span>.</p>
<div class="cell" data-execution_count="3">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>x0 <span class="op">=</span> y0 <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>dx <span class="op">=</span> dy <span class="op">=</span> <span class="fl">1e-5</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>dzdx <span class="op">=</span> (f(x0 <span class="op">+</span> dx, y0) <span class="op">-</span> f(x0, y0)) <span class="op">/</span> dx</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>dzdy <span class="op">=</span> (f(x0, y0 <span class="op">+</span> dy) <span class="op">-</span> f(x0, y0)) <span class="op">/</span> dy</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>grad <span class="op">=</span> [dzdx, dzdy]</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'grad = </span><span class="sc">{</span>grad<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>grad = [2.00001000001393, 2.00001000001393]</code></pre>
</div>
</div>
</section>
<section id="visualizing-gradients" class="level3" data-number="7.1.2">
<h3 data-number="7.1.2" class="anchored" data-anchor-id="visualizing-gradients"><span class="header-section-number">7.1.2</span> Visualizing Gradients</h3>
<p>In the case of the ordinary univariate derivative <span class="math inline">\(\frac{dy}{dx}\)</span>, we could think of it geometrically as the slope of the tangent line to <span class="math inline">\(y=f(x)\)</span> at a point <span class="math inline">\((x_0,y_0)\)</span>. We can do something similar for the gradient <span class="math inline">\(\frac{dz}{d\mathbf{x}}\)</span> by thinking of it as the vector of slopes defining a tangent plane to <span class="math inline">\(z=f(\mathbf{x})\)</span> at a point <span class="math inline">\((\mathbf{x}_0, z_0)\)</span>.</p>
<p>Suppose <span class="math inline">\(z=f(x,y)\)</span>. Let <span class="math inline">\((x_0,y_0,z_0) \in \mathbb{R}^3\)</span> be a point in 3D space, with <span class="math inline">\(z_0=f(x_0,y_0)\)</span>. This is just a point on the 2D surface of <span class="math inline">\(z=f(x,y)\)</span>. Now, it doesn’t make much sense to talk about a single <em>line</em> that hugs this point, since there can now be infinitely many lines that hug that point. What we instead want to do is think about a <em>plane</em> that hugs the surface. This will be called the <strong>tangent plane</strong>. It’s given by</p>
<p><span class="math display">\[z = z_0 + \frac{\partial}{\partial x}f(x_0,y_0) (x - x_0) + \frac{\partial}{\partial y}f(x_0,y_0) (y - y_0),\]</span></p>
<p>or in vector notation just, <span class="math display">\[z = z_0 + \frac{d}{d\mathbf{x}} f(\mathbf{x}_0) \cdot (\mathbf{x} - \mathbf{x}_0), \quad \text{or} \quad z = z_0 + \mathbf{g}(\mathbf{x}_0) \cdot (\mathbf{x} - \mathbf{x}_0).\]</span></p>
<p>This tangent plane will hug the surface of the function at the point <span class="math inline">\((x_0,y_0,z_0)\)</span>.</p>
<p>Here’s an example, where I’ll calculate the tangent plane to <span class="math inline">\(z=x^2+y^2\)</span> at the point <span class="math inline">\((1,1)\)</span>. Since I showed above that the gradient in this case is <span class="math inline">\((2x, 2y)\)</span>, the tangent line becomes <span class="math inline">\(z=2 + 2(x-1) + 2(y-1)\)</span>. Everything is done in an analogous way to the tangent line calculation from before.</p>
<div class="cell" data-execution_count="4">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>f <span class="op">=</span> <span class="kw">lambda</span> x, y: x<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> y<span class="op">**</span><span class="dv">2</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>dfdx <span class="op">=</span> <span class="kw">lambda</span> x, y: (<span class="dv">2</span> <span class="op">*</span> x, <span class="dv">2</span> <span class="op">*</span> y)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>x0 <span class="op">=</span> y0 <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>z0 <span class="op">=</span> f(x0, y0)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">2</span> <span class="op">*</span> x0, <span class="dv">2</span> <span class="op">*</span> x0, <span class="dv">100</span>)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">2</span> <span class="op">*</span> y0, <span class="dv">2</span> <span class="op">*</span> y0, <span class="dv">100</span>)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>f_tangent <span class="op">=</span> <span class="kw">lambda</span> x, y: <span class="dv">2</span> <span class="op">*</span> (x <span class="op">-</span> x0) <span class="op">+</span> <span class="dv">2</span> <span class="op">*</span> (y <span class="op">-</span> y0) <span class="op">+</span> <span class="dv">2</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>plot_tangent_plane(x, y, x0, y0, f, f_tangent, dfdx, plot_grad<span class="op">=</span><span class="va">True</span>, grad_scale<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>                   title<span class="op">=</span><span class="ss">f'Tangent Plane to $z=x^2+y^2$ at $</span><span class="sc">{</span>(x0, y0, z0)<span class="sc">}</span><span class="ss">$'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="multivariate-calculus_files/figure-html/cell-5-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>If you look at the plane, the partial of <span class="math inline">\(z\)</span> with respect to <span class="math inline">\(x\)</span> turns out to represent the slope of the line running along the plane <em>parallel</em> to the x-axis at the point <span class="math inline">\((1,1)\)</span>. Similarly, the partial of <span class="math inline">\(z\)</span> with respect to <span class="math inline">\(y\)</span> represents the slope of the line running along the plane parallel to the y-axis at the point <span class="math inline">\((1,1)\)</span>.</p>
<p>The gradient vector (shown in red) is both of these together, which gives a vector <span class="math inline">\((2, 2)\)</span> that points in the steepest direction up the surface from the point <span class="math inline">\((1,1)\)</span>. Said differently, the gradient vector is the direction of <em>steepest ascent</em>.</p>
<p>This fact can be visualized easier by looking at the contour plot. In the contour plot, the tangent plane will appear as a line hugging tangent to the contour at the point <span class="math inline">\((1,1)\)</span>. The gradient vector will always point outward <em>perpendicular</em> to this line in the direction of steepest ascent of the function.</p>
<div class="cell" data-execution_count="5">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>plot_tangent_contour(x, y, x0, y0, f, f_tangent, dfdx, title<span class="op">=</span><span class="ss">f'Tangent to $z=x^2+y^2$ at $</span><span class="sc">{</span>(x0, y0, z0)<span class="sc">}</span><span class="ss">$'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="multivariate-calculus_files/figure-html/cell-6-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Here’s an argument for why this is true. A contour is <em>by definition</em> a curve where <span class="math inline">\(z\)</span> is constant. Imagine taking the surface of <span class="math inline">\(z=f(x,y)\)</span> and at each <span class="math inline">\(z\)</span> value slicing the surface parallel to the xy-plane. That’s all a contour is. This means that along any given contour we must have <span class="math inline">\(dz=0\)</span>, since <span class="math inline">\(z\)</span> can’t change. But by the chain rule we already know</p>
<p><span class="math display">\[dz = \frac{dz}{d\mathbf{x}} \cdot d\mathbf{x}.\]</span></p>
<p>But since <span class="math inline">\(dz=0\)</span>, this means <span class="math display">\[\frac{dz}{d\mathbf{x}} \cdot d\mathbf{x} = 0.\]</span></p>
<p>Now, recall two vectors <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{x}\)</span> are orthogonal (i.e.&nbsp;perpendicular) if <span class="math inline">\(\mathbf{x} \cdot \mathbf{y} = 0\)</span>. I’ve thus shown that the gradient vector <span class="math inline">\(\mathbf{g}\)</span> must be perpendicular to the differential vector <span class="math inline">\(d\mathbf{x}\)</span> along contours where <span class="math inline">\(z\)</span> is constant.</p>
<p>Since we’re confined to a contour of constant <span class="math inline">\(z\)</span>, any small changes <span class="math inline">\(d\mathbf{x}\)</span> as we move around the contour must be <em>parallel</em> to the contour, otherwise <span class="math inline">\(dz\)</span> wouldn’t be zero. This means <span class="math inline">\(\mathbf{g}\)</span> must be <em>perpendicular</em> to the line tangent to the contour at <span class="math inline">\((1,1)\)</span>. That is, the gradient at <span class="math inline">\((1,1)\)</span> is a vector pointing outward in the direction of steep ascent from the point <span class="math inline">\((1,1)\)</span>.</p>
</section>
<section id="the-hessian" class="level3" data-number="7.1.3">
<h3 data-number="7.1.3" class="anchored" data-anchor-id="the-hessian"><span class="header-section-number">7.1.3</span> The Hessian</h3>
<p>In the univariate case, we had not just first derivatives <span class="math inline">\(\frac{dy}{dx}\)</span>, but second derivatives <span class="math inline">\(\frac{d^2y}{dx^2}\)</span> too. In the multivariate case we can take second partial derivatives as well in the usual way, but there are now <span class="math inline">\(2^2=4\)</span> different ways to calculate second derivatives,</p>
<p><span class="math display">\[\frac{\partial^2 z}{\partial x^2}, \frac{\partial^2 z}{\partial x \partial y}, \frac{\partial^2 z}{\partial y \partial x}, \frac{\partial^2 z}{\partial y^2}.\]</span></p>
<p>Note the partials are by convention applied from right to left. Thankfully this doesn’t matter, since for well-behaved functions the mixed partials <em>commute</em> with each other, i.e. <span class="math display">\[\frac{\partial^2 z}{\partial x \partial y} = \frac{\partial^2 z}{\partial y \partial x}.\]</span></p>
<p>Just as we could group first partial derivatives into a vector to get the gradient, we can group second partial derivatives into a <em>matrix</em> to get what’s called the <strong>Hessian</strong> matrix, <span class="math display">\[
\frac{d^2 z}{d\mathbf{x}^2} =
\begin{pmatrix}
\frac{\partial^2 z}{\partial x^2} &amp; \frac{\partial^2 z}{\partial x \partial y} \\
\frac{\partial^2 z}{\partial y \partial x} &amp; \frac{\partial^2 z}{\partial y^2}
\end{pmatrix}.
\]</span></p>
<p>The Hessian is the multivariate generalization of the full second derivative, just as the gradient vector is the generalization of the full first derivative. I’ll often write the Hessian matrix with the symbol <span class="math inline">\(\mathbf{H}\)</span> or <span class="math inline">\(\mathbf{H}(\mathbf{x})\)</span> for brevity.</p>
<p>Just as the second derivative of a univariate function can be interpreted geometrically as representing the curvature of the <em>curve</em> <span class="math inline">\(y=f(x)\)</span>, the Hessian of a multivariate function represents the curvature of the <em>surface</em> <span class="math inline">\(z=f(x,y)\)</span>. This comes from looking at the multivariate tangent parabola</p>
<p><span class="math display">\[z = z_0 + \mathbf{g}(\mathbf{x}_0) \cdot (\mathbf{x} - \mathbf{x}_0) + \frac{1}{2}(\mathbf{x} - \mathbf{x}_0)^\top \mathbf{H}(\mathbf{x}_0) (\mathbf{x} - \mathbf{x}_0).\]</span></p>
<p>The curvature of the function can be obtained by looking at the eigenvalues of the Hessian at <span class="math inline">\(\mathbf{x}= \mathbf{x}_0\)</span>. Large eigenvalues represent steep curvature, while small eigenvalues represent shallow curvature. The sign of the eigenvalues indicate whether the function</p>
<ol type="1">
<li>Bowls upward: both eigenvalues are non-negative,</li>
<li>Bowls downward: both eigenvalues are non-positive,</li>
<li>Saddles: one eigenvalue is positive, one is eigenvalue negative.</li>
</ol>
<p>Case (3) creates what’s called a <strong>saddlepoint</strong>, a point where the function slopes upwards in one direction, but downward in the other, creating the shape of something that resembles a horse’s saddle.</p>
<p>For the same working example <span class="math inline">\(z=x^2+y^2\)</span>, we’d have</p>
<p><span class="math display">\[
\mathbf{H} = \frac{d^2 z}{d\mathbf{x}^2} =
\begin{pmatrix}
2 &amp; 2 \\
2 &amp; 2
\end{pmatrix},
\]</span></p>
<p>that is, the Hessian of this function is constant, since no elements depend on <span class="math inline">\(x\)</span> or <span class="math inline">\(y\)</span>.</p>
<p>The eigenvalues of this Hessian are <span class="math inline">\(\lambda=4,0\)</span>, both of which are non-negative. Since the Hessian is constant, this means the function bowls upward at <em>all</em> points <span class="math inline">\((x,y)\)</span>. This also means this Hessian matrix is positive semi-definite.</p>
<div class="cell" data-execution_count="6">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>H <span class="op">=</span> sp.Matrix([[<span class="dv">2</span>, <span class="dv">2</span>], [<span class="dv">2</span>, <span class="dv">2</span>]])</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>eigs <span class="op">=</span> H.eigenvals()</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'eigenvalues = </span><span class="sc">{</span><span class="bu">list</span>(eigs.keys())<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>eigenvalues = [4, 0]</code></pre>
</div>
</div>
<p>When a function’s Hessian is positive semi-definite, i.e.&nbsp;it bowls upward, it’s called a <strong>convex function</strong>. Convex functions are very important in optimization since convex functions always have a unique global minimum. Classical machine learning algorithms often take advantage of this fact.</p>
<p>What about higher derivatives of multivariate functions? It turns out the <span class="math inline">\(k\)</span><sup>th</sup> derivative of a multivariate function is a rank-<span class="math inline">\(k\)</span> tensor. This makes higher derivatives especially nasty, so we rarely see them.</p>
</section>
<section id="differentiation-in-n-dimensions" class="level3" data-number="7.1.4">
<h3 data-number="7.1.4" class="anchored" data-anchor-id="differentiation-in-n-dimensions"><span class="header-section-number">7.1.4</span> Differentiation in <span class="math inline">\(n\)</span> Dimensions</h3>
<p>Similarly, we can define all of these quantities for any n-dimensional multivariate function <span class="math inline">\(y=f(\mathbf{x})=f(x_0,x_1,\cdots,x_{n-1}).\)</span> The partial derivative of <span class="math inline">\(y\)</span> with respect to some <span class="math inline">\(x_i\)</span> is the one whose only first order perturbation is <span class="math inline">\(x_i+dx_i\)</span>, with the rest staying fixed,</p>
<p><span class="math display">\[\frac{\partial y}{\partial x_i} = \frac{f(x_0,x_1,\cdots,x_i+dx_i,\cdots,x_{n-1}) - f(x_0,x_1,\cdots,x_i,\cdots,x_{n-1})}{dx_i}.\]</span></p>
<p>That is, it’s the derivative of <span class="math inline">\(y\)</span> with respect to <span class="math inline">\(x_i\)</span> where all other inputs <span class="math inline">\(x_j \neq x_i\)</span> are held constant. The chain rule extends by adding a term for each <span class="math inline">\(dx_i\)</span>,</p>
<p><span class="math display">\[dy = \sum_{i=0}^{n-1} \frac{\partial y}{\partial x_i} dx_i = \frac{\partial y}{\partial x_0} dx_0 + \frac{\partial y}{\partial x_1} dx_1 + \cdots + \frac{\partial y}{\partial x_{n-1}} dx_{n-1},\]</span></p>
<p>Or, written as a dot product of <span class="math inline">\(n\)</span> dimensional vectors,</p>
<p><span class="math display">\[\begin{align*}
\frac{dy}{d\mathbf{x}} &amp;= \bigg(\frac{\partial y}{\partial x_0}, \frac{\partial y}{\partial x_1},\cdots,\frac{\partial y}{\partial x_{n-1}} \bigg), \\
d\mathbf{x} &amp;= (dx_0, dx_1,\cdots,dx_{n-1}), \\
dy &amp;= \frac{dy}{d\mathbf{x}} \cdot d\mathbf{x}.
\end{align*}\]</span></p>
<p>I’ll calculate a quick example with the <span class="math inline">\(n\)</span> dimensional generalization of our running quadratic function,</p>
<p><span class="math display">\[y = x_0^2 + x_1^2 + \cdots + x_{n-1}^2 = \sum_{i=0}^{n-1} x_i^2.\]</span></p>
<p>Since each partial derivative gives <span class="math inline">\(\frac{\partial y}{\partial x_i} = 2x_i\)</span>, the <strong>gradient</strong> for this function should be the n-dimensional vector</p>
<p><span class="math display">\[\mathbf{g} = \frac{dy}{d\mathbf{x}} = (2x_0, 2x_1, \cdots, 2x_{n-1}) = 2\mathbf{x}.\]</span></p>
<p>Using numpy we can efficiently calculate this function with the vectorized command <code>np.sum(x ** 2)</code>. I’ll choose our point of interest to be the vector <span class="math inline">\(\mathbf{x}_0\)</span> of all ones. I’ll define a helper function <code>dfdxi</code> to calculate the i<sup>th</sup> partial derivative at <span class="math inline">\(\mathbf{x}_0\)</span>. Note <code>dx</code> will be a vector of all zeros except at <code>dx[i] = dxi</code>. This will then be used in the function <code>dfdx</code> to calculate the gradient. It will loop over every index, calculate each partial, and put them in a vector <code>grad</code>. Observe that yet again we have a gradient vector of all twos to within an error of around <code>1e-5</code>, except instead of 1 or 2 elements we have 100 of them.</p>
<div class="cell" data-execution_count="7">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> dfdxi(f, x0, i, dxi<span class="op">=</span><span class="fl">1e-5</span>):</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>    dx <span class="op">=</span> np.zeros(<span class="bu">len</span>(x0))</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>    dx[i] <span class="op">=</span> dxi</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>    dydxi <span class="op">=</span> (f(x0 <span class="op">+</span> dx) <span class="op">-</span> f(x0)) <span class="op">/</span> dxi</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> dydxi</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> dfdx(f, x0, dxi<span class="op">=</span><span class="fl">1e-5</span>):</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.array([dfdxi(f, x0, i, dxi<span class="op">=</span>dxi) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(x0))])</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>f <span class="op">=</span> <span class="kw">lambda</span> x: np.<span class="bu">sum</span>(x <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>x0 <span class="op">=</span> np.ones(<span class="dv">100</span>)</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>grad <span class="op">=</span> dfdx(f, x0)</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'grad.shape = </span><span class="sc">{</span>grad<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'grad = </span><span class="ch">\n</span><span class="sc">{</span>grad<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>grad.shape = (100,)
grad = 
[2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001
 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001
 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001
 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001
 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001
 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001
 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001
 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001
 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001
 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001
 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001
 2.00001]</code></pre>
</div>
</div>
<p>Just like with univariate functions, we can use the tangent plane to approximate the behavior of a multivariate function <span class="math inline">\(f(\mathbf{x})\)</span> near a point <span class="math inline">\(\mathbf{x}_0\)</span>. This gives the <strong>first order approximation</strong></p>
<p><span class="math display">\[f(\mathbf{x}) \approx f(\mathbf{x}_0) + \frac{d}{d\mathbf{x}} f(\mathbf{x}_0) \cdot (\mathbf{x} - \mathbf{x}_0).\]</span></p>
<p>The error in this approximation will again be quadratic in the distance between the two vectors, <span class="math inline">\(||\mathbf{x} - \mathbf{x}_0||^2\)</span>.</p>
<p>The Hessian matrix of second partial derivatives also extends to <span class="math inline">\(n\)</span> dimensional scalar-valued functions <span class="math inline">\(y = f(\mathbf{x})\)</span>. The difference is that instead of just <span class="math inline">\(2^2=4\)</span> second partials, we now have <span class="math inline">\(n^2\)</span> possible second partials. These can be organized into an <span class="math inline">\(n \times n\)</span> matrix</p>
<p><span class="math display">\[
\mathbf{H} = \frac{d^2 y}{d\mathbf{x}^2} =
\begin{pmatrix}
\frac{\partial^2 y}{\partial x_0^2} &amp; \frac{\partial^2 y}{\partial x_0 \partial x_1} &amp; \cdots &amp; \frac{\partial^2 y}{\partial x_0 \partial x_{n-1}} \\
\frac{\partial^2 y}{\partial x_1 \partial x_0} &amp; \frac{\partial^2 y}{\partial x_1^2} &amp; \cdots &amp; \frac{\partial^2 y}{\partial x_1 \partial x_{n-1}} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\frac{\partial^2 y}{\partial x_{n-1} \partial x_0} &amp; \frac{\partial^2 y}{\partial x_{n-1} \partial x_1} &amp; \cdots &amp; \frac{\partial^2 y}{\partial x_{n-1}^2}
\end{pmatrix}.
\]</span></p>
<p>The mixed partials again typically all commute, which means <span class="math inline">\(\mathbf{H}\)</span> is a symmetric matrix, i.e.&nbsp;<span class="math inline">\(\mathbf{H}^\top = \mathbf{H}\)</span>. The eigenvalues of <span class="math inline">\(\mathbf{H}\)</span> again determine the curvature of the function at any point <span class="math inline">\(\mathbf{x}=\mathbf{x}_0\)</span>. If the eigenvalues of the Hessian are all non-negative, <span class="math inline">\(\mathbf{H}\)</span> will be positive semi-definite, i.e.&nbsp;<span class="math inline">\(\mathbf{H} \succcurlyeq 0\)</span>. In the case, the function <span class="math inline">\(f(\mathbf{x})\)</span> will be a convex function and hence bowl upwards. If the Hessian isn’t positive semidefinite it’ll usually have saddlepoints, usually far more saddlepoints than minima or maxima in fact.</p>
<p>We can use the Hessian to define a <strong>second-order approximation</strong> to a multivariate function <span class="math inline">\(f(\mathbf{x})\)</span> near a point <span class="math inline">\(\mathbf{x}_0\)</span>,</p>
<p><span class="math display">\[f(\mathbf{x}) \approx f(\mathbf{x}_0) + \mathbf{g}(\mathbf{x}_0) \cdot (\mathbf{x} - \mathbf{x}_0) + \frac{1}{2}(\mathbf{x} - \mathbf{x}_0)^\top \mathbf{H}(\mathbf{x}_0) (\mathbf{x} - \mathbf{x}_0).\]</span></p>
<p>The error in this approximation will be cubic in the distance between the two vectors, <span class="math inline">\(||\mathbf{x} - \mathbf{x}_0||^3\)</span>.</p>
</section>
<section id="the-jacobian" class="level3" data-number="7.1.5">
<h3 data-number="7.1.5" class="anchored" data-anchor-id="the-jacobian"><span class="header-section-number">7.1.5</span> The Jacobian</h3>
<p>Thus far we’ve seen the following two types of functions:</p>
<ul>
<li>scalar-valued functions of a scalar variable: <span class="math inline">\(y=f(x)\)</span>,</li>
<li>scalar-valued functions of a vector variable: <span class="math inline">\(y = f(\mathbf{x})\)</span>.</li>
</ul>
<p>As you might expect, we can also have the equivalent vector-valued functions:</p>
<ul>
<li>vector-valued functions of a scalar variable: <span class="math inline">\(\mathbf{y} = f(x)\)</span>,</li>
<li>vector-valued functions of a vector variable: <span class="math inline">\(\mathbf{y} = f(\mathbf{x})\)</span>.</li>
</ul>
<p>The most relevant of these two for machine learning purposes is the vector-valued function of a vector variable <span class="math inline">\(\mathbf{y} = f(\mathbf{x})\)</span>. These functions are just extensions of the scalar-valued vector variable functions <span class="math inline">\(y = f(\mathbf{x})\)</span> we’ve been working with so far, except now we can have <span class="math inline">\(m\)</span> scalar-valued functions <span class="math inline">\(y_i = f_i(\mathbf{x})\)</span>, which when put together make up a <em>vector</em> output</p>
<p><span class="math display">\[\mathbf{y} = (y_0,y_1,\cdots,y_{m-1}) = (f_0(\mathbf{x}),f_1(\mathbf{x}),\cdots,f_{m-1}(\mathbf{x})).\]</span></p>
<p>To define the gradient of a vector-valued function, we just take the gradient of each output element <span class="math inline">\(y_i=f_i(\mathbf{x})\)</span>. Doing this over all <span class="math inline">\(m\)</span> output elements will give <span class="math inline">\(m\)</span> gradients each of size <span class="math inline">\(n\)</span>,</p>
<p><span class="math display">\[\frac{dy_0}{d\mathbf{x}}, \ \frac{dy_1}{d\mathbf{x}}, \ \cdots, \ \frac{dy_{m-1}}{d\mathbf{x}}.\]</span></p>
<p>By treating all these gradients as row vectors, we can assemble them into a single <span class="math inline">\(m \times n\)</span> matrix to get <em>the</em> derivative of the vector-valued function <span class="math inline">\(\mathbf{y} = f(\mathbf{x})\)</span>. This matrix is usually called the <strong>Jacobian</strong> matrix, sometimes denoted in short-hand by the symbol <span class="math inline">\(\mathbf{J}\)</span>. It’s defined as the <span class="math inline">\(m \times n\)</span> of all possible first partial derivatives,</p>
<p><span class="math display">\[
\mathbf{J} = \frac{d\mathbf{y}}{d\mathbf{x}} =
\begin{pmatrix}
\frac{\partial y_0}{\partial x_0} &amp; \frac{\partial y_0}{\partial x_1} &amp; \cdots &amp; \frac{\partial y_0}{\partial x_{n-1}} \\
\frac{\partial y_1}{\partial x_0} &amp; \frac{\partial y_1}{\partial x_1} &amp; \cdots &amp; \frac{\partial y_1}{\partial x_{n-1}} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\frac{\partial y_{m-1}}{\partial x_0} &amp; \frac{\partial y_{m-1}}{\partial x_1} &amp; \cdots &amp; \frac{\partial y_{m-1}}{\partial x_{n-1}}
\end{pmatrix}.
\]</span></p>
<p>To see an example of a vector-valued function, consider the function <span class="math inline">\(\mathbf{y} = f(\mathbf{x})\)</span> given by</p>
<p><span class="math display">\[
\mathbf{y} =
\begin{pmatrix}
y_0 \\
y_1
\end{pmatrix} =
\begin{pmatrix}
x_0^3 + x_1^2 \\
2 x_0 - x_1^4 \\
\end{pmatrix}.
\]</span></p>
<p>This is really two functions <span class="math inline">\(y_0 = x_0^3 + x_1^2\)</span> and <span class="math inline">\(y_1 = 2 x_0 - x_1^4\)</span>. Here’s what its Jacobian would look like,</p>
<p><span class="math display">\[
\mathbf{J} =
\begin{pmatrix}
\frac{\partial y_0}{\partial x_0} &amp; \frac{\partial y_0}{\partial x_1} \\
\frac{\partial y_1}{\partial x_0} &amp; \frac{\partial y_1}{\partial x_1}
\end{pmatrix} =
\begin{pmatrix}
3 x_0^2 &amp; 2 x_1  \\
2 &amp; -4 x_1^3
\end{pmatrix}.
\]</span></p>
<p>Notice each row of the Jacobian is the gradient of the elements of <span class="math inline">\(\mathbf{y}\)</span>, as you’d expect,</p>
<p><span class="math display">\[\frac{dy_0}{d\mathbf{x}} = (3x_0^2, 2x_1), \qquad \frac{dy_1}{d\mathbf{x}} = (2, -4x_1^3).\]</span></p>
</section>
<section id="application-the-softmax-function" class="level3" data-number="7.1.6">
<h3 data-number="7.1.6" class="anchored" data-anchor-id="application-the-softmax-function"><span class="header-section-number">7.1.6</span> Application: The Softmax Function</h3>
<p>A more interesting example of vector-valued functions and Jacobians that’s very relevant to machine learning is the <strong>softmax</strong> function, defined by</p>
<p><span class="math display">\[
\mathbf{y} = \text{softmax}(\mathbf{x}) =
\begin{pmatrix}
y_0 \\
y_1 \\
\dots \\
y_{n-1}
\end{pmatrix} =
\begin{pmatrix}
\frac{1}{Z} e^{x_0} \\
\frac{1}{Z} e^{x_1} \\
\dots \\
\frac{1}{Z} e^{x_{n-1}}
\end{pmatrix},
\]</span></p>
<p>where <span class="math inline">\(Z = \sum_k e^{x_k}\)</span> is a normalizing constant, often called the <strong>partition function</strong>. This function shows up in machine learning as a way to create probabilities out of <span class="math inline">\(n\)</span> categories. It takes inputs <span class="math inline">\(x_i\)</span> of any real value and scales them so that <span class="math inline">\(0 \leq y_i \leq 1\)</span> and <span class="math inline">\(\sum_i y_i = 1\)</span>, so that the output is a valid probability vector.</p>
<p>The softmax is useful in defining models for multi-class classification problems, since it can be used to classify things into one of <span class="math inline">\(n\)</span> classes. To classify an object as type <span class="math inline">\(k\)</span>, choose the index <span class="math inline">\(k\)</span> such that <span class="math inline">\(y_k\)</span> is the largest probability in the probability vector <span class="math inline">\(\mathbf{y}\)</span>. More on this in future lessons.</p>
<p>Here’s an example illustrating what the softmax function does. I’ll define a vector <span class="math inline">\(x\)</span> of size <span class="math inline">\(n=5\)</span> by randomly sampling from the interval <span class="math inline">\([-1,1]\)</span>. I’ll use a quick lambda function to implement the softmax. Observe what the softmax seems to do is take the elements of <span class="math inline">\(x\)</span> and re-scale them so they’re all in the interval <span class="math inline">\([0,1]\)</span>. The outputs also indeed sum to one by construction.</p>
<div class="cell" data-execution_count="8">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.random.randn(<span class="dv">5</span>)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'x = </span><span class="sc">{</span>x<span class="sc">.</span><span class="bu">round</span>(<span class="dv">3</span>)<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>softmax <span class="op">=</span> <span class="kw">lambda</span> x: np.exp(<span class="op">-</span>x) <span class="op">/</span> np.<span class="bu">sum</span>(np.exp(<span class="op">-</span>x))</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> softmax(x)</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'y = </span><span class="sc">{</span>y<span class="sc">.</span><span class="bu">round</span>(<span class="dv">3</span>)<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'sum(y) = </span><span class="sc">{</span>y<span class="sc">.</span><span class="bu">sum</span>()<span class="sc">.</span><span class="bu">round</span>(<span class="dv">10</span>)<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>x = [-0.542 -0.126 -0.854  1.209  0.322]
y = [0.276 0.182 0.377 0.048 0.116]
sum(y) = 1.0</code></pre>
</div>
</div>
<p>Note you would <em>not</em> want to implement the softmax this way at scale due to numerical instability. We’ll get back to this stuff in much more depth in a later lesson.</p>
<p>Since we’ll need it later on anyway, let’s go ahead and calculate the Jacobian of the softmax function. Let’s work term by term, focusing on the <span class="math inline">\(j\)</span>th partial derivative of <span class="math inline">\(y_i=\frac{1}{Z} e^{x_i}\)</span>. First, notice that the derivative of the partition function is</p>
<p><span class="math display">\[\frac{\partial Z}{\partial x_j} = \frac{\partial}{\partial x_j} \sum_k e^{x_k} = \sum_k \frac{\partial}{\partial x_j} e^{x_k} = e^{x_j}\]</span></p>
<p>since the only term in the sum containing <span class="math inline">\(x_j\)</span> is <span class="math inline">\(e^{x_j}\)</span>. Using this along with the quotient rule, we thus have</p>
<p><span class="math display">\[
J_{i,j} = \frac{\partial y_i}{\partial x_j} = \frac{\partial}{\partial x_j} \frac{e^{x_i}}{Z} =
\frac{1}{Z^2}\bigg(Z \frac{\partial e^{x_i}}{\partial x_j} - e^{x_i} \frac{\partial Z}{\partial x_j} \bigg) =
\begin{cases}
\frac{e^{x_i}}{Z} \big(1 - \frac{e^{x_i}}{Z}\big), &amp; i = j \\
-\frac{e^{x_i}}{Z} \frac{e^{x_j}}{Z}, &amp; i \neq j
\end{cases} \ = \
\begin{cases}
y_i (1 - y_i), &amp; i = k \\
-y_i y_j &amp; i, \neq k.
\end{cases}
\]</span></p>
<p>Putting all this into the Jacobian matrix, the <span class="math inline">\(i=j\)</span> terms go in the diagonal, and the <span class="math inline">\(i \neq j\)</span> terms go in the off-diagonals, hence</p>
<p><span class="math display">\[
\mathbf{J} =
\begin{pmatrix}
y_0 (1 - y_0) &amp; -y_0 y_1 &amp; \cdots &amp; -y_0 y_{n-1} \\
-y_1 y_0 &amp; y_1 (1 - y_1) &amp; \cdots &amp; -y_1 y_{n-1} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
-y_{n-1} y_0 &amp; -y_{n-1} y_1 &amp; \cdots &amp; y_{n-1} (1 - y_{n-1})
\end{pmatrix}
\]</span></p>
<p>If you play with this expression a little bit, you’ll see we can write this softmax Jacobian efficiently as <span class="math inline">\(\mathbf{J} = \text{diag}(\mathbf{y}) - \mathbf{y} \mathbf{y}^\top\)</span>.</p>
<p>Here’s the Jacobian for the above example where <span class="math inline">\(n=5\)</span>, which I’ll call <code>grad</code>. It’s common in machine learning to blur the distinction between Jacobians and gradients and just call everything a gradient. Notice <code>grad</code> is a <span class="math inline">\(5 \times 5\)</span> matrix.</p>
<div class="cell" data-execution_count="9">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>grad <span class="op">=</span> np.diag(y) <span class="op">-</span> y <span class="op">@</span> y.T</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'grad = </span><span class="ch">\n</span><span class="sc">{</span>grad<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>grad = 
[[ 0.00841128 -0.26767389 -0.26767389 -0.26767389 -0.26767389]
 [-0.26767389 -0.08553925 -0.26767389 -0.26767389 -0.26767389]
 [-0.26767389 -0.26767389  0.10971353 -0.26767389 -0.26767389]
 [-0.26767389 -0.26767389 -0.26767389 -0.21971262 -0.26767389]
 [-0.26767389 -0.26767389 -0.26767389 -0.26767389 -0.15124236]]</code></pre>
</div>
</div>
<p><strong>Aside:</strong> We’ve talked about functions with scalar and vector inputs or outputs. What about functions with matrix or tensor inputs or outputs? We could just as well define scalar-valued functions of a matrix variable <span class="math inline">\(y = f(\mathbf{X})\)</span>, matrix-valued function of a matrix variable <span class="math inline">\(\mathbf{Y} = f(\mathbf{X})\)</span>, etc. The derivative rules extend into these cases as well, but things get a lot more complicated. Taking any kind of derivative of these kinds of functions can cause the rank of the derivative to blow up. For example, the derivative of a (rank-2) matrix with respect to another (rank-2) matrix is now a rank-4 tensor. For at least partly this reason, derivatives of such functions are less commonly used. See <a href="https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf">this</a> comprehensive paper if you’re interested in how to take derivatives of matrices.</p>
</section>
<section id="gradient-and-jacobian-rules" class="level3" data-number="7.1.7">
<h3 data-number="7.1.7" class="anchored" data-anchor-id="gradient-and-jacobian-rules"><span class="header-section-number">7.1.7</span> Gradient and Jacobian Rules</h3>
<p>Here are a few common gradient and Jacobian rules. I’ll state them assuming a vector-valued function with a vector input unless the scalar-valued form looks different, in which case I’ll state it explicitly. Don’t worry too much about how to derive these. Don’t even try to memorize them. This is just a reference.</p>
<table class="table">
<colgroup>
<col style="width: 30%">
<col style="width: 30%">
<col style="width: 39%">
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>Name</strong></td>
<td style="text-align: left;"><strong>Gradient or Jacobian</strong></td>
<td style="text-align: left;"><strong>Scalar Equivalent</strong></td>
</tr>
<tr class="even">
<td style="text-align: left;">Constant Rule</td>
<td style="text-align: left;"><span class="math inline">\(\frac{d}{d\mathbf{x}} (c\mathbf{y}) = c\frac{d\mathbf{y}}{d\mathbf{x}}\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\frac{d}{dx} (cy) = c\frac{dy}{dx}\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Addition Rule</td>
<td style="text-align: left;"><span class="math inline">\(\frac{d}{d\mathbf{x}}(\mathbf{u} + \mathbf{v}) = \frac{d\mathbf{u}}{d\mathbf{x}} + \frac{d\mathbf{v}}{d\mathbf{x}}\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\frac{d}{dx}(u + v) = \frac{du}{dx} + \frac{dv}{dx}\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">Product Rule (scalar-valued)</td>
<td style="text-align: left;"><span class="math inline">\(\frac{d}{d\mathbf{x}}(uv) = u\frac{dv}{d\mathbf{x}} + v\frac{du}{d\mathbf{x}}\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\frac{d}{dx}(uv) = u\frac{dv}{dx} + v\frac{du}{dx}\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Product Rule (dot products)</td>
<td style="text-align: left;"><span class="math inline">\(\frac{d}{d\mathbf{x}}(\mathbf{u}^\top \mathbf{v}) = \mathbf{u}^\top \frac{d\mathbf{v}}{d\mathbf{x}} + \mathbf{v}^\top \frac{d\mathbf{u}}{d\mathbf{x}}\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\frac{d}{dx}(uv) = u\frac{dv}{dx} + v\frac{du}{dx}\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">Chain Rule (scalar-valued, vector-valued)</td>
<td style="text-align: left;"><span class="math inline">\(\frac{dz}{d\mathbf{x}} = \big(\frac{dz}{d\mathbf{y}}\big)^\top \frac{d\mathbf{y}}{d\mathbf{x}}\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\frac{dz}{dx} = \frac{dz}{dy} \frac{dy}{dx}\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Chain Rule (both vector-valued)</td>
<td style="text-align: left;"><span class="math inline">\(\frac{d\mathbf{z}}{d\mathbf{x}} = \frac{d\mathbf{z}}{d\mathbf{y}} \frac{d\mathbf{y}}{d\mathbf{x}}\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\frac{dz}{dx} = \frac{dz}{dy} \frac{dy}{dx}\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">Constant Function</td>
<td style="text-align: left;"><span class="math inline">\(\frac{d}{d\mathbf{x}} \mathbf{c} = \mathbf{0}\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\frac{d}{dx} c = 0\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Squared Two-Norm</td>
<td style="text-align: left;"><span class="math inline">\(\frac{d}{d\mathbf{x}} ||\mathbf{x}||^2 = \frac{d}{d\mathbf{x}} \mathbf{x}^\top \mathbf{x} = 2 \mathbf{x}\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\frac{d}{dx} x^2 = 2x\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">Linear Combination</td>
<td style="text-align: left;"><span class="math inline">\(\frac{d}{d\mathbf{x}} \mathbf{c}^\top \mathbf{x} = \mathbf{c}\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\frac{d}{dx} cx = c\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Symmetric Quadratic Form</td>
<td style="text-align: left;"><span class="math inline">\(\frac{d}{d\mathbf{x}} \mathbf{x}^\top \mathbf{S} \mathbf{x} = 2 \mathbf{S} \mathbf{x}\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\frac{d}{dx} sx^2 = 2sx\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">Affine Function</td>
<td style="text-align: left;"><span class="math inline">\(\frac{d}{d\mathbf{x}} (\mathbf{A}\mathbf{x} + \mathbf{b}) = \mathbf{A}^\top\)</span> or <span class="math inline">\(\frac{d}{d\mathbf{x}} (\mathbf{x}^\top \mathbf{A} + \mathbf{b}) = \mathbf{A}\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\frac{d}{dx} (ax+b) = a\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Squared Error Function</td>
<td style="text-align: left;"><span class="math inline">\(\frac{d}{d\mathbf{x}} ||\mathbf{A}\mathbf{x}-\mathbf{b}||^2 = 2\mathbf{A}^\top (\mathbf{A}\mathbf{x}-\mathbf{b})\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\frac{d}{dx} (ax-b)^2 = 2a(ax-b)\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">Cross Entropy Function</td>
<td style="text-align: left;"><span class="math inline">\(\frac{d}{d\mathbf{x}} (-\mathbf{c}^\top \log \mathbf{x}) = -\frac{\mathbf{c}}{\mathbf{x}}\)</span> (element-wise division)</td>
<td style="text-align: left;"><span class="math inline">\(\frac{d}{dx} (-c \log x) = -\frac{c}{x}\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">ReLU Function</td>
<td style="text-align: left;"><span class="math inline">\(\frac{d}{d\mathbf{x}} \max(\mathbf{0}, \mathbf{x}) = \text{diag}(\mathbf{x} \geq \mathbf{0})\)</span> (element-wise <span class="math inline">\(\geq\)</span>)</td>
<td style="text-align: left;"><span class="math inline">\(\frac{d}{dx} \max(0, x) = \text{$1$ if $x \geq 0$ else $0$}\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">Softmax Function</td>
<td style="text-align: left;"><span class="math inline">\(\frac{d}{d\mathbf{x}} \text{softmax}(\mathbf{x}) = \text{diag}(\mathbf{y}) - \mathbf{y} \mathbf{y}^\top\)</span> where <span class="math inline">\(\mathbf{y} = \text{softmax}(\mathbf{x})\)</span></td>
<td style="text-align: left;"></td>
</tr>
</tbody>
</table>
<p>You <em>can</em> calculate gradients and Jacobians in sympy, though in my opinion it can be kind of painful except in the simplest cases. Here’s an example, where I’ll calculate the Jacobian of the squared error function <span class="math inline">\(||\mathbf{A}\mathbf{x}-\mathbf{b}||^2\)</span>.</p>
<p><strong>Aside:</strong> There is also a nice online <a href="https://www.matrixcalculus.org/">tool</a> that lets you do this somewhat more easily.</p>
<div class="cell" data-execution_count="10">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> sp.Symbol(<span class="st">'m'</span>)</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> sp.Symbol(<span class="st">'n'</span>)</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> sp.MatrixSymbol(<span class="st">'A'</span>, m, n)</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> sp.MatrixSymbol(<span class="st">'x'</span>, n, <span class="dv">1</span>)</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> sp.MatrixSymbol(<span class="st">'b'</span>, m, <span class="dv">1</span>)</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> (A <span class="op">*</span> x <span class="op">-</span> b).T <span class="op">*</span> (A <span class="op">*</span> x <span class="op">-</span> b)</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>dydx <span class="op">=</span> y.diff(x)</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'y = </span><span class="sc">{</span>y<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'dydx = </span><span class="sc">{</span>dydx<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>y = (-b.T + x.T*A.T)*(A*x - b)
dydx = 2*A.T*(A*x - b)</code></pre>
</div>
</div>
</section>
</section>
<section id="multivariate-integration" class="level2" data-number="7.2">
<h2 data-number="7.2" class="anchored" data-anchor-id="multivariate-integration"><span class="header-section-number">7.2</span> Multivariate Integration</h2>
<section id="integration-in-2-dimensions" class="level3" data-number="7.2.1">
<h3 data-number="7.2.1" class="anchored" data-anchor-id="integration-in-2-dimensions"><span class="header-section-number">7.2.1</span> Integration in 2 Dimensions</h3>
<p>We can also integrate multivariate functions like <span class="math inline">\(z=f(x,y)\)</span>. Geometrically these integrals translate into calculating the <em>volume</em> under the surface of <span class="math inline">\(z=f(x,y)\)</span>. I’ll very briefly touch on this.</p>
<p>The idea here is to approximate the volume <span class="math inline">\(V\)</span> under a surface not with <span class="math inline">\(N\)</span> <em>rectangles</em> of width <span class="math inline">\(dx\)</span> and height <span class="math inline">\(f(x)\)</span>, but instead with <span class="math inline">\(N \cdot M\)</span> <em>rectangular prisms</em> of base area <span class="math inline">\(dA = dx \cdot dy\)</span> and height <span class="math inline">\(z=f(x,y)\)</span>,</p>
<p><span class="math display">\[V = \int_R f(x,y) dA = \sum_{n=0}^{N-1} \sum_{m=0}^{M-1} f(x_n,y_m) dxdy = f(x_0,y_0) dxdy + f(x_0,y_1) dxdy + \cdots + f(x_1,y_0) dxdy + \cdots + f(x_{N-1},y_{M-1}) dxdy.\]</span></p>
<p>Rather than integrate from one endpoint <span class="math inline">\(a\)</span> to another endpoint <span class="math inline">\(b\)</span>, we now have to integrate over a 2D region in the xy-plane that I’ll call <span class="math inline">\(R\)</span>.</p>
<p>If <span class="math inline">\(R\)</span> is just a rectangle in the xy-plane, say <span class="math inline">\(R = [a,b] \times [c,d]\)</span> we can break the integral <span class="math inline">\(\int_R dA\)</span> into two integrals <span class="math inline">\(\int_a^b dx\)</span> and <span class="math inline">\(\int_c^d dy\)</span>. If we can <em>also</em> factor <span class="math inline">\(f(x,y) = g(x)h(y)\)</span>, we can further break the integral into a product of two univariate integrals,</p>
<p><span class="math display">\[\int_R f(x,y) dA = \int_a^b \int_c^d f(x,y) dxdy = \bigg(\int_a^b g(x) dx \bigg) \bigg( \int_c^d h(y) dy \bigg).\]</span></p>
<p>As an example, suppose we wanted to integrate the function <span class="math inline">\(f(x,y) = x^2 \sqrt{y}\)</span> over the rectangle <span class="math inline">\(R = [0,1] \times [0,1]\)</span>. This function factors into a product of two functions <span class="math inline">\(g(x) = x^2\)</span> and <span class="math inline">\(h(y) = \sqrt{y}\)</span>. We can thus integrate each individually to get</p>
<p><span class="math display">\[\int_0^1 \int_0^1 x^2 \sqrt{y} dxdy = \bigg(\int_0^1 x^2 dx\bigg) \bigg(\int_0^1 \sqrt{y} dy\bigg) = \frac{1}{3} x^3 \bigg |_{x=0}^1 \cdot \frac{2}{3} y^{3/2} \bigg |_{y=0}^1 = \frac{1}{3} \cdot \frac{2}{3} = \frac{2}{9}.\]</span></p>
<p>In general <span class="math inline">\(R\)</span> won’t be a rectangle, but some arbitrary shape. And <span class="math inline">\(f(x,y)\)</span> won’t usually factor. When this is the case we usually have to fall back to numerical integration methods.</p>
</section>
<section id="application-integrating-the-gaussian" class="level3" data-number="7.2.2">
<h3 data-number="7.2.2" class="anchored" data-anchor-id="application-integrating-the-gaussian"><span class="header-section-number">7.2.2</span> Application: Integrating the Gaussian</h3>
<p>One of the most important functions in machine learning, if not all of science, is the Gaussian function</p>
<p><span class="math display">\[y = e^{-\frac{1}{2} x^2}.\]</span></p>
<p>The Gaussian is the function that gives the well-known bell-curve shape.</p>
<div class="cell" data-execution_count="11">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.arange(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>, <span class="fl">0.1</span>)</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>f <span class="op">=</span> <span class="kw">lambda</span> x:  np.exp(<span class="op">-</span><span class="dv">1</span> <span class="op">/</span> <span class="dv">2</span> <span class="op">*</span> x <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>plot_function(x, f, xlim<span class="op">=</span>(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>), ylim<span class="op">=</span>(<span class="op">-</span><span class="dv">0</span>, <span class="fl">1.5</span>), title<span class="op">=</span><span class="st">'Gaussian Function'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="multivariate-calculus_files/figure-html/cell-12-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>It’s very important in many applications of probability and statistics to be able to integrate the Gaussian function between two points <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>,</p>
<p><span class="math display">\[\int_a^b e^{-\frac{1}{2} x^2} dx.\]</span></p>
<p>Unfortunately, this turns out to be <em>impossible</em> to do analytically, because the Gaussian function has no indefinite integral. No matter how hard you try, you’ll never find an elementary function <span class="math inline">\(F(x)\)</span> whose derivative is <span class="math inline">\(f(x)=e^{-\frac{1}{2} x^2}\)</span>.</p>
<p>One special case, however, where we <em>can</em> integrate the Gaussian analytically is when the region is the whole real line,</p>
<p><span class="math display">\[\int_{-\infty}^\infty e^{-\frac{1}{2} x^2} dx.\]</span></p>
<p>It’s surprising we can even do this. We can do it using a trick. The trick is to <em>square</em> the Gaussian. Consider instead the function</p>
<p><span class="math display">\[f(x,y) = e^{-\frac{1}{2} x^2} e^{-\frac{1}{2} y^2} = e^{-\frac{1}{2} (x^2+y^2)}.\]</span></p>
<p>Consider now the bivariate integral</p>
<p><span class="math display">\[\int_{\mathbb{R}^2} f(x,y) dxdy = \int_{-\infty}^\infty \int_{-\infty}^\infty e^{-\frac{1}{2} (x^2+y^2)} dxdy.\]</span></p>
<p>This integral doesn’t on the face of it look any easier, but we can do something with it that we can’t with the univariate integral: change variables. I won’t go into detail here, but if we define 2 new variables <span class="math inline">\(r\)</span> and <span class="math inline">\(\theta\)</span> (which turn out to be polar coordinates)</p>
<p><span class="math display">\[r^2 = x^2+y^2, \quad \tan \theta = -\frac{y}{x},\]</span></p>
<p>then <span class="math inline">\(dxdy = rdrd\theta\)</span>, and we can re-write the bivariate integral as</p>
<p><span class="math display">\[\int_{\mathbb{R}^2} f(x,y) dxdy = \int_0^\infty \int_0^{2\pi} e^{-\frac{1}{2}r^2} rdrd\theta = \bigg(\int_0^{2\pi} d\theta \bigg) \bigg( \int_0^\infty re^{-\frac{1}{2}r^2} dr \bigg).\]</span></p>
<p>This is just a product of two univariate integrals that we can evaluate. The first integral is easy,</p>
<p><span class="math display">\[\int_0^{2\pi} d\theta = \theta \bigg |_{\theta=0}^{2\pi} = 2\pi.\]</span></p>
<p>The second integral is a little harder, but we can solve it by using another change of variables <span class="math inline">\(u=r^2\)</span>, so <span class="math inline">\(du = 2rdr\)</span>, to get</p>
<p><span class="math display">\[\int_0^\infty re^{-\frac{1}{2}r^2} dr = \int_0^\infty e^{-\frac{1}{2}u} \bigg(\frac{1}{2} du\bigg) = \frac{1}{2} \int_0^\infty e^{-\frac{1}{2}u} du = -e^{-\frac{1}{2}u} \bigg |_{u=0}^\infty = -(e^{-\infty} - 1) = 1,\]</span></p>
<p>since <span class="math inline">\(e^{-\infty} = \frac{1}{e^{\infty}} = \frac{1}{\infty} = 0\)</span>. Putting these together, the bivariate integral is thus</p>
<p><span class="math display">\[\int_{-\infty}^\infty \int_{-\infty}^\infty e^{-\frac{1}{2} (x^2+y^2)} dxdy = 2\pi.\]</span></p>
<p>Since <span class="math inline">\(e^{-\frac{1}{2} (x^2+y^2)} = e^{-\frac{1}{2} x^2} e^{-\frac{1}{2} y^2}\)</span>, we can factor this integral into a product to get</p>
<p><span class="math display">\[2\pi = \int_{-\infty}^\infty \int_{-\infty}^\infty e^{-\frac{1}{2} x^2} dxdy = \bigg(\int_{-\infty}^\infty e^{-\frac{1}{2} x^2} dx \bigg) \bigg(\int_{-\infty}^\infty e^{-\frac{1}{2} y^2} dy\bigg).\]</span></p>
<p>Both of the integrals on the right are the same, so they must equal the same number, call it <span class="math inline">\(A\)</span>. We thus have an equation <span class="math inline">\(A^2 = 2\pi\)</span>, which we can solve to get the area under each integral, which is <span class="math inline">\(A=\sqrt{2\pi}\)</span>. Thus, we’ve arrived at the final result for the univariate integral of the Gaussian,</p>
<p><span class="math display">\[\int_{-\infty}^\infty e^{-\frac{1}{2} x^2} dx = \sqrt{2\pi} \approx 2.507.\]</span></p>
<p>Any time from now on you see the factors of <span class="math inline">\(\sqrt{2\pi}\)</span> in a Gaussian function, this is where they come from.</p>
<p>It’s interesting that we can integrate a function all the way from <span class="math inline">\(-\infty\)</span> to <span class="math inline">\(\infty\)</span> and still get a finite number. This is because Gaussian functions rapidly decay, so most of their area ends up being around <span class="math inline">\(x=0\)</span>. In fact, the interval <span class="math inline">\([-3,3]\)</span> alone contains 99.7% of the area of the under the bell curve!</p>
<p>Here’s the same integral verified using sympy. Note the unusual notation sympy uses for <span class="math inline">\(\infty\)</span>, which is <code>sp.oo</code>.</p>
<div class="cell" data-execution_count="12">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> sp.Symbol(<span class="st">'x'</span>) </span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> sp.exp(<span class="op">-</span>sp.Rational(<span class="dv">1</span>, <span class="dv">2</span>) <span class="op">*</span> x <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>integral <span class="op">=</span> y.integrate((x, <span class="op">-</span>sp.oo, sp.oo))</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'y = </span><span class="sc">{</span>y<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'integral = </span><span class="sc">{</span>integral<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>y = exp(-x**2/2)
integral = sqrt(2)*sqrt(pi)</code></pre>
</div>
</div>
</section>
<section id="integration-in-n-dimensions" class="level3" data-number="7.2.3">
<h3 data-number="7.2.3" class="anchored" data-anchor-id="integration-in-n-dimensions"><span class="header-section-number">7.2.3</span> Integration in <span class="math inline">\(n\)</span> Dimensions</h3>
<p>The same idea extends to <span class="math inline">\(n\)</span> dimensional functions <span class="math inline">\(y=f(x_0,\cdots,x_{n-1})\)</span>. In this case we’re calculating the <span class="math inline">\(n+1\)</span> dimensional <em>hypervolume</em> <span class="math inline">\(V_{n+1}\)</span> under the <span class="math inline">\(n\)</span> dimensional <em>manifold</em> <span class="math inline">\(y=f(x_0,\cdots,x_{n-1})\)</span>. The hyperrectangles would now have base hyperarea <span class="math inline">\(dA_n = dx_0dx_1\cdots dx_{n-1}\)</span> and height <span class="math inline">\(y\)</span>, so</p>
<p><span class="math display">\[V_{n+1} = \int_{R_n} f(x_0,\cdots,x_{n-1}) dA_n = \sum_{\text{all hyperrectangles}} f(x_0,\cdots,x_{n-1}) dx_0dx_1\cdots dx_{n-1}.\]</span></p>
<p>If we’re sufficiently lucky, we can factor a multivariate integral into a product of univariate integrals. We can do this as long as</p>
<ul>
<li>the multivariate function <span class="math inline">\(f(x_0,\cdots,x_{n-1})\)</span> factors into a product of univariate functions <span class="math display">\[f(x_0,x_1,\cdots,x_{n-1}) = f_0(x_0) f_1(x_1) \cdots f_{n-1}(x_{n-1}),\]</span></li>
<li>the integration region <span class="math inline">\(R_n\)</span> is a product of rectangles, <span class="math display">\[R_n = [a_0,b_0] \times [a_1,b_1] \times \cdots \times [a_{n-1},b_{n-1}].\]</span></li>
</ul>
<p>When this is the case, we can simplify the integral to</p>
<p><span class="math display">\[\int_{R_n} f(x_0,x_1,\cdots,x_{n-1}) dA_n = \bigg(\int_{a_0}^{b_0} f_0(x_0) dx_0\bigg) \bigg(\int_{a_1}^{b_1} f_1(x_1) dx_1\bigg) \cdots \bigg(\int_{a_{n-1}}^{b_{n-1}} f_{n-1}(x_{n-1}) dx_{n-1}\bigg).\]</span></p>
<p>We can then evaluate each univariate integral one-by-one and put the results together to get the full multivariate integral.</p>
<p>As a quick example, suppose we wanted to integrate the following multivariate Gaussian function over all space,</p>
<p><span class="math display">\[f(x_0,x_1,\cdots,x_{n-1}) = \exp\bigg(-\frac{1}{2}||\mathbf{x}||^2\bigg) = \exp\bigg(-\frac{1}{2}\sum_{i=0}^{n-1}x_i^2\bigg) = \prod_{i=0}^{n-1} \exp\bigg(-\frac{1}{2}x_i^2\bigg).\]</span></p>
<p>Since each product on the right is independent, the integral splits up into a product itself, so we have</p>
<p><span class="math display">\[\int_{\mathbb{R}^n} f(x_0,x_1,\cdots,x_{n-1}) dx_0dx_1\cdots dx_{n-1} = \prod_{i=0}^{n-1} \int_{-\infty}^\infty \exp\bigg(-\frac{1}{2}x_i^2\bigg) dx_i = \big(\sqrt{2\pi}\big)^n = (2\pi)^{n/2}.\]</span></p>
<p>If you don’t understand what’s going on here, that’s fine. When you see multivariate integrals come up in future lessons, just think of them as a way to calculate the volumes under surfaces. That’s the most important thing to take away.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation column-page-right">
  <div class="nav-page nav-page-previous">
      <a href="../notebooks/matrix-algebra.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Matrix Algebra</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../notebooks/probability.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Basic Probability</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>