<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Mathematics for Machine Learning - 6&nbsp; Matrix Algebra</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../notebooks/multivariate-calculus.html" rel="next">
<link href="../notebooks/vector-spaces.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Matrix Algebra</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Mathematics for Machine Learning</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">Preface</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/basic-math.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Basic Math</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/numerical-computing.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Numerical Computation</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/basic-calculus.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Calculus</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/linear-systems.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Linear Systems</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/vector-spaces.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Vector Spaces</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/matrix-algebra.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Matrix Algebra</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/multivariate-calculus.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Multivariate Calculus</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/probability.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Basic Probability</span></a>
  </div>
</li>
    </ul>
    </div>
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#properties-of-matrices" id="toc-properties-of-matrices" class="nav-link active" data-scroll-target="#properties-of-matrices"><span class="toc-section-number">6.1</span>  Properties of Matrices</a>
  <ul class="collapse">
  <li><a href="#matrix-spaces" id="toc-matrix-spaces" class="nav-link" data-scroll-target="#matrix-spaces"><span class="toc-section-number">6.1.1</span>  Matrix Spaces</a></li>
  <li><a href="#transposes" id="toc-transposes" class="nav-link" data-scroll-target="#transposes"><span class="toc-section-number">6.1.2</span>  Transposes</a></li>
  <li><a href="#inverses" id="toc-inverses" class="nav-link" data-scroll-target="#inverses"><span class="toc-section-number">6.1.3</span>  Inverses</a></li>
  <li><a href="#determinant-and-trace" id="toc-determinant-and-trace" class="nav-link" data-scroll-target="#determinant-and-trace"><span class="toc-section-number">6.1.4</span>  Determinant and Trace</a></li>
  <li><a href="#linear-independence-and-rank" id="toc-linear-independence-and-rank" class="nav-link" data-scroll-target="#linear-independence-and-rank"><span class="toc-section-number">6.1.5</span>  Linear Independence and Rank</a></li>
  <li><a href="#outer-products" id="toc-outer-products" class="nav-link" data-scroll-target="#outer-products"><span class="toc-section-number">6.1.6</span>  Outer Products</a></li>
  </ul></li>
  <li><a href="#special-matrices" id="toc-special-matrices" class="nav-link" data-scroll-target="#special-matrices"><span class="toc-section-number">6.2</span>  Special Matrices</a>
  <ul class="collapse">
  <li><a href="#diagonal-matrices" id="toc-diagonal-matrices" class="nav-link" data-scroll-target="#diagonal-matrices"><span class="toc-section-number">6.2.1</span>  Diagonal Matrices</a></li>
  <li><a href="#symmetric-matrices" id="toc-symmetric-matrices" class="nav-link" data-scroll-target="#symmetric-matrices"><span class="toc-section-number">6.2.2</span>  Symmetric Matrices</a></li>
  <li><a href="#upper-and-lower-triangular-matrices" id="toc-upper-and-lower-triangular-matrices" class="nav-link" data-scroll-target="#upper-and-lower-triangular-matrices"><span class="toc-section-number">6.2.3</span>  Upper and Lower Triangular Matrices</a></li>
  <li><a href="#orthogonal-matrices" id="toc-orthogonal-matrices" class="nav-link" data-scroll-target="#orthogonal-matrices"><span class="toc-section-number">6.2.4</span>  Orthogonal Matrices</a></li>
  <li><a href="#block-matrices" id="toc-block-matrices" class="nav-link" data-scroll-target="#block-matrices"><span class="toc-section-number">6.2.5</span>  Block Matrices</a></li>
  <li><a href="#sparse-matrices" id="toc-sparse-matrices" class="nav-link" data-scroll-target="#sparse-matrices"><span class="toc-section-number">6.2.6</span>  Sparse Matrices</a></li>
  </ul></li>
  <li><a href="#matrix-factorizations" id="toc-matrix-factorizations" class="nav-link" data-scroll-target="#matrix-factorizations"><span class="toc-section-number">6.3</span>  Matrix Factorizations</a>
  <ul class="collapse">
  <li><a href="#lu-factorization" id="toc-lu-factorization" class="nav-link" data-scroll-target="#lu-factorization"><span class="toc-section-number">6.3.1</span>  LU Factorization</a></li>
  <li><a href="#qr-factorization" id="toc-qr-factorization" class="nav-link" data-scroll-target="#qr-factorization"><span class="toc-section-number">6.3.2</span>  QR Factorization</a></li>
  <li><a href="#spectral-decomposition" id="toc-spectral-decomposition" class="nav-link" data-scroll-target="#spectral-decomposition"><span class="toc-section-number">6.3.3</span>  Spectral Decomposition</a></li>
  <li><a href="#positive-definiteness" id="toc-positive-definiteness" class="nav-link" data-scroll-target="#positive-definiteness"><span class="toc-section-number">6.3.4</span>  Positive Definiteness</a></li>
  <li><a href="#singular-value-decomposition" id="toc-singular-value-decomposition" class="nav-link" data-scroll-target="#singular-value-decomposition"><span class="toc-section-number">6.3.5</span>  Singular Value Decomposition</a></li>
  <li><a href="#low-rank-approximations" id="toc-low-rank-approximations" class="nav-link" data-scroll-target="#low-rank-approximations"><span class="toc-section-number">6.3.6</span>  Low-Rank Approximations</a></li>
  </ul></li>
  </ul>
</nav>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content column-page-right" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Matrix Algebra</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>In this lesson I’ll continue on with the topic of linear algebra. So far I’ve covered the basics of matrices and vectors, including how they arise from systems of linear equations, and how they can be understood geometrically via vector spaces and linear maps. In this lesson I’ll focus mainly on understanding matrices directly. Specifically, we’ll look at common matrix operations and important matrix factorizations. I’ll also briefly talk about tensors. Let’s get started.</p>
<div class="cell" data-execution_count="1">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sympy <span class="im">as</span> sp</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> utils.math_ml <span class="im">import</span> <span class="op">*</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>plt.rcParams[<span class="st">"figure.figsize"</span>] <span class="op">=</span> (<span class="dv">4</span>, <span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<section id="properties-of-matrices" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="properties-of-matrices"><span class="header-section-number">6.1</span> Properties of Matrices</h2>
<section id="matrix-spaces" class="level3" data-number="6.1.1">
<h3 data-number="6.1.1" class="anchored" data-anchor-id="matrix-spaces"><span class="header-section-number">6.1.1</span> Matrix Spaces</h3>
<p>Just like vectors, matrices can be thought of as objects in their own <strong>matrix space</strong>. A matrix space is just a vector space, except it has two dimensions <span class="math inline">\(m\)</span> and <span class="math inline">\(n\)</span>. We’ll denote the matrix space of <span class="math inline">\(m \times n\)</span> matrices with the symbol <span class="math inline">\(\mathbb{R}^{m \times n}\)</span>. Just like vector spaces, matrix spaces must be closed under linear combinations. If <span class="math inline">\(\mathbf{A}, \mathbf{B} \in \mathbb{R}^{m \times n}\)</span> are two matrices, then any matrix linear combination <span class="math inline">\(\mathbf{C} = a\mathbf{A} + b\mathbf{B}\)</span> must also be a valid <span class="math inline">\(m \times n\)</span> matrix in <span class="math inline">\(\mathbb{R}^{m \times n}\)</span>. This means matrices behave the same way under addition and scalar multiplication as vectors do.</p>
<p>While this fact should be kind of obvious by now, here’s an example anyway. I’ll choose <span class="math inline">\(\mathbf{A}\)</span> and <span class="math inline">\(\mathbf{B}\)</span> to both be <span class="math inline">\(2 \times 2\)</span> here. Adding them together or scalar multiplying them should also obviously give a matrix that’s <span class="math inline">\(2 \times 2\)</span>, since everything is element-wise.</p>
<div class="cell" data-execution_count="2">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>a <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.array(</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    [[<span class="dv">1</span>, <span class="dv">1</span>], </span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>     [<span class="dv">1</span>, <span class="dv">1</span>]])</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>B <span class="op">=</span> np.array(</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    [[<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>], </span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>     [<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>]])</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'</span><span class="sc">{</span>a<span class="sc">}</span><span class="ss">A = </span><span class="ch">\n</span><span class="sc">{</span><span class="dv">5</span> <span class="op">*</span> A<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'A + B = </span><span class="ch">\n</span><span class="sc">{</span>A <span class="op">+</span> B<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>5A = 
[[5 5]
 [5 5]]
A + B = 
[[2 0]
 [0 2]]</code></pre>
</div>
</div>
<p>Since every matrix corresponds to a linear map <span class="math inline">\(\mathbf{F}(\mathbf{x}) = \mathbf{A}\mathbf{x}\)</span>, the space of matrices also corresponds to the space of linear maps from vectors <span class="math inline">\(\mathbf{x} \in \mathbb{R}^n\)</span> to vectors <span class="math inline">\(\mathbf{y} \in \mathbb{R}^m\)</span>. Recall that the composition of linear maps is equivalent to matrix multiplication. If <span class="math inline">\(\mathbf{F}(\mathbf{y}) = \mathbf{A}\mathbf{y}\)</span> and <span class="math inline">\(\mathbf{G}(\mathbf{x}) = \mathbf{B}\mathbf{x}\)</span> are two linear maps, then their composition is equivalent to the matrix product of the two maps,</p>
<p><span class="math display">\[\mathbf{z}=\mathbf{F}(\mathbf{G}(\mathbf{x})) = \mathbf{A}\mathbf{B}\mathbf{x}.\]</span></p>
<p>The composition, and hence the matrix multiplication operation, only makes sense when the two matrices are <strong>compatible</strong>, i.e.&nbsp;<span class="math inline">\(\mathbf{A} \in \mathbb{R}^{m \times n}\)</span> and <span class="math inline">\(\mathbf{B} \in \mathbb{R}^{n \times p}\)</span>. It also follows from this relationship to linear maps (which are of course just functions) that matrix multiplication is associative, i.e.&nbsp;we can put parenthesis wherever we like,</p>
<p><span class="math display">\[\mathbf{A}\mathbf{B}\mathbf{C} = (\mathbf{A}\mathbf{B})\mathbf{C} = \mathbf{A}(\mathbf{B}\mathbf{C}).\]</span></p>
<p>Do remember, however, that matrix multiplication (and function composition) doesn’t commute, i.e.&nbsp;<span class="math inline">\(\mathbf{A}\mathbf{B} \neq \mathbf{B}\mathbf{A}\)</span>, even when the two matrices <em>are</em> compatible.</p>
</section>
<section id="transposes" class="level3" data-number="6.1.2">
<h3 data-number="6.1.2" class="anchored" data-anchor-id="transposes"><span class="header-section-number">6.1.2</span> Transposes</h3>
<p>Recall that every matrix <span class="math inline">\(\mathbf{A} \in \mathbb{R}^{m \times n}\)</span> has a <strong>transpose</strong> matrix <span class="math inline">\(\mathbf{A}^\top \in \mathbb{R}^{n \times m}\)</span> that’s defined as the same matrix, but with the indices swapped,</p>
<p><span class="math display">\[(A^\top)_{i,j} = A_{j,i}.\]</span></p>
<p>Here’s a quick example for a <span class="math inline">\(2 \times 3\)</span> matrix <span class="math inline">\(\mathbf{A}\)</span>.</p>
<div class="cell" data-execution_count="3">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.array(</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    [[<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>], </span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>     [<span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">6</span>]])</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'A^T = </span><span class="ch">\n</span><span class="sc">{</span>A<span class="sc">.</span>T<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>A^T = 
[[1 4]
 [2 5]
 [3 6]]</code></pre>
</div>
</div>
<p>What happens if we multiply two transposed matrices? Suppose <span class="math inline">\(\mathbf{A}\)</span> is <span class="math inline">\(m \times n\)</span> and <span class="math inline">\(\mathbf{B}\)</span> is <span class="math inline">\(n \times p\)</span>. Then <span class="math inline">\(\mathbf{A}\mathbf{B}\)</span> is <span class="math inline">\(m \times p\)</span>. That means its transpose <span class="math inline">\((\mathbf{A}\mathbf{B})^\top\)</span> should be <span class="math inline">\(p \times m\)</span>. But <span class="math inline">\(\mathbf{A}^\top\)</span> is <span class="math inline">\(n \times m\)</span> and <span class="math inline">\(\mathbf{B}^\top\)</span> is <span class="math inline">\(p \times n\)</span>. This implies that the transpose of the product can only make sense if it’s the product of the transposes, but in opposite order so the shapes match up right,</p>
<p><span class="math display">\[(\mathbf{A}\mathbf{B})^\top = \mathbf{B}^\top \mathbf{A}^\top.\]</span></p>
<p>This is not really a proof of this fact. If you want a proof, what you’ll want to do is look at the individual elements of each side, and show the equation must be true element-by-element. I won’t bore you with this. I’ll just give you an example with numpy so you can see they have to be equal. I’ll take <span class="math inline">\(\mathbf{A}\)</span> to be <span class="math inline">\(3 \times 2\)</span> and <span class="math inline">\(\mathbf{B}\)</span> to be <span class="math inline">\(2 \times 3\)</span>, which means <span class="math inline">\((\mathbf{A}\mathbf{B})^\top\)</span> should be <span class="math inline">\(2 \times 2\)</span>. Recall you can transpose a matrix in numpy using <code>A.T</code> or <code>np.transpose(A)</code>.</p>
<div class="cell" data-execution_count="4">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.array(</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    [[<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>], </span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>     [<span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">6</span>]])</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>B <span class="op">=</span> np.array(</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    [[<span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">2</span>], </span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>     [<span class="op">-</span><span class="dv">3</span>, <span class="op">-</span><span class="dv">4</span>], </span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>     [<span class="op">-</span><span class="dv">5</span>, <span class="op">-</span><span class="dv">6</span>]])</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'(AB)^T = </span><span class="ch">\n</span><span class="sc">{</span>(A <span class="op">@</span> B)<span class="sc">.</span>T<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'B^T A^T = </span><span class="ch">\n</span><span class="sc">{</span>B<span class="sc">.</span>T <span class="op">@</span> A<span class="sc">.</span>T<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>(AB)^T = 
[[-22 -49]
 [-28 -64]]
B^T A^T = 
[[-22 -49]
 [-28 -64]]</code></pre>
</div>
</div>
</section>
<section id="inverses" class="level3" data-number="6.1.3">
<h3 data-number="6.1.3" class="anchored" data-anchor-id="inverses"><span class="header-section-number">6.1.3</span> Inverses</h3>
<p>When a matrix is square, i.e.&nbsp;<span class="math inline">\(\mathbf{A}\)</span> is <span class="math inline">\(n \times n\)</span>, we can think of it as mapping vectors to other vectors in the same vector space <span class="math inline">\(\mathbb{R}^n\)</span>. The identity map (the “do nothing” map) always maps a vector to itself. It corresponds to the <span class="math inline">\(n \times n\)</span> <strong>identity matrix</strong></p>
<p><span class="math display">\[
\mathbf{I} =
\begin{pmatrix}
1 &amp; 0 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; 1 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; 0 &amp; \cdots &amp; 1 \\
\end{pmatrix}.
\]</span></p>
<p>Here’s an example. I’ll use <code>np.eye(n)</code> to generate the identity matrix for <span class="math inline">\(n=5\)</span>.</p>
<div class="cell" data-execution_count="5">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>I <span class="op">=</span> np.eye(<span class="dv">5</span>)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'I = </span><span class="ch">\n</span><span class="sc">{</span>I<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>I = 
[[1. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0.]
 [0. 0. 1. 0. 0.]
 [0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 1.]]</code></pre>
</div>
</div>
<p>Recall the inverse of a square matrix <span class="math inline">\(\mathbf{A}\)</span> is the matrix <span class="math inline">\(\mathbf{A}^{-1}\)</span> satisfying</p>
<p><span class="math display">\[\mathbf{A}^{-1}\mathbf{A} = \mathbf{A}\mathbf{A}^{-1} = \mathbf{I}.\]</span></p>
<p>The inverse matrix <span class="math inline">\(\mathbf{A}^{-1}\)</span> will exist exactly when the <strong>determinant</strong> of <span class="math inline">\(\mathbf{A}\)</span> is nonzero, i.e.&nbsp;<span class="math inline">\(\text{det}(\mathbf{A}) \neq 0\)</span>. If the determinant <em>is</em> zero, then the matrix is <strong>singular</strong>, and no inverse can be found no matter how hard you look for one.</p>
<p>Recall that in numpy you can invert a square matrix using <code>np.linalg.inv(A)</code>. It’s usually not a good idea to do so because inverting a matrix is numerically unstable, but you can in principle. The inverse calculation runs in <span class="math inline">\(O(n^3)\)</span> time just like multiplication.</p>
<p>Here’s an example where <span class="math inline">\(\mathbf{A}\)</span> is <span class="math inline">\(2 \times 2\)</span>. You can already see from this example the numerical loss of precision creeping in, since neither <span class="math inline">\(\mathbf{A}^{-1}\mathbf{A}\)</span> nor <span class="math inline">\(\mathbf{A}\mathbf{A}^{-1}\)</span> exactly yield the identity matrix.</p>
<div class="cell" data-execution_count="6">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.array(</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>    [[<span class="dv">1</span>, <span class="dv">2</span>], </span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>     [<span class="dv">3</span>, <span class="dv">4</span>]])</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>A_inv <span class="op">=</span> np.linalg.inv(A)</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'A^(-1) = </span><span class="ch">\n</span><span class="sc">{</span>A_inv<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'A^(-1) A = </span><span class="ch">\n</span><span class="sc">{</span>A_inv <span class="op">@</span> A<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'A A^(-1) = </span><span class="ch">\n</span><span class="sc">{</span>A <span class="op">@</span> A_inv<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>A^(-1) = 
[[-2.   1. ]
 [ 1.5 -0.5]]
A^(-1) A = 
[[1.00000000e+00 0.00000000e+00]
 [1.11022302e-16 1.00000000e+00]]
A A^(-1) = 
[[1.0000000e+00 0.0000000e+00]
 [8.8817842e-16 1.0000000e+00]]</code></pre>
</div>
</div>
<p>Just like with the transpose, we can ask what happens if we try to invert the product of two matrices. You can convince yourself that the same kind of rule holds: the inverse of a product is the product of the inverses in <em>reverse order</em>,</p>
<p><span class="math display">\[(\mathbf{A}\mathbf{B})^{-1} = \mathbf{B}^{-1} \mathbf{A}^{-1}.\]</span></p>
<p>Here’s a <span class="math inline">\(2 \times 2\)</span> “proof” of this fact.</p>
<div class="cell" data-execution_count="7">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.array(</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>    [[<span class="dv">1</span>, <span class="dv">2</span>], </span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>     [<span class="dv">3</span>, <span class="dv">4</span>]])</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>B <span class="op">=</span> np.array(</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>    [[<span class="dv">1</span>, <span class="dv">0</span>], </span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>     [<span class="dv">1</span>, <span class="dv">1</span>]])</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>A_inv <span class="op">=</span> np.linalg.inv(A)</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>B_inv <span class="op">=</span> np.linalg.inv(B)</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>AB_inv <span class="op">=</span> np.linalg.inv(A <span class="op">@</span> B)</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'(AB)^(-1) = </span><span class="ch">\n</span><span class="sc">{</span>AB_inv<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'B^(-1) A^(-1) = </span><span class="ch">\n</span><span class="sc">{</span>B_inv <span class="op">@</span> A_inv<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>(AB)^(-1) = 
[[-2.   1. ]
 [ 3.5 -1.5]]
B^(-1) A^(-1) = 
[[-2.   1. ]
 [ 3.5 -1.5]]</code></pre>
</div>
</div>
<p>I encourage you to check this result using the fact I derived from the last lesson for <span class="math inline">\(2 \times 2\)</span> matrices,</p>
<p><span class="math display">\[
\mathbf{A} =
\begin{pmatrix}
a &amp; b \\
c &amp; d \\
\end{pmatrix} \quad \Longrightarrow \quad
\mathbf{A}^{-1} = \frac{1}{ad-bc}
\begin{pmatrix}
d &amp; -b \\
-c &amp; a \\
\end{pmatrix}.
\]</span></p>
</section>
<section id="determinant-and-trace" class="level3" data-number="6.1.4">
<h3 data-number="6.1.4" class="anchored" data-anchor-id="determinant-and-trace"><span class="header-section-number">6.1.4</span> Determinant and Trace</h3>
<p>Notice something from this formula. Since <span class="math inline">\(\text{det}(\mathbf{A}) = ad - bc\)</span> in this case, we can evidently write</p>
<p><span class="math display">\[\mathbf{A}^{-1} = \frac{1}{\text{det}(\mathbf{A})} \mathbf{\tilde A},\]</span></p>
<p>where <span class="math inline">\(\mathbf{\tilde A}\)</span> is some kind of matrix related to <span class="math inline">\(\mathbf{A}\)</span>. The properties of <span class="math inline">\(\mathbf{\tilde A}\)</span> aren’t important (it’s called the <em>adjugate</em> if you’re curious). But this general fact turns out to be true for any <span class="math inline">\(n \times n\)</span> matrix, except the formula for the determinant gets a lot more complicated. What’s important is that <span class="math inline">\(\mathbf{A}^{-1}\)</span> is <em>inversely proportional</em> to the determinant. That’s why we can’t allow <span class="math inline">\(\text{det}(\mathbf{A}) = 0\)</span>, because then <span class="math inline">\(\mathbf{A}^{-1}\)</span> blows up due to the division by zero.</p>
<p>Now, I’ve already said <span class="math inline">\((\mathbf{A}\mathbf{B})^{-1} = \mathbf{B}^{-1} \mathbf{A}^{-1}\)</span>. If then</p>
<p><span class="math display">\[\mathbf{A}^{-1} = \frac{1}{\text{det}(\mathbf{A})}\mathbf{\tilde A}, \quad \mathbf{B}^{-1} = \frac{1}{\text{det}(\mathbf{B})}\mathbf{\tilde B},\]</span></p>
<p>it’s evidently the case that</p>
<p><span class="math display">\[(\mathbf{AB})^{-1} = \frac{1}{\text{det}(\mathbf{AB})}\mathbf{\tilde{AB}} = \frac{1}{\text{det}(\mathbf{A}) \cdot \text{det}(\mathbf{B})}\mathbf{\tilde B}\mathbf{\tilde A} = \mathbf{B}^{-1} \mathbf{A}^{-1}.\]</span></p>
<p>Provided that <span class="math inline">\(\mathbf{\tilde{AB}}=\mathbf{\tilde B}\mathbf{\tilde A}\)</span>, which is true, it thus follows that</p>
<p><span class="math display">\[\text{det}(\mathbf{A}\mathbf{B}) = \text{det}(\mathbf{A}) \cdot \text{det}(\mathbf{B}) = \text{det}(\mathbf{B}) \cdot \text{det}(\mathbf{A}).\]</span></p>
<p>Said differently, the determinant of a matrix product is just the product of their individual determinants.</p>
<p>In general, the determinant of an <span class="math inline">\(n \times n\)</span> matrix <span class="math inline">\(\mathbf{A}\)</span> is a nasty <span class="math inline">\(n\)</span> degree multivariate polynomial of the elements of <span class="math inline">\(\mathbf{A}\)</span>. There’s no reliably easy way to calculate it except for small <span class="math inline">\(n\)</span> matrices. In numpy, you can use <code>np.linalg.det(A)</code> to calculate the determinant, but just as with inverses, this is a numerically unstable operation, and so should be avoided where possible. Moreover, it runs in <span class="math inline">\(O(n^3)\)</span> time, which is just as slow as matrix multiplication.</p>
<p>Here’s an example. I’ll verify this “product rule” for determinants using two <span class="math inline">\(3 \times 3\)</span> matrices. The determinant of both matrices turns out to be <span class="math inline">\(6\)</span>, which means their product should have determinant <span class="math inline">\(36\)</span>.</p>
<div class="cell" data-execution_count="8">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.array(</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>    [[<span class="dv">3</span>, <span class="dv">0</span>, <span class="dv">0</span>],</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>     [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">0</span>],</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>     [<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>]])</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>B <span class="op">=</span> np.array(</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>    [[<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>],</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>     [<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">1</span>],</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>     [<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">3</span>]])</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>det_A <span class="op">=</span> np.linalg.det(A)</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>det_B <span class="op">=</span> np.linalg.det(B)</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>det_AB <span class="op">=</span> np.linalg.det(A <span class="op">@</span> B)</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'det(A) = </span><span class="sc">{</span>det_A<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'det(B) = </span><span class="sc">{</span>det_B<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'det(AB) = </span><span class="sc">{</span>det_AB<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>det(A) = 6.0
det(B) = 6.0
det(AB) = 36.0</code></pre>
</div>
</div>
<p>Notice in both cases the determinant happens to be the product of the diagonal elements</p>
<p><span class="math display">\[\text{det}(\mathbf{A}) = \text{det}(\mathbf{B}) = 1 \cdot 2 \cdot 3 = 6.\]</span></p>
<p>I rigged the result to come out this way. It’s not always true. It’s only true when a matrix is either lower triangular (the elements <em>above</em> the diagonal are all zero), upper triangular (the elements <em>below</em> the diagonal are all zero), or diagonal (the elements <em>off</em> the diagonal are all zero). In this example, <span class="math inline">\(\mathbf{A}\)</span> was lower triangular and <span class="math inline">\(\mathbf{B}\)</span> was upper triangular. I chose both to have the same diagonal elements (in different order) on purpose.</p>
<p>More generally, if <span class="math inline">\(\mathbf{A}\)</span> is diagonal or upper/lower triangular, then</p>
<p><span class="math display">\[\text{det}(\mathbf{A}) = \prod_{i=0}^{n-1} A_{i,i} = A_{0,0} A_{1,1} \cdots A_{n-1,n-1}.\]</span></p>
<p>It’s not yet obvious, but we can always “change” a square matrix <span class="math inline">\(\mathbf{A}\)</span> into one of these three kinds of matrices, and then calculate the determinant of <span class="math inline">\(\mathbf{A}\)</span> this way. There are a few ways to do this. I’ll cover these when I get to matrix factorizations below.</p>
<p>Some other properties of the determinant that you can verify are,</p>
<ul>
<li><span class="math inline">\(\text{det}(\mathbf{I}) = 1\)</span>.</li>
<li><span class="math inline">\(\text{det}(\mathbf{A}^\top) = \text{det}(\mathbf{A})\)</span>.</li>
<li><span class="math inline">\(\text{det}(\mathbf{A}^{-1}) = \frac{1}{\text{det}(\mathbf{A})}\)</span>.</li>
<li><span class="math inline">\(\text{det}(c\mathbf{A}) = c^n\text{det}(\mathbf{A})\)</span>.</li>
</ul>
<p>The determinant is one important way to get a scalar out of a matrix. Another useful scalar is the <strong>trace</strong>, which is far simpler to calculate. The trace of a matrix <span class="math inline">\(\mathbf{A}\)</span> is the sum of its diagonal elements, usually written</p>
<p><span class="math display">\[\text{tr}(\mathbf{A}) = \sum_{i=0}^{n-1} A_{i,i} = A_{0,0} + A_{1,1} + \cdots + A_{n-1,n-1}.\]</span></p>
<p>Unlike the determinant, the trace doesn’t split up over products. It instead splits over addition,</p>
<p><span class="math display">\[\text{tr}(\mathbf{A} + \mathbf{B}) = \text{tr}(\mathbf{A}) + \text{tr}(\mathbf{B}).\]</span></p>
<p>This is very easy to verify from the fact that the sum is element-wise, so <span class="math inline">\(\sum (A+B)_{i,i} = \sum A_{i,i} + \sum B_{i,i}\)</span>.</p>
<p>Some other fairly trivial properties the trace satisfies are,</p>
<ul>
<li><span class="math inline">\(\text{tr}(\mathbf{I}) = n\)</span>.</li>
<li><span class="math inline">\(\text{tr}(\mathbf{A}^\top) = \text{tr}(\mathbf{A})\)</span>.</li>
<li><span class="math inline">\(\text{tr}(c\mathbf{A}) = c\text{tr}(\mathbf{A})\)</span>.</li>
<li><span class="math inline">\(\text{tr}(\mathbf{A}\mathbf{B}) = \text{tr}(\mathbf{B}\mathbf{A})\)</span>.</li>
</ul>
<p>Here’s a “proof” of the last result on the same <span class="math inline">\(3 \times 3\)</span> matrices above. In numpy, you can calculate the trace using <code>np.trace</code>. It’s not unstable like the determinant is, and it’s fast to calculate since it’s only summing the <span class="math inline">\(n\)</span> diagonal terms, which is <span class="math inline">\(O(n)\)</span> time.</p>
<div class="cell" data-execution_count="9">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>tr_AB <span class="op">=</span> np.trace(A <span class="op">@</span> B)</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>tr_BA <span class="op">=</span> np.trace(B <span class="op">@</span> A)</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'tr(AB) = </span><span class="sc">{</span>tr_AB<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'tr(BA) = </span><span class="sc">{</span>tr_BA<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>tr(AB) = 13
tr(BA) = 13</code></pre>
</div>
</div>
<p>It’s kind of obvious what the determinant is good for. It tells you how invertible a matrix is. But what does the trace tell you? It turns out both the trace and the determinant also tell you something important about the <em>scale</em> of the matrix. We’ll see this in more depth below when we talk about eigenvalues.</p>
</section>
<section id="linear-independence-and-rank" class="level3" data-number="6.1.5">
<h3 data-number="6.1.5" class="anchored" data-anchor-id="linear-independence-and-rank"><span class="header-section-number">6.1.5</span> Linear Independence and Rank</h3>
<p>We can always think of a matrix in terms of its <strong>column vectors</strong>. If <span class="math inline">\(\mathbf{A}\)</span> is <span class="math inline">\(m \times n\)</span>, it has <span class="math inline">\(n\)</span> column vectors <span class="math inline">\(\mathbf{a}_0, \mathbf{a}_1, \cdots, \mathbf{a}_{n-1}\)</span> each of size <span class="math inline">\(m\)</span>. Concatenated together in order, the column vectors form the matrix itself,</p>
<p><span class="math display">\[
\mathbf{A} =
\begin{pmatrix}
\mathbf{a}_0 &amp; \mathbf{a}_1 &amp; \cdots &amp; \mathbf{a}_{n-1}
\end{pmatrix}.
\]</span></p>
<p>It turns out these column vectors also tell us how invertible a matrix is, but in a more general and useful way than the determinant does. Roughly speaking, a matrix is invertible if we can’t write any one column vector as a function of the other column vectors. This is just the definition of linear independence.</p>
<p>Recall a set of vectors <span class="math inline">\(\mathbf{x}_0, \mathbf{x}_1, \cdots, \mathbf{x}_{k-1}\)</span> is <em>linearly independent</em> if no one vector is a linear combination of the rest,</p>
<p><span class="math display">\[\mathbf{x}_j \neq \sum_{i \neq j} c_i \mathbf{x}_j.\]</span></p>
<p>If one vector <em>is</em> a linear combination of the rest, they’re <em>linearly dependent</em>.</p>
<p>An <span class="math inline">\(n \times n\)</span> matrix <span class="math inline">\(\mathbf{A}\)</span> is invertible if and only if its column vectors are all linearly independent. Equivalently, the column vectors span an <span class="math inline">\(n\)</span>-dimensional vector space. To see why this is true, let’s look at a <span class="math inline">\(2 \times 2\)</span> matrix <span class="math inline">\(\mathbf{A}\)</span> with column vectors <span class="math inline">\(\mathbf{a}=\binom{a}{b}\)</span> and <span class="math inline">\(\mathbf{b}=\binom{c}{d}\)</span>,</p>
<p><span class="math display">\[
\mathbf{A} = \begin{pmatrix} \mathbf{a} &amp; \mathbf{b} \end{pmatrix} =
\begin{pmatrix}
a &amp; b \\
c &amp; d \\
\end{pmatrix}.
\]</span></p>
<p>Now, if <span class="math inline">\(\mathbf{a}\)</span> and <span class="math inline">\(\mathbf{b}\)</span> are linearly <em>dependent</em>, then <span class="math inline">\(\mathbf{b}\)</span> must be a scalar multiple of <span class="math inline">\(\mathbf{a}\)</span>, say <span class="math inline">\(\mathbf{b} = \beta \mathbf{a}\)</span>. Then <span class="math inline">\(\mathbf{A}\)</span> would look like</p>
<p><span class="math display">\[
\mathbf{A} = \begin{pmatrix} \mathbf{a} &amp; \beta \mathbf{a} \end{pmatrix} =
\begin{pmatrix}
a &amp; \beta a \\
c &amp; \beta c \\
\end{pmatrix}.
\]</span></p>
<p>This means its determinant would be <span class="math inline">\(\text{det}(\mathbf{A}) = \beta ac - \beta ac = 0\)</span>, which of course means <span class="math inline">\(\mathbf{A}\)</span> can’t be invertible.</p>
<p>Graphically, saying the column vectors are linearly dependent is saying they’ll map any vector onto the same subspace. For the <span class="math inline">\(2 \times 2\)</span> case, that means any vector <span class="math inline">\(\mathbf{v}\)</span> hit by <span class="math inline">\(\mathbf{A}\)</span> will get mapped onto the same line, no matter what <span class="math inline">\(\mathbf{v}\)</span> you pick. The matrix is collapsing, or <em>projecting</em>, the vector space down to a lower-dimensional subspace.</p>
<p>Here’s a plot of this idea. I’ll make <span class="math inline">\(\mathbf{A}\)</span> have two linearly dependent columns, then plot its action on several different vectors, plotted in black. Acting on these by <span class="math inline">\(\mathbf{A}\)</span> will map them to the red vectors, which all lie on the same line in the plane. They’re all collapsing onto the same subspace, evidently the line <span class="math inline">\(y=-x\)</span>.</p>
<div class="cell" data-execution_count="10">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>beta <span class="op">=</span> <span class="fl">1.5</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>a0 <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>]).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>a1 <span class="op">=</span> beta <span class="op">*</span> a0</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.hstack([a0, a1])</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>v <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">1</span>]).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> np.array([<span class="op">-</span><span class="dv">1</span>, <span class="dv">0</span>]).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>u <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="op">-</span><span class="dv">3</span>]).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>vectors <span class="op">=</span> [x.flatten() <span class="cf">for</span> x <span class="kw">in</span> [v, A <span class="op">@</span> v, w, A <span class="op">@</span> w, u, A <span class="op">@</span> u]]</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>plot_vectors(vectors, colors<span class="op">=</span>[<span class="st">'black'</span>, <span class="st">'red'</span>] <span class="op">*</span> <span class="dv">3</span>, title<span class="op">=</span><span class="st">'Linearly Dependence'</span>,</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>             labels<span class="op">=</span>[<span class="st">'$\mathbf</span><span class="sc">{v}</span><span class="st">$'</span>, <span class="st">'$\mathbf</span><span class="sc">{A}</span><span class="st">\mathbf</span><span class="sc">{v}</span><span class="st">$'</span>] <span class="op">+</span> [<span class="st">''</span>] <span class="op">*</span> <span class="dv">4</span>,</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>             text_offsets<span class="op">=</span>[[<span class="dv">0</span>, <span class="dv">0</span>]] <span class="op">*</span> <span class="dv">6</span>, headwidth<span class="op">=</span><span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="matrix-algebra_files/figure-html/cell-11-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The number of linearly independent column vectors a matrix has is called its <strong>rank</strong>, written <span class="math inline">\(\text{rank}(\mathbf{A})\)</span>. Clearly it’ll always be the case that <span class="math inline">\(\text{rank}(\mathbf{A}) \leq n\)</span>. When <span class="math inline">\(\text{rank}(\mathbf{A}) = n\)</span> exactly the matrix is called <strong>full rank</strong>. Only full rank square matrices are invertible.</p>
<p>Here’s an example. I’ll use <code>np.linalg.matrix_rank(A)</code> to calculate the rank of the above <span class="math inline">\(2 \times 2\)</span> example. Since <span class="math inline">\(\text{rank}(\mathbf{A})=1&lt;2\)</span>, the matrix <span class="math inline">\(\mathbf{A}\)</span> must be singular, as I’ve of course already shown.</p>
<div class="cell" data-execution_count="11">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>rank <span class="op">=</span> np.linalg.matrix_rank(A)</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'rank(A) = </span><span class="sc">{</span>rank<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>rank(A) = 1</code></pre>
</div>
</div>
</section>
<section id="outer-products" class="level3" data-number="6.1.6">
<h3 data-number="6.1.6" class="anchored" data-anchor-id="outer-products"><span class="header-section-number">6.1.6</span> Outer Products</h3>
<p>We’ll frequently be interested in <strong>low rank</strong> matrices, which are matrices whose rank is much much less than the dimension, i.e.&nbsp;<span class="math inline">\(\text{rank}(\mathbf{A}) &lt;&lt; n\)</span>. As we’ll see, low rank matrices are special because they can efficiently compress the information contained in a matrix, which often allows us to represent data more efficiently, or clean up data by denoising away “unnecessary” dimensions. In fact, approximating a matrix with a lower rank matrix is the whole idea behind dimension reduction, one of the core areas of unsupervised learning.</p>
<p>The most useful low-rank matrices are the outer products of two vectors. If <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{y}\)</span> are size <span class="math inline">\(n\)</span> vectors, define their <strong>outer product</strong> by</p>
<p><span class="math display">\[
\mathbf{x} \mathbf{y}^\top =
\begin{pmatrix}
x_0 y_0 &amp; x_0 y_1 &amp; \cdots &amp; x_0 y_{n-1} \\
x_1 y_0 &amp; x_1 y_1 &amp; \cdots &amp; x_1 y_{n-1} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
x_{n-1} y_0 &amp; x_{n-1} y_1 &amp; \cdots &amp; x_{n-1} y_{n-1} \\
\end{pmatrix}.
\]</span></p>
<p>Each column vector <span class="math inline">\(\mathbf{a}_j\)</span> of the outer product matrix is linearly proportional to the first column <span class="math inline">\(\mathbf{a}_0\)</span>, since</p>
<p><span class="math display">\[\mathbf{a}_j = \mathbf{x} y_j = \mathbf{x} y_0 \frac{y_j}{y_0} = \frac{y_j}{y_0} \mathbf{a}_0.\]</span></p>
<p>This means that only one column vector is linearly independent, which implies <span class="math inline">\(\text{rank}(\mathbf{x} \mathbf{y}^\top)=1\)</span>. The outer product is evidently rank-1, and hence highly singular. You’d never be able to invert it. But it is useful as we’ll see soon.</p>
<p>Here’s an example of an outer product calculation. You can either calculate <code>x @ y.T</code> directly or use <code>np.outer(x, y)</code>. Since both vectors are size <span class="math inline">\(3\)</span>, the outer product should be a <span class="math inline">\(3 \times 3\)</span> matrix with rank-1.</p>
<div class="cell" data-execution_count="12">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>])</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.array([<span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">1</span>])</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>outer <span class="op">=</span> np.outer(x, y)</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'xy^T = </span><span class="ch">\n</span><span class="sc">{</span>outer<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'rank(xy^T) = </span><span class="sc">{</span>np<span class="sc">.</span>linalg<span class="sc">.</span>matrix_rank(outer)<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>xy^T = 
[[3 2 1]
 [6 4 2]
 [9 6 3]]
rank(xy^T) = 1</code></pre>
</div>
</div>
<p>You can think of the outer product matrix as a kind of projection matrix. It always projects vectors onto the same one-dimensional line in <span class="math inline">\(\mathbb{R}^n\)</span>. Why? Suppose <span class="math inline">\(\mathbf{v}\)</span> is some vector. If we hit it with the outer product matrix <span class="math inline">\(\mathbf{x} \mathbf{y}^\top\)</span>, using the fact matrix multiplication is associative, we get</p>
<p><span class="math display">\[(\mathbf{x} \mathbf{y}^\top) \mathbf{v} = \mathbf{x} (\mathbf{y}^\top \mathbf{v}) = (\mathbf{y} \cdot \mathbf{v}) \mathbf{x}.\]</span></p>
<p>That is, <span class="math inline">\(\mathbf{v}\)</span> just gets projected onto the space spanned by the vector <span class="math inline">\(\mathbf{x}\)</span>. Evidently the other outer product vector <span class="math inline">\(\mathbf{y}\)</span> determines how long the projection vector will be. Here’s a visual representation of this idea for 2-dimensional vectors. Take</p>
<p><span class="math display">\[\begin{align*}
\mathbf{x} &amp;= (1, 1) \\
\mathbf{y} &amp;= (1, -1) \\
\mathbf{v}_0 &amp;= (-1, 2) \quad &amp;\Longrightarrow \quad (\mathbf{y} \cdot \mathbf{v}_0) \mathbf{x} &amp;= (-3, -3) \\
\mathbf{v}_1 &amp;= (2, 0) \quad &amp;\Longrightarrow \quad (\mathbf{y} \cdot \mathbf{v}_1) \mathbf{x} &amp;= (2, 2) \\
\mathbf{v}_2 &amp;= (2, -1) \quad &amp;\Longrightarrow \quad (\mathbf{y} \cdot \mathbf{v}_2) \mathbf{x} &amp;= (3, 3). \\
\end{align*}\]</span></p>
<p>Applying the outer product <span class="math inline">\(\mathbf{x} \mathbf{y}^\top\)</span> to each <span class="math inline">\(\mathbf{v}_i\)</span> should project each vector onto the space spanned by <span class="math inline">\(\mathbf{x}=(1, 1)\)</span>, which is just the line <span class="math inline">\(y=x\)</span>. Notice the projections are all proportional to <span class="math inline">\((1, 1)\)</span>, as they should be. In the plot below, each vector and its projection have the same color. The outer product vectors <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{y}\)</span> are shown in black.</p>
<div class="cell" data-execution_count="13">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">1</span>]).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>]).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>vs <span class="op">=</span> [np.array([<span class="op">-</span><span class="dv">1</span>, <span class="dv">2</span>]).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>), </span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>      np.array([<span class="dv">2</span>, <span class="dv">0</span>]).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>), </span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>      np.array([<span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>]).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)]</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>ws <span class="op">=</span> [(x <span class="op">@</span> y.T) <span class="op">@</span> v <span class="cf">for</span> v <span class="kw">in</span> vs]</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>vectors <span class="op">=</span> [vector.flatten() <span class="cf">for</span> vector <span class="kw">in</span> vs <span class="op">+</span> ws <span class="op">+</span> [x, y]]</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>plot_vectors(</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>    vectors, colors<span class="op">=</span>[<span class="st">'salmon'</span>, <span class="st">'limegreen'</span>, <span class="st">'steelblue'</span>] <span class="op">*</span> <span class="dv">2</span> <span class="op">+</span> [<span class="st">'black'</span>, <span class="st">'black'</span>], headwidth<span class="op">=</span><span class="dv">5</span>, width<span class="op">=</span><span class="fl">0.01</span>,</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>    labels<span class="op">=</span>[<span class="st">'$\mathbf</span><span class="sc">{v}</span><span class="st">_0$'</span>, <span class="st">'$\mathbf</span><span class="sc">{v}</span><span class="st">_1$'</span>, <span class="st">'$\mathbf</span><span class="sc">{v}</span><span class="st">_2$'</span>] <span class="op">+</span> [<span class="st">''</span>] <span class="op">*</span> <span class="dv">3</span> <span class="op">+</span> [<span class="st">'$\mathbf</span><span class="sc">{x}</span><span class="st">$'</span>, <span class="st">'$\mathbf</span><span class="sc">{y}</span><span class="st">$'</span>],</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>    text_offsets <span class="op">=</span> [[<span class="dv">0</span>, <span class="fl">0.2</span>], [<span class="dv">0</span>, <span class="fl">0.2</span>], [<span class="fl">0.1</span>, <span class="op">-</span><span class="fl">0.3</span>]] <span class="op">+</span> [[<span class="dv">0</span>,<span class="dv">0</span>]] <span class="op">*</span> <span class="dv">3</span> <span class="op">+</span> [[<span class="op">-</span><span class="fl">0.4</span>, <span class="fl">0.15</span>], [<span class="dv">0</span>, <span class="op">-</span><span class="fl">0.3</span>]], ticks_every<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>    title<span class="op">=</span><span class="st">'Outer Product Projections'</span>, zorders<span class="op">=</span>[<span class="dv">0</span>, <span class="dv">5</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">6</span>, <span class="dv">7</span>], xlim<span class="op">=</span>(<span class="op">-</span><span class="fl">3.5</span>, <span class="fl">3.5</span>), ylim<span class="op">=</span>(<span class="op">-</span><span class="fl">3.5</span>, <span class="fl">3.5</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="matrix-algebra_files/figure-html/cell-14-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="special-matrices" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="special-matrices"><span class="header-section-number">6.2</span> Special Matrices</h2>
<p>There are many classes of matrices that have various special properties. I’ll quickly introduce a few that’ll be of interest to us in machine learning.</p>
<section id="diagonal-matrices" class="level3" data-number="6.2.1">
<h3 data-number="6.2.1" class="anchored" data-anchor-id="diagonal-matrices"><span class="header-section-number">6.2.1</span> Diagonal Matrices</h3>
<p>Probably the most basic class of matrices are the diagonal matrices. A <strong>diagonal matrix</strong> is an <span class="math inline">\(m \times n\)</span> matrix <span class="math inline">\(\mathbf{D}\)</span> whose elements are only non-zero on the diagonals, i.e.&nbsp;<span class="math inline">\(D_{i,j} = 0\)</span> if <span class="math inline">\(i \neq j\)</span>. For example, the following <span class="math inline">\(3 \times 3\)</span> matrix is diagonal since its only non-zero values lie on the diagonal,</p>
<p><span class="math display">\[
\mathbf{D} =
\begin{pmatrix}
1 &amp; 0 &amp; 0 \\
0 &amp; 2 &amp; 0 \\
0 &amp; 0 &amp; 3 \\
\end{pmatrix}.
\]</span></p>
<p>We’ve already seen an important diagonal matrix a few times, the identity matrix <span class="math inline">\(\mathbf{I}\)</span>. The identity matrix is the diagonal matrix whose diagonal entries are all ones. It’s common to short-hand a diagonal matrix by just specifying its diagonal entries as a vector. In this notation, we’d use the short-hand</p>
<p><span class="math display">\[\mathbf{D} = \text{diag}(1,2,3).\]</span></p>
<p>to refer to the matrix in the above example. It means exactly the same thing, we’re just only specifying the diagonal elements. This is also the easiest way to define a diagonal matrix in numpy, by using <code>np.diag</code>. Notice that a diagonal matrix contains <span class="math inline">\(n^2\)</span> elements, but we only need to specify <span class="math inline">\(n\)</span> of them to fully determine what the matrix is (i.e.&nbsp;the diagonal elements themselves).</p>
<p>In a sense, a diagonal matrix can only scale a vector it acts on, not rotate it or reflect it. This is because multiplying diagonal matrix with a vector is equivalent to element-wise multiplying the diagonal elements with the vector, which causes each vector component to get stretched by some amount. For example, if <span class="math inline">\(\mathbf{x}=(1,1,1)\)</span>, when the above example <span class="math inline">\(\mathbf{D}\)</span> acts on it, we’d get</p>
<p><span class="math display">\[
\mathbf{D}\mathbf{x} =
\begin{pmatrix}
1 &amp; 0 &amp; 0 \\
0 &amp; 2 &amp; 0 \\
0 &amp; 0 &amp; 3 \\
\end{pmatrix}
\begin{pmatrix}
1 \\
1 \\
1 \\
\end{pmatrix} =
\begin{pmatrix}
1 \\
2 \\
3 \\
\end{pmatrix} =
\begin{pmatrix}
1 \\
2 \\
3 \\
\end{pmatrix} \circ
\begin{pmatrix}
1 \\
1 \\
1 \\
\end{pmatrix}.
\]</span></p>
<p>Here’s an example of how to define a diagonal matrix in numpy using <code>np.diag</code>. I’ll define the same matrix as the above example, and then act on the same vector to show it just scales the entries.</p>
<div class="cell" data-execution_count="14">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>D <span class="op">=</span> np.diag([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>])</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>]).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'D = diag(1,2,3) = </span><span class="ch">\n</span><span class="sc">{</span>D<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Dx = </span><span class="sc">{</span>(D <span class="op">@</span> x)<span class="sc">.</span>flatten()<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>D = diag(1,2,3) = 
[[1 0 0]
 [0 2 0]
 [0 0 3]]
Dx = [1 2 3]</code></pre>
</div>
</div>
</section>
<section id="symmetric-matrices" class="level3" data-number="6.2.2">
<h3 data-number="6.2.2" class="anchored" data-anchor-id="symmetric-matrices"><span class="header-section-number">6.2.2</span> Symmetric Matrices</h3>
<p>Another special class of matrices important to machine learning is the symmetric matrix. A <strong>symmetric matrix</strong> is a square matrix <span class="math inline">\(\mathbf{S}\)</span> that equals its own transpose, i.e.&nbsp;<span class="math inline">\(\mathbf{S}^\top = \mathbf{S}\)</span>. They’re called symmetric matrices because their lower diagonals and upper diagonals are mirror images. Symmetric matrices can be thought of as the matrix equivalent of a real number.</p>
<p>For example, consider the matrix <span class="math display">\[
\mathbf{S} =
\begin{pmatrix}
1 &amp; -1 &amp; -2 \\
-1 &amp; 2 &amp; 1 \\
-2 &amp; 1 &amp; 3 \\
\end{pmatrix}.
\]</span></p>
<p>This matrix is symmetric since the upper diagonal and lower diagonal are the same, i.e.&nbsp;<span class="math inline">\(S_{i,j} = S_{j,i}\)</span>. Symmetric matrices are very important as we’ll see. They’re the matrix generalization of the idea of a real number.</p>
<p>Since the lower diagonal and upper diagonal of a symmetric matrix always equal, we only need to specify what the diagonal and upper diagonal are to fully determine the matrix. If <span class="math inline">\(\mathbf{S}\)</span> contains <span class="math inline">\(n^2\)</span> entries, only <span class="math display">\[n + \frac{1}{2}(n^2 - n) = \frac{1}{2}n(n+1)\]</span></p>
<p>of those elements are actually unique. This fact can be used to shave a lot of time off of algorithms involving symmetric matrices. In numpy, you can check a matrix <span class="math inline">\(\mathbf{S}\)</span> is symmetric by checking that it equals its transpose. Due to numerical roundoff, you may want to wrap the condition inside <code>np.allclose</code>.</p>
<div class="cell" data-execution_count="15">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>S <span class="op">=</span> np.array([</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">2</span>],</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>    [<span class="op">-</span><span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">1</span>],</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>    [<span class="op">-</span><span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">3</span>]])</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>is_symmetric <span class="op">=</span> <span class="kw">lambda</span> A: np.allclose(A, A.T)</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>is_symmetric(S)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="15">
<pre><code>True</code></pre>
</div>
</div>
</section>
<section id="upper-and-lower-triangular-matrices" class="level3" data-number="6.2.3">
<h3 data-number="6.2.3" class="anchored" data-anchor-id="upper-and-lower-triangular-matrices"><span class="header-section-number">6.2.3</span> Upper and Lower Triangular Matrices</h3>
<p>Closely related to diagonal matrices are lower and upper triangular matrices. An <span class="math inline">\(m \times n\)</span> matrix <span class="math inline">\(\mathbf{L}\)</span> is <strong>lower-triangular</strong> if the entries in its <em>upper</em> diagonal are zero, i.e.&nbsp;<span class="math inline">\(L_{i,j} = 0\)</span> when <span class="math inline">\(i &lt; j\)</span>. Similarly, an <span class="math inline">\(m \times n\)</span> matrix <span class="math inline">\(\mathbf{U}\)</span> is <strong>upper-triangular</strong> if the entries in its <em>lower</em> diagonal are zero, i.e.&nbsp;<span class="math inline">\(U_{i,j} = 0\)</span> when <span class="math inline">\(i &gt; j\)</span>. I’ve already showed an example of these when I covered determinants. Here they are again,</p>
<p><span class="math display">\[
\mathbf{L} =
\begin{pmatrix}
3 &amp; 0 &amp; 0 \\
1 &amp; 2 &amp; 0 \\
1 &amp; 1 &amp; 1 \\
\end{pmatrix}, \qquad \mathbf{U} =
\begin{pmatrix}
1 &amp; 1 &amp; 1 \\
0 &amp; 2 &amp; 1 \\
0 &amp; 0 &amp; 3 \\
\end{pmatrix}.
\]</span></p>
<p>Upper and lower triangular (and diagonal) matrices are useful because it’s easy to invert them and calculate their determinants. Just like symmetric matrices, only <span class="math inline">\(\frac{1}{2}n(n+1)\)</span> unique elements are needed to fully specify these matrices since an entire off-diagonal is all zeros.</p>
</section>
<section id="orthogonal-matrices" class="level3" data-number="6.2.4">
<h3 data-number="6.2.4" class="anchored" data-anchor-id="orthogonal-matrices"><span class="header-section-number">6.2.4</span> Orthogonal Matrices</h3>
<p>The next class of matrices I’ll introduce are more subtle, but very important geometrically. These are the orthogonal matrices. An <strong>orthogonal matrix</strong> is an <span class="math inline">\(n \times n\)</span> matrix <span class="math inline">\(\mathbf{Q}\)</span> whose transpose is its inverse, i.e.</p>
<p><span class="math display">\[\mathbf{Q}^\top = \mathbf{Q}^{-1} \quad \text{or} \quad \mathbf{Q}^\top \mathbf{Q}=\mathbf{I}.\]</span></p>
<p>As an example, consider the following matrix,</p>
<p><span class="math display">\[
\mathbf{Q} = \frac{1}{\sqrt{2}}
\begin{pmatrix}
1 &amp; 1 \\
1 &amp; -1 \\
\end{pmatrix} =
\begin{pmatrix}
\frac{1}{\sqrt{2}} &amp; \frac{1}{\sqrt{2}} \\
\frac{1}{\sqrt{2}} &amp; -\frac{1}{\sqrt{2}} \\
\end{pmatrix}.
\]</span></p>
<p>We can check <span class="math inline">\(\mathbf{Q}\)</span> is orthogonal by checking it satisfies the condition <span class="math inline">\(\mathbf{Q}^\top \mathbf{Q}=\mathbf{I}\)</span>,</p>
<p><span class="math display">\[
\mathbf{Q}^\top \mathbf{Q} =
\frac{1}{\sqrt{2}}
\begin{pmatrix}
1 &amp; 1 \\
1 &amp; -1 \\
\end{pmatrix}
\frac{1}{\sqrt{2}}
\begin{pmatrix}
1 &amp; 1 \\
1 &amp; -1 \\
\end{pmatrix} =
\frac{1}{2}
\begin{pmatrix}
2 &amp; 0 \\
0 &amp; 2 \\
\end{pmatrix} =
\begin{pmatrix}
1 &amp; 0 \\
0 &amp; 1 \\
\end{pmatrix} = \mathbf{I}.
\]</span></p>
<p>Notice from this example that the column vectors <span class="math inline">\(\mathbf{q}_0, \mathbf{q}_1\)</span> form an orthonormal basis for <span class="math inline">\(\mathbb{R}^2\)</span>, since</p>
<p><span class="math display">\[\mathbf{q}_0 \cdot \mathbf{q}_1 = 0, \quad \mathbf{q}_0 \cdot \mathbf{q}_0 = \mathbf{q}_1 \cdot \mathbf{q}_1 = 1.\]</span></p>
<p>This is a general fact. The column vectors of an orthogonal matrix <span class="math inline">\(\mathbf{Q}\)</span> form a complete set of orthonormal basis vectors for <span class="math inline">\(\mathbb{R}^n\)</span>. Conversely, we can always form an orthogonal matrix by first finding an orthonormal basis and then creating column vectors out of the basis vectors. This is usually the way orthogonal matrices are constructed in practice using algorithms like the <a href="https://en.wikipedia.org/wiki/Gram%E2%80%93Schmidt_process">Gram-Schmidt Algorithm</a>.</p>
<p>It’s not at all obvious, but the fact that the column vectors of <span class="math inline">\(\mathbf{Q}\)</span> form an orthonormal basis constrains the number of unique elements <span class="math inline">\(\mathbf{Q}\)</span> is allowed to have. Requiring each <span class="math inline">\(\mathbf{q}_i\)</span> means <span class="math inline">\(n\)</span> total elements are already determined. The further requirement that the column vectors be mutually orthogonal determines another <span class="math inline">\(\frac{1}{2}n(n-1)\)</span>. This means <span class="math inline">\(\mathbf{Q}\)</span> only has <span class="math inline">\(n^2 - n - \frac{1}{2}n(n-1) = \frac{1}{2}n(n-1)\)</span> unique elements. For example, when <span class="math inline">\(\mathbf{Q}\)</span> is <span class="math inline">\(2 \times 2\)</span> it only has <span class="math inline">\(\frac{1}{2}2(2-1)=1\)</span> unique element. The other <span class="math inline">\(3\)</span> are all determined by that one element. This unique element can be thought of as a rotation angle. I’ll come back to this in a minute.</p>
<p>An important fact about orthogonal matrices is that they preserve the dot products between vectors. If <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{y}\)</span> are two vectors, then</p>
<p><span class="math display">\[(\mathbf{Q} \mathbf{x}) \cdot (\mathbf{Q}\mathbf{y}) = \mathbf{x} \cdot \mathbf{y}.\]</span></p>
<p>This follows from the fact that <span class="math inline">\((\mathbf{Q} \mathbf{x})^\top (\mathbf{Q} \mathbf{y}) = \mathbf{x}^\top \mathbf{Q}^\top\mathbf{Q}\mathbf{y} = \mathbf{x}^\top \mathbf{I} \mathbf{y} = \mathbf{x}^\top \mathbf{y}\)</span>. Since the dot product encodes the notions of length and angle, this fact implies that orthogonal matrices can’t change the lengths of vectors, nor the angles between vectors. Orthogonal matrices preserve the <em>geometry</em> of the vector space.</p>
<p>This fact suggests some deep intuition about what orthogonal matrices do. If they can’t change the lengths of vectors or the angles between them, then all they can do is <em>rotate</em> vectors or <em>reflect</em> them across some line. In fact, it turns out any <span class="math inline">\(2 \times 2\)</span> orthogonal matrix can be written in the form</p>
<p><span class="math display">\[
\mathbf{Q} =
\begin{pmatrix}
\cos \theta &amp; \mp \sin \theta \\
\sin \theta &amp; \pm \cos \theta \\
\end{pmatrix},
\]</span></p>
<p>where <span class="math inline">\(\theta\)</span> is some angle (expressed in radians). When the right column vector is <span class="math inline">\(\binom{-\sin\theta}{\cos\theta}\)</span>, <span class="math inline">\(\mathbf{Q}\)</span> is a pure <strong>rotation matrix</strong>. It will rotate any vector in the plane by an angle <span class="math inline">\(\theta\)</span>, counterclockwise if <span class="math inline">\(\theta &gt; 0\)</span>, and clockwise if <span class="math inline">\(\theta &lt; 0\)</span>. When the right column vector is <span class="math inline">\(\binom{\sin\theta}{-\cos\theta}\)</span>, <span class="math inline">\(\mathbf{Q}\)</span> becomes a <strong>reflection matrix</strong>; it’ll reflect vectors about the line at an angle <span class="math inline">\(\frac{\theta}{2}\)</span> with the x-axis. The combination of these two together can generate any 2D rotation or reflection.</p>
<p>Here’s a visual of this idea. I’ll take the unit vector <span class="math inline">\(\mathbf{e}_x=(1,0)\)</span> and use <span class="math inline">\(\mathbf{Q}\)</span> to rotate it by some angle, in this case <span class="math inline">\(\theta = 45^\circ\)</span>. Note the need to convert the angle to radians by multiplying the angle in degrees by <span class="math inline">\(\frac{\pi}{180}\)</span>. You should be able to confirm that the red vector is indeed the black vector <span class="math inline">\(\mathbf{e}_x\)</span> rotated counterclockwise by <span class="math inline">\(45^\circ\)</span> to the new vector <span class="math inline">\(\mathbf{Q}\mathbf{e}_x = 2^{-1/2}(1,1)\)</span>. The factor of <span class="math inline">\(2^{-1/2}\)</span> appears to keep the vector normalized to unit length.</p>
<div class="cell" data-execution_count="16">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>theta_degrees <span class="op">=</span> <span class="dv">45</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>theta <span class="op">=</span> theta_degrees <span class="op">*</span> np.pi <span class="op">/</span> <span class="dv">180</span></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>Q <span class="op">=</span> np.array([</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>    [np.cos(theta), <span class="op">-</span>np.sin(theta)], </span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>    [np.sin(theta), np.cos(theta)]])</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>ex <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">0</span>]).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>Qex <span class="op">=</span> Q <span class="op">@</span> ex</span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>plot_vectors([ex.flatten(), Qex.flatten()], colors<span class="op">=</span>[<span class="st">'black'</span>, <span class="st">'red'</span>], title<span class="op">=</span><span class="ss">f'$</span><span class="sc">{</span>theta_degrees<span class="sc">}</span><span class="ss">^\circ$ Rotation'</span>,</span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>             labels<span class="op">=</span>[<span class="st">'$\mathbf</span><span class="sc">{e}</span><span class="st">_x$'</span>, <span class="st">'$\mathbf</span><span class="sc">{Q}</span><span class="st">\mathbf</span><span class="sc">{e}</span><span class="st">_x$'</span>], text_offsets<span class="op">=</span>[[<span class="op">-</span><span class="fl">0.1</span>, <span class="fl">0.1</span>], [<span class="dv">0</span>, <span class="dv">0</span>]],</span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a>             ticks_every<span class="op">=</span><span class="dv">1</span>, xlim<span class="op">=</span>(<span class="op">-</span><span class="fl">0.5</span>, <span class="fl">1.5</span>), ylim<span class="op">=</span>(<span class="op">-</span><span class="fl">0.5</span>, <span class="fl">1.5</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="matrix-algebra_files/figure-html/cell-17-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>I’ll finish this section by noting that orthogonal matrices always have determinant <span class="math inline">\(\pm 1\)</span>. You can see this by applying the determinant product formula to <span class="math inline">\(\mathbf{Q}^\top \mathbf{Q}=\mathbf{I}\)</span>,</p>
<p><span class="math display">\[1 = \text{det}(\mathbf{I}) = \text{det}(\mathbf{Q}^\top \mathbf{Q}) = \text{det}(\mathbf{Q}^\top) \cdot \text{det}(\mathbf{Q}) = \big(\text{det}(\mathbf{Q})\big)^2,\]</span></p>
<p>which implies <span class="math inline">\(\text{det}(\mathbf{Q}) = \pm 1\)</span>. This evidently divides orthogonal matrices into two distinct classes:</p>
<ul>
<li><span class="math inline">\(\text{det}(\mathbf{Q}) = +1\)</span>: These are the orthogonal matrices that correspond to pure rotations.</li>
<li><span class="math inline">\(\text{det}(\mathbf{Q}) = -1\)</span>: These are the orthogonal matrices that correspond to reflections.</li>
</ul>
<p>I’ll verify that the rotation matrix I just plotted indeed has determinant <span class="math inline">\(+1\)</span>.</p>
<div class="cell" data-execution_count="17">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'det(Q) = </span><span class="sc">{</span>np<span class="sc">.</span>linalg<span class="sc">.</span>det(Q)<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>det(Q) = 1.0</code></pre>
</div>
</div>
</section>
<section id="block-matrices" class="level3" data-number="6.2.5">
<h3 data-number="6.2.5" class="anchored" data-anchor-id="block-matrices"><span class="header-section-number">6.2.5</span> Block Matrices</h3>
<p>Sometimes a matrix might look kind of diagonal or triangular, but not exactly. For example, consider the following matrix,</p>
<p><span class="math display">\[
\mathbf{A} =
\begin{pmatrix}
1 &amp; 2 &amp; 0 &amp; 0 \\
3 &amp; 4 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 5 &amp; 6 \\
0 &amp; 0 &amp; 7 &amp; 8 \\
\end{pmatrix}.
\]</span></p>
<p>This matrix looks kind of diagonal, but not exactly. Notice, however, that we can think about this matrix as being composed of four sub-matrices, or <strong>blocks</strong>,</p>
<p><span class="math display">\[
\mathbf{A}_{0,0} =
\begin{pmatrix}
1 &amp; 2 \\
3 &amp; 4 \\
\end{pmatrix}, \quad
\mathbf{A}_{0,1} =
\begin{pmatrix}
0 &amp; 0 \\
0 &amp; 0 \\
\end{pmatrix}, \quad
\mathbf{A}_{1,0} =
\begin{pmatrix}
0 &amp; 0 \\
0 &amp; 0 \\
\end{pmatrix}, \quad
\mathbf{A}_{1,1} =
\begin{pmatrix}
5 &amp; 6 \\
7 &amp; 8 \\
\end{pmatrix}.
\]</span></p>
<p>If we think of <span class="math inline">\(\mathbf{A}\)</span> in terms of these 4 blocks, we can write it simply as</p>
<p><span class="math display">\[
\mathbf{A} =
\begin{pmatrix}
1 &amp; 2 &amp; 0 &amp; 0 \\
3 &amp; 4 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 5 &amp; 6 \\
0 &amp; 0 &amp; 7 &amp; 8 \\
\end{pmatrix} =
\begin{pmatrix}
\begin{pmatrix} 1 &amp; 2 \\ 3 &amp; 4 \\ \end{pmatrix} &amp; \begin{pmatrix} 0 &amp; 0 \\ 0 &amp; 0 \\ \end{pmatrix} \\
\begin{pmatrix} 0 &amp; 0 \\ 0 &amp; 0 \\ \end{pmatrix} &amp; \begin{pmatrix} 5 &amp; 6 \\ 7 &amp; 8 \\ \end{pmatrix} \\
\end{pmatrix} =
\begin{pmatrix}
\mathbf{A}_{0,0} &amp; \mathbf{A}_{0,1} \\
\mathbf{A}_{1,0} &amp; \mathbf{A}_{1,1} \\
\end{pmatrix}.
\]</span></p>
<p>Where the braces are doesn’t really effect anything other than how the elements are indexed. What matters is we can express this <span class="math inline">\(4 \times 4\)</span> matrix as a <span class="math inline">\(2 \times 2\)</span> <strong>block matrix</strong> that semantically represents the exact same matrix. Notice that in block form <span class="math inline">\(\mathbf{A}\)</span> is now a diagonal matrix. We call a matrix that can be blocked into diagonal form like this <strong>block diagonal</strong>. Block diagonal matrices are the most useful of the block matrices. If a block matrix is upper or lower triangular, we’d call it a <strong>block upper triangular</strong> or <strong>block lower triangular</strong> matrix, respectively.</p>
<p>When matrices are in block form, you can manipulate them exactly the way you would if they weren’t. The only difference is that you have to remember matrix multiplication doesn’t commute. For example, we could write <span class="math inline">\(\mathbf{A}^2 = \mathbf{A}\mathbf{A}\)</span> in block form as</p>
<p><span class="math display">\[
\mathbf{A}^2 = \mathbf{A}\mathbf{A} =
\begin{pmatrix}
\mathbf{A}_{0,0} &amp; \mathbf{A}_{0,1} \\
\mathbf{A}_{1,0} &amp; \mathbf{A}_{1,1} \\
\end{pmatrix}
\begin{pmatrix}
\mathbf{A}_{0,0} &amp; \mathbf{A}_{0,1} \\
\mathbf{A}_{1,0} &amp; \mathbf{A}_{1,1} \\
\end{pmatrix} =
\begin{pmatrix}
\mathbf{A}_{0,0}\mathbf{A}_{0,0} + \mathbf{A}_{0,1}\mathbf{A}_{1,0}  &amp; \mathbf{A}_{0,0}\mathbf{A}_{0,1} + \mathbf{A}_{0,1}\mathbf{A}_{1,1} \\
\mathbf{A}_{1,0}\mathbf{A}_{0,0} + \mathbf{A}_{1,1}\mathbf{A}_{1,0} &amp; \mathbf{A}_{1,0}\mathbf{A}_{0,1} + \mathbf{A}_{1,1}\mathbf{A}_{1,1} \\
\end{pmatrix}.
\]</span></p>
<p>This would give the exact same answer as multiplying <span class="math inline">\(\mathbf{A}^2 = \mathbf{A}\mathbf{A}\)</span> in regular form, except we’d have extra braces floating around that we can ignore.</p>
<p>Now, you might ask why we even care about blocking matrices like this. Probably the most important reason we care is <em>hardware</em>. Computer memory is typically divided into a sequence of fixed-sized blocks. When we want to operate on an array, the system has to go into memory and fetch where those array values are stored, perform the array operation, and then place the answer back into memory. During the fetch step, it will take a long time if the array elements are located far away from each other in different blocks. For this reason, programs tend to place array elements nearby each other in memory. But when one block runs out, the program has to go find a new block of memory to place the other elements. This suggests that if we want to efficiently fetch array elements from memory, we should do so block by block. That is, we should find a way to partition the array so each block of the array fits in the same block of memory. If we do this, we can perform operations much faster than we would if the program had to search new blocks every time it needed to perform an operation.</p>
<p>The best example of this is matrix multiplication. While it might take <span class="math inline">\(O(n^3)\)</span> time to multiply two arrays <em>in theory</em>, don’t forget that asymptotic notation has a hidden constant term out front that we ignore. In real life, that constant can make a big difference. If we try to multiply two matrices without doing any blocking, we’d have a much larger constant than if we first blocked the matrices into blocks that would fit in one block of memory. In fact, this is what the LAPACK routines behind functions like <code>np.matmul</code> do. They don’t just naively multiply two matrices by running over a triple loop. They first block both matrices into sub-matrix blocks that fit efficiently in memory, and then run the triple loop block-style before putting everything back together. It’s for this reason more than anything else that numpy array methods run much faster than anything you’d write in python. When it comes to array operations, numpy and the LAPACK routines will swim laps around anything you’d code up yourself.</p>
</section>
<section id="sparse-matrices" class="level3" data-number="6.2.6">
<h3 data-number="6.2.6" class="anchored" data-anchor-id="sparse-matrices"><span class="header-section-number">6.2.6</span> Sparse Matrices</h3>
<p>A very useful class of matrices in applications are the sparse matrices. <strong>Sparse matrices</strong> are defined by the property that most of their entries are zero. Only a sparse number of elements are non-zero. When a matrix is sparse, we can often more efficiently store its elements using a different data structure that only keeps track of the non-zero elements and where they occur in the matrix. We can then define matrix algorithms in a way that they only operate on the non-zero entries, which can considerably speed up computation.</p>
<p>For example, suppose an <span class="math inline">\(n \times n\)</span> matrix <span class="math inline">\(\mathbf{A}\)</span> is sparse, with <span class="math inline">\(k \ll n\)</span> non-zero entries. If we wanted to multiply <span class="math inline">\(\mathbf{A}\)</span> with itself, it would usually take <span class="math inline">\(O(n^3)\)</span> time and require <span class="math inline">\(O(n^2)\)</span> words of memory. But, since <span class="math inline">\(\mathbf{A}\)</span> is sparse, we could multiply it with itself in <span class="math inline">\(O(k^3)\)</span> and use only <span class="math inline">\(O(k)\)</span> words of memory. The speedup comes from the fact that we only need to keep track of the non-zero elements when adding and multiplying elements in the matrix, which means we only need to keep track of the <span class="math inline">\(k\)</span> non-zero elements in the multiplication.</p>
<p>For ordinary sized matrices, treating them as sparse doesn’t really benefit you much. It’s when matrices get <em>huge</em> that sparse methods can be useful. One example of this that comes up in machine learning is when we want to represent a corpus of text as a matrix of data. In that case, each row would be a document of text, and each column would be a word in the vocabulary of all possible words. Vocabulary sizes can get huge, often millions of words. If you have, say, 10,000 documents, that means you’d have a 10,000 by 1,000,000 matrix of data, which is pretty huge. Fortunately, any one document only contain a handful of words in the total vocabulary. This means the data matrix is sparse, and we can efficiently manipulate it using sparse methods.</p>
<p>Numpy doesn’t have any direct methods to work with sparse matrices, but scipy does. To define a matrix as sparse in scipy, use <code>scipy.sparse.csr_matrix</code>. This will encode a sparse matrix using a <em>CSR matrix</em>, which is one of several ways to efficiently represent a sparse matrix. Once we’ve encoded a matrix as sparse, we can more or less use any of the operations we’re used to. To convert a sparse matrix back to a normal, <em>dense</em> matrix, use <code>A.todense()</code>. Here’s an example. I’ll convert the following matrix into sparse form,</p>
<p><span class="math display">\[
\mathbf{A} =
\begin{pmatrix}
1 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 2 &amp; 0 &amp; 0 \\
3 &amp; 0 &amp; 0 &amp; 4 \\
0 &amp; 5 &amp; 0 &amp; 6 \\
\end{pmatrix}.
\]</span></p>
<p>Notice how it’s only keeping track of the non-zero values and where in the matrix they occur.</p>
<div class="cell" data-execution_count="18">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.sparse <span class="im">import</span> csr_matrix</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.array([</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>], </span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">0</span>], </span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">3</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">4</span>], </span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">0</span>, <span class="dv">5</span>, <span class="dv">0</span>, <span class="dv">6</span>]])</span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a>A_sparse <span class="op">=</span> csr_matrix(A)</span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'A_sparse = </span><span class="ch">\n</span><span class="sc">{</span>A_sparse<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>A_sparse = 
  (0, 0)    1
  (1, 1)    2
  (2, 0)    3
  (2, 3)    4
  (3, 1)    5
  (3, 3)    6</code></pre>
</div>
</div>
<div class="cell" data-execution_count="19">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>A_dense <span class="op">=</span> A_sparse.todense()</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'A_dense = </span><span class="ch">\n</span><span class="sc">{</span>A_dense<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>A_dense = 
[[1 0 0 0]
 [0 2 0 0]
 [3 0 0 4]
 [0 5 0 6]]</code></pre>
</div>
</div>
</section>
</section>
<section id="matrix-factorizations" class="level2" data-number="6.3">
<h2 data-number="6.3" class="anchored" data-anchor-id="matrix-factorizations"><span class="header-section-number">6.3</span> Matrix Factorizations</h2>
<p>Given any two compatible matrices <span class="math inline">\(\mathbf{A}\)</span> and <span class="math inline">\(\mathbf{B}\)</span>, we can get a third matrix <span class="math inline">\(\mathbf{C}\)</span> by matrix multiplication, <span class="math inline">\(\mathbf{C} = \mathbf{A}\mathbf{B}\)</span>. Now suppose we wanted to go the other way. Given a matrix <span class="math inline">\(\mathbf{C}\)</span>, how can we <em>factor</em> it back out into a product <span class="math inline">\(\mathbf{A}\mathbf{B}\)</span>? This is the idea behind matrix factorization. In practice, we’re interested in factoring a matrix into a product of special types of matrices that are easier to work with, like symmetric, diagonal, or orthogonal matrices.</p>
<section id="lu-factorization" class="level3" data-number="6.3.1">
<h3 data-number="6.3.1" class="anchored" data-anchor-id="lu-factorization"><span class="header-section-number">6.3.1</span> LU Factorization</h3>
<p>Probably the most basic matrix factorization is the LU Factorization. LU factorization factors an <span class="math inline">\(m \times n\)</span> matrix <span class="math inline">\(\mathbf{A}\)</span> into a product of a lower triangular matrix <span class="math inline">\(\mathbf{L}\)</span> and an upper triangular matrix <span class="math inline">\(\mathbf{U}\)</span>, <span class="math display">\[\mathbf{A} = \mathbf{L}\mathbf{U}.\]</span></p>
<p>The LU factorization is most useful for solving a system of linear equations. If <span class="math inline">\(\mathbf{A}\mathbf{x}=\mathbf{b}\)</span>, we can do an LU factorization of <span class="math inline">\(\mathbf{A}\)</span> and write the system as <span class="math inline">\(\mathbf{LUx} = \mathbf{b}\)</span>. This can then be solved by breaking it into two steps, known as <em>forward substitution</em> and <em>back substitution</em>,</p>
<ul>
<li>Forward substitution: Solve <span class="math inline">\(\mathbf{Ly} = \mathbf{b}\)</span> for <span class="math inline">\(\mathbf{y}\)</span>.</li>
<li>Back Substitution: Solve <span class="math inline">\(\mathbf{Ux} = \mathbf{y}\)</span> for <span class="math inline">\(\mathbf{x}\)</span>.</li>
</ul>
<p>These two steps are easy to do since each system can be solved by substitution, working from the “tip” of the triangle down. The LU factorization is essentially what matrix solvers like <code>np.linalg.solve</code> do to solve linear systems.</p>
<p>Of course, the question still remains how to actually factor <span class="math inline">\(\mathbf{A}\)</span> into <span class="math inline">\(\mathbf{L}\mathbf{U}\)</span>. I won’t describe the algorithm to do this, or any matrix factorization really, since their inner workings aren’t that relevant to machine learning. If you’re curious, LU factorization is done using some variant of an algorithm known as <a href="https://en.wikipedia.org/wiki/Gaussian_elimination">Gaussian Elimination</a>. Note the LU factorization in general is a cubic time algorithm, i.e.&nbsp;<span class="math inline">\(O(n^3)\)</span> if <span class="math inline">\(\mathbf{A}\)</span> is <span class="math inline">\(n \times n\)</span>.</p>
<p>The LU factorization can also be used to compute the determinant of a square matrix. Since <span class="math inline">\(\mathbf{L}\)</span> and <span class="math inline">\(\mathbf{U}\)</span> are triangular, their determinant is just the product of their diagonals. Using the product rule for determinants then gives</p>
<p><span class="math display">\[\text{det}(\mathbf{A}) = \text{det}(\mathbf{LU}) = \text{det}(\mathbf{L}) \cdot \text{det}(\mathbf{U}) = \prod_{i=0}^{n-1} L_{i,i} \cdot U_{i,i}.\]</span></p>
<p>The LU factorization can also be used to compute the inverse of a square matrix. The idea is to solve the <em>matrix system</em> of equations</p>
<p><span class="math display">\[\mathbf{A} \mathbf{X} = \mathbf{I},\]</span></p>
<p>assuming <span class="math inline">\(\mathbf{X}=\mathbf{A}^{-1}\)</span> are the <span class="math inline">\(n^2\)</span> unknown variables you’re solving for. This system can be solved by using the same technique of forward substitution plus back substitution. Note that solving for both the determinant and inverse this way each takes <span class="math inline">\(O(n^3)\)</span> time due to the LU decomposition. This is one reason why you should probably avoid calculating these quantities explicitly unless you really need them.</p>
<p>Strangely, numpy doesn’t have a built-in LU factorization solver, but scipy does using <code>scipy.linalg.lu</code>. It factors a matrix into not two, but three products, <span class="math inline">\(\mathbf{A}=\mathbf{PLU}\)</span>. The <span class="math inline">\(\mathbf{P}\)</span> is a <em>permutation matrix</em>. It just accounts for the fact that sometimes you need to swap the rows before doing the LU factorization. I won’t go into that. Here’s the LU factorization of the above example matrix. I’ll also verify that <span class="math inline">\(\mathbf{A}=\mathbf{LU}\)</span>.</p>
<div class="cell" data-execution_count="20">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.linalg <span class="im">import</span> lu</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.array([[<span class="dv">1</span>, <span class="dv">1</span>], </span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>]])</span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>P, L, U <span class="op">=</span> lu(A)</span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'L = </span><span class="ch">\n</span><span class="sc">{</span>L<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'U = </span><span class="ch">\n</span><span class="sc">{</span>U<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'LU = </span><span class="ch">\n</span><span class="sc">{</span>L <span class="op">@</span> U<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>L = 
[[1. 0.]
 [1. 1.]]
U = 
[[ 1.  1.]
 [ 0. -2.]]
LU = 
[[ 1.  1.]
 [ 1. -1.]]</code></pre>
</div>
</div>
</section>
<section id="qr-factorization" class="level3" data-number="6.3.2">
<h3 data-number="6.3.2" class="anchored" data-anchor-id="qr-factorization"><span class="header-section-number">6.3.2</span> QR Factorization</h3>
<p>Another useful factorization is to factor an <span class="math inline">\(m \times n\)</span> matrix <span class="math inline">\(\mathbf{A}\)</span> into a product of an <span class="math inline">\(m \times m\)</span> orthogonal matrix <span class="math inline">\(\mathbf{Q}\)</span> and an <span class="math inline">\(m \times n\)</span> upper triangular matrix <span class="math inline">\(\mathbf{R}\)</span>,</p>
<p><span class="math display">\[\mathbf{A} = \mathbf{QR}.\]</span></p>
<p>The QR factorization is useful if we want to create an orthonormal basis out of the column vectors of <span class="math inline">\(\mathbf{A}\)</span>, since <span class="math inline">\(\mathbf{Q}\)</span> will give a complete set of basis vectors built from orthogonalizing <span class="math inline">\(\mathbf{A}\)</span>. It’s also useful for calculating other random things of interest. Like LU factorization, it can be used to calculate determinants, since</p>
<p><span class="math display">\[\text{det}(\mathbf{A}) = \text{det}(\mathbf{QR}) = \text{det}(\mathbf{Q}) \cdot \text{det}(\mathbf{R}) = 1 \cdot \text{det}(\mathbf{R}) = \prod_{i=0}^{n-1} R_{i,i}.\]</span></p>
<p>It can also be used to find the inverse matrix. Use the fact that <span class="math inline">\(\mathbf{A}^{-1} = (\mathbf{QR})^{-1} = \mathbf{R}^{-1} \mathbf{Q}^\top\)</span>, since <span class="math inline">\(\mathbf{Q}\)</span> is orthogonal. The matrix <span class="math inline">\(\mathbf{R}^{-1}\)</span> can be calculated efficiently via back-substitution since <span class="math inline">\(\mathbf{R}\)</span> just a triangular matrix. Both the determinant and inverse calculation again take <span class="math inline">\(O(n^3)\)</span> time because the QR factorization does.</p>
<p>In practice, the QR factorization is done using algorithms like the <a href="https://en.wikipedia.org/wiki/Gram%E2%80%93Schmidt_process">Gram-Schmidt method</a> or <a href="https://en.wikipedia.org/wiki/Householder_reflection">Householder reflections</a>. Just like LU factorization, QR factorization is in general an <span class="math inline">\(O(n^3)\)</span> algorithm. In numpy, you can get the QR factorization using <code>np.linalg.qr(A)</code>.</p>
<p>The exact QR factorization I described is technically called the <em>full</em> QR factorization, since it orthogonalizes all of the columns, even if <span class="math inline">\(\mathbf{A}\)</span> isn’t full rank. Usually by default the algorithms only orthogonalize the first <span class="math inline">\(r=\text{rank}(\mathbf{A})\)</span> columns. If you want to return the full QR factorization in numpy, you need to pass in the keyword argument <code>mode = 'complete'</code>.</p>
<p>Here’s the full QR factorization of the same matrix from before.</p>
<div class="cell" data-execution_count="21">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.array([[<span class="dv">1</span>, <span class="dv">1</span>], </span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>]])</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>Q, R <span class="op">=</span> np.linalg.qr(A, mode<span class="op">=</span><span class="st">'complete'</span>)</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Q = </span><span class="ch">\n</span><span class="sc">{</span>Q<span class="sc">.</span><span class="bu">round</span>(<span class="dv">10</span>)<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'R = </span><span class="ch">\n</span><span class="sc">{</span>R<span class="sc">.</span><span class="bu">round</span>(<span class="dv">10</span>)<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'QR = </span><span class="ch">\n</span><span class="sc">{</span>Q <span class="op">@</span> R<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Q = 
[[-0.70710678 -0.70710678]
 [-0.70710678  0.70710678]]
R = 
[[-1.41421356  0.        ]
 [ 0.         -1.41421356]]
QR = 
[[ 1.  1.]
 [ 1. -1.]]</code></pre>
</div>
</div>
</section>
<section id="spectral-decomposition" class="level3" data-number="6.3.3">
<h3 data-number="6.3.3" class="anchored" data-anchor-id="spectral-decomposition"><span class="header-section-number">6.3.3</span> Spectral Decomposition</h3>
<p>The spectral decomposition is a way to factor a symmetric matrix <span class="math inline">\(\mathbf{S}\)</span> into a product of an orthonormal matrix <span class="math inline">\(\mathbf{X}\)</span> and a diagonal matrix <span class="math inline">\(\mathbf{\Lambda}\)</span>,</p>
<p><span class="math display">\[\mathbf{S} = \mathbf{X \Lambda X}^\top.\]</span></p>
<p>The matrix <span class="math inline">\(\mathbf{\Lambda}\)</span> is called the <strong>eigenvalue matrix</strong>, and <span class="math inline">\(\mathbf{X}\)</span> is the <strong>eigenvector matrix</strong>. The diagonal entries of <span class="math inline">\(\mathbf{\Lambda}\)</span> are called the <strong>eigenvalues</strong> of <span class="math inline">\(\mathbf{S}\)</span>, denoted <span class="math inline">\(\lambda_i\)</span>,</p>
<p><span class="math display">\[\mathbf{\Lambda} = \text{diag}(\lambda_0, \lambda_1, \cdots, \lambda_n).\]</span></p>
<p>The column vectors of <span class="math inline">\(\mathbf{X}\)</span> are called the <strong>eigenvectors</strong> of <span class="math inline">\(\mathbf{S}\)</span>, denoted <span class="math inline">\(\mathbf{x}_i\)</span>,</p>
<p><span class="math display">\[\mathbf{X} = \begin{pmatrix} \mathbf{x}_0 &amp; \mathbf{x}_1 &amp; \cdots &amp; \mathbf{x}_{n-1} \end{pmatrix}.\]</span></p>
<p>Eigenvalues and eigenvectors arise from trying to find special “characteristic” lines in the vector space <span class="math inline">\(\mathbb{R}^n\)</span> that stay fixed when acted on by <span class="math inline">\(\mathbf{S}\)</span>. Let <span class="math inline">\(\mathbf{x}\)</span> be the unit vector along one of these lines. Saying <span class="math inline">\(\mathbf{S}\)</span> can’t rotate <span class="math inline">\(\mathbf{x}\)</span> is equivalent to saying it can only <em>scale</em> <span class="math inline">\(\mathbf{x}\)</span> by some value <span class="math inline">\(\lambda\)</span>. Finding these special characteristic lines is thus equivalent to solving the equation</p>
<p><span class="math display">\[\mathbf{S}\mathbf{x} = \lambda \mathbf{x}\]</span></p>
<p>for <span class="math inline">\(\lambda\)</span> and <span class="math inline">\(\mathbf{x}\)</span>. The vector <span class="math inline">\(\mathbf{x}\)</span> is the eigenvector (German for “characteristic vector”). The scalar <span class="math inline">\(\lambda\)</span> is its corresponding eigenvalue (German for “characteristic value”). We can rewrite this equation as <span class="math inline">\((\mathbf{S} - \lambda \mathbf{I})\mathbf{x} = \mathbf{0}\)</span>, where <span class="math inline">\(\mathbf{0}\)</span> is the zero vector. Taking the determinant of <span class="math inline">\(\mathbf{S} - \lambda \mathbf{I}\)</span> and insisting it must be singular gives a polynomial equation, called the <strong>characteristic equation</strong>, that can (in principle) be solved for the eigenvalue <span class="math inline">\(\lambda\)</span>,</p>
<p><span class="math display">\[\text{det}(\mathbf{S} - \lambda \mathbf{I}) = 0.\]</span></p>
<p>For example, if <span class="math inline">\(\mathbf{S}\)</span> is a symmetric <span class="math inline">\(2 \times 2\)</span> matrix, we have</p>
<p><span class="math display">\[
\mathbf{S} =
\begin{pmatrix}
a &amp; b \\
b &amp; d \\
\end{pmatrix} \quad \Longrightarrow \quad
\mathbf{S} - \lambda \mathbf{I} =
\begin{pmatrix}
a-\lambda &amp; b \\
b &amp; d-\lambda \\
\end{pmatrix} \quad \Longrightarrow \quad
\text{det}(\mathbf{S} - \lambda \mathbf{I}) = (a - \lambda)(d - \lambda) - b^2 = \lambda^2 - (a + d)\lambda + (ad-b^2) = 0.
\]</span></p>
<p>Notice that <span class="math inline">\(\text{tr}(\mathbf{S}) = a + d\)</span> and <span class="math inline">\(\text{det}(\mathbf{S}) = ad-b^2\)</span>, so the characteristic equation in this special <span class="math inline">\(2 \times 2\)</span> cases reduces to</p>
<p><span class="math display">\[\lambda^2 - \text{tr}(\mathbf{S})\lambda + \text{det}(\mathbf{S}) = 0.\]</span></p>
<p>This is a quadratic equation whose solution is the two eigenvalues <span class="math inline">\(\lambda_0, \lambda_1\)</span>. Once the eigenvalues are known, they can be plugged back into the linear equation <span class="math inline">\((\mathbf{S} - \lambda \mathbf{I})\mathbf{x} = \mathbf{0}\)</span> to solve for the eigenvectors <span class="math inline">\(\mathbf{x}_0, \mathbf{x}_1\)</span>, e.g.&nbsp;using LU factorization.</p>
<p>Just to put some numbers in, take the following specific <span class="math inline">\(2 \times 2\)</span> matrix</p>
<p><span class="math display">\[
\mathbf{S} =
\begin{pmatrix}
2 &amp; 1 \\
1 &amp; 2 \\
\end{pmatrix}.
\]</span></p>
<p>Since <span class="math inline">\(\text{tr}(\mathbf{S})=2+2=4\)</span> and <span class="math inline">\(\text{det}(\mathbf{S})=2 \cdot 2 - 1 \cdot 1 = 3\)</span>, the characteristic equation is</p>
<p><span class="math display">\[\lambda^2 - 4\lambda + 3 = 0 \quad \Longrightarrow \quad (\lambda-1)(\lambda - 3) = 0 \quad \Longrightarrow \quad \lambda=1, 3.\]</span></p>
<p>The eigenvalues for this matrix are thus <span class="math inline">\(\lambda_0 = 3\)</span> and <span class="math inline">\(\lambda_1 = 1\)</span>. Note it’s conventional to order the eigenvalues from largest to smallest, though it isn’t required. The eigenvectors are gotten by solving the two systems</p>
<p><span class="math display">\[
(\mathbf{S} - \lambda_0 \mathbf{I})\mathbf{x}_0 = \mathbf{0} \quad \Longrightarrow \quad
\begin{pmatrix}
2-3 &amp; 1 \\
1 &amp; 2-3 \\
\end{pmatrix}
\begin{pmatrix}
x_0 \\
y_0 \\
\end{pmatrix} =
\begin{pmatrix}
0 \\
0 \\
\end{pmatrix} \quad \Longrightarrow \quad
\mathbf{x}_0 =
\frac{1}{\sqrt{2}}
\begin{pmatrix}
1 \\
1 \\
\end{pmatrix} \approx
\begin{pmatrix}
0.707 \\
0.707 \\
\end{pmatrix},
\]</span></p>
<p><span class="math display">\[
(\mathbf{S} - \lambda_1 \mathbf{I})\mathbf{x}_1 = \mathbf{0} \quad \Longrightarrow \quad
\begin{pmatrix}
2-1 &amp; 1 \\
1 &amp; 2-1 \\
\end{pmatrix}
\begin{pmatrix}
x_1 \\
y_1 \\
\end{pmatrix} =
\begin{pmatrix}
0 \\
0 \\
\end{pmatrix} \quad \Longrightarrow \quad
\mathbf{x}_1 =
\frac{1}{\sqrt{2}}
\begin{pmatrix}
1 \\
-1 \\
\end{pmatrix} \approx
\begin{pmatrix}
0.707 \\
-0.707 \\
\end{pmatrix}.
\]</span></p>
<p>You can easily check that <span class="math inline">\(\mathbf{x}_0\)</span> and <span class="math inline">\(\mathbf{x}_1\)</span> are orthogonal. Note the eigenvectors here have been normalized so <span class="math inline">\(||\mathbf{x}_0||=||\mathbf{x}_1||=1\)</span>. This isn’t required, but it’s the most common convention to ensure the eigenvector matrix <span class="math inline">\(\mathbf{X}\)</span> is a properly orthogonal.</p>
<p>Here’s a plot of what this looks like. I’ll show that <span class="math inline">\(\mathbf{v}_0=\sqrt{2}\mathbf{x}_0=(1,1)\)</span> gets scaled by a factor of <span class="math inline">\(\lambda_0=3\)</span> when acted on by <span class="math inline">\(\mathbf{S}\)</span>. Similarly, I’ll show that <span class="math inline">\(\mathbf{v}_1=\sqrt{2}\mathbf{x}_1=(1,-1)\)</span> gets scaled by a factor of <span class="math inline">\(\lambda_1=1\)</span> (i.e.&nbsp;not at all) when acted on by <span class="math inline">\(\mathbf{S}\)</span>. Importantly, notice that <span class="math inline">\(\mathbf{S}\)</span> doesn’t rotate either vector. They stay along their characteristic lines, or <strong>eigenspaces</strong>, which in this example are the lines <span class="math inline">\(y=\pm x\)</span>.</p>
<div class="cell" data-code_folding="[]" data-execution_count="22">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>S <span class="op">=</span> np.array([</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">2</span>, <span class="dv">1</span>], </span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">1</span>, <span class="dv">2</span>]])</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>v0 <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">1</span>]).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a>Sv0 <span class="op">=</span> S <span class="op">@</span> v0</span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a>v1 <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>]).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a>Sv1 <span class="op">=</span> S <span class="op">@</span> v1</span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a>vectors <span class="op">=</span> [x.flatten() <span class="cf">for</span> x <span class="kw">in</span> [v0, Sv0, v1, Sv1]]</span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a>plot_vectors(</span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a>    vectors, colors<span class="op">=</span>[<span class="st">'black'</span>, <span class="st">'red'</span>, <span class="st">'black'</span>, <span class="st">'blue'</span>], xlim<span class="op">=</span>(<span class="op">-</span><span class="dv">1</span>, <span class="dv">4</span>), ylim<span class="op">=</span>(<span class="op">-</span><span class="dv">2</span>, <span class="dv">4</span>), zorders<span class="op">=</span>[<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">3</span>], </span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a>    labels<span class="op">=</span>[<span class="st">'$\mathbf</span><span class="sc">{v}</span><span class="st">_0$'</span>, <span class="st">'$\mathbf</span><span class="sc">{S}</span><span class="st">\mathbf</span><span class="sc">{v}</span><span class="st">_0$'</span>, <span class="st">'$\mathbf</span><span class="sc">{v}</span><span class="st">_1$'</span>, <span class="st">'$\mathbf</span><span class="sc">{S}</span><span class="st">\mathbf</span><span class="sc">{v}</span><span class="st">_1$'</span>],</span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a>    text_offsets<span class="op">=</span>[[<span class="op">-</span><span class="fl">0.45</span>, <span class="fl">0.25</span>], [<span class="fl">0.05</span>, <span class="fl">0.15</span>], [<span class="fl">0.1</span>, <span class="op">-</span><span class="fl">0.5</span>], [<span class="fl">0.05</span>, <span class="fl">0.3</span>]], </span>
<span id="cb39-13"><a href="#cb39-13" aria-hidden="true" tabindex="-1"></a>    title<span class="op">=</span><span class="st">'Eigenspaces of $\mathbf</span><span class="sc">{S}</span><span class="st">$'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="matrix-algebra_files/figure-html/cell-23-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>A result I won’t prove, called the <strong>spectral theorem</strong>, guarantees that the eigenvalues of a symmetric matrix will be real-valued, and that the eigenvectors will form an orthonormal basis for <span class="math inline">\(\mathbb{R}^n\)</span>. This is why <span class="math inline">\(\mathbf{X}\)</span> ends up being an orthogonal matrix. The fact that the eigenvalues have to be real is why we can think of symmetric matrices as the matrix generalization of a real number.</p>
<p>The spectral decomposition <span class="math inline">\(\mathbf{S} = \mathbf{X \Lambda X}^\top\)</span> is just a matrix way of writing the individual equations <span class="math inline">\(\mathbf{S}\mathbf{x} = \lambda \mathbf{x}\)</span>. Grouping the eigenvectors and eigenvalues into matrices, we can write these equations in one go as <span class="math inline">\(\mathbf{S}\mathbf{X} = \mathbf{\Lambda} \mathbf{X}\)</span>, which is just the spectral decomposition.</p>
<p>Back to our working example, putting the eigenvalues and eigenvectors into their respective matrices gives</p>
<p><span class="math display">\[
\mathbf{\Lambda} =
\begin{pmatrix}
3 &amp; 0 \\
0 &amp; 1 \\
\end{pmatrix}, \qquad
\mathbf{X} =
\frac{1}{\sqrt{2}}
\begin{pmatrix}
1 &amp; 1 \\
1 &amp; -1 \\
\end{pmatrix}.
\]</span></p>
<p>That is, the symmetric matrix <span class="math inline">\(\mathbf{S}\)</span> factorizes into the spectral decomposition</p>
<p><span class="math display">\[
\mathbf{S} = \mathbf{X \Lambda X}^\top =
\frac{1}{\sqrt{2}}
\begin{pmatrix}
1 &amp; 1 \\
1 &amp; -1 \\
\end{pmatrix}
\begin{pmatrix}
3 &amp; 0 \\
0 &amp; 1 \\
\end{pmatrix}
\frac{1}{\sqrt{2}}
\begin{pmatrix}
1 &amp; 1 \\
1 &amp; -1 \\
\end{pmatrix}.
\]</span></p>
<p>We can find the spectral decomposition of a symmetric matrix in numpy using <code>np.linalg.eigh(S)</code>. Note that <code>np.linalg.eig(S)</code> will also work, but <code>eigh</code> is more efficient for symmetric matrices than <code>eig</code>. In either case, they return a pair of arrays, the first being the <em>diagonals</em> of <span class="math inline">\(\mathbf{\Lambda}\)</span>, the second being <span class="math inline">\(\mathbf{X}\)</span>. I’ll also verify that the spectral decomposition indeed gives <span class="math inline">\(\mathbf{S}\)</span>.</p>
<div class="cell" data-execution_count="23">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>S <span class="op">=</span> np.array([[<span class="dv">2</span>, <span class="dv">1</span>], </span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">1</span>, <span class="dv">2</span>]])</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>lambdas, X <span class="op">=</span> np.linalg.eigh(S)</span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>Lambda <span class="op">=</span> np.diag(lambdas)</span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Lambda = </span><span class="ch">\n</span><span class="sc">{</span>Lambda<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'X = </span><span class="ch">\n</span><span class="sc">{</span>X<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'X Lambda X^T = </span><span class="ch">\n</span><span class="sc">{</span>X <span class="op">@</span> Lambda <span class="op">@</span> X<span class="sc">.</span>T<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Lambda = 
[[1. 0.]
 [0. 3.]]
X = 
[[-0.70710678  0.70710678]
 [ 0.70710678  0.70710678]]
X Lambda X^T = 
[[2. 1.]
 [1. 2.]]</code></pre>
</div>
</div>
<p>Notice something from the example I just worked. It turns out that <span class="math inline">\(\text{tr}(\mathbf{S}) = 4 = \lambda_0 + \lambda_1\)</span> and <span class="math inline">\(\text{det}(\mathbf{S}) = 3 = \lambda_0 \lambda_1\)</span>. This fact turns out to always be true for <span class="math inline">\(n \times n\)</span> symmetric matrices, namely if <span class="math inline">\(\mathbf{S}\)</span> has eigenvalues <span class="math inline">\(\lambda_0, \lambda_1, \cdots, \lambda_{n-1}\)</span>, then</p>
<p><span class="math display">\[\begin{align*}
\text{tr}(\mathbf{S}) &amp;= \sum_{i=0}^{n-1} \lambda_i = \lambda_0 + \lambda_1 + \cdots + \lambda_{n-1}, \\
\text{det}(\mathbf{S}) &amp;= \prod_{i=0}^{n-1} \lambda_i = \lambda_0 \cdot \lambda_1 \cdots \lambda_{n-1}.
\end{align*}\]</span></p>
<p>This fact implies that <span class="math inline">\(\mathbf{S}\)</span> will be invertible if and only if all the eigenvalues are non-zero, since otherwise we’d have <span class="math inline">\(\text{det}(\mathbf{S})=0\)</span>.</p>
<p>Given how important the spectral decomposition is to many applications, there are a lot of different algorithms for finding it, each with its own trade-offs. One popular algorithm for doing so is the <em>QR algorithm</em>. Roughly speaking, the QR algorithm works as follows:</p>
<ul>
<li>Start with <span class="math inline">\(\mathbf{S}_0 = \mathbf{S}\)</span>.</li>
<li>For some number of iterations <span class="math inline">\(t=0,1,\cdots, T-1\)</span> do the following:
<ul>
<li>Calculate the QR factorization of <span class="math inline">\(\mathbf{S}_t\)</span>: <span class="math inline">\(\mathbf{Q}_{t+1}, \mathbf{R}_{t+1} = \text{qr}(\mathbf{S}_t)\)</span>.</li>
<li>Update <span class="math inline">\(\mathbf{S}_t\)</span> by reversing the factorization order: <span class="math inline">\(\mathbf{S}_{t+1} = \mathbf{R}_{t+1} \mathbf{Q}_{t+1}\)</span>.</li>
</ul></li>
<li>Take <span class="math inline">\(\mathbf{\Lambda} \approx \mathbf{S}_{T-1}\)</span> and <span class="math inline">\(\mathbf{X} \approx \mathbf{Q}_{T-1}\)</span>.</li>
</ul>
<p>Due to the QR factorizations and matrix multiplications, this algorithm will be <span class="math inline">\(O(n^3)\)</span> at each step, which all together gives a time complexity of <span class="math inline">\(O(Tn^3)\)</span>. It’s not at all obvious from what I’ve said why the QR algorithm even works. In fact, to work well it requires a few small <a href="https://en.wikipedia.org/wiki/QR_algorithm">modifications</a> I won’t go into.</p>
</section>
<section id="positive-definiteness" class="level3" data-number="6.3.4">
<h3 data-number="6.3.4" class="anchored" data-anchor-id="positive-definiteness"><span class="header-section-number">6.3.4</span> Positive Definiteness</h3>
<p>The eigenvalues of a symmetric matrix <span class="math inline">\(\mathbf{S}\)</span> are important because they in some sense specify how much <span class="math inline">\(\mathbf{S}\)</span> tends to stretch vectors in different directions. Most important for machine learning purposes though is the <em>sign</em> of the eigenvalues. The sign of the eigenvalues of a symmetric matrix essentially determine how hard it is to optimize a given function. This is especially relevant in machine learning, since training a model is all about optimizing the loss function of a model’s predictions against the data.</p>
<p>If <span class="math inline">\(\mathbf{S}\)</span> is <span class="math inline">\(n \times n\)</span>, it will have <span class="math inline">\(n\)</span> eigenvalues <span class="math inline">\(\lambda_0, \lambda_1, \cdots, \lambda_{n-1}\)</span>. Ignoring the fact that each eigenvalue can be zero, each one will be either positive or negative. That means the <em>sequence</em> of eigenvalues can have <span class="math inline">\(2^n\)</span> possible arrangements of signs. For example, when <span class="math inline">\(n=3\)</span>, we could have any of the <span class="math inline">\(2^3=8\)</span> possible sign arrangements for the eigenvalues <span class="math inline">\((\lambda_0, \lambda_1, \lambda_2)\)</span>,</p>
<p><span class="math display">\[(+, +, +), \ (+, +, -), \ (+, -, +), \ (-, +, +), \ (+, -, -), \ (-, +, -), \ (-, -, +), \ (-, -, -).\]</span></p>
<p>Most of these arrangements will have mixed signs, but there will always be exactly two arrangements that don’t, namely when the eigenvalues are all positive, and when the eigenvalues are all negative. Most useful in applications like machine learning are when the eigenvalues are positive, or non-negative.</p>
<p>A symmetric matrix whose eigenvalues are all positive is called <strong>positive definite</strong>. A positive definite matrix is essentially the matrix equivalent of a positive real number. When <span class="math inline">\(\mathbf{S}\)</span> is positive definite, we sometimes write <span class="math inline">\(\mathbf{S} \succ 0\)</span> to make the analogy clear. Similarly, if the eigenvalues are all non-negative, the matrix is called <strong>positive semi-definite</strong>, sometimes written <span class="math inline">\(\mathbf{S} \succcurlyeq 0\)</span>. A positive semi-definite matrix is the matrix generalization of a non-negative number. Clearly any positive definite matrix is also positive semi-definite.</p>
<p>By writing <span class="math inline">\(\mathbf{S}=\mathbf{X \Lambda X}^\top\)</span> and expanding everything out term-by-term, it’s not hard to show that,</p>
<ul>
<li>if <span class="math inline">\(\mathbf{S}\)</span> is positive definite, then <span class="math inline">\(\mathbf{x}^\top \mathbf{S} \mathbf{x} &gt; 0\)</span> for any non-zero vector <span class="math inline">\(\mathbf{x} \in \mathbb{R}^n\)</span>,</li>
<li>if <span class="math inline">\(\mathbf{S}\)</span> is positive semi-definite, then <span class="math inline">\(\mathbf{x}^\top \mathbf{S} \mathbf{x} \geq 0\)</span> for any non-zero vector <span class="math inline">\(\mathbf{x} \in \mathbb{R}^n\)</span>.</li>
</ul>
<p>Expressions of the form <span class="math inline">\(\mathbf{x}^\top \mathbf{S} \mathbf{x}\)</span> are called <strong>quadratic forms</strong>. They’ll always be scalars since all they are is a dot product <span class="math inline">\(\mathbf{x} \cdot \mathbf{S} \mathbf{x}\)</span>. This is why a positive definite matrix extends the idea of a positive number, since a positive number <span class="math inline">\(a\)</span> would satisfy <span class="math inline">\(xax=ax^2 &gt; 0\)</span> for any non-zero scalar <span class="math inline">\(x\)</span>. Similarly for the positive semi-definite case, a non-negative scalar would satisfy <span class="math inline">\(xax \geq 0\)</span>.</p>
<p>Note that two of the most important matrices in machine learning, the Hessian and the covariance matrix, are both positive semi-definite, so these things do come up in applications. As you’d probably guess, the easiest way to determine if a symmetric matrix is positive definite or semi-definite is to just calculate the eigenvalues and check their signs. For example, I showed before that the matrix</p>
<p><span class="math display">\[
\mathbf{S} =
\begin{pmatrix}
2 &amp; 1 \\
1 &amp; 2 \\
\end{pmatrix}
\]</span></p>
<p>has eigenvalues <span class="math inline">\(\lambda = 3, 1\)</span>. Since both of these are positive, <span class="math inline">\(\mathbf{S}\)</span> is positive definite. It’s also positive semi-definite since they’re both non-negative. To check if a matrix is positive definite, for example, in numpy, you can do something like the following. Modify the inequality accordingly for the other types.</p>
<div class="cell" data-execution_count="24">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> is_positive_definite(S):</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>    eigvals <span class="op">=</span> np.linalg.eigvals(S)</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.<span class="bu">all</span>(eigvals <span class="op">&gt;</span> <span class="dv">0</span>)</span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a>S <span class="op">=</span> np.array([[<span class="dv">2</span>, <span class="dv">1</span>], </span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">1</span>, <span class="dv">2</span>]])</span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a>is_positive_definite(S)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="24">
<pre><code>True</code></pre>
</div>
</div>
<p>Suppose <span class="math inline">\(a \neq 0\)</span> is some non-negative number. We know we can take its square root to get another non-negative number <span class="math inline">\(\sqrt{a} \neq 0\)</span>. Positive semi-definite matrices have a similar property. If <span class="math inline">\(\mathbf{S} \succcurlyeq 0\)</span>, then we can find a “square root” matrix <span class="math inline">\(\mathbf{R}\)</span> such that</p>
<p><span class="math display">\[\mathbf{S} = \mathbf{R} \mathbf{R}^\top.\]</span></p>
<p>It turns out though that matrices have many possible square roots, not just one. For this reason, we might as well choose an <span class="math inline">\(\mathbf{R}\)</span> that has some convenient form. One useful form is to assume that <span class="math inline">\(\mathbf{R}\)</span> is triangular. When we do this, we get what’s called the <strong>Cholesky Factorization</strong>. If <span class="math inline">\(\mathbf{S}\)</span> is positive semi-definite, we’ll factor <span class="math inline">\(\mathbf{S}\)</span> into as,</p>
<p><span class="math display">\[\mathbf{S} = \mathbf{L} \mathbf{L}^\top,\]</span></p>
<p>where <span class="math inline">\(\mathbf{L}\)</span> is some lower triangular matrix, which also means <span class="math inline">\(\mathbf{L}^\top\)</span> is upper triangular.</p>
<p>Here’s an example. Let’s try to find the Cholesky factorization of the same symmetric matrix from before,</p>
<p><span class="math display">\[
\mathbf{S} =
\begin{pmatrix}
2 &amp; 1 \\
1 &amp; 2 \\
\end{pmatrix}.
\]</span></p>
<p>Take <span class="math inline">\(\mathbf{L}\)</span> to be</p>
<p><span class="math display">\[
\mathbf{L} =
\begin{pmatrix}
\sqrt{2} &amp; 0 \\
\frac{1}{\sqrt{2}} &amp; \sqrt{\frac{3}{2}} \\
\end{pmatrix} \approx
\begin{pmatrix}
1.414 &amp; 0 \\
0.707 &amp; 1.225 \\
\end{pmatrix}.
\]</span></p>
<p>Then</p>
<p><span class="math display">\[
\mathbf{L}\mathbf{L}^\top =
\begin{pmatrix}
\sqrt{2} &amp; 0 \\
\frac{1}{\sqrt{2}} &amp; \sqrt{\frac{3}{2}} \\
\end{pmatrix}
\begin{pmatrix}
\sqrt{2} &amp; \frac{1}{\sqrt{2}} \\
0 &amp; \sqrt{\frac{3}{2}} \\
\end{pmatrix} =
\begin{pmatrix}
2 &amp; 1 \\
1 &amp; 2 \\
\end{pmatrix} =
\mathbf{S},
\]</span></p>
<p>hence <span class="math inline">\(\mathbf{L}\)</span> is the Cholesky factor or square root of <span class="math inline">\(\mathbf{S}\)</span>. How I came up with <span class="math inline">\(\mathbf{L}\)</span> here isn’t important. We can do the Cholesky factorization in numpy using <code>np.linalg.cholesky</code>. Let’s check my answer above is correct. Looks like it is.</p>
<div class="cell" data-execution_count="25">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>S <span class="op">=</span> np.array([[<span class="dv">2</span>, <span class="dv">1</span>], </span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>              [<span class="dv">1</span>, <span class="dv">2</span>]])</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a>L <span class="op">=</span> np.linalg.cholesky(S)</span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'L = </span><span class="ch">\n</span><span class="sc">{</span>L<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'L L^T = </span><span class="ch">\n</span><span class="sc">{</span>L <span class="op">@</span> L<span class="sc">.</span>T<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>L = 
[[1.41421356 0.        ]
 [0.70710678 1.22474487]]
L L^T = 
[[2. 1.]
 [1. 2.]]</code></pre>
</div>
</div>
<p>In practice, the Cholesky factorization is calculated using variants on the same algorithms used to calculate the LU factorization. Indeed, the Cholesky and LU factorizations have a lot in common. Both methods factor a matrix into a product of lower and upper triangular matrices. The major difference is that Cholesky only needs to find one lower triangular matrix <span class="math inline">\(\mathbf{L}\)</span>, not two. Like LU factorization, Cholesky runs in <span class="math inline">\(O(n^3)\)</span> time, but only uses half the FLOPS that LU does.</p>
<p>This means positive semi-definite matrices have more efficient algorithms than general matrices do. For example, suppose you wanted to solve a linear system <span class="math inline">\(\mathbf{A}\mathbf{x}=\mathbf{b}\)</span>. If you knew <span class="math inline">\(\mathbf{A}\)</span> was positive semi-definite, you could solve the system at half the cost by calculating the Cholesky factorization <span class="math inline">\(\mathbf{A}=\mathbf{L}\mathbf{L}^\top\)</span>, and then using substitution to solve for <span class="math inline">\(\mathbf{x}\)</span>. Similar approaches apply for other matrix quantities, like the inverse or determinant.</p>
</section>
<section id="singular-value-decomposition" class="level3" data-number="6.3.5">
<h3 data-number="6.3.5" class="anchored" data-anchor-id="singular-value-decomposition"><span class="header-section-number">6.3.5</span> Singular Value Decomposition</h3>
<p>The spectral decomposition is mostly useful for square symmetric matrices. Yet, the properties of eigenvalues and eigenvectors seem to be incredibly useful for understanding how a matrix behaves. They say something useful about the characteristic scales and directions of a matrix and its underlying linear operator. It turns out we <em>can</em> generalize the spectral decomposition to arbitrary matrices, but with some slight modifications. This modified factorization is called the <strong>singular value decomposition</strong>, or <strong>SVD</strong> for short.</p>
<p>Suppose <span class="math inline">\(\mathbf{A}\)</span> is some arbitrary <span class="math inline">\(m \times n\)</span> matrix. It turns out we can <em>always</em> factor <span class="math inline">\(\mathbf{A}\)</span> into a product of the form</p>
<p><span class="math display">\[\mathbf{A} = \mathbf{U} \mathbf{\Sigma} \mathbf{V}^\top,\]</span></p>
<p>where <span class="math inline">\(\mathbf{U}\)</span> is an <span class="math inline">\(m \times m\)</span> orthogonal matrix called the <strong>left singular matrix</strong>, <span class="math inline">\(\mathbf{V}\)</span> is a different <span class="math inline">\(n \times n\)</span> orthogonal matrix called the <strong>left singular matrix</strong>, and <span class="math inline">\(\mathbf{\Sigma}\)</span> is an <span class="math inline">\(m \times n\)</span> diagonal matrix called the <strong>singular value matrix</strong>.</p>
<p>The singular value matrix <span class="math inline">\(\mathbf{\Sigma}\)</span> is a rectangular diagonal matrix. This means the diagonal will only have <span class="math inline">\(k=\min(m, n)\)</span> entries. The diagonal entries are called the <strong>singular values</strong> of <span class="math inline">\(\mathbf{A}\)</span>, usually denoted <span class="math inline">\(\sigma_0, \sigma_1, \cdots, \sigma_{k-1}\)</span>. Unlike eigenvalues, singular values are required to be non-negative.</p>
<p>The column vectors of <span class="math inline">\(\mathbf{U}\)</span> and <span class="math inline">\(\mathbf{V}\)</span> are called the left and right <strong>singular vectors</strong> respectively. Since both matrices are orthogonal, their singular vectors will form an orthonormal basis for <span class="math inline">\(\mathbb{R}^m\)</span> and <span class="math inline">\(\mathbb{R}^n\)</span> respectively.</p>
<p>Notice that whereas with the spectral composition <span class="math inline">\(\mathbf{S} = \mathbf{X} \mathbf{\Lambda} \mathbf{X}^\top\)</span> has only a single orthogonal matrix <span class="math inline">\(\mathbf{X}\)</span>, the SVD has two different orthogonal matrices <span class="math inline">\(\mathbf{U}\)</span> and <span class="math inline">\(\mathbf{V}\)</span> to worry about, and each one is a different size. Also, while <span class="math inline">\(\mathbf{\Lambda}\)</span> can contain eigenvalues of any sign, <span class="math inline">\(\mathbf{\Sigma}\)</span> can only contain singular values that are nonnegative.</p>
<p>Nonetheless, the two factorizations are related by the following fact: The <em>singular values</em> of <span class="math inline">\(\mathbf{A}\)</span> are the <em>eigenvalues</em> of the symmetric matrix <span class="math inline">\(\mathbf{S} = \mathbf{A}^\top \mathbf{A}\)</span>. Not only that, they’re also the eigenvalues of the transposed symmetric matrix <span class="math inline">\(\mathbf{S}^\top = \mathbf{A} \mathbf{A}^\top\)</span>. This fact gives one way you could actually calculate the SVD. The singular value matrix <span class="math inline">\(\mathbf{\Sigma}\)</span> will just be the eigenvalue matrix of <span class="math inline">\(\mathbf{S}\)</span> (and <span class="math inline">\(\mathbf{S}^\top\)</span>). The left singular matrix <span class="math inline">\(\mathbf{U}\)</span> will be the eigenvector matrix of <span class="math inline">\(\mathbf{S}\)</span>. The right singular matrix <span class="math inline">\(\mathbf{V}\)</span> will be the eigenvector matrix of <span class="math inline">\(\mathbf{S}^\top\)</span>. Very roughly speaking, this is what many SVD algorithms use, e.g.&nbsp;by applying the QR algorithm on both <span class="math inline">\(\mathbf{S}\)</span> and <span class="math inline">\(\mathbf{S}^\top\)</span>.</p>
<p>Calculating the SVD by hand is much more of a pain than the spectral decomposition is because you have to do it twice, once on <span class="math inline">\(\mathbf{S}\)</span> and once on <span class="math inline">\(\mathbf{S}^\top\)</span>. I’ll spare you the agony of this calculation, and just use numpy to calculate the SVD of the following matrix,</p>
<p><span class="math display">\[
\mathbf{A} =
\begin{pmatrix}
1 &amp; 1 \\
1 &amp; 0 \\
1 &amp; -1 \\
\end{pmatrix}.
\]</span></p>
<p>We can use <code>np.linalg.svd(A)</code> to calculate the SVD of <span class="math inline">\(\mathbf{A}\)</span>. It’ll return a triplet of arrays, in order <span class="math inline">\(\mathbf{U}\)</span>, the diagonal of <span class="math inline">\(\mathbf{\Sigma}\)</span>, and <span class="math inline">\(\mathbf{V}^T\)</span>. Note to get the full <span class="math inline">\(\mathbf{\Sigma}\)</span> you can’t just use <code>np.diag</code> since <span class="math inline">\(\mathbf{\Sigma}\)</span> won’t be square here. You have to add a row of zeros after to make the calculation work out. I’ll do this just using a loop and filling in the diagonals manually.</p>
<p>Notice that the two singular values are positive, <span class="math inline">\(\sigma_0 = \sqrt{3} \approx 1.732\)</span> and <span class="math inline">\(\sigma_1 = \sqrt{2} \approx 1.414\)</span>. In this example, the right singular matrix <span class="math inline">\(\mathbf{V}\)</span> is just <span class="math inline">\(\text{diag}(-1, 1)\)</span>, which is clearly orthogonal. The left singular matrix <span class="math inline">\(\mathbf{U}\)</span> is a little harder to see, but it’s also orthogonal. Finally, the product <span class="math inline">\(\mathbf{U}\mathbf{\Sigma}\mathbf{V}^\top\)</span> indeed gives <span class="math inline">\(\mathbf{A}\)</span>.</p>
<div class="cell" data-execution_count="26">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.array([</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">1</span>, <span class="dv">1</span>],</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">1</span>, <span class="dv">0</span>],</span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>]])</span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a>m, n <span class="op">=</span> A.shape</span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a>k <span class="op">=</span> <span class="bu">min</span>(m, n)</span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a>U, sigma, Vt <span class="op">=</span> np.linalg.svd(A)</span>
<span id="cb46-8"><a href="#cb46-8" aria-hidden="true" tabindex="-1"></a>Sigma <span class="op">=</span> np.zeros((m, n))</span>
<span id="cb46-9"><a href="#cb46-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(k):</span>
<span id="cb46-10"><a href="#cb46-10" aria-hidden="true" tabindex="-1"></a>    Sigma[i, i] <span class="op">=</span> sigma[i]</span>
<span id="cb46-11"><a href="#cb46-11" aria-hidden="true" tabindex="-1"></a>USVt <span class="op">=</span> U <span class="op">@</span> Sigma <span class="op">@</span> Vt</span>
<span id="cb46-12"><a href="#cb46-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'U = </span><span class="ch">\n</span><span class="sc">{</span>U<span class="sc">.</span><span class="bu">round</span>(<span class="dv">10</span>)<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb46-13"><a href="#cb46-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Sigma = </span><span class="ch">\n</span><span class="sc">{</span>Sigma<span class="sc">.</span><span class="bu">round</span>(<span class="dv">10</span>)<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb46-14"><a href="#cb46-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'V = </span><span class="ch">\n</span><span class="sc">{</span>Vt<span class="sc">.</span>T<span class="sc">.</span><span class="bu">round</span>(<span class="dv">10</span>)<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb46-15"><a href="#cb46-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'U Sigma V^T = </span><span class="ch">\n</span><span class="sc">{</span>USVt<span class="sc">.</span><span class="bu">round</span>(<span class="dv">10</span>)<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>U = 
[[-0.57735027  0.70710678  0.40824829]
 [-0.57735027  0.         -0.81649658]
 [-0.57735027 -0.70710678  0.40824829]]
Sigma = 
[[1.73205081 0.        ]
 [0.         1.41421356]
 [0.         0.        ]]
V = 
[[-1.  0.]
 [-0.  1.]]
U Sigma V^T = 
[[ 1.  1.]
 [ 1.  0.]
 [ 1. -1.]]</code></pre>
</div>
</div>
<p>To give you an intuition is to what the SVD is doing, suppose <span class="math inline">\(\mathbf{x} \in \mathbb{R}^n\)</span> is some size-<span class="math inline">\(n\)</span> vector. Suppose we want to operate on <span class="math inline">\(\mathbf{x}\)</span> with <span class="math inline">\(\mathbf{A}\)</span> to get a new vector <span class="math inline">\(\mathbf{v} = \mathbf{A}\mathbf{x}\)</span>. Writing <span class="math inline">\(\mathbf{A} = \mathbf{U} \mathbf{\Sigma} \mathbf{V}^\top\)</span>, we can do this operation in a sequence of three successive steps:</p>
<ol type="1">
<li>Calculate <span class="math inline">\(\mathbf{y} = \mathbf{V}^\top \mathbf{x}\)</span>: The output is also a size-<span class="math inline">\(n\)</span> vector <span class="math inline">\(\mathbf{y} \in \mathbb{R}^n\)</span>. Since <span class="math inline">\(\mathbf{V}\)</span> is orthogonal, this action can only rotate (or reflect) <span class="math inline">\(\mathbf{x}\)</span> by some angle in space.</li>
<li>Calculate <span class="math inline">\(\mathbf{z} = \mathbf{\Sigma}\mathbf{y}\)</span>: The output is now a size-<span class="math inline">\(k\)</span> vector <span class="math inline">\(\mathbf{z} \in \mathbb{R}^k\)</span>. Since <span class="math inline">\(\mathbf{\Sigma}\)</span> is diagonal, it can only stretch <span class="math inline">\(\mathbf{y}\)</span> along the singular directions of <span class="math inline">\(\mathbf{V}\)</span>, not rotate it.</li>
<li>Calculate <span class="math inline">\(\mathbf{v} = \mathbf{U}\mathbf{z}\)</span>: The output is now a size-<span class="math inline">\(m\)</span> vector <span class="math inline">\(\mathbf{v} \in \mathbb{R}^m\)</span>. Since <span class="math inline">\(\mathbf{U}\)</span> is orthogonal, this action can only rotate (or reflect) <span class="math inline">\(\mathbf{z}\)</span> by some angle in space.</li>
</ol>
<p>The final output is thus a vector <span class="math inline">\(\mathbf{v} = \mathbf{A}\mathbf{x}\)</span> that first got rotated in <span class="math inline">\(\mathbb{R}^n\)</span>, then scaled in <span class="math inline">\(\mathbb{R}^k\)</span>, then rotated again in <span class="math inline">\(\mathbb{R}^m\)</span>. So you can visualize this better let’s take a specific example. To make everything show up on one plot I’ll choose a <span class="math inline">\(2 \times 2\)</span> matrix, so <span class="math inline">\(m=n=k=2\)</span>, for example</p>
<p><span class="math display">\[
\mathbf{A} =
\begin{pmatrix}
1 &amp; 2 \\
1 &amp; 1 \\
\end{pmatrix}.
\]</span></p>
<p>The singular values to this matrix turn out to be <span class="math inline">\(\sigma_0 \approx 2.618\)</span> and <span class="math inline">\(\sigma_1 \approx 0.382\)</span>. What I’m going to do is randomly sample a bunch of unit vectors <span class="math inline">\(\mathbf{x}\)</span>, then apply the successive operations above to each vector. The original vectors <span class="math inline">\(\mathbf{x}\)</span> are shown in red, the vectors <span class="math inline">\(\mathbf{y} = \mathbf{V}^\top \mathbf{x}\)</span> in blue, the vectors <span class="math inline">\(\mathbf{z} = \mathbf{\Sigma}\mathbf{y}\)</span> in green, and finally the vectors <span class="math inline">\(\mathbf{v} = \mathbf{U}\mathbf{z}\)</span> in black. Notice that the red vectors just kind of fill in the unit circle, since they’re all unit vectors of length one. The blue vectors also fill in the unit circle, since <span class="math inline">\(\mathbf{V}^\top\)</span> can only rotate vectors, not stretch them. The green vectors then get stretched out into an elliptical shape due to <span class="math inline">\(\mathbf{\Sigma}\)</span>. The distortion of the ellipse depends on the “distortion ratio” <span class="math inline">\(\frac{\sigma_0}{\sigma_1} \approx 6.85\)</span>. This means one axis gets stretched about <span class="math inline">\(6.85\)</span> times as much as the other. Finally, since <span class="math inline">\(\mathbf{U}\)</span> can only rotate vectors, the black vectors then rotate these stretched vectors into their final position.</p>
<div class="cell" data-execution_count="27">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.array([</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">1</span>, <span class="dv">2</span>],</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>    [<span class="dv">1</span>, <span class="dv">1</span>]])</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>m, n <span class="op">=</span> A.shape</span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a>k <span class="op">=</span> <span class="bu">min</span>(m, n)</span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a>U, sigma, Vt <span class="op">=</span> np.linalg.svd(A)</span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a>Sigma <span class="op">=</span> np.diag(sigma)</span>
<span id="cb48-8"><a href="#cb48-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Sigma = </span><span class="ch">\n</span><span class="sc">{</span>Sigma<span class="sc">.</span><span class="bu">round</span>(<span class="dv">10</span>)<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Sigma = 
[[2.61803399 0.        ]
 [0.         0.38196601]]</code></pre>
</div>
</div>
<div class="cell" data-execution_count="28">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a>plot_svd(A)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="matrix-algebra_files/figure-html/cell-29-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The “distortion ratio” <span class="math inline">\(\frac{\sigma_0}{\sigma_1}\)</span> mentioned above can actually be used as a measure of how invertible a matrix is. It’s called the <em>condition number</em>, denoted <span class="math inline">\(\kappa\)</span>. For a general <span class="math inline">\(n \times n\)</span> matrix, the <strong>condition number</strong> is defined as the ratio of the <em>largest</em> to the <em>smallest</em> singular value,</p>
<p><span class="math display">\[\kappa = \frac{\sigma_0}{\sigma_{k-1}}.\]</span></p>
<p>The higher the condition number is, the harder it is to invert <span class="math inline">\(\mathbf{A}\)</span>. A condition number of <span class="math inline">\(\kappa=1\)</span> is when the singular values are the same. These are easiest to invert. Matrices with low <span class="math inline">\(\kappa\)</span> are called called <strong>well-conditioned</strong> matrices. The identity matrix has <span class="math inline">\(\kappa=1\)</span>, for example. If one of the singular values is <span class="math inline">\(0\)</span> then <span class="math inline">\(\kappa\)</span> will be infinite, meaning the matrix isn’t invertible at all. Matrices with high <span class="math inline">\(\kappa\)</span> are called <strong>ill-conditioned</strong> matrices. For this reason, the condition number is very often used in calculations when it’s important to make sure that <span class="math inline">\(\mathbf{A}\)</span> isn’t singular or close to singular. In numpy, you can calculate the condition number of a matrix directly by using <code>np.linalg.cond(A)</code>.</p>
</section>
<section id="low-rank-approximations" class="level3" data-number="6.3.6">
<h3 data-number="6.3.6" class="anchored" data-anchor-id="low-rank-approximations"><span class="header-section-number">6.3.6</span> Low-Rank Approximations</h3>
<p>The SVD is useful for many reasons. In fact, it’s probably the single most useful factorization in all of applied linear algebra. One reason this is true is because <em>every matrix</em> has one. When in doubt, if you can’t figure out how to do something with a matrix, you can take its SVD and try to work with those three matrices one-by-one. While that’s nice, the more useful application of the SVD to machine learning is that it’s a good way to compress or denoise data. To see why we need to look at the SVD in a slightly different way.</p>
<p>Suppose <span class="math inline">\(\mathbf{A}\)</span> is some <span class="math inline">\(m \times n\)</span> matrix. Suppose <span class="math inline">\(\mathbf{u}_0, \mathbf{u}_1, \cdots, \mathbf{u}_{m-1}\)</span> are the column vectors of <span class="math inline">\(\mathbf{U}\)</span>, and <span class="math inline">\(\mathbf{v}_0, \mathbf{v}_1, \cdots, \mathbf{v}_{n-1}\)</span> are the column vectors of <span class="math inline">\(\mathbf{V}\)</span>. Suppose <span class="math inline">\(\sigma_0, \sigma_1, \cdots, \sigma_{k-1}\)</span> are the singular values of <span class="math inline">\(\mathbf{A}\)</span>, by convention ordered from largest to smallest. Then writing out the SVD in terms of the column vectors, and multiplying everything out matrix multiplication style, we have</p>
<p><span class="math display">\[
\mathbf{A} = \mathbf{U} \mathbf{\Sigma} \mathbf{V}^\top =
\begin{pmatrix}
\mathbf{u}_0 &amp; \mathbf{u}_1 &amp; \cdots &amp; \mathbf{u}_{m-1}
\end{pmatrix}
\text{diag}\big(\sigma_0, \sigma_1, \cdots, \sigma_{k-1}\big)
\begin{pmatrix}
\mathbf{v}_0^\top \\ \mathbf{v}_1^\top \\ \cdots \\ \mathbf{v}_{n-1}^\top
\end{pmatrix} =
\sum_{i=0}^{k-1} \sigma_i \mathbf{u}_i \mathbf{v}_i^\top =
\sigma_0 \mathbf{u}_0 \mathbf{v}_0^\top + \sigma_1 \mathbf{u}_1 \mathbf{v}_1^\top + \cdots + \sigma_{k-1} \mathbf{u}_{k-1} \mathbf{v}_{k-1}^\top.
\]</span></p>
<p>That is, we can write <span class="math inline">\(\mathbf{A}\)</span> as a sum of outer products over the singular vectors, each weighted by its singular value. That’s fine. But why is it useful? All I did was re-write the SVD in a different form, after all. The gist of it is that we can use this formula to approximate <span class="math inline">\(\mathbf{A}\)</span> by a lower-dimensional matrix. Supposing we only kept the first <span class="math inline">\(d &lt; k\)</span> terms of the right-hand side and dropped the rest, we’d have</p>
<p><span class="math display">\[\mathbf{A} \approx \mathbf{U}_d \mathbf{\Sigma}_d \mathbf{V}_d^\top = \sigma_0 \mathbf{u}_0 \mathbf{v}_0^\top + \sigma_1 \mathbf{u}_1 \mathbf{v}_1^\top + \cdots + \sigma_{d-1} \mathbf{u}_{d-1} \mathbf{v}_{d-1}^\top.\]</span></p>
<p>This approximation will be a rank-<span class="math inline">\(d\)</span> matrix again of size <span class="math inline">\(m \times n\)</span>. It’s rank <span class="math inline">\(d\)</span> because it’s a sum of <span class="math inline">\(d\)</span> “independent” rank-1 matrices. When <span class="math inline">\(d&lt;&lt;k\)</span>, this is called the <strong>low-rank approximation</strong>. While this approximation is low <em>rank</em> it still has size <span class="math inline">\(m \times n\)</span>. It’s the inner dimensions that got cut from <span class="math inline">\(k\)</span> to <span class="math inline">\(d\)</span>, not the outer dimensions. To get a true low-dimensional approximation, we need to multiply both sides by <span class="math inline">\(\mathbf{V}_d\)</span>,</p>
<p><span class="math display">\[\mathbf{A}_d =  \mathbf{A} \mathbf{V}_d = \mathbf{U}_d \mathbf{\Sigma}_d.\]</span></p>
<p>We’re now approximating the <span class="math inline">\(m \times n\)</span> matrix <span class="math inline">\(\mathbf{A}\)</span> with an <span class="math inline">\(m \times d\)</span> matrix I’ll call <span class="math inline">\(\mathbf{A}_d\)</span>. Said differently, we’re <em>compressing</em> the <span class="math inline">\(n\)</span> columns of <span class="math inline">\(\mathbf{A}\)</span> down to just <span class="math inline">\(d&lt;&lt;n\)</span> columns. Note that we’re not <em>dropping</em> the last <span class="math inline">\(n-d\)</span> columns, we’re building new columns that best approximate <em>all</em> of the old columns.</p>
<p>Let’s try to understand why low rank approximations are useful, and that they indeed do give good approximations to large matrices. To do so, consider the following example. I’m going to load some data from a well-known dataset in machine learning called MNIST. It’s a dataset of images of handwritten digits. When the low-rank approximation is applied to data, it’s called <strong>principle components analysis</strong>, or <strong>PCA</strong>. PCA is probably the most fundamental dimension reduction algorithm, a way of compressing high-dimensional data into lower-dimensional data.</p>
<p>Each image is size <span class="math inline">\(28 \times 28\)</span>, which flatten out into <span class="math inline">\(n = 28 \cdot 28 = 784\)</span> dimensions. I’ll load <span class="math inline">\(m=1000\)</span> random samples from the MNIST dataset. This will create a matrix <span class="math inline">\(\mathbf{A}\)</span> of shape <span class="math inline">\(1000 \times 784\)</span>. I’ll go ahead and calculate the SVD to get <span class="math inline">\(\mathbf{U}\)</span>, <span class="math inline">\(\mathbf{\Sigma}\)</span>, and <span class="math inline">\(\mathbf{V}^\top\)</span>. In this case, <span class="math inline">\(k=\min(m,n)=784\)</span>, so these matrices will have sizes <span class="math inline">\(1000 \times 1000\)</span>, <span class="math inline">\(1000 \times 784\)</span>, and <span class="math inline">\(784 \times 784\)</span> respectively. As I mentioned before, numpy only returns the non-zero diagonals of <span class="math inline">\(\mathbf{\Sigma}\)</span>, which is a size <span class="math inline">\(k=784\)</span> vector of the singular values. Thankfully, that’s all we’ll need here.</p>
<div class="cell" data-execution_count="29">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> sample_mnist(size<span class="op">=</span>m)</span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a>U, sigma, Vt <span class="op">=</span> np.linalg.svd(A)</span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a>A.shape, U.shape, sigma.shape, Vt.shape</span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'A.shape = </span><span class="sc">{</span>A<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb51-6"><a href="#cb51-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'U.shape = </span><span class="sc">{</span>U<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb51-7"><a href="#cb51-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'sigma.shape = </span><span class="sc">{</span>sigma<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb51-8"><a href="#cb51-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Vt.shape = </span><span class="sc">{</span>Vt<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>A.shape = (1000, 784)
U.shape = (1000, 1000)
sigma.shape = (784,)
Vt.shape = (784, 784)</code></pre>
</div>
</div>
<p>Think of each <em>row</em> of <span class="math inline">\(\mathbf{A}\)</span> as representing a single image in the dataset, and each <em>column</em> of <span class="math inline">\(\mathbf{A}\)</span> as representing a single pixel of the image.</p>
<p>Since these are images, I might as well show you what they look like. To do that, just pick a random row from the matrix. Each row will be a flattened image. To turn it into an image, we can just reshape the row to have shape <span class="math inline">\(28 \times 28\)</span>, then plot it using <code>plt.imshow</code>. Below, I’m picking off the first row, which turns out to be an image of a handwritten <span class="math inline">\(0\)</span>.</p>
<div class="cell" data-execution_count="30">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> A[<span class="dv">0</span>, :].reshape(<span class="dv">28</span>, <span class="dv">28</span>)</span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>plt.imshow(img, cmap<span class="op">=</span><span class="st">'Greys'</span>)</span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)</span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="matrix-algebra_files/figure-html/cell-31-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Let’s start by taking <span class="math inline">\(d=2\)</span>. Why? Because when <span class="math inline">\(d=2\)</span> we can plot each image as a point in the xy-plane! This suggests a powerful application of the low-rank approximation, to visualize high-dimensional data. To calculate <span class="math inline">\(\mathbf{A}_d\)</span>, we’ll need to truncate <span class="math inline">\(\mathbf{U}\)</span>, <span class="math inline">\(\mathbf{\Sigma}\)</span>, and <span class="math inline">\(\mathbf{V}^\top\)</span>. To make the shapes come out right, we’ll want to drop the first <span class="math inline">\(d\)</span> <em>columns</em> of <span class="math inline">\(\mathbf{U}\)</span> and the first <span class="math inline">\(d\)</span> <em>rows</em> of <span class="math inline">\(\mathbf{V}^\top\)</span>. Once we’ve got these, we can calculate <span class="math inline">\(\mathbf{A}_d\)</span>, which in this case will be size <span class="math inline">\(1000 \times 2\)</span>.</p>
<div class="cell" data-execution_count="31">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a>d <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a>U_d, sigma_d, Vt_d <span class="op">=</span> U[:, :d], sigma[:d], Vt[:d, :]</span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a>A_d <span class="op">=</span> A <span class="op">@</span> Vt_d.T</span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'U_d.shape = </span><span class="sc">{</span>U_d<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'sigma_d = </span><span class="sc">{</span>sigma_d<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Vt_d.shape = </span><span class="sc">{</span>Vt_d<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb54-7"><a href="#cb54-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'A_d.shape = </span><span class="sc">{</span>A_d<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>U_d.shape = (1000, 2)
sigma_d = [197.89062659  66.60026657]
Vt_d.shape = (2, 784)
A_d.shape = (1000, 2)</code></pre>
</div>
</div>
<p>Now we have <span class="math inline">\(m=1000\)</span> “images”, each with <span class="math inline">\(d=2\)</span> “variables”. This means we can plot them in the xy-plane, taking <span class="math inline">\(x\)</span> to be the first column <code>A_d[:, 0]</code>, and <span class="math inline">\(y\)</span> to be the second column <code>A_d[:, 1]</code>. Here’s a scatter plot of all images projected down to 2 dimensions. I can’t make out any patterns in the plot, and you probably can’t either. But at least we’ve found an interesting and sometimes useful way to visualize high-dimensional data.</p>
<div class="cell" data-execution_count="32">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a>plt.scatter(A_d[:, <span class="dv">0</span>], A_d[:, <span class="dv">1</span>], s<span class="op">=</span><span class="dv">1</span>, alpha<span class="op">=</span><span class="fl">0.8</span>)</span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>plt.xticks([])</span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a>plt.yticks([])</span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="ss">f'</span><span class="sc">{</span>m<span class="sc">}</span><span class="ss"> MNIST Images'</span>)</span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="matrix-algebra_files/figure-html/cell-33-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>How good is our approximation? We can use the singular values to figure this out. In the low rank approximation, we’re keeping <span class="math inline">\(d\)</span> singular values and dropping the remaining <span class="math inline">\(k-d\)</span>. Throwing away those remaining singular values is throwing away information about our original matrix <span class="math inline">\(\mathbf{A}\)</span>. To figure out how much information we’re keeping in our approximation, we can just look at the ratio of the sum of singular values kept to the total sum of all singular values,</p>
<p><span class="math display">\[R_d = \frac{\sigma_0 + \sigma_1 + \cdots + \sigma_{d-1}}{\sigma_0 + \sigma_1 + \cdots + \sigma_{k-1}}.\]</span></p>
<p>This ratio is sometimes called the <strong>explained variance</strong> for reasons I’ll get into in a future lesson.</p>
<p>In the rank-2 case I just worked out, this ratio turns out to be <span class="math inline">\(R_2 = \frac{\sigma_0 + \sigma_1}{\sum \sigma_i} \approx 0.087\)</span>. That is, this rank-2 approximation is preserving about 8.7% of the information in the original data.</p>
<div class="cell" data-execution_count="33">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>R_d <span class="op">=</span> np.<span class="bu">sum</span>(sigma_d) <span class="op">/</span> np.<span class="bu">sum</span>(sigma)</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'R_d = </span><span class="sc">{</span>R_d<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>R_d = 0.08740669535517863</code></pre>
</div>
</div>
<p>That’s pretty bad. We can do better. Let’s take <span class="math inline">\(d=100\)</span> and see how well that does. Of course, we won’t be able to plot the data in the xy-plane anymore, but it’ll better represent the original data. We’re now at <span class="math inline">\(R_d \approx 0.643\)</span>, which means we’re preserving about 64.3% of the information in the original data, and we’re doing it using only <span class="math inline">\(\frac{100}{784} \approx 0.127\)</span>, or 12.7% of the total columns of <span class="math inline">\(\mathbf{A}\)</span>.</p>
<div class="cell" data-execution_count="34">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a>d <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>U_d, sigma_d, Vt_d <span class="op">=</span> U[:, :d], sigma[:d], Vt[:d, :]</span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a>A_d <span class="op">=</span> A <span class="op">@</span> Vt_d.T</span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'A_d.shape = </span><span class="sc">{</span>A_d<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>A_d.shape = (1000, 100)</code></pre>
</div>
</div>
<div class="cell" data-execution_count="35">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a>R_d <span class="op">=</span> np.<span class="bu">sum</span>(sigma_d) <span class="op">/</span> np.<span class="bu">sum</span>(sigma)</span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'R_d = </span><span class="sc">{</span>R_d<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>R_d = 0.6433751746962163</code></pre>
</div>
</div>
<p>Another way to see how good our compression is is to “unproject” the compressed images and plot them. To unproject <span class="math inline">\(\mathbf{A}_d\)</span>, just multiply on the right again by <span class="math inline">\(\mathbf{V}^\top\)</span> to get the original <span class="math inline">\(m \times n\)</span> matrix approximation again,</p>
<p><span class="math display">\[\mathbf{A} \approx \mathbf{A}_d \mathbf{V}^\top.\]</span></p>
<p>Once I’ve done that, I can just pluck a random row from the approximation, resize it, and plot it using <code>plt.imshow</code>, just like before. Notice this time we can still clearly see the handwritten <span class="math inline">\(0\)</span>, but it’s a bit grainer than it was before. The edges aren’t as sharp. Nevertheless, we can still make out the digit pretty solidly.</p>
<div class="cell" data-execution_count="36">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> (A_d <span class="op">@</span> Vt_d)[<span class="dv">0</span>, :].reshape(<span class="dv">28</span>, <span class="dv">28</span>)</span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a>plt.imshow(img, cmap<span class="op">=</span><span class="st">'Greys'</span>)</span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)</span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="matrix-algebra_files/figure-html/cell-37-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>But why is this approach good for compression anyway? After all, we still have to unproject the rows back into the original <span class="math inline">\(m \times n\)</span> space. Maybe think about it this way. If you just stored the full matrix <span class="math inline">\(\mathbf{A}\)</span>, you’d have tot store <span class="math inline">\(m \cdot n\)</span> total numbers. In this example, that’s <span class="math inline">\(1000 \cdot 784 = 784000\)</span> numbers you’d have to store in memory.</p>
<p>But suppose now we do the low rank approximation. What we can then do is just store <span class="math inline">\(\mathbf{A}_d\)</span> and <span class="math inline">\(\mathbf{V}\)</span> instead. That means we’d instead store <span class="math inline">\(m \cdot d + d \cdot n\)</span> total numbers. In our example, that comes out to <span class="math inline">\(1000 \cdot 100 + 100 \cdot 784 = 100000 + 78400 = 178400\)</span>, which is only <span class="math inline">\(\frac{178400}{784000} \approx 0.227\)</span> or 22.7% of the numbers we’d have to store otherwise. We’ve thus compressed our data by a factor of about <span class="math inline">\(\frac{1}{0.227} \approx 4.4\)</span>. That’s a 4.4x compression of the original images.</p>
<p>Now, this kind of PCA compression isn’t <em>perfect</em>, or <strong>lossless</strong>, since we can’t recover the original images <em>exactly</em>. But we can still recover the most fundamental features of the image, which in this case are the handwritten digits. This kind of compression is <strong>lossy</strong>, since it irreversibly throws away some information in the original data. Yet, it still maintains enough information to be useful in many settings.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation column-page-right">
  <div class="nav-page nav-page-previous">
      <a href="../notebooks/vector-spaces.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Vector Spaces</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../notebooks/multivariate-calculus.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Multivariate Calculus</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>