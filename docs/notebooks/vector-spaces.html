<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Machine Learning For The 2020s - 6&nbsp; Vector Spaces</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../notebooks/matrix-algebra.html" rel="next">
<link href="../notebooks/linear-systems.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Vector Spaces</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Machine Learning For The 2020s</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">Preface</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/basic-math.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Basic Math</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/numerical-computing.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Numerical Computation</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/vectorization.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Arrays and Vectorization</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/basic-calculus.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Basic Calculus</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/linear-systems.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Systems of Linear Equations</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/vector-spaces.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Vector Spaces</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/matrix-algebra.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Matrix Algebra</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/multivariate-calculus.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Multivariate Calculus</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/probability.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Probability Distributions</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#geometry-of-vectors" id="toc-geometry-of-vectors" class="nav-link active" data-scroll-target="#geometry-of-vectors">Geometry of Vectors</a>
  <ul class="collapse">
  <li><a href="#visualizing-vectors" id="toc-visualizing-vectors" class="nav-link" data-scroll-target="#visualizing-vectors">Visualizing Vectors</a></li>
  <li><a href="#vector-operations" id="toc-vector-operations" class="nav-link" data-scroll-target="#vector-operations">Vector Operations</a></li>
  <li><a href="#the-dot-product" id="toc-the-dot-product" class="nav-link" data-scroll-target="#the-dot-product">The Dot Product</a></li>
  <li><a href="#projections" id="toc-projections" class="nav-link" data-scroll-target="#projections">Projections</a></li>
  <li><a href="#linear-independence" id="toc-linear-independence" class="nav-link" data-scroll-target="#linear-independence">Linear Independence</a></li>
  <li><a href="#basis-vectors" id="toc-basis-vectors" class="nav-link" data-scroll-target="#basis-vectors">Basis Vectors</a></li>
  <li><a href="#cosine-similarity" id="toc-cosine-similarity" class="nav-link" data-scroll-target="#cosine-similarity">Cosine Similarity</a></li>
  <li><a href="#other-norms" id="toc-other-norms" class="nav-link" data-scroll-target="#other-norms">Other Norms</a></li>
  </ul></li>
  <li><a href="#linear-maps" id="toc-linear-maps" class="nav-link" data-scroll-target="#linear-maps">Linear Maps</a></li>
  <li><a href="#n-dimensional-vector-spaces" id="toc-n-dimensional-vector-spaces" class="nav-link" data-scroll-target="#n-dimensional-vector-spaces"><span class="math inline">\(n\)</span>-dimensional Vector Spaces</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content column-body" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Vector Spaces</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>In this lesson I’ll continue on the topic of linear algebra by discussing vector spaces. Vector spaces are essential for abstracting linear algebra away from systems of equations and for visualizing linear algebra objects like vectors and matrices. Let’s get started.</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sympy <span class="im">as</span> sp</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> utils.math_ml <span class="im">import</span> <span class="op">*</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="geometry-of-vectors" class="level1">
<h1>Geometry of Vectors</h1>
<p>I’ve introduced matrices and vectors so far kind of organically as the natural way to write and solve systems of linear equations. They’re good for a lot more than solving linear systems, however. For one thing, they possess important geometric properties. I’m now going to re-define the concepts covered so far, but in more geometric terms.</p>
<section id="visualizing-vectors" class="level2">
<h2 class="anchored" data-anchor-id="visualizing-vectors">Visualizing Vectors</h2>
<p>Let’s go back to the simple 2-dimensional case. Imagine you have a point in the xy-plane, call it <span class="math inline">\((x,y)\)</span>. Now, we can think of this as a single point, but we can also imagine it differently. Suppose there was an arrow pointing from the origin <span class="math inline">\((0,0)\)</span> to the point <span class="math inline">\((x,y)\)</span>. For example, if the point was <span class="math inline">\((1,1)\)</span>, this arrow might look like this.</p>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>point <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">1</span>])</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>plot_vectors(point, title<span class="op">=</span><span class="ss">f'Arrow From $(0,0)$ To $(1,1)$'</span>, ticks_every<span class="op">=</span><span class="fl">0.5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="vector-spaces_files/figure-html/cell-3-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Unlike the <em>point</em> <span class="math inline">\((x,y)\)</span>, the <em>arrow</em> <span class="math inline">\((x,y)\)</span> has both a length and a direction. Its length is given by the Pythagorean Theorem. If the triangle has base <span class="math inline">\(x\)</span> and height <span class="math inline">\(y\)</span>, then the length of the arrow is just its hypotenuse, i.e.&nbsp;<span class="math inline">\(r = \sqrt{x^2 + y^2}\)</span>. The direction of the arrow is its angle <span class="math inline">\(\theta\)</span> with respect to the x-axis. This angle is just given by the inverse tangent of height over base, i.e.&nbsp;<span class="math inline">\(\theta = \tan^{-1}\big(\frac{y}{x}\big)\)</span>.</p>
<p>In the example plotted, the length is <span class="math inline">\(r=\sqrt{1+1}=\sqrt{2}\)</span>, and the angle is <span class="math inline">\(\theta = \tan^{-1}(1) = 45^\circ\)</span>. These two values uniquely specify the arrow, assuming it starts at the origin. If we know the length and direction, we know exactly which arrow we’re talking about.</p>
<p>What I’ve just shown is another way to define a vector. A <strong>vector</strong> is an arrow in the plane. Said differently, a vector is just a point that’s also been endowed with a length (or magnitude) and a direction. The x and y values are called <strong>components</strong> of a vector. Usually we’ll write a vector in bold-face and its components in regular type but with subscripts indicating which component. For example, <span class="math inline">\(\mathbf{v}=(v_x,v_y)\)</span>. Here’s the same arrow I plotted above, but explicitly labeled as a vector <span class="math inline">\(\mathbf{v}=(1,1)\)</span>. Its components are <span class="math inline">\(v_x=1\)</span> and <span class="math inline">\(v_y=1\)</span>.</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>v <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">1</span>])</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>plot_vectors(v, title<span class="op">=</span><span class="st">'$\mathbf</span><span class="sc">{v}</span><span class="st">=(1,1)$'</span>, labels<span class="op">=</span>[<span class="st">'$\mathbf</span><span class="sc">{v}</span><span class="st">$'</span>], ticks_every<span class="op">=</span><span class="fl">0.5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="vector-spaces_files/figure-html/cell-4-output-1.png" class="img-fluid"></p>
</div>
</div>
<p><strong>Notation:</strong> It’s common to represent vectors in a few different ways depending on the situation. One way to represent a vector is as a <em>column</em> vector. This is what I did when doing matrix-vector multiplication. Another way, what I just introduced, is a <em>flat</em> vector, or a 1-dimensional array. This is more common when thinking about a vector geometrically. Yet <em>another</em> way is to think of a vector as a <em>row</em> vector, which is the transpose of a column vector. All of these representations conceptually represent the same object, but their shapes are different. Here’s an example: The size-2 vector <span class="math inline">\(\mathbf{v}=(1,1)\)</span> can be written in 3 different but all equivalent ways:</p>
<p><span class="math display">\[\begin{align*}
&amp;\text{Flat vector of shape } (2,): \mathbf{v} = (1,1), \\
&amp;\text{Column vector of shape } (2,1): \mathbf{v} = \begin{pmatrix}
1 \\
1 \\
\end{pmatrix}, \\
&amp;\text{Row vector of shape } (1,2): \mathbf{v}^\top = \begin{pmatrix}
1 &amp; 1 \\
\end{pmatrix}.
\end{align*}\]</span></p>
<p>Be careful when working with vectors in code to make sure you’re using the right shapes for the right situation or you’ll get shape mismatch errors (or worse a silent bug).</p>
</section>
<section id="vector-operations" class="level2">
<h2 class="anchored" data-anchor-id="vector-operations">Vector Operations</h2>
<p>The magnitude, or length, of <span class="math inline">\(\mathbf{v}\)</span> is typically denoted by the symbol <span class="math inline">\(||\mathbf{v}||\)</span>, called a <strong>norm</strong>,</p>
<p><span class="math display">\[||\mathbf{v}|| = \sqrt{v_x^2 + v_y^2}.\]</span></p>
<p>In the above example with <span class="math inline">\(\mathbf{v}=(1,1)\)</span>, its norm is <span class="math inline">\(||\mathbf{v}||=\sqrt{1+1}=\sqrt{2} \approx 1.414\)</span>.</p>
<p>Notice that the norm must be non-negative since it’s the square root of a sum of squares, i.e.&nbsp;<span class="math inline">\(||\mathbf{v}|| \geq 0\)</span>. This should sound right, after all negative lengths don’t make any sense.</p>
<p>What happens if we scale a vector <span class="math inline">\(\mathbf{v}\)</span> by some scalar <span class="math inline">\(c\)</span>? By the rules of scalar-vector multiplication, the new vector should be <span class="math inline">\(c\mathbf{v}=(cx,cy)\)</span>. Since the new vector has length <span class="math inline">\(||c\mathbf{v}||\)</span>, a little math shows that</p>
<p><span class="math display">\[||c\mathbf{v}|| = \sqrt{(cv_x)^2 + (cv_y)^2} = \sqrt{c^2(v_x^2 + v_y^2)} = |c| \sqrt{v_x^2 + v_y^2} = |c| \cdot ||\mathbf{v}||.\]</span></p>
<p>That is, the re-scaled vector <span class="math inline">\(c\mathbf{v}\)</span> just gets its length re-scaled by <span class="math inline">\(c\)</span>. That’s why <span class="math inline">\(c\)</span> is called a scalar. It rescales vectors. Notice if <span class="math inline">\(c\)</span> is negative, the length stays the same, but the direction gets reversed <span class="math inline">\(180^\circ\)</span> since in that case <span class="math inline">\(c\mathbf{v} = c(v_x, v_y) = -|c|(v_x,v_y)\)</span>.</p>
<p>Here’s what vector scaling looks like geometrically. I’ll plot the vector <span class="math inline">\(\mathbf{v}=(1,1)\)</span> again, but scaled by two numbers, one <span class="math inline">\(c=2\)</span>, the other <span class="math inline">\(c=-1\)</span>. When <span class="math inline">\(c=2\)</span>, the vector just doubles its length. That’s the light blue arrow. When <span class="math inline">\(c=-1\)</span>, the vector reverses its direction <span class="math inline">\(180^\circ\)</span>, but maintains its length since <span class="math inline">\(|c|=1\)</span>. That’s the light orange arrow.</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>v <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">1</span>])</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>plot_vectors([v, <span class="op">-</span>v, <span class="dv">2</span><span class="op">*</span>v], xlim<span class="op">=</span>(<span class="op">-</span><span class="dv">2</span>,<span class="dv">3</span>), ylim<span class="op">=</span>(<span class="op">-</span><span class="dv">2</span>,<span class="dv">3</span>), title<span class="op">=</span><span class="ss">f'Scaling Vectors'</span>, headwidth<span class="op">=</span><span class="dv">7</span>, ticks_every<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>             labels<span class="op">=</span>[<span class="st">'$\mathbf</span><span class="sc">{v}</span><span class="st">$'</span>, <span class="st">'$-\mathbf</span><span class="sc">{v}</span><span class="st">$'</span>, <span class="st">'$2\mathbf</span><span class="sc">{v}</span><span class="st">$'</span>], </span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>             colors<span class="op">=</span>[<span class="st">'black'</span>, <span class="st">'salmon'</span>, <span class="st">'steelblue'</span>],</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>             text_offsets<span class="op">=</span>[[<span class="op">-</span><span class="fl">0.2</span>, <span class="fl">0.2</span>], [<span class="op">-</span><span class="fl">0.2</span>, <span class="fl">0.4</span>], [<span class="op">-</span><span class="fl">0.2</span>, <span class="fl">0.2</span>]])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="vector-spaces_files/figure-html/cell-5-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>What does adding two vectors do? Let <span class="math inline">\(\mathbf{v}=(v_x,v_y)\)</span> and <span class="math inline">\(\mathbf{w}=(w_x,w_y)\)</span> be two vectors in the plane. Then their sum is <span class="math inline">\(\mathbf{v}+\mathbf{w} = (v_x+w_x,v_y+w_y)\)</span>. I’ll plot an example below with <span class="math inline">\(\mathbf{v}=(1,1)\)</span> and <span class="math inline">\(\mathbf{w}=(1,3)\)</span>. Their sum should be</p>
<p><span class="math display">\[\mathbf{v}+\mathbf{w}=(1+1,1+3)=(2,4).\]</span></p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>v <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">1</span>])</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">3</span>])</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>plot_vectors([v, w, v <span class="op">+</span> w], xlim<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">3</span>), ylim<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">5</span>), title<span class="op">=</span><span class="ss">f'Adding Two Vectors'</span>, ticks_every<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>             labels<span class="op">=</span>[<span class="st">'$\mathbf</span><span class="sc">{v}</span><span class="st">$'</span>, <span class="st">'$\mathbf</span><span class="sc">{w}</span><span class="st">$'</span>, <span class="st">'$\mathbf</span><span class="sc">{v}</span><span class="st">+\mathbf</span><span class="sc">{w}</span><span class="st">$'</span>], </span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>             colors<span class="op">=</span>[<span class="st">'salmon'</span>, <span class="st">'steelblue'</span>, <span class="st">'black'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="vector-spaces_files/figure-html/cell-6-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>It may not be obvious yet what vector addition is doing geometrically. Let me plot it slightly differently. What I’ll do is plot the vectors “head to tail” by taking the <em>tail</em> of <span class="math inline">\(\mathbf{w}\)</span> and placing it at the <em>head</em> of <span class="math inline">\(\mathbf{v}\)</span>. Then the head of this translated <span class="math inline">\(\mathbf{w}\)</span> vector points at the head of the sum <span class="math inline">\(\mathbf{v}+\mathbf{w}\)</span>. We can do this “head to tail” stuff since the base of a vector is irrelevant. We can place the arrow wherever we want as long as we maintain its length and direction.</p>
<p>Informally speaking, to add two vectors, just stack them on top of each other head to tail, and draw an arrow from the starting point to the ending point. You can geometrically add arbitrarily many vectors this way, not just two. Just keep stacking them.</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>plot_vectors([v, w, v <span class="op">+</span> w], xlim<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">3</span>), ylim<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">5</span>), title<span class="op">=</span><span class="ss">f'Adding Two Vectors (Head to Tail)'</span>,</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>             colors<span class="op">=</span>[<span class="st">'salmon'</span>, <span class="st">'steelblue'</span>, <span class="st">'black'</span>],</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>             tails<span class="op">=</span>[[<span class="dv">0</span>, <span class="dv">0</span>], [v[<span class="dv">0</span>], v[<span class="dv">1</span>]], [<span class="dv">0</span>, <span class="dv">0</span>]], text_offsets<span class="op">=</span>[[<span class="op">-</span><span class="fl">0.5</span>, <span class="op">-</span><span class="fl">0.85</span>], [<span class="fl">0.5</span>, <span class="op">-</span><span class="fl">0.8</span>], [<span class="op">-</span><span class="fl">1.4</span>, <span class="op">-</span><span class="fl">1.6</span>]],</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>             labels<span class="op">=</span>[<span class="st">'$\mathbf</span><span class="sc">{v}</span><span class="st">$'</span>, <span class="st">'$\mathbf</span><span class="sc">{w}</span><span class="st">$'</span>, <span class="st">'$\mathbf</span><span class="sc">{v}</span><span class="st">+\mathbf</span><span class="sc">{w}</span><span class="st">$'</span>],</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>             zorders <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>], ticks_every<span class="op">=</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="vector-spaces_files/figure-html/cell-7-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>The norm satisfies what’s known as the <strong>triangle inequality</strong>: If <span class="math inline">\(\mathbf{v}\)</span> and <span class="math inline">\(\mathbf{w}\)</span> are two vectors, then the length of their sum is less than the sum of their individual lengths, i.e.</p>
<p><span class="math display">\[||\mathbf{v}+\mathbf{w}|| \leq ||\mathbf{v}|| + ||\mathbf{w}||.\]</span></p>
<p>You can see this by staring at the plot above. The added lengths of <span class="math inline">\(\mathbf{v}\)</span> and <span class="math inline">\(\mathbf{w}\)</span> is larger than the length of their sum <span class="math inline">\(\mathbf{v}+\mathbf{w}\)</span>. In fact, the only time the lengths will be equal is if <span class="math inline">\(\mathbf{v}\)</span> and <span class="math inline">\(\mathbf{w}\)</span> are parallel to each other.</p>
<p>What about subtracting two vectors? By combining the rules for scalar multiplication and vector addition, you can convince yourself that the difference of two vectors is also element-wise,</p>
<p><span class="math display">\[\mathbf{v}-\mathbf{w} = (v_x-w_x,v_y-w_y).\]</span></p>
<p>To visualize what subtracting two vectors looks like, notice we can write subtraction as a sum like this, <span class="math inline">\(\mathbf{w} + (\mathbf{v}-\mathbf{w}) = \mathbf{v}\)</span>. Now use the same trick for adding vectors, only this time placing <span class="math inline">\((\mathbf{v}-\mathbf{w})\)</span> at the head of <span class="math inline">\(\mathbf{w}\)</span>, and noticing that it points to the sum of the two, which is <span class="math inline">\(\mathbf{v}\)</span>.</p>
<p>An easy way to remember what subtracting two vectors looks like is to connect the two vectors you’re subtracting with a line segment, and place the head on the first vector. This trick will never fail you.</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>v <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">1</span>])</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">3</span>])</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>plot_vectors([v, w, v <span class="op">-</span> w], xlim<span class="op">=</span>(<span class="op">-</span><span class="fl">0.5</span>, <span class="fl">1.5</span>), ylim<span class="op">=</span>(<span class="op">-</span><span class="fl">0.5</span>, <span class="fl">3.5</span>), title<span class="op">=</span><span class="ss">f'Subtracting Two Vectors'</span>, headwidth<span class="op">=</span><span class="dv">4</span>,</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>             ticks_every<span class="op">=</span><span class="dv">1</span>, colors<span class="op">=</span>[<span class="st">'salmon'</span>, <span class="st">'steelblue'</span>, <span class="st">'black'</span>],</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>             tails<span class="op">=</span>[[<span class="dv">0</span>, <span class="dv">0</span>], [<span class="dv">0</span>, <span class="dv">0</span>], [w[<span class="dv">0</span>], w[<span class="dv">1</span>]]], text_offsets<span class="op">=</span>[[<span class="op">-</span><span class="fl">0.5</span>, <span class="op">-</span><span class="fl">0.8</span>], [<span class="op">-</span><span class="fl">0.5</span>, <span class="op">-</span><span class="dv">1</span>], [<span class="fl">1.05</span>, <span class="fl">3.8</span>]],</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>             labels<span class="op">=</span>[<span class="st">'$\mathbf</span><span class="sc">{v}</span><span class="st">$'</span>, <span class="st">'$\mathbf</span><span class="sc">{w}</span><span class="st">$'</span>, <span class="st">'$\mathbf</span><span class="sc">{v}</span><span class="st">-\mathbf</span><span class="sc">{w}</span><span class="st">$'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="vector-spaces_files/figure-html/cell-8-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="the-dot-product" class="level2">
<h2 class="anchored" data-anchor-id="the-dot-product">The Dot Product</h2>
<p>It turns out we can understand both the lengths and angles of vectors in terms of a single operation called the <strong>dot product</strong>, also called the inner or scalar product. The dot product is a kind of multiplication between two vectors that returns a scalar. If <span class="math inline">\(\mathbf{v}=(v_x,v_y)\)</span> and <span class="math inline">\(\mathbf{w}=(w_x,w_y)\)</span> are two vectors in the plane, their dot product is defined as</p>
<p><span class="math display">\[\mathbf{v} \cdot \mathbf{w} = v_x w_x + v_y w_y.\]</span></p>
<p>That is, the dot product is just the sum of the element-wise products of the two vectors.</p>
<p>In terms of vectorized numpy code, the dot product is just the operation <code>np.sum(v * w)</code>. Numpy also has a convenience function <code>np.dot(v, w)</code> that calculates it directly. Here’s the calculation of the dot product between the two vectors <span class="math inline">\(\mathbf{v}=(5,-1)\)</span> and <span class="math inline">\(\mathbf{w}=(2,4)\)</span>. The answer should be</p>
<p><span class="math display">\[\mathbf{v} \cdot \mathbf{w} = 5 \cdot 2 + (-1) \cdot 4 = 10 - 4 = 6.\]</span></p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>v <span class="op">=</span> np.array([<span class="dv">5</span>, <span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> np.array([<span class="dv">2</span>, <span class="dv">4</span>])</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'v . w = </span><span class="sc">{</span>np<span class="sc">.</span>dot(v, w)<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>np.<span class="bu">sum</span>(v <span class="op">*</span> w) <span class="op">==</span> np.dot(v, w)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>v . w = 6</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="8">
<pre><code>True</code></pre>
</div>
</div>
<p><strong>Algorithm Analysis:</strong> Evaluating the dot product uses <span class="math inline">\(2n-1\)</span> or <span class="math inline">\(O(n)\)</span> total FLOPS, since for a vector of size <span class="math inline">\(n\)</span> there are <span class="math inline">\(n\)</span> multiplications and <span class="math inline">\(n-1\)</span> additions.</p>
<p>Here are some fairly trivial properties the dot product satisfies. These follow straight from the definition.</p>
<ul>
<li>The dot product of a vector with itself is nonnegative: <span class="math inline">\(\mathbf{v} \cdot \mathbf{v} \geq 0\)</span>.</li>
<li>It commutes: <span class="math inline">\(\mathbf{v} \cdot \mathbf{w} = \mathbf{w} \cdot \mathbf{v}\)</span>.</li>
<li>It distributes over scalar multiplication: <span class="math inline">\(c\mathbf{v} \cdot \mathbf{w} = \mathbf{v} \cdot c\mathbf{w} = c(\mathbf{v} \cdot \mathbf{w})\)</span>.</li>
<li>It distributes over vector addition: <span class="math inline">\((\mathbf{u} + \mathbf{v}) \cdot \mathbf{w} = \mathbf{u} \cdot \mathbf{w} + \mathbf{v} \cdot \mathbf{w}\)</span> and <span class="math inline">\(\mathbf{v} \cdot (\mathbf{u}+\mathbf{w}) = \mathbf{v} \cdot \mathbf{u} + \mathbf{v} \cdot \mathbf{w}\)</span>.</li>
</ul>
<p><strong>Notation</strong>: The dot product is often written in several different ways in different fields. Another notation arises by thinking of the dot product as the matrix multiplication of a <em>row vector</em> <span class="math inline">\(\mathbf{v}^\top = \begin{pmatrix}v_x &amp; v_y \end{pmatrix}\)</span> with a <em>column vector</em> <span class="math inline">\(\mathbf{w} = \begin{pmatrix} w_x \\ w_y \end{pmatrix}\)</span>. In that case,</p>
<p><span class="math display">\[
\mathbf{v}^\top \mathbf{w} =
\begin{pmatrix} v_x &amp; v_y \end{pmatrix}
\begin{pmatrix} w_x \\ w_y \end{pmatrix}
= v_x w_x + v_y w_y = \mathbf{v} \cdot \mathbf{w}.
\]</span></p>
<p>This is the most commonly used notation for the dot product in machine learning. I’ll use it more frequently after this lesson.</p>
<p>We can write the norm or length of a vector in terms of the dot product. Observe that by dotting <span class="math inline">\(\mathbf{v}\)</span> with itself, I get</p>
<p><span class="math display">\[\mathbf{v} \cdot \mathbf{v} = v_x^2 + v_y^2 = ||\mathbf{v}||^2.\]</span></p>
<p>Taking the square root of both sides, you can see that the norm or length of a vector is just the square root of its dot product with itself,</p>
<p><span class="math display">\[||\mathbf{v}|| = \sqrt{\mathbf{v} \cdot \mathbf{v}}.\]</span></p>
<p>We can also talk about the <strong>distance</strong> between any two vectors <span class="math inline">\(\mathbf{v}\)</span> and <span class="math inline">\(\mathbf{w}\)</span>. Denote the distance between these two vectors as <span class="math inline">\(d(\mathbf{v}, \mathbf{w})\)</span>. Since the difference vector is <span class="math inline">\(\mathbf{v} - \mathbf{w}\)</span>, the distance between the two vectors is evidently just the length of the difference vector,</p>
<p><span class="math display">\[d(\mathbf{v}, \mathbf{w}) = ||\mathbf{v} - \mathbf{w}|| = \sqrt{(\mathbf{v} - \mathbf{w}) \cdot (\mathbf{v} - \mathbf{w})} = \sqrt{(v_x-w_x)^2 - (v_y-w_y)^2}.\]</span></p>
<p>For example, the distance between the two vectors <span class="math inline">\(\mathbf{v}=(1,1)\)</span> and <span class="math inline">\(\mathbf{w}=(1, 0)\)</span> is</p>
<p><span class="math display">\[d(\mathbf{v}, \mathbf{w}) = ||\mathbf{v} - \mathbf{w}|| = \sqrt{(1-1)^2 + (1-0)^2} = 1.\]</span></p>
<p>If a vector <span class="math inline">\(\mathbf{e}\)</span> has norm <span class="math inline">\(||\mathbf{e}||=1\)</span> it’s called a <strong>unit vector</strong>. We can convert any non-zero vector <span class="math inline">\(\mathbf{v}\)</span> into a unit vector by dividing by its norm, which is called <strong>normalizing</strong> <span class="math inline">\(\mathbf{v}\)</span>. The unit vector gotten from normalizing <span class="math inline">\(\mathbf{v}\)</span> I’ll call <span class="math inline">\(\mathbf{e_v}\)</span>. It’s given by</p>
<p><span class="math display">\[\mathbf{e_v} = \frac{\mathbf{v}}{||\mathbf{v}||}.\]</span></p>
<p>For example, if <span class="math inline">\(\mathbf{v}=(1,1)\)</span>, its norm is <span class="math inline">\(||\mathbf{v}||=\sqrt{2}\)</span>, so if we wanted to normalize it into a new unit vector <span class="math inline">\(\mathbf{e_v}\)</span>, we’d have</p>
<p><span class="math display">\[\mathbf{e_v} = \frac{\mathbf{v}}{||\mathbf{v}||} = \frac{\mathbf{v}}{\sqrt{2}} = \frac{1}{\sqrt{2}}(1,1) \approx (0.707, 0.707).\]</span></p>
<p>Unit vectors will always point in the same direction as the vector used to normalize them. The only difference is they’ll have length one. In the plane, unit vectors will always lie along the unit circle. Here’s a plot of this idea using the previous example.</p>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>v <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">2</span>])</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>ev <span class="op">=</span> v <span class="op">/</span> np.sqrt(<span class="dv">2</span>)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>plot_vectors([v, ev], title<span class="op">=</span><span class="st">'$\mathbf</span><span class="sc">{e}</span><span class="st">_v = ||\mathbf</span><span class="sc">{v}</span><span class="st">||^{-1} \mathbf</span><span class="sc">{v}</span><span class="st">$'</span>, ticks_every<span class="op">=</span><span class="fl">0.5</span>, zorders<span class="op">=</span>[<span class="dv">0</span>, <span class="dv">1</span>],</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>             text_offsets<span class="op">=</span>[[<span class="fl">0.01</span>, <span class="fl">0.05</span>], [<span class="op">-</span><span class="fl">0.2</span>, <span class="fl">0.2</span>]], colors<span class="op">=</span>[<span class="st">'steelblue'</span>, <span class="st">'red'</span>],</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>             labels<span class="op">=</span>[<span class="st">'$\mathbf</span><span class="sc">{v}</span><span class="st">$'</span>, <span class="st">'$\mathbf</span><span class="sc">{e}</span><span class="st">_v$'</span>], headwidth<span class="op">=</span><span class="dv">6</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="vector-spaces_files/figure-html/cell-10-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="projections" class="level2">
<h2 class="anchored" data-anchor-id="projections">Projections</h2>
<p>Let <span class="math inline">\(\mathbf{e}_x=(1,0)\)</span>. It’s the unit vector pointing along the positive x-axis. Notice the dot product between <span class="math inline">\(\mathbf{v}=(v_x, v_y)\)</span> and <span class="math inline">\(\mathbf{e}_x\)</span> is just</p>
<p><span class="math display">\[\mathbf{v} \cdot \mathbf{e}_x = v_x \cdot 1 + v_y \cdot 0 = v_x.\]</span></p>
<p>Evidently the dot product <span class="math inline">\(\mathbf{v} \cdot \mathbf{e}_x\)</span> “picks” out the x-component of <span class="math inline">\(\mathbf{v}\)</span>, namely <span class="math inline">\(v_x\)</span>. The vector <span class="math inline">\(v_x \mathbf{e}_x = (v_x,0)\)</span> gotten by rescaling <span class="math inline">\(\mathbf{e}_x\)</span> by <span class="math inline">\(v_x\)</span> is called the <strong>projection</strong> of <span class="math inline">\(\mathbf{v}\)</span> onto the x-axis. It’s the vector you’d get by dropping <span class="math inline">\(\mathbf{v}\)</span> perpendicular to the x-axis.</p>
<p>Similarly, if <span class="math inline">\(\mathbf{e}_y = (0,1)\)</span> is the unit vector along the positive y-axis, we can “pick out” the y-component of <span class="math inline">\(\mathbf{v}\)</span> by taking the dot product of <span class="math inline">\(\mathbf{v}\)</span> with <span class="math inline">\(\mathbf{e}_y\)</span>, i.e.&nbsp;<span class="math inline">\(v_y = \mathbf{v} \cdot \mathbf{e}_y\)</span>. The vector <span class="math inline">\(v_y\mathbf{e}_y\)</span> is the projection of <span class="math inline">\(\mathbf{v}\)</span> onto the y-axis.</p>
<p>Evidently, then, <span class="math inline">\(\mathbf{v}\)</span> is just the sum of projections of <span class="math inline">\(\mathbf{v}\)</span> onto all of the axes,</p>
<p><span class="math display">\[\mathbf{v} = v_x \mathbf{e_x} + v_y \mathbf{e_y}.\]</span></p>
<p>This is yet another way to express a vector in terms of its components. Just project down onto the axes and sum up the linear combination.</p>
<p>Here’s what this looks like when <span class="math inline">\(\mathbf{v}=(0.5,1)\)</span>. In this example, the projection onto the x-axis is just <span class="math inline">\(v_x \mathbf{e}_x=(0.5, 0)\)</span>, and the projection onto the y-axis is just <span class="math inline">\(v_y \mathbf{e_y}=(0,1)\)</span>. Using these projections, we can write <span class="math inline">\(\mathbf{v}=(0.5,1)\)</span> as <span class="math inline">\(\mathbf{v} = 0.5 \mathbf{e}_x + \mathbf{e}_y\)</span>.</p>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>v <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">2</span>])</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>ex <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">0</span>])</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>ey <span class="op">=</span> np.array([<span class="dv">0</span>, <span class="dv">1</span>])</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>plot_vectors([v, v[<span class="dv">0</span>] <span class="op">*</span> ex, v[<span class="dv">1</span>] <span class="op">*</span> ey], title<span class="op">=</span><span class="st">'Projections Of $\mathbf</span><span class="sc">{v}</span><span class="st">$'</span>, ticks_every<span class="op">=</span><span class="fl">0.5</span>,</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>             text_offsets<span class="op">=</span>[[<span class="fl">0.02</span>, <span class="fl">0.1</span>], [<span class="op">-</span><span class="fl">0.1</span>, <span class="fl">0.2</span>], [<span class="fl">0.05</span>, <span class="fl">0.05</span>]], colors<span class="op">=</span>[<span class="st">'red'</span>, <span class="st">'steelblue'</span>, <span class="st">'steelblue'</span>],</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>             labels<span class="op">=</span>[<span class="st">'$\mathbf</span><span class="sc">{v}</span><span class="st">$'</span>, <span class="st">'$v_x \mathbf</span><span class="sc">{e}</span><span class="st">_x$'</span>, <span class="st">'$v_y \mathbf</span><span class="sc">{e}</span><span class="st">_y$'</span>], headwidth<span class="op">=</span><span class="dv">4</span>,</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>             xlim<span class="op">=</span>(<span class="op">-</span><span class="fl">0.5</span>, <span class="fl">2.5</span>), ylim<span class="op">=</span>(<span class="op">-</span><span class="fl">0.5</span>, <span class="fl">2.5</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="vector-spaces_files/figure-html/cell-11-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="linear-independence" class="level2">
<h2 class="anchored" data-anchor-id="linear-independence">Linear Independence</h2>
<p>I just showed we can decompose any vector <span class="math inline">\(\mathbf{v} \in \mathbb{R}^2\)</span> into its projections <span class="math inline">\(\mathbf{v} = v_x \mathbf{e}_x + v_y \mathbf{e}_y\)</span>. The fact we can do this is because the unit vectors <span class="math inline">\(\mathbf{e}_x\)</span> and <span class="math inline">\(\mathbf{e}_y\)</span> are special, for a few reasons.</p>
<p>The first reason these vectors are special is that they don’t lie along the same line in the plane. Said differently, we can’t write one vector as a scalar multiple of the other, <span class="math inline">\(\mathbf{e}_x \neq c \mathbf{e}_y\)</span> for any scalar <span class="math inline">\(c\)</span>. Vectors with this property are called <em>linearly independent</em>.</p>
<p>More generally, a set of <span class="math inline">\(k\)</span> vectors <span class="math inline">\(\mathbf{v}_0, \mathbf{v}_1, \cdots, \mathbf{v}_{k-1}\)</span> is called <strong>linearly independent</strong> if no one vector <span class="math inline">\(\mathbf{v}_j\)</span> in the set can be written as a linear combination of the rest, i.e.&nbsp;for <em>any</em> choice of scalars <span class="math inline">\(c_i\)</span>,</p>
<p><span class="math display">\[\mathbf{v}_j \neq \sum_{i \neq j} c_i \mathbf{v}_i.\]</span></p>
<p>A set of vectors that isn’t linearly independent is called <strong>linearly dependent</strong>. In a linearly dependent set, you can always express at least one vector as a linear combination of the rest, for example by finding a choice of scalars <span class="math inline">\(c_i\)</span>, you could write <span class="math inline">\(\mathbf{v}_0\)</span> as</p>
<p><span class="math display">\[\mathbf{v}_0 = \sum_{i=1}^{k-1} c_i \mathbf{v}_i = c_1 \mathbf{v}_1 + c_2 \mathbf{v}_2 + \cdots + c_{k-1} \mathbf{v}_{k-1}.\]</span></p>
<p>Linearly dependent sets of vectors are redundant in a sense. We have more than we need. We can always keep dropping vectors from the set until the ones remaining are linearly independent.</p>
<p>The vector space spanned by all linear combinations of a set of vectors is called the <strong>span</strong> of that set. The span of a single vector will always be a <em>line</em>, since a linear combination of any one vector is just the scalar multiples of that vector. The span of any <em>two</em> linearly independent vectors will always be a <em>plane</em>. The span of <span class="math inline">\(k\)</span> linearly independent vectors will form a <span class="math inline">\(k\)</span>-dimensional <em>hyperplane</em>.</p>
<p>As a simple example, consider the following set of vectors in the plane,</p>
<p><span class="math display">\[\begin{align*}
\mathbf{v}_0 &amp;= (1, 0), \\
\mathbf{v}_1 &amp;= (0, 1), \\
\mathbf{v}_2 &amp;= (1, 1).
\end{align*}\]</span></p>
<p>If you stare at these for a second, you’ll see that <span class="math inline">\(\mathbf{v}_2 = \mathbf{v}_0 + \mathbf{v}_1\)</span>, so this set can’t be linearly independent. The third vector is redundant. Any two vectors in this set span the exact same plane <span class="math inline">\(\mathbb{R}^2\)</span>. In fact, you’ll never have more than 2 linearly independent vectors of size 2. Why?</p>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>v0 <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">0</span>])</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>v1 <span class="op">=</span> np.array([<span class="dv">0</span>, <span class="dv">1</span>])</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>v2 <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">1</span>])</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>plot_vectors(</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>    [v2, v0, v1], colors<span class="op">=</span>[<span class="st">'salmon'</span>, <span class="st">'steelblue'</span>, <span class="st">'limegreen'</span>], xlim<span class="op">=</span>(<span class="op">-</span><span class="fl">0.5</span>, <span class="fl">1.5</span>), ylim<span class="op">=</span>(<span class="op">-</span><span class="fl">0.5</span>, <span class="fl">1.5</span>),</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>    ticks_every<span class="op">=</span><span class="fl">0.5</span>, zorders<span class="op">=</span>[<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>], headwidth<span class="op">=</span><span class="dv">5</span>, text_offsets<span class="op">=</span>[[<span class="fl">0.03</span>, <span class="fl">0.05</span>], [<span class="fl">0.03</span>,<span class="fl">0.05</span>], [<span class="fl">0.03</span>,<span class="fl">0.05</span>]],</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>    title<span class="op">=</span><span class="st">'$\mathbf</span><span class="sc">{v}</span><span class="st">_0$, $\mathbf</span><span class="sc">{v}</span><span class="st">_1$, $\mathbf</span><span class="sc">{v}</span><span class="st">_2=\mathbf</span><span class="sc">{v}</span><span class="st">_0+\mathbf</span><span class="sc">{v}</span><span class="st">_1$'</span>, </span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>    labels<span class="op">=</span>[<span class="st">'$\mathbf</span><span class="sc">{v}</span><span class="st">_2$'</span>, <span class="st">'$\mathbf</span><span class="sc">{v}</span><span class="st">_0$'</span>, <span class="st">'$\mathbf</span><span class="sc">{v}</span><span class="st">_1$'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="vector-spaces_files/figure-html/cell-12-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>For vectors in <span class="math inline">\(\mathbb{R}^2\)</span>, there are only two possibilities, they either lie on the same line, or they span the whole plane. This follows from the fact that any vector <span class="math inline">\(\mathbf{v}\)</span> can be decomposed as <span class="math inline">\(\mathbf{v} = v_x \mathbf{e}_x + v_y \mathbf{e}_y\)</span>. An implication of this fact is that a set of vectors in <span class="math inline">\(\mathbb{R}^2\)</span> can only be linearly independent if it contains only one or two vectors. If it contains a third vector, that vector <em>must</em> be a linear combination of the other two. The maximum number of linearly independent vectors in a set is the <strong>dimension</strong> of the vector space. Since <span class="math inline">\(\mathbb{R}^2\)</span> is 2-dimensional, it can only sustain 2 linearly independent vectors at a time.</p>
</section>
<section id="basis-vectors" class="level2">
<h2 class="anchored" data-anchor-id="basis-vectors">Basis Vectors</h2>
<p>In <span class="math inline">\(\mathbb{R}^2\)</span>, if we can find any two vectors <span class="math inline">\(\mathbf{a}\)</span> and <span class="math inline">\(\mathbf{b}\)</span> that are linearly independent, then we can write any other vector <span class="math inline">\(\mathbf{v}\)</span> as a linear combination of those two vectors,</p>
<p><span class="math display">\[\mathbf{v} = v_a \mathbf{a} + v_b \mathbf{b}.\]</span></p>
<p>The set <span class="math inline">\(\{\mathbf{a}, \mathbf{b}\}\)</span> is called a <em>basis</em>. We can use vectors in this set as a “basis” to write any other vector.</p>
<p>More generally, a set of <span class="math inline">\(k\)</span> vectors <span class="math inline">\(\mathbf{v}_0, \mathbf{v}_1, \cdots, \mathbf{v}_{k-1}\)</span> form a <strong>basis</strong> for a vector space if the following two conditions hold,</p>
<ol type="1">
<li>The vectors are all linearly independent,</li>
<li>The vectors span the full vector space.</li>
</ol>
<p>Another way of saying the same thing is that a basis is a set of exactly <span class="math inline">\(n\)</span> linearly independent vectors, where <span class="math inline">\(n\)</span> is the dimension of the vector space. A basis contains the minimal number of vectors needed to span the vector space.</p>
<p>The special vectors <span class="math inline">\(\mathbf{e}_x\)</span> and <span class="math inline">\(\mathbf{e}_y\)</span> form a basis for <span class="math inline">\(\mathbb{R}^2\)</span>, since we can write any other vector as a linear combination of those two. Not only are these two vectors a basis, however. They satisfy two other useful properties,</p>
<ol type="1">
<li>They’re both unit vectors, <span class="math inline">\(||\mathbf{e}_x|| = ||\mathbf{e}_y|| = 1\)</span>.</li>
<li>They’re <strong>orthogonal</strong> to each other, that is, <span class="math inline">\(\mathbf{e}_x \cdot \mathbf{e}_y = 0\)</span>.</li>
</ol>
<p>A basis satisfying these two properties is called an <strong>orthonormal basis</strong>. An orthonormal basis is special in that it allows us to pick out the components of a vector directly by just taking dot products with the basis vectors. It’s only true in an orthonormal basis that we can write the components of a vector <span class="math inline">\(\mathbf{v}\)</span> as,</p>
<p><span class="math display">\[\begin{align*}
v_x &amp;= \mathbf{v} \cdot \mathbf{e}_x, \\
v_y &amp;= \mathbf{v} \cdot \mathbf{e}_y.
\end{align*}\]</span></p>
<p>The set <span class="math inline">\(\{\mathbf{e}_x, \mathbf{e}_y\}\)</span> is only one example of an orthonormal basis for <span class="math inline">\(\mathbb{R}^2\)</span>. It’s called the <strong>standard basis</strong>, since it’s the basis whose vectors point along the usual positive x and y axes.</p>
<p>Expressing any vector in terms of its basis is just projecting the vector down onto each of the basis axes. Let’s do a quick example. Let <span class="math inline">\(\mathbf{v}=(1.25,2)\)</span> be a vector. Decomposed into the standard basis we just get</p>
<p><span class="math display">\[\mathbf{v} = 1.25 \mathbf{e}_x + 2 \mathbf{e}_y.\]</span></p>
<p>Graphically this just looks as follows. We’ve already seen a plot like this, except this time I’m including the basis vectors <span class="math inline">\(\mathbf{e}_x\)</span> and <span class="math inline">\(\mathbf{e}_y\)</span> explicitly. Notice that the two basis vectors form a <span class="math inline">\(90^\circ\)</span> angle, i.e.&nbsp;they’re perpendicular. I’ll show in a moment that this is implied by the fact that <span class="math inline">\(\mathbf{e}_x \cdot \mathbf{e}_y = 0\)</span>.</p>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>v <span class="op">=</span> np.array([<span class="fl">1.25</span>, <span class="dv">2</span>])</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>ex <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">0</span>])</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>ey <span class="op">=</span> np.array([<span class="dv">0</span>, <span class="dv">1</span>])</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>plot_vectors(</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>    [v, v[<span class="dv">0</span>] <span class="op">*</span> ex, v[<span class="dv">1</span>] <span class="op">*</span> ey, ex, ey], colors<span class="op">=</span>[<span class="st">'red'</span>, <span class="st">'steelblue'</span>, <span class="st">'steelblue'</span>, <span class="st">'black'</span>, <span class="st">'black'</span>], </span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>    ticks_every<span class="op">=</span><span class="dv">1</span>, zorders<span class="op">=</span>[<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>], headwidth<span class="op">=</span><span class="dv">5</span>,</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>    text_offsets<span class="op">=</span>[[<span class="dv">0</span>,<span class="dv">0</span>], [<span class="dv">0</span>,<span class="fl">0.2</span>], [<span class="fl">0.05</span>,<span class="dv">0</span>], [<span class="op">-</span><span class="fl">0.2</span>,<span class="fl">0.2</span>], [<span class="fl">0.05</span>,<span class="dv">0</span>]],</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>    title<span class="op">=</span><span class="st">'$\mathbf</span><span class="sc">{v}</span><span class="st">=v_x \mathbf</span><span class="sc">{e}</span><span class="st">_x + v_y \mathbf</span><span class="sc">{e}</span><span class="st">_y$'</span>, </span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>    labels<span class="op">=</span>[<span class="st">'$\mathbf</span><span class="sc">{v}</span><span class="st">$'</span>, <span class="st">'$v_x \mathbf</span><span class="sc">{e}</span><span class="st">_x$'</span>, <span class="st">'$v_y \mathbf</span><span class="sc">{e}</span><span class="st">_y$'</span>, <span class="st">'$\mathbf</span><span class="sc">{e}</span><span class="st">_x$'</span>, <span class="st">'$\mathbf</span><span class="sc">{e}</span><span class="st">_y$'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="vector-spaces_files/figure-html/cell-13-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Of course, I already said the standard basis isn’t the <em>only</em> orthonormal basis for <span class="math inline">\(\mathbb{R}^2\)</span> we could choose. Here’s an example of another one that would work equally well. Let <span class="math inline">\(\mathbf{e}_a=\frac{1}{\sqrt{2}} (1,1)\)</span> and <span class="math inline">\(\mathbf{e}_b=\frac{1}{\sqrt{2}} (-1,1)\)</span>. Notice that both vectors have unit length, and they’re orthogonal since <span class="math inline">\(\mathbf{e}_a \cdot \mathbf{e}_b = 0\)</span>. Thus, they form an orthonormal basis for <span class="math inline">\(\mathbb{R}^2\)</span>. In this basis, <span class="math inline">\(\mathbf{v}=(1.25, 2)\)</span> would be written</p>
<p><span class="math display">\[\mathbf{v} = (\mathbf{v} \cdot \mathbf{e}_a) \mathbf{e}_a + (\mathbf{v} \cdot \mathbf{e}_b) \mathbf{e}_b \approx 2.298 \mathbf{e}_a + 0.530 \mathbf{e}_b.\]</span></p>
<p>This is a very different representation for <span class="math inline">\(\mathbf{v}\)</span>. Nevertheless, the two basis vectors are still perpendicular to each other. You can see a plot of this below.</p>
<p>There are infinitely many orthonormal bases for <span class="math inline">\(\mathbb{R}^2\)</span>. Just take any two perpendicular vectors in the plane and normalize them to unit length and they’ll form a valid orthonormal basis.</p>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>v <span class="op">=</span> np.array([<span class="fl">1.25</span>, <span class="dv">2</span>])</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>ea <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">1</span>]) <span class="op">/</span> np.sqrt(<span class="dv">2</span>)</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>eb <span class="op">=</span> np.array([<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>]) <span class="op">/</span> np.sqrt(<span class="dv">2</span>)</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>vectors <span class="op">=</span> [v, np.dot(v, ea) <span class="op">*</span> ea, np.dot(v, eb) <span class="op">*</span> eb, ea, eb]</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>plot_vectors(</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>    vectors, ticks_every<span class="op">=</span><span class="dv">1</span>, zorders<span class="op">=</span>[<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">5</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">2</span>], headwidth<span class="op">=</span><span class="dv">7</span>,</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>    colors<span class="op">=</span>[<span class="st">'red'</span>, <span class="st">'steelblue'</span>, <span class="st">'steelblue'</span>, <span class="st">'black'</span>, <span class="st">'black'</span>],</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>    text_offsets<span class="op">=</span>[[<span class="dv">0</span>, <span class="dv">0</span>], [<span class="fl">0.03</span>, <span class="dv">0</span>], [<span class="op">-</span><span class="fl">0.3</span>, <span class="op">-</span><span class="fl">0.65</span>], [<span class="op">-</span><span class="fl">0.1</span>, <span class="op">-</span><span class="fl">0.48</span>], [<span class="op">-</span><span class="fl">0.2</span>, <span class="fl">0.15</span>]],</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>    title<span class="op">=</span><span class="st">'$\mathbf</span><span class="sc">{v}</span><span class="st">=v_a \mathbf</span><span class="sc">{e}</span><span class="st">_a + v_b \mathbf</span><span class="sc">{e}</span><span class="st">_b$'</span>, </span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>    labels<span class="op">=</span>[<span class="st">'$\mathbf</span><span class="sc">{v}</span><span class="st">$'</span>, <span class="st">'$v_a \mathbf</span><span class="sc">{e}</span><span class="st">_a$'</span>, <span class="st">'$v_b \mathbf</span><span class="sc">{e}</span><span class="st">_b$'</span>, <span class="st">'$\mathbf</span><span class="sc">{e}</span><span class="st">_a$'</span>, <span class="st">'$\mathbf</span><span class="sc">{e}</span><span class="st">_b$'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="vector-spaces_files/figure-html/cell-14-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="cosine-similarity" class="level2">
<h2 class="anchored" data-anchor-id="cosine-similarity">Cosine Similarity</h2>
<p>Just like we can express the length of a vector using the dot product, it turns out we can also express the <em>angle</em> between any two vectors in the plane using the dot product. If <span class="math inline">\(\theta\)</span> is the angle between two vectors <span class="math inline">\(\mathbf{v}\)</span> and <span class="math inline">\(\mathbf{w}\)</span>, it turns out the dot product is given by</p>
<p><span class="math display">\[\mathbf{v} \cdot \mathbf{w} = ||\mathbf{v}|| \cdot ||\mathbf{w}|| \cos \theta.\]</span></p>
<p>Note that both sides of this equation are scalars since the dot product is a scalar and the product of norms is a scalar. If you’re good at trigonometry, you can convince yourself this formula must be true by projecting <span class="math inline">\(\mathbf{v}\)</span> onto <span class="math inline">\(\mathbf{w}\)</span> similar to the way we did projections onto the x and y axes before. The difference this time is that the component of <span class="math inline">\(\mathbf{v}\)</span> in the direction of <span class="math inline">\(\mathbf{w}\)</span> is not <span class="math inline">\(v_x\)</span> or <span class="math inline">\(v_y\)</span> anymore, but instead <span class="math inline">\(||\mathbf{v}|| \cos \theta\)</span>.</p>
<p>You can see two special cases of this formula by looking at what happens when the two vectors are <em>parallel</em> or <em>perpendicular</em>. If the two vectors are parallel, then <span class="math inline">\(\theta = 0^\circ, 180^\circ\)</span>, so <span class="math inline">\(\cos \theta = \pm 1\)</span>, so <span class="math inline">\(\mathbf{v} \cdot \mathbf{w} = \pm ||\mathbf{v}|| \cdot ||\mathbf{w}||\)</span>. More importantly, if the two vectors are perpendicular, then <span class="math inline">\(\theta = 90^\circ, 270^\circ\)</span>, so <span class="math inline">\(\cos \theta = 0\)</span>, so <span class="math inline">\(\mathbf{v} \cdot \mathbf{w} = 0\)</span>. That is, perpendicular vectors are <em>orthogonal</em>. They mean the same thing.</p>
<p>It’s more common to express this formula with <span class="math inline">\(\cos \theta\)</span> on one side and the vector terms on the other so you can solve for the angle (or more commonly just the cosine of the angle). In this case, we have <span class="math display">\[\cos \theta = \frac{\mathbf{v} \cdot \mathbf{w}}{||\mathbf{v}|| \cdot ||\mathbf{w}||}.\]</span></p>
<p>What matters more than anything is what this formula says and how to use it. Suppose, for example, you want to find the angle between the two vectors <span class="math inline">\(\mathbf{v} = (1,1)\)</span> and <span class="math inline">\(\mathbf{w} = (0, -1)\)</span>. Then you’d have</p>
<p><span class="math display">\[\begin{align*}
\mathbf{v} \cdot \mathbf{w} &amp;= 1 \cdot 0 + 1 \cdot (-1) = -1, \\
||\mathbf{v}|| &amp;= \sqrt{1^2 + 1^2} = \sqrt{2}, \\
||\mathbf{w}|| &amp;= \sqrt{0^2 + (-1)^2} = 1.
\end{align*}\]</span></p>
<p>Plugging this into the cosine formula gives,</p>
<p><span class="math display">\[
\cos \theta = \frac{-1}{\sqrt{2}} \quad \Longrightarrow \quad \theta = \cos^{-1}\bigg(\frac{-1}{\sqrt{2}}\bigg) = 135^\circ.
\]</span></p>
<p>You can verify this is correct by plotting the two vectors and confirming that they’re about <span class="math inline">\(135^\circ\)</span> from each other, which corresponds to about 1.25 quarter turns around a circle. It’s interesting to note that the dot product will only be negative when the angle between the two vectors is obtuse, i.e.&nbsp;more than <span class="math inline">\(90^\circ\)</span>, which is of course the case here.</p>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>v <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">1</span>])</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>w <span class="op">=</span> np.array([<span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>plot_vectors([v, w], title<span class="op">=</span><span class="st">'$\mathbf</span><span class="sc">{v}</span><span class="st"> \cdot \mathbf</span><span class="sc">{w}</span><span class="st"> = ||\mathbf</span><span class="sc">{v}</span><span class="st">||||\mathbf</span><span class="sc">{w}</span><span class="st">|| \cos </span><span class="ch">\\</span><span class="st">theta$'</span>, </span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>             text_offsets<span class="op">=</span>[[<span class="dv">0</span>, <span class="dv">0</span>], [<span class="fl">0.1</span>, <span class="dv">0</span>]], ticks_every<span class="op">=</span><span class="fl">0.5</span>, xlim<span class="op">=</span>(<span class="op">-</span><span class="dv">1</span>, <span class="dv">2</span>), ylim<span class="op">=</span>(<span class="op">-</span><span class="fl">1.5</span>, <span class="fl">1.5</span>),</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>             labels<span class="op">=</span>[<span class="st">'$\mathbf</span><span class="sc">{v}</span><span class="st">$'</span>, <span class="st">'$\mathbf</span><span class="sc">{w}</span><span class="st">$'</span>], colors<span class="op">=</span>[<span class="st">'red'</span>, <span class="st">'steelblue'</span>], headwidth<span class="op">=</span><span class="dv">7</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="vector-spaces_files/figure-html/cell-15-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>In machine learning, this formula for <span class="math inline">\(\cos \theta\)</span> is called the <strong>cosine similarity</strong>. The reason for this is that the dot product itself is a measure of how similar two vectors are. To see why, consider two special cases:</p>
<ul>
<li>The two vectors are parallel: This is as large as the dot product between two vectors can get in absolute value. The vectors are as similar as they can be in a sense. Up to a scalar multiple, they contain the same information.</li>
<li>The two vectors are perpendicular: This is as small as the dot product between two vectors can get in absolute value. The two vectors are as different as they can be in a sense. They share pretty much no information. Information about one vector tells you basically nothing about the other.</li>
</ul>
<p>The cosine similarity is a function of two input vectors <span class="math inline">\(\mathbf{v}\)</span> and <span class="math inline">\(\mathbf{w}\)</span>. Since we don’t actually care about the angle <span class="math inline">\(\theta\)</span> usually, we’ll more often denote the cosine similarity using a notation like <span class="math inline">\(\cos(\mathbf{v},\mathbf{w})\)</span> to make it clear it’s a function of its two input vectors,</p>
<p><span class="math display">\[\cos(\mathbf{v},\mathbf{w}) = \frac{\mathbf{v} \cdot \mathbf{w}}{||\mathbf{v}|| \cdot ||\mathbf{w}||}.\]</span></p>
<p>Note the cosine similarity is just a normalized dot product, since dividing by the norms forces <span class="math inline">\(-1 \leq \cos(\mathbf{v},\mathbf{w}) \leq 1\)</span>. It thus captures the same idea of similarity that the dot product does, but it’s more useful when the lengths of vectors get out of control. This is particularly likely to happen in high dimensions, when <span class="math inline">\(n &gt;&gt; 2\)</span>. This is the so-called “curse of dimensionality”. We’ll come back to this idea in future lessons.</p>
<p>Here’s a quick implementation of the cosine similarity function using numpy. There’s no built-in function to do it, but it’s easy enough to implement by making judicious use of the <code>np.dot</code> function. It should give the same answer found above for <span class="math inline">\(\cos \theta\)</span>, which is <span class="math inline">\(-\frac{1}{\sqrt{2}} \approx -0.707\)</span>.</p>
<div class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> cosine_similarity(v, w):</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.dot(v, w) <span class="op">/</span> np.sqrt(np.dot(v, v) <span class="op">*</span> np.dot(w, w))</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'cos(v, w) = </span><span class="sc">{</span>cosine_similarity(v, w)<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>cos(v, w) = -0.7071067811865475</code></pre>
</div>
</div>
<p><strong>Algorithm Analysis:</strong> Like the dot product, this function uses only <span class="math inline">\(O(n)\)</span> FLOPS. There are three independent dot product operations happening here, each adding <span class="math inline">\(O(n)\)</span> FLOPS. Since the outputs of dot products are scalars, the multiply and divide only add one FLOP each. The square root isn’t obvious, but you can assume it takes some constant number of FLOPS as well. The total must therefore be <span class="math inline">\(O(n)\)</span>.</p>
</section>
<section id="other-norms" class="level2">
<h2 class="anchored" data-anchor-id="other-norms">Other Norms</h2>
<p>It turns out that the norm I defined above is only <em>one</em> way to measure the length of a vector. It’s the most natural way to do so sense it corresponds to your intuitive notions of length, which itself relates to the Pythagorean Theorem. There are other ways to quantify vector length as well that aren’t as intuitive. Because they do sometimes show up in machine learning I’ll briefly mention a couple of these here.</p>
<p>The norm I’ve covered is called the <strong>2-norm</strong>. It’s called this because it involves squares and square roots. We can write it in the form</p>
<p><span class="math display">\[||\mathbf{v}|| = ||\mathbf{v}||_2 = \big(v_x^2 + v_y^2 \big)^{1/2}.\]</span></p>
<p>It turns out we can replace the twos with any other positive number <span class="math inline">\(p&gt;1\)</span> to get generalized norms, called <strong>p-norms</strong>,</p>
<p><span class="math display">\[||\mathbf{v}||_p = \big(v_x^p + v_y^p \big)^{1/p}.\]</span></p>
<p>The p-norms cover a large class of norms, since any <span class="math inline">\(1 \leq p \leq \infty\)</span> can define a valid norm. The 2-norm, as you’d guess, occurs when <span class="math inline">\(p=2\)</span>. A couple of other norms that show up in machine learning are the <strong>1-norm</strong> when <span class="math inline">\(p=1\)</span>, and the <strong>infinity norm</strong> when <span class="math inline">\(p=\infty\)</span>. For 2-dimensional vectors, these norms are</p>
<p><span class="math display">\[\begin{align*}
||\mathbf{v}||_1 &amp;= |v_x| + |v_y|, \\
||\mathbf{v}||_\infty &amp;= \max\big(|v_x|, |v_y|\big).
\end{align*}\]</span></p>
<p>Here’s an example. I’ll calculate the <span class="math inline">\(p=1, 2, \infty\)</span> norms for the vector <span class="math inline">\(\mathbf{v}=(1,-2)\)</span>. We have,</p>
<p><span class="math display">\[\begin{align*}
||\mathbf{v}||_1 &amp;= |1| + |-2| = 1 + 2 = 3, \\
||\mathbf{v}||_2 &amp;= \sqrt{1^2 + (-2)^2} = \sqrt{1 + 4} = \sqrt{5} \approx 2.236, \\
||\mathbf{v}||_\infty &amp;= \max\big(|1|, |-2|\big) = \max(1, 2) = 2.
\end{align*}\]</span></p>
<p>Notice that <span class="math inline">\(||\mathbf{v}||_1 \geq ||\mathbf{v}||_2 \geq ||\mathbf{v}||_\infty\)</span>. This is a general fact.</p>
<p>It’s a little hard right now to describe why these norms are useful in machine learning since we don’t currently have the context. Just know that these norms do come up sometimes. I’ll go into more depth on the uses of these different norms as we apply them. In practice though, we’ll probably work with the regular 2-norm maybe 90% of the time.</p>
<p>In numpy, you can calculate any <span class="math inline">\(p\)</span>-norm using the function <code>np.linalg.norm(v, ord=p)</code>. Here’s an example.</p>
<div class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>v <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="op">-</span><span class="dv">2</span>])</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'1-Norm of v: </span><span class="sc">{</span>np<span class="sc">.</span>linalg<span class="sc">.</span>norm(v, <span class="bu">ord</span><span class="op">=</span><span class="dv">1</span>)<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'2-Norm of v: </span><span class="sc">{</span>np<span class="sc">.</span>linalg<span class="sc">.</span>norm(v, <span class="bu">ord</span><span class="op">=</span><span class="dv">2</span>)<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Infinity-Norm of v: </span><span class="sc">{</span>np<span class="sc">.</span>linalg<span class="sc">.</span>norm(v, <span class="bu">ord</span><span class="op">=</span>np.inf)<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>1-Norm of v: 3.0
2-Norm of v: 2.23606797749979
Infinity-Norm of v: 2.0</code></pre>
</div>
</div>
</section>
</section>
<section id="linear-maps" class="level1">
<h1>Linear Maps</h1>
<p>So where do matrices fit into all this vector space stuff? It turns out that matrices correspond to functions between vectors to vectors. These are called <strong>linear maps</strong>. A linear map is a vector-valued function from one vector space to another that preserves the properties of vectors. In <span class="math inline">\(\mathbb{R}^2\)</span>, a linear map is a function between vectors <span class="math inline">\(\mathbf{v}=(v_x,v_y)\)</span> and <span class="math inline">\(\mathbf{w}=(w_x,w_y)\)</span> of the form</p>
<p><span class="math display">\[\mathbf{w} = (w_x, w_y) = (av_x + bv_y, cv_x + dv_y) = \mathbf{F}(\mathbf{v}).\]</span></p>
<p>That is, each component of the output vector <span class="math inline">\(\mathbf{w}\)</span> is a linear combination of the input vector <span class="math inline">\(\mathbf{v}\)</span>. Now, if you stare at this function for a little bit, you should see that this kind of looks like a <span class="math inline">\(2 \times 2\)</span> system of linear equations,</p>
<p><span class="math display">\[\begin{alignat*}{3}
   av_x &amp; {}+{} &amp;  bv_y &amp; {}={} &amp; w_x \\
   cv_x &amp; {}+{} &amp;  dv_y &amp; {}={} &amp; w_y.
\end{alignat*}\]</span></p>
<p>This of course means the linear map is equivalent to a matrix-vector equation. If we identify <span class="math inline">\(\mathbf{v}\)</span> and <span class="math inline">\(\mathbf{w}\)</span> with <span class="math inline">\(2 \times 1\)</span> column vectors, and define a <span class="math inline">\(\mathbf{A}\)</span> by</p>
<p><span class="math display">\[
\mathbf{A} =
\begin{pmatrix}
a &amp; b \\
c &amp; d
\end{pmatrix},
\]</span></p>
<p>then the linear map <span class="math inline">\(\mathbf{w} = \mathbf{F}(\mathbf{v})\)</span> is equivalent to the matrix-vector equation <span class="math inline">\(\mathbf{w}=\mathbf{A}\mathbf{v}\)</span>, or</p>
<p><span class="math display">\[\mathbf{F}(\mathbf{v}) = \mathbf{A}\mathbf{v}.\]</span></p>
<p>In fact, <em>every</em> linear map <span class="math inline">\(\mathbf{F}(\mathbf{v})\)</span> can be identified with some matrix equation <span class="math inline">\(\mathbf{A}\mathbf{v}\)</span>. Knowing <span class="math inline">\(\mathbf{A}\)</span> (in some basis) is equivalent to knowing the linear map itself.</p>
<p>But why are linear maps important? The main reason is that they preserve linear structure. Notice that I can define a line through any vector <span class="math inline">\(\mathbf{v}\)</span> by scaling it with some parameter <span class="math inline">\(t\)</span>. If I apply a linear map to this line I’d get <span class="math inline">\(\mathbf{F}(t\mathbf{v}) = t\mathbf{F}(\mathbf{v})\)</span>. Check it yourself from the definition. Said differently, linear maps map lines to lines, thus preserving the linear structure of the vector space. The new line won’t usually be the <em>original</em> line. It may get rotated. But it’s still a line.</p>
<p>Let’s try to visualize what a linear map does by defining a particular <span class="math inline">\(2 \times 2\)</span> matrix <span class="math inline">\(\mathbf{A}\)</span> and seeing how it acts on inputs <span class="math inline">\(\mathbf{v}\)</span>.</p>
<div class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.array([[<span class="dv">1</span>, <span class="op">-</span><span class="fl">0.1</span>], [<span class="fl">0.1</span>, <span class="dv">1</span>]])</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>v <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">1</span>]).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>plot_vectors([v.flatten(), (A <span class="op">@</span> v).flatten()], colors<span class="op">=</span>[<span class="st">'black'</span>, <span class="st">'red'</span>],</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>             labels<span class="op">=</span>[<span class="st">'$\mathbf</span><span class="sc">{{</span><span class="st">v</span><span class="sc">}}</span><span class="st">$'</span>, <span class="st">'$\mathbf</span><span class="sc">{{</span><span class="st">A</span><span class="sc">}}</span><span class="st">\mathbf</span><span class="sc">{{</span><span class="st">v</span><span class="sc">}}</span><span class="st">$'</span>], text_offsets<span class="op">=</span>[[<span class="fl">0.01</span>, <span class="op">-</span><span class="fl">0.1</span>], [<span class="op">-</span><span class="fl">0.1</span>, <span class="fl">0.05</span>]],</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>             title<span class="op">=</span><span class="st">'Linear Map: $\mathbf</span><span class="sc">{{</span><span class="st">w</span><span class="sc">}}</span><span class="st"> = \mathbf</span><span class="sc">{{</span><span class="st">A</span><span class="sc">}}</span><span class="st">\mathbf</span><span class="sc">{{</span><span class="st">v</span><span class="sc">}}</span><span class="st">$'</span>,  xlim<span class="op">=</span>(<span class="dv">0</span>, <span class="fl">1.5</span>), ylim<span class="op">=</span>(<span class="dv">0</span>, <span class="fl">1.5</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="vector-spaces_files/figure-html/cell-18-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Why stop there? Let’s apply the linear map a whole bunch of times recursively and see what happens to the output vectors <span class="math inline">\(\mathbf{w}\)</span>. I’ll plot each of the <span class="math inline">\(k=63\)</span> vectors in the following sequence, <span class="math display">\[\mathbf{v}, \ \mathbf{A}\mathbf{v}, \ \mathbf{A}^2\mathbf{v}, \ \cdots, \ \mathbf{A}^{63}\mathbf{v}.\]</span></p>
<p>Note each power <span class="math inline">\(\mathbf{A}^j\)</span> here is a <em>matrix power</em>, defined by applying <em>matrix multiplication</em> over and over <span class="math inline">\(k\)</span> times. It’s <em>not</em> the element-wise power.</p>
<p>For the particular values in this plot I’ll choose <span class="math inline">\(\mathbf{v}=\begin{pmatrix} 1 \\ 1 \end{pmatrix}\)</span> and <span class="math inline">\(\mathbf{A}=\begin{pmatrix} 1 &amp; -0.1 \\ 0.1 &amp; 1 \end{pmatrix}\)</span>. The original vector <span class="math inline">\(\mathbf{v}\)</span> is colored black. Notice that each linear map is slowly rotating the vector counterclockwise and also slightly stretching it. By the time it gets back around it’s already maybe 40% longer than the original vector.</p>
<p>In fact, <em>every</em> linear map between two vectors in the plane will do at least one of these two things: rotate the input vector in the plane, or stretch (or shrink) it by some factor. I just chose a particularly nice one to plot.</p>
<div class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>k <span class="op">=</span> <span class="dv">63</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>vectors <span class="op">=</span> [(np.linalg.matrix_power(A, i) <span class="op">@</span> v).flatten() <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(k)]</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>title <span class="op">=</span> <span class="ss">f"""</span></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a><span class="ss">$\mathbf</span><span class="ch">{{</span><span class="ss">v</span><span class="ch">}}</span><span class="ss">, \mathbf</span><span class="ch">{{</span><span class="ss">A</span><span class="ch">}}</span><span class="ss">\mathbf</span><span class="ch">{{</span><span class="ss">v</span><span class="ch">}}</span><span class="ss">, \mathbf</span><span class="ch">{{</span><span class="ss">A</span><span class="ch">}}</span><span class="ss">^2\mathbf</span><span class="ch">{{</span><span class="ss">v</span><span class="ch">}}</span><span class="ss">, \cdots, \mathbf</span><span class="ch">{{</span><span class="ss">A</span><span class="ch">}}</span><span class="ss">^</span><span class="ch">{{</span><span class="sc">{</span>k<span class="sc">}</span><span class="ch">}}</span><span class="ss">\mathbf</span><span class="ch">{{</span><span class="ss">v</span><span class="ch">}}</span><span class="ss">$</span></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a><span class="ss">"""</span>.strip()</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>plot_vectors(vectors, colors<span class="op">=</span>[<span class="st">'black'</span>]<span class="op">+</span>[<span class="st">'red'</span>]<span class="op">*</span>(k<span class="op">-</span><span class="dv">1</span>), title<span class="op">=</span>title, xlim<span class="op">=</span>(<span class="op">-</span><span class="fl">2.5</span>, <span class="fl">2.5</span>), ylim<span class="op">=</span>(<span class="op">-</span><span class="fl">2.5</span>, <span class="fl">2.5</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="vector-spaces_files/figure-html/cell-19-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>What do you suppose the transpose of <span class="math inline">\(\mathbf{A}\)</span> does in this particular example? That is, suppose you use <span class="math inline">\(\mathbf{A}^\top\)</span> instead. Notice what would happen is the minus sign would move from the upper right to the lower left. You can verify that this will just cause the matrix to spin vectors the other way, clockwise instead of counterclockwise.</p>
<p>In fact, what I’ve just shown is a special case of what happens when you compose linear maps: Composition of linear maps is <em>equivalent</em> to matrix multiplication. If <span class="math inline">\(\mathbf{F}(\mathbf{w}) = \mathbf{A}\mathbf{w}\)</span> and <span class="math inline">\(\mathbf{G}(\mathbf{v}) = \mathbf{B}\mathbf{v}\)</span> are two linear maps, then their composite function <span class="math inline">\(\mathbf{F}(\mathbf{G}(\mathbf{v}))\)</span> is another linear map given by</p>
<p><span class="math display">\[\mathbf{F}(\mathbf{G}(\mathbf{v})) = \mathbf{A}\mathbf{B}\mathbf{v}.\]</span></p>
<p>This is perhaps the <em>real</em> reason matrix multiplication is important. Because linear maps are important, and applying multiple linear maps in sequence is just matrix multiplication.</p>
<p>Two other special linear maps worth being aware of are the identity map and the inverse map. The identity map is the map <span class="math inline">\(\mathbf{F}(\mathbf{v}) = \mathbf{I}\mathbf{v}\)</span>. What does it do to <span class="math inline">\(\mathbf{v}\)</span>? Let’s see. We can get the identity matrix in numpy using <code>np.eye(n)</code>, where <code>n</code> is the dimension (in this case 2).</p>
<p>It looks like nothing is happening. That is, <span class="math inline">\(\mathbf{I}\mathbf{v} = \mathbf{v}\)</span>. You can verify this by writing this out in components and seeing what the matrix-vector product is. In fact, <span class="math inline">\(\mathbf{I}\mathbf{v} = \mathbf{v}\)</span> is <em>always</em> true, for any dimension, and any vector <span class="math inline">\(\mathbf{v}\)</span>.</p>
<p><strong>Aside:</strong> Notice that I had to <em>flatten</em> the vectors here to do the plot. That’s because I’ve sneakily defined vectors in two different ways, first as a <em>column</em> vector of shape <span class="math inline">\((2,1)\)</span> and <em>then</em> as a <em>flat</em> vector of shape <span class="math inline">\((2,)\)</span>.</p>
<div class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>I <span class="op">=</span> np.eye(<span class="dv">2</span>)</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>plot_vectors([v.flatten(), (I <span class="op">@</span> v).flatten()], zorders<span class="op">=</span>[<span class="dv">0</span>, <span class="dv">1</span>],</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>             title<span class="op">=</span><span class="st">'Identity Map: $\mathbf</span><span class="sc">{F}</span><span class="st">(\mathbf</span><span class="sc">{v}</span><span class="st">)=\mathbf</span><span class="sc">{I}</span><span class="st">\mathbf</span><span class="sc">{v}</span><span class="st">$'</span>,</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>             labels<span class="op">=</span>[<span class="st">'$\mathbf</span><span class="sc">{v}</span><span class="st">$'</span>, <span class="st">'$\mathbf</span><span class="sc">{I}</span><span class="st">\mathbf</span><span class="sc">{v}</span><span class="st">$'</span>],</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>             colors<span class="op">=</span>[<span class="st">'black'</span>, <span class="st">'red'</span>], xlim<span class="op">=</span>(<span class="op">-</span><span class="fl">0.5</span>, <span class="fl">1.5</span>), ylim<span class="op">=</span>(<span class="op">-</span><span class="fl">0.5</span>, <span class="fl">1.5</span>), </span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>             text_offsets<span class="op">=</span>[[<span class="dv">0</span>, <span class="op">-</span><span class="fl">0.25</span>], [<span class="op">-</span><span class="fl">0.2</span>, <span class="fl">0.1</span>]])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="vector-spaces_files/figure-html/cell-20-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>The inverse map is just the linear map that undoes the original linear map <span class="math inline">\(\mathbf{F}(\mathbf{v}) = \mathbf{A}\mathbf{v}\)</span>, i.e.</p>
<p><span class="math display">\[\mathbf{F}^{-1}(\mathbf{v}) = \mathbf{A}^{-1}\mathbf{v}.\]</span></p>
<p>You can see what this does by applying the two maps in succession. Here’s an example of doing this with the vector <span class="math inline">\(\mathbf{v}=(1,1)\)</span> and the <span class="math inline">\(90^\circ\)</span> rotation matrix</p>
<p><span class="math display">\[\mathbf{A}=\begin{pmatrix} 0 &amp; 1 \\ -1 &amp; 0 \end{pmatrix}.\]</span></p>
<p>Applying <span class="math inline">\(\mathbf{F}(\mathbf{v})\)</span> followed by <span class="math inline">\(\mathbf{F}^{-1}(\mathbf{v})\)</span> just gives the same vector <span class="math inline">\(\mathbf{v}\)</span> back. This just follows from the fact that <span class="math inline">\(\mathbf{A}^{-1}\mathbf{A}=\mathbf{I}\)</span>, so the composition <span class="math inline">\(\mathbf{F}^{-1}(\mathbf{F}(\mathbf{v}))=\mathbf{v}\)</span>.</p>
<div class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.array([[<span class="dv">0</span>, <span class="dv">1</span>], [<span class="op">-</span><span class="dv">1</span>, <span class="dv">0</span>]])</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>plot_vectors([v.flatten(), (A <span class="op">@</span> v).flatten(), (np.linalg.inv(A) <span class="op">@</span> A <span class="op">@</span> v).flatten()], zorders<span class="op">=</span>[<span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">0</span>],</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>             title<span class="op">=</span><span class="st">'Inverse Map: $\mathbf</span><span class="sc">{F}</span><span class="st">^{-1}(\mathbf</span><span class="sc">{v}</span><span class="st">)=\mathbf</span><span class="sc">{A}</span><span class="st">^{-1}\mathbf</span><span class="sc">{v}</span><span class="st">$'</span>,</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>             labels<span class="op">=</span>[<span class="st">'$\mathbf</span><span class="sc">{v}</span><span class="st">$'</span>, <span class="st">'$\mathbf</span><span class="sc">{A}</span><span class="st">\mathbf</span><span class="sc">{v}</span><span class="st">$'</span>, <span class="st">'$\mathbf</span><span class="sc">{A}</span><span class="st">^{-1}\mathbf</span><span class="sc">{A}</span><span class="st">\mathbf</span><span class="sc">{v}</span><span class="st">$'</span>],</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>             colors<span class="op">=</span>[<span class="st">'black'</span>, <span class="st">'red'</span>, <span class="st">'blue'</span>], xlim<span class="op">=</span>(<span class="op">-</span><span class="fl">0.5</span>, <span class="dv">2</span>), ylim<span class="op">=</span>(<span class="op">-</span><span class="fl">1.5</span>, <span class="fl">1.5</span>), </span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>             text_offsets<span class="op">=</span>[[<span class="dv">0</span>, <span class="op">-</span><span class="fl">0.3</span>], [<span class="fl">0.05</span>, <span class="op">-</span><span class="fl">0.05</span>], [<span class="dv">0</span>, <span class="fl">0.1</span>]])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="vector-spaces_files/figure-html/cell-21-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>I’ll close this section by mentioning that we’re often not interested in <em>linear maps</em> in practice, but <em>affine maps</em>. Recall that a simple linear function might have the form <span class="math inline">\(y=ax\)</span>. An affine function has the form <span class="math inline">\(y=ax+b\)</span>. That is, an affine function is just a linear function shifted upward by <span class="math inline">\(b\)</span>. The same idea extends to maps between vectors.</p>
<p>An <strong>affine map</strong> is just a linear map shifted by some constant translation vector <span class="math inline">\(\mathbf{b}\)</span>,</p>
<p><span class="math display">\[\mathbf{F}(\mathbf{v}) = \mathbf{A}\mathbf{v} + \mathbf{b}.\]</span></p>
<p>The only difference between an affine map and a linear map is that vectors will get not just scaled and rotated, but also <em>translated</em> by <span class="math inline">\(\mathbf{b}\)</span>. In machine learning, the translation vector <span class="math inline">\(\mathbf{b}\)</span> often called a <strong>bias vector</strong>.</p>
<p>Here’s a plot of what this looks like using the same previous matrix, and a bias vector <span class="math inline">\(\mathbf{b}=(-1, -1)\)</span>. I want to show that <span class="math inline">\(\mathbf{A}\mathbf{v} + \mathbf{b}\)</span> is really just the vector <span class="math inline">\(\mathbf{A}\mathbf{v}\)</span>, but translated so its tail lies at <span class="math inline">\(\mathbf{b}\)</span>. To do so, I’ll plot the vector <span class="math inline">\(\mathbf{A}\mathbf{v} + \mathbf{b}\)</span> with its tail at the origin, as well as the vector <span class="math inline">\(\mathbf{A}\mathbf{v}\)</span> with its tail shifted to the point <span class="math inline">\(\mathbf{b}\)</span>. What matters is that the head of both of these vectors is the same. The main vector <span class="math inline">\(\mathbf{A}\mathbf{v} + \mathbf{b}\)</span> is shown in red.</p>
<div class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.array([[<span class="dv">0</span>, <span class="dv">1</span>], [<span class="op">-</span><span class="dv">1</span>, <span class="dv">0</span>]])</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>v <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">1</span>]).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>b <span class="op">=</span> np.array([<span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>]).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>vectors <span class="op">=</span> [x.flatten() <span class="cf">for</span> x <span class="kw">in</span> [v, A <span class="op">@</span> v, A <span class="op">@</span> v <span class="op">+</span> b, b]]</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>plot_vectors(</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>    vectors, xlim<span class="op">=</span>(<span class="op">-</span><span class="fl">1.5</span>, <span class="fl">1.5</span>), ylim<span class="op">=</span>(<span class="op">-</span><span class="fl">2.5</span>, <span class="fl">1.5</span>), headwidth<span class="op">=</span><span class="dv">5</span>, colors<span class="op">=</span>[<span class="st">'black'</span>, <span class="st">'blue'</span>, <span class="st">'red'</span>, <span class="st">'green'</span>],</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>    labels<span class="op">=</span>[<span class="st">'$\mathbf</span><span class="sc">{{</span><span class="st">v</span><span class="sc">}}</span><span class="st">$'</span>, <span class="st">'$\mathbf</span><span class="sc">{{</span><span class="st">A</span><span class="sc">}}</span><span class="st">\mathbf</span><span class="sc">{{</span><span class="st">v</span><span class="sc">}}</span><span class="st">$'</span>, </span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>            <span class="st">'$\mathbf</span><span class="sc">{{</span><span class="st">A</span><span class="sc">}}</span><span class="st">\mathbf</span><span class="sc">{{</span><span class="st">v</span><span class="sc">}}</span><span class="st"> + \mathbf</span><span class="sc">{{</span><span class="st">b</span><span class="sc">}}</span><span class="st">$'</span>, <span class="st">'$\mathbf</span><span class="sc">{{</span><span class="st">b</span><span class="sc">}}</span><span class="st">$'</span>], </span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>    text_offsets<span class="op">=</span>[[<span class="dv">0</span>, <span class="dv">0</span>], [<span class="op">-</span><span class="fl">1.4</span>, <span class="op">-</span><span class="fl">1.25</span>], [<span class="fl">0.07</span>, <span class="dv">0</span>], [<span class="op">-</span><span class="fl">0.2</span>, <span class="fl">0.1</span>]], </span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>    tails<span class="op">=</span>[[<span class="dv">0</span>, <span class="dv">0</span>], [b[<span class="dv">0</span>][<span class="dv">0</span>], b[<span class="dv">1</span>][<span class="dv">0</span>]], [<span class="dv">0</span>, <span class="dv">0</span>], [<span class="dv">0</span>, <span class="dv">0</span>]],</span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>    title<span class="op">=</span><span class="st">'Affine Map: $\mathbf</span><span class="sc">{F}</span><span class="st">(\mathbf</span><span class="sc">{v}</span><span class="st">) = \mathbf</span><span class="sc">{A}</span><span class="st">\mathbf</span><span class="sc">{v}</span><span class="st"> + \mathbf</span><span class="sc">{b}</span><span class="st">$'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="vector-spaces_files/figure-html/cell-22-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="n-dimensional-vector-spaces" class="level1">
<h1><span class="math inline">\(n\)</span>-dimensional Vector Spaces</h1>
<p>It may seem like everything I’ve said is special for the case of <span class="math inline">\(n=2\)</span> dimensions, but it’s really not. Every single thing I’ve said extends exactly how you’d expect to vectors of arbitrary size <span class="math inline">\(n\)</span>. The only difference now is that you can’t visualize the stuff anymore. You just have to trust the math. I’ll restate all of the definitions from above here, but for <span class="math inline">\(n\)</span>-dimensional vector spaces instead.</p>
<p>A <strong>vector</strong> of size <span class="math inline">\(n\)</span> can be defined as a 1-dimensional array of real numbers <span class="math inline">\(x_0,x_1,x_2,\cdots,x_{n-1}\)</span>,</p>
<p><span class="math display">\[\mathbf{x} = (x_0,x_1,x_2,\cdots,x_{n-1}).\]</span></p>
<p>Vectors can be added together, and multiplied by scalars. Vector addition is defined element-wise. If <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{y}\)</span> are two vectors, then</p>
<p><span class="math display">\[\mathbf{x} + \mathbf{y} = (x_0+y_0, x_1+y_1, \cdots, x_{n-1}+y_{n-1}).\]</span></p>
<p>To keep a running example through this section, I’ll use numpy to create two vectors <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{y}\)</span> each of size <span class="math inline">\(n=10\)</span>. Here’s their vector sum.</p>
<div class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">5</span>, <span class="dv">4</span>, <span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">1</span>])</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>])</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'x + y = </span><span class="sc">{</span>x <span class="op">+</span> y<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>x + y = [2 2 2 4 6 5 3 3 3 1]</code></pre>
</div>
</div>
<p>Scalar multiplication is defined similarly. If <span class="math inline">\(c \in \mathbb{R}\)</span> is some scalar and <span class="math inline">\(\mathbf{x}\)</span> is some vector, then</p>
<p><span class="math display">\[c\mathbf{x} = (cx_0,cx_1,\cdots,cx_{n-1}).\]</span></p>
<div class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>c <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'c * x = </span><span class="sc">{</span>c <span class="op">*</span> x<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>c * x = [ 5 10 15 20 25 25 20 15 10  5]</code></pre>
</div>
</div>
<p>Vectors of size <span class="math inline">\(n\)</span> live in the <span class="math inline">\(n\)</span>-dimensional <strong>vector space</strong> <span class="math inline">\(\mathbb{R}^n\)</span>. By definition, any <strong>linear combination</strong> of two vectors must also live in the same vector space. That is, if <span class="math inline">\(\mathbf{x}, \mathbf{y} \in \mathbb{R}^n\)</span> are two vectors and <span class="math inline">\(a,b \in \mathbb{R}\)</span> are two scalars, then <span class="math inline">\(a \mathbf{x} + b \mathbf{y} \in \mathbb{R}^n\)</span>.</p>
<p>The <strong>dot product</strong> or <strong>inner product</strong> between two vectors <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{y}\)</span> of size <span class="math inline">\(n\)</span> is defined as their sum product, i.e.</p>
<p><span class="math display">\[\mathbf{x} \cdot \mathbf{y} = x_0y_0 + x_1y_1 + \cdots + x_{n-1}y_{n-1}.\]</span></p>
<div class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'x . y = </span><span class="sc">{</span>np<span class="sc">.</span>dot(x, y)<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>x . y = 1</code></pre>
</div>
</div>
<p>The <strong>norm</strong> (technically the <strong>2-norm</strong>) of a vector is defined as the square root of its dot product with itself, i.e.</p>
<p><span class="math display">\[||\mathbf{x}|| = ||\mathbf{x}||_2 = \sqrt{\mathbf{x} \cdot \mathbf{x}} = \sqrt{x_0^2 + x_1^2 + \cdots + x_{n-1}^2}.\]</span></p>
<p>This is just the <span class="math inline">\(n\)</span>-dimensional generalization of the Pythagorean Theorem. We can also consider other <span class="math inline">\(p\)</span> norms as well. In particular, the cases when <span class="math inline">\(p=1\)</span> and <span class="math inline">\(p=\infty\)</span> sometimes show up in applications,</p>
<p><span class="math display">\[\begin{align*}
||\mathbf{x}||_1 &amp;= \sum_{i=0}^{n-1} |x_i| = |x_0| + |x_1| + \cdots + |x_{n-1}|, \\
||\mathbf{x}||_\infty &amp;= \max_{i=0,\cdots,n-1} |x_i| = \max\big(|x_0|, |x_1|, \cdots, |x_{n-1}|\big).
\end{align*}\]</span></p>
<p>It will always be the case that <span class="math inline">\(||\mathbf{x}||_1 \geq ||\mathbf{x}||_2 \geq ||\mathbf{x}||_\infty\)</span>.</p>
<div class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'1-Norm of x: </span><span class="sc">{</span>np<span class="sc">.</span>linalg<span class="sc">.</span>norm(x, <span class="bu">ord</span><span class="op">=</span><span class="dv">1</span>)<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'2-Norm of x: </span><span class="sc">{</span>np<span class="sc">.</span>linalg<span class="sc">.</span>norm(x, <span class="bu">ord</span><span class="op">=</span><span class="dv">2</span>)<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Infinity-Norm of x: </span><span class="sc">{</span>np<span class="sc">.</span>linalg<span class="sc">.</span>norm(x, <span class="bu">ord</span><span class="op">=</span>np.inf)<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>1-Norm of x: 30.0
2-Norm of x: 10.488088481701515
Infinity-Norm of x: 5.0</code></pre>
</div>
</div>
<p>The <strong>distance</strong> <span class="math inline">\(d(\mathbf{x}, \mathbf{y})\)</span> between two vectors <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{y}\)</span> is just the norm of their difference vector,</p>
<p><span class="math display">\[d(\mathbf{x}, \mathbf{y}) = ||\mathbf{x}-\mathbf{y}|| = \sum_{i=0}^{n-1} \sqrt{(x_i-y_i)^2} = \sqrt{(x_0-y_0)^2 + (x_1-y_1)^2 + \cdots + (x_{n-1}-y_{n-1})^2}.\]</span></p>
<p>We can define the angle between any two vectors <span class="math inline">\(\mathbf{x}\)</span> and <span class="math inline">\(\mathbf{y}\)</span> of size <span class="math inline">\(n\)</span> by making use of the same identity for the dot product, which still holds in <span class="math inline">\(n\)</span> dimensions,</p>
<p><span class="math display">\[\mathbf{x} \cdot \mathbf{y} = ||\mathbf{x}|| \cdot ||\mathbf{y}|| \cos \theta.\]</span></p>
<p>Using this identity, we can define the <strong>cosine similarity</strong> <span class="math inline">\(\cos(\mathbf{x}, \mathbf{y})\)</span> by solving for <span class="math inline">\(\cos \theta\)</span>,</p>
<p><span class="math display">\[\cos(\mathbf{x}, \mathbf{y}) = \frac{\mathbf{x} \cdot \mathbf{y}}{||\mathbf{x}|| \cdot ||\mathbf{y}||}.\]</span></p>
<p>The dot product is a measure of how similar two vectors are, and the cosine similarity is a <em>normalized</em> measure of how similar two vectors are, since dividing by the norms forces <span class="math inline">\(-1 \leq \cos \theta \leq 1\)</span>.</p>
<div class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'cos(x, y) = </span><span class="sc">{</span>cosine_similarity(x, y)<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>cos(x, y) = 0.04264014327112208</code></pre>
</div>
</div>
<p>A set of vectors <span class="math inline">\(\mathbf{x}_0, \mathbf{x}_1, \cdots, \mathbf{x}_{k-1}\)</span> is <strong>linearly independent</strong> if no one vector is a linear combination of the rest,</p>
<p><span class="math display">\[\mathbf{x}_j \neq \sum_{i \neq j} c_i \mathbf{x}_j.\]</span></p>
<p>If one vector <em>is</em> a linear combination of the rest, they’re <strong>linearly dependent</strong>. If there are exactly <span class="math inline">\(n\)</span> linear independent vectors in the set, it’s called a <strong>basis</strong>.</p>
<p>We can define the <strong>standard basis</strong> on <span class="math inline">\(\mathbb{R}^n\)</span> with the following complete set of size <span class="math inline">\(n\)</span> unit vectors,</p>
<p><span class="math display">\[\begin{align*}
\mathbf{e}_0 &amp;= (1, 0, 0, \cdots, 0), \\
\mathbf{e}_1 &amp;= (0, 1, 0, \cdots, 0), \\
\vdots \ &amp;= \qquad \vdots \\
\mathbf{e}_{n-1} &amp;= (0, 0, 0, \cdots, 1).
\end{align*}\]</span></p>
<p>The standard basis is an <strong>orthonormal basis</strong> since each vector is a unit vector and they’re all mutually orthogonal, i.e.</p>
<p><span class="math display">\[
\mathbf{e}_i \cdot \mathbf{e}_j = \delta_{ij} =
\begin{cases}
1 &amp; i = j, \\
0 &amp; i \neq j.
\end{cases}
\]</span></p>
<p><strong>Notation:</strong> The symbol <span class="math inline">\(\delta_{ij}\)</span> is called the <strong>Kronecker delta</strong>. It’s just a shorthand way of writing something is <span class="math inline">\(1\)</span> if <span class="math inline">\(i=j\)</span> and <span class="math inline">\(0\)</span> if <span class="math inline">\(i \neq j\)</span>.</p>
<div class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>e <span class="op">=</span> [ei.flatten().astype(<span class="bu">int</span>) <span class="cf">for</span> ei <span class="kw">in</span> np.eye(n)]</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'e3 = </span><span class="sc">{</span>e[<span class="dv">3</span>]<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'e8 = </span><span class="sc">{</span>e[<span class="dv">8</span>]<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'e3 . e3 = </span><span class="sc">{</span>np<span class="sc">.</span>dot(e[<span class="dv">3</span>], e[<span class="dv">3</span>])<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'e3 . e8 = </span><span class="sc">{</span>np<span class="sc">.</span>dot(e[<span class="dv">3</span>], e[<span class="dv">8</span>])<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>e3 = [0 0 0 1 0 0 0 0 0 0]
e8 = [0 0 0 0 0 0 0 0 1 0]
e3 . e3 = 1
e3 . e8 = 0</code></pre>
</div>
</div>
<p>If a basis is orthonormal, any vector <span class="math inline">\(\mathbf{x}\)</span> can be decomposed into a linear combination of the basis elements by taking the dot product <span class="math inline">\(\mathbf{x} \cdot \mathbf{e}_i\)</span>. For the standard basis, these just give the vector components <span class="math inline">\(x_i\)</span>,</p>
<p><span class="math display">\[\mathbf{x} = \sum_{i=0}^{n-1} (\mathbf{x} \cdot \mathbf{e}_i) \mathbf{e}_i = \sum_{i=0}^{n-1} x_i \mathbf{e}_i = x_0 \mathbf{e}_0 + x_1 \mathbf{e}_1 + \cdots x_{n-1} \mathbf{e}_{n-1}.\]</span></p>
<p>Each term <span class="math inline">\(x_i \mathbf{e}_i\)</span> in the sum corresponds to the <strong>projection</strong> of <span class="math inline">\(\mathbf{x}\)</span> onto the <span class="math inline">\(i\)</span><sup>th</sup> axis. Each axis in <span class="math inline">\(\mathbb{R}^n\)</span> is still a single line, but now there are <span class="math inline">\(n\)</span> of these axis lines, all perpendicular to each other.</p>
<p>A <strong>linear map</strong> is a vector-valued function <span class="math inline">\(\mathbf{y}=\mathbf{F}(\mathbf{x})\)</span> between vector spaces that preserves the linear structure of the spaces. In general, <span class="math inline">\(\mathbf{x} \in \mathbb{R}^m\)</span> and <span class="math inline">\(\mathbf{y} \in \mathbb{R}^n\)</span> need not be in the same vector spaces. Either way, a linear map can always be expressed as a matrix-vector equation <span class="math inline">\(\mathbf{y}=\mathbf{A}\mathbf{x}\)</span>, where <span class="math inline">\(\mathbf{A}\)</span> is some <span class="math inline">\(m \times n\)</span> matrix. More generally, an <strong>affine map</strong> is a linear map shifted by some <strong>bias vector</strong> <span class="math inline">\(\mathbf{b} \in \mathbb{R}^m\)</span>. Affine maps can always be expressed as a shifted matrix-vector equation, <span class="math inline">\(\mathbf{y}=\mathbf{A}\mathbf{x} + \mathbf{b}\)</span>.</p>
<p><strong>Aside:</strong> Roughly speaking a neural network is just a composite function of successive affine maps, except that one adds a <em>nonlinearity</em> function <span class="math inline">\(\boldsymbol{\sigma}(\mathbf{x})\)</span> in between each successive affine map to make it nonlinear. For example, the following nonlinear function could represent a “one hidden layer” neural network,</p>
<p><span class="math display">\[\mathbf{y} = \boldsymbol{\sigma}_2\big(\mathbf{A}_2\boldsymbol{\sigma}_1(\mathbf{A}_1\mathbf{x} + \mathbf{b}_1) + \mathbf{b}_2\big).\]</span></p>
<p>The nonlinearity functions that get chosen are rarely exotic. Most of the time they’re all just the ReLU function <span class="math inline">\(\boldsymbol{\sigma}(\mathbf{x})=\max(\mathbf{0}, \mathbf{x})\)</span>, except in the output layer. The coefficients in the matrices and bias vectors become the parameters of the network and get learned from the training data.</p>
<p>Just as with linear maps in the plane, linear maps in higher dimensions always preserve lines. Not just lines in fact, but planes and hyperplanes as well. These generalizations of lines are called <strong>linear subspaces</strong>. Linear subspaces will always be hyperplanes in <span class="math inline">\(n\)</span>-dimensional space that pass through the origin. Think of them as planes passing through the origin, but in more dimensions. If the hyperplane spanned by <span class="math inline">\(\mathbf{x}_0, \mathbf{x}_1, \cdots, \mathbf{x}_{k-1}\)</span> is some <span class="math inline">\(k\)</span>-dimensional linear subspace of <span class="math inline">\(\mathbb{R}^n\)</span>, then its <strong>image</strong> under the linear map will be a new <span class="math inline">\(k\)</span>-dimensional linear subspace in <span class="math inline">\(\mathbb{R}^m\)</span> (if <span class="math inline">\(k \leq m\)</span>, otherwise it’ll just be the full vector space <span class="math inline">\(\mathbb{R}^m\)</span> itself). Any linear combination of vectors in a given subspace will stay inside that subspace. It’s <em>closed</em> under vector space operations. For all practical purposes it’s a new vector space <span class="math inline">\(\mathbb{R}^k\)</span> unto itself.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation column-body">
  <div class="nav-page nav-page-previous">
      <a href="../notebooks/linear-systems.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Systems of Linear Equations</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../notebooks/matrix-algebra.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Matrix Algebra</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>