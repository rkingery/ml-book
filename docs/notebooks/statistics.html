<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Mathematics and Computer Science for Machine Learning - 13&nbsp; Math for ML: Statistics</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../notebooks/multivariate-probability.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Math for ML: Statistics</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Mathematics and Computer Science for Machine Learning</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">Preface</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/algorithms.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Programming and Algorithms</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/basic-math.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Basic Math</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/numerical-computing.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Numerical Computation</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/calculus.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Calculus</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/linear-systems.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Linear Systems</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/vectors.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Vector Spaces</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/matrices.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Matrix Algebra</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/tensors.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Tensor Algebra</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/multivariate-calculus.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Multivariate Calculus</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/optimization.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Optimization</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/probability.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Basic Probability</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/multivariate-probability.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Multivariate Distributions</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../notebooks/statistics.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Math for ML: Statistics</span></a>
  </div>
</li>
    </ul>
    </div>
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#statistics" id="toc-statistics" class="nav-link active" data-scroll-target="#statistics"><span class="toc-section-number">13.1</span>  Statistics</a></li>
  <li><a href="#optimization-not-finished" id="toc-optimization-not-finished" class="nav-link" data-scroll-target="#optimization-not-finished"><span class="toc-section-number">13.2</span>  Optimization [NOT FINISHED]</a>
  <ul class="collapse">
  <li><a href="#gradient-descent" id="toc-gradient-descent" class="nav-link" data-scroll-target="#gradient-descent"><span class="toc-section-number">13.2.1</span>  Gradient Descent</a></li>
  </ul></li>
  <li><a href="#moments-of-a-distribution" id="toc-moments-of-a-distribution" class="nav-link" data-scroll-target="#moments-of-a-distribution"><span class="toc-section-number">13.3</span>  Moments of a Distribution</a>
  <ul class="collapse">
  <li><a href="#univariate-moments" id="toc-univariate-moments" class="nav-link" data-scroll-target="#univariate-moments"><span class="toc-section-number">13.3.1</span>  Univariate Moments</a></li>
  <li><a href="#multivariate-moments" id="toc-multivariate-moments" class="nav-link" data-scroll-target="#multivariate-moments"><span class="toc-section-number">13.3.2</span>  Multivariate Moments</a></li>
  <li><a href="#example-bernoulli-distribution" id="toc-example-bernoulli-distribution" class="nav-link" data-scroll-target="#example-bernoulli-distribution"><span class="toc-section-number">13.3.3</span>  Example: Bernoulli Distribution</a></li>
  <li><a href="#list-of-means-and-variances" id="toc-list-of-means-and-variances" class="nav-link" data-scroll-target="#list-of-means-and-variances"><span class="toc-section-number">13.3.4</span>  List of Means and Variances</a></li>
  </ul></li>
  </ul>
</nav>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content column-page-right" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Math for ML: Statistics</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>In this final Part 5 of the series on Math for ML, …</p>
<p>Topics: - Basic statistics: test statistics of a random variable (mean, median, variance, std dev, max, min, range), correlation (pearson vs spearman), outliers and how they affect statistics like correlations - Statistical theory: maximum likelihood estimation, negative log likelihood and loss function, Bayesian (optional) - Statistical inference: confidence intervals, statistical tests, p-values, etc (maybe) - Information theory: entropy, continuous entropy, cross entropy (maybe) - Complexity theory: analyzing algorithms, big O notation - Cover bootstrapping, and use it on some examples of getting confidence intervals of estimators (also used for bagging, “bootstrap aggregation”)</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> utils.math_ml <span class="im">import</span> <span class="op">*</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<section id="statistics" class="level2" data-number="13.1">
<h2 data-number="13.1" class="anchored" data-anchor-id="statistics"><span class="header-section-number">13.1</span> Statistics</h2>
<p>Statistics is at its root the use of probability to study <strong>data</strong>. Data can be any real-world measurement, in essentially any form. Statistics treats data as either fixed values or random variables depending on the situation. If the data has already been observed we assume it’s fixed. If not, we assume it’s random and try to model it with a probability distribution. For the purposes of this lesson we’ll assume data comes in the form of some 1D or 2D array and that each data point takes on a numerical value, either an integer or real number.</p>
<p>Let’s start with a simple 1D array of <span class="math inline">\(m\)</span> samples <span class="math inline">\(\mathbf{x} = (x_0,x_1,\cdots,x_{m-1})\)</span>. We’d like to know what univariate distribution <span class="math inline">\(p(x)\)</span> would generate the samples <span class="math inline">\(x_0,x_1,\cdots,x_{m-1}\)</span>. If we can get this probability distribution even approximately, we can gain a lot of insight into the nature of the data itself.</p>
<p>Unfortunately, it’s really hard to figure out what distribution data is coming from with only a finite amount of data in all but the simplest cases. What we often thus settle for instead is to <em>assume</em> the data come from a certain class of distribution, and then try to estimate what the parameters of that distribution would have to be to ensure that distribution fits the data well. In this sense, a whole lot of statistics boils down to how to estimate the parameters of a given distribution from the data.</p>
<p>Some of the most important parameters to estimate from an array of data are its moments. The hope is to find, without knowing the data’s distribution, the best estimate of that distribution’s moments from the data given. This is where the formulas you’re probably used to come in for things like mean, variance, standard deviation, etc. Traditionally to avoid getting these moment estimates mixed up with the true distribution’s moments, we call them <strong>sample moments</strong>. I’ll define them below for the univariate case.</p>
<p><strong>Sample Mean:</strong> <span class="math display">\[\overline{x} = \frac{1}{m}\sum_{i=0}^{m-1} x_i = \frac{1}{m}(x_0 + x_1 + \cdots + x_{m-1})\]</span></p>
<p><strong>Sample Variance:</strong> <span class="math display">\[s^2 = \frac{1}{m}\sum_{i=0}^{m-1} (x_i-\overline{x})^2 = \frac{1}{m}\big((x_0-\overline{x})^2 + \cdots + (x_{m-1}-\overline{x})^2\big)\]</span></p>
<p><strong>Sample Standard Deviation:</strong> <span class="math display">\[s = \sqrt{s^2} = \sqrt{\frac{1}{m}\sum_{i=0}^{m-1} (x_i-\overline{x})^2}\]</span></p>
<p>Other quantities that might be of interest to estimate aren’t moments at all. One example is the <strong>median</strong>, which is defined as the midpoint of a distribution, i.e.&nbsp;the <span class="math inline">\(x\)</span> such that <span class="math inline">\(p(x)=1/2\)</span>. The median is another way of estimating the center of a distribution, but has slightly different properties than the mean. One of those properties is that the mean depends only on the <em>rank order</em> of values, not on what numbers those values take on. This implies that, unlike the mean, the median is invariant to points “far away from the center”, called <strong>outliers</strong>.</p>
<p>We can estimate the sample median, call it <span class="math inline">\(M\)</span>, of an array <span class="math inline">\(x_0,x_1,\cdots,x_{m-1}\)</span> by sorting them in ascending order and plucking out the midpoint. If <span class="math inline">\(m\)</span> is odd, this is just <span class="math inline">\(M = x_{m//2+1}\)</span>. If <span class="math inline">\(m\)</span> is even we by convention take the median as the average of the two midpoints <span class="math inline">\(M = \frac{1}{2}\big(x_{m//2} + x_{m//2+1}\big)\)</span>.</p>
<p>Other quantities we might want to estimate from the data are the sample <strong>minimum</strong>, the sample <strong>maximum</strong>, and the sample <strong>range</strong>, which is defined as the difference between the sample max and min.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># annoying fact: np.var uses 1/m version while torch.var uses 1/(m-1) version</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="co"># derive that MLEs for x ~ N(mu, sigma^2) are xbar and s^2 (the 1/m versions)</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co"># mention that the uniform is the max entropy distribution assuming nothing (principle of indifference),</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co"># and the Gaussian is assuming fixed mean and variance (why Gaussians are popular aside from CLT)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="optimization-not-finished" class="level2" data-number="13.2">
<h2 data-number="13.2" class="anchored" data-anchor-id="optimization-not-finished"><span class="header-section-number">13.2</span> Optimization [NOT FINISHED]</h2>
<p>For the purposes of machine learning, by far the most important application of differentiation and calculus in general is to optimization. <strong>Optimization</strong> is the problem of finding the “best” values with respect to some function. Usually in machine learning, by “best” we mean finding the minimum value of a <em>loss function</em>, which is a function that measures agreement between a model’s prediction and the data it sees. Finding the minimum value of the loss function essentially means we’ve found the best weights for our model, the ones that give the highest accuracy on the data.</p>
<p>An interesting fact is that for a reasonably smooth function, its minimum value will <em>always</em> be at a point where the derivative is zero. To see why, consider our tangent line plot of <span class="math inline">\(y=x^2\)</span> from before. What happens if we set our point of interest to be <span class="math inline">\(x_0=0\)</span>? Clearly that’s the minimum of this function. At this point, the tangent line hugs the parabola horizontally, which means it’s a point where the slope is zero.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>f <span class="op">=</span> <span class="kw">lambda</span> x: x <span class="op">**</span> <span class="dv">2</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>dfdx <span class="op">=</span> <span class="kw">lambda</span> x: <span class="dv">2</span> <span class="op">*</span> x</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>x0 <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>y0 <span class="op">=</span> f(x0)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.arange(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>, <span class="fl">0.1</span>)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>f_tangent <span class="op">=</span> <span class="kw">lambda</span> x: y0 <span class="op">+</span> dfdx(x0) <span class="op">*</span> (x <span class="op">-</span> x0)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>plot_function(x, (f, f_tangent), (<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>), (<span class="op">-</span><span class="dv">2</span>, <span class="dv">10</span>), title<span class="op">=</span><span class="ss">f'Tangent of $y=x^2$ at $</span><span class="sc">{</span>(x0,y0)<span class="sc">}</span><span class="ss">$'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>This same fact also holds for the maximum of a function as well. Not just the maximum, but any other point where the function is flat, called <strong>saddle points</strong>. As an example, the origin is a saddle-point of the function <span class="math inline">\(y=x^3\)</span>. These general points where the derivative is zero (min, max, or saddle point) are called <strong>stationary points</strong>.</p>
<p>In machine learning we usually care most about the minimum. I’ll just mention that we can formulate any maximum problem as a minimum problem by just multiplying the function by -1, which flips the function upside down, turning any maxima into minima.</p>
<p>Now, suppose we have a univariate function <span class="math inline">\(y=f(x)\)</span>. The problem of (unconstrained) optimization is to find a point <span class="math inline">\(x^*\)</span> such that <span class="math inline">\(y^* = f(x^*)\)</span> is the <strong>minimum</strong> value of <span class="math inline">\(f(x)\)</span>, i.e.&nbsp; <span class="math display">\[y^* = \min f(x) \leq f(x) \text{ for all } x.\]</span> The special point <span class="math inline">\(x^*\)</span> that minimizes the function is called the <strong>argmin</strong>, written <span class="math display">\[x^* = \text{argmin } f(x).\]</span></p>
<p>I need to mention a subtle point. What do I mean when I say “the minimum”? When I say <span class="math inline">\(y^* \leq f(x)\)</span> for all <span class="math inline">\(x\)</span>, which <span class="math inline">\(x\)</span> values am I talking about? This means we’re really only talking about the minimum over some <em>range</em> of <span class="math inline">\(x\)</span> values. We have to specify what that range is. If the range is the whole real line, it really is <em>the</em> minimum, usually called the <strong>global minimum</strong>. If it’s over some subset of the real line it may not be the global minimum since we’re not looking at every <span class="math inline">\(x\)</span>. It’s only the minimum in our region of interest. This sort of region-specific minimum is called a <strong>local minimum</strong>.</p>
<p>While this seems like a subtle point, it is an important one in machine learning. Some algorithms, like deep learning algorithms, can only reliably find a local minimum. Finding the global minimum can be harder unless there’s only one minimum to begin with. These simple functions are called <strong>convex functions</strong>. Our above example of <span class="math inline">\(y=x^2\)</span> is a convex function. It only has one minimum, and the function just slopes up around it on both sides in a bowl shape. Deep learning loss functions on the other hand are nasty, wiggly things with lots of bumps and valleys. Such functions are called <strong>non-convex functions</strong>. In general they’ll have lots of local minima.</p>
<p>So back to the fact about the derivative being zero at the minimum, what we “proved” by example is that at the point <span class="math inline">\(x^*\)</span> we should have <span class="math display">\[\frac{d}{dx}f(x^*)=0.\]</span> Another useful way to state the same fact is to think in terms of infinitesimals: At <span class="math inline">\(x^*\)</span>, any infinitesimal perturbation <span class="math inline">\(dx\)</span> won’t change the value of the function at all, <span class="math inline">\(f(x^*+dx) = f(x^*)\)</span>. This is just another way of stating that <span class="math inline">\(dy=0\)</span> at <span class="math inline">\(x^*\)</span>. The fact that small perturbations don’t change the function’s value is unique to minima and other stationary points.</p>
<p>Let’s verify this fact with the same example <span class="math inline">\(y=x^2\)</span> by looking at small perturbations around <span class="math inline">\(x=0\)</span>. Since <span class="math inline">\(f(0)=0\)</span> is a minimum, any perturbation should just give <span class="math inline">\(0\)</span> as well. Choosing a <span class="math inline">\(dx\)</span> of <code>1e-5</code>, we can see that the function’s perturbed value <span class="math inline">\(f(0+dx)\)</span> is only about <code>1e-10</code>, essentially negligible since <span class="math inline">\(dx^2 \approx 0\)</span> for infinitesimals. This won’t be true for any other value of <span class="math inline">\(x\)</span>, e.g.&nbsp;<span class="math inline">\(x=1\)</span>, which has a much larger change of <code>2e-5</code>, which is on the order of <span class="math inline">\(dx\)</span>, as expected.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>dx <span class="op">=</span> <span class="fl">1e-5</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>f(<span class="dv">0</span> <span class="op">+</span> dx) <span class="op">-</span> f(<span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>f(<span class="dv">1</span> <span class="op">+</span> dx) <span class="op">-</span> f(<span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Pretty much everything I’ve said on optimization extends naturally to higher dimensions. That’s why I went into so much detail on the simple univariate case. It’s easier to explain and visualize. To extend to <span class="math inline">\(n\)</span> dimensions we basically just need to convert inputs into vectors and derivatives into gradients. Other than this the formulas all look basically the same.</p>
<p>Suppose we have now a scalar-valued multivariate function <span class="math inline">\(z=f(\mathbf{x})=f(x_1,\cdots,x_n)\)</span>. The problem of (unconstrained) optimization is to find a vector <span class="math inline">\(\mathbf{x}^* \in \mathbb{R}^n\)</span> such that <span class="math inline">\(z^* = f(\mathbf{x}^*)\)</span> is the <strong>minimum</strong> value of <span class="math inline">\(f(\mathbf{x})\)</span>, i.e.&nbsp; <span class="math display">\[z^* = \min f(\mathbf{x}) \leq f(\mathbf{x}) \text{ for all } \mathbf{x} \in \mathbb{R}^n.\]</span> The vector <span class="math inline">\(\mathbf{x}^*\)</span> that minimizes the function is called the <strong>argmin</strong>, written <span class="math display">\[\mathbf{x}^* = \text{argmin } f(\mathbf{x}).\]</span></p>
<p>Just as the derivative is zero at the minimum in the univariate case, the <em>gradient</em> is the <em>zero vector</em> at the minimum in the multivariate case, <span class="math display">\[\frac{d}{d\mathbf{x}}f(\mathbf{x^*})=\mathbf{0}.\]</span> Another way of stating the same fact is that at the minimum <span class="math inline">\(f(\mathbf{x^*} + d\mathbf{x}) = f(\mathbf{x^*})\)</span> for any infinitesimal perturbation vector <span class="math inline">\(d\mathbf{x}\)</span>. Equivalently, <span class="math inline">\(dz=0\)</span>.</p>
<section id="gradient-descent" class="level3" data-number="13.2.1">
<h3 data-number="13.2.1" class="anchored" data-anchor-id="gradient-descent"><span class="header-section-number">13.2.1</span> Gradient Descent</h3>
<p>So if the minimum is so important how do we actually find the thing? For simple functions like <span class="math inline">\(y=x^2\)</span> we can do it just by plotting the function, or by trial and error. We can also do it analytically by solving the equation <span class="math inline">\(\frac{dy}{dx}\big|_{x^*}=0\)</span> for <span class="math inline">\(x^*\)</span>. But for complicated functions, or functions we can’t exactly write down, this isn’t feasible. We need an algorithmic way to do it.</p>
<p>Let’s try something simple. Since the derivative at <span class="math inline">\(x\)</span> tells us the slope of the function at <span class="math inline">\(x\)</span>, it’s in some sense telling us how far we are away from the minimum. Suppose we perturb <span class="math inline">\(x\)</span> to <span class="math inline">\(dx\)</span>. Then <span class="math inline">\(y=f(x)\)</span> gets perturbed to <span class="math inline">\(y+dy=f(x+dx)\)</span>. Now, observe the almost trivial fact that <span class="math display">\[dy = \frac{dy}{dx}dx.\]</span> So if <span class="math inline">\(\frac{dy}{dx}\)</span> is <em>large</em>, small changes in <span class="math inline">\(x\)</span> will result in large changes in <span class="math inline">\(y\)</span>. Similarly, if <span class="math inline">\(\frac{dy}{dx}\)</span> is <em>small</em>, then small changes in <span class="math inline">\(x\)</span> will result in small changes in <span class="math inline">\(y\)</span>. But we demonstrated above that if we’re near the minimum we <em>know</em> that changes in <span class="math inline">\(y\)</span> will be tiny if <span class="math inline">\(dx\)</span> is small. Thus, the derivative serves as a kind of “how close are we to the minimum” metric.</p>
<p>But that’s not all the derivative tells us. Since the sign of the derivative indicates which way the slope is slanting, it also tells us which direction the minimum is in. If you’re at a point on the function, the minimum will always be in the direction that’s sloping downward from you. Since the slope slants upward in the direction of the sign of the derivative, and we want to move downward the other way, <strong>the minimum will be in the direction of the negative of the derivative</strong>.</p>
<p>More formally, suppose we want to find the minimum of <span class="math inline">\(y=f(x)\)</span>. To start, we’ll pick a point <span class="math inline">\(x_0\)</span> at random. Doesn’t matter too much how. Pick a step size, we’ll call it <span class="math inline">\(\alpha\)</span>. This will multiply the derivative and tell us how big of a step to take towards the minimum (more on why this is important in a second). Now, we’ll take a step towards the minimum <span class="math display">\[x_1 = x_0 - \alpha \frac{dy}{dx}\bigg|_{x_0}.\]</span> This puts us at a new point <span class="math inline">\(x_1\)</span>, which will be closer to the argmin <span class="math inline">\(x^*\)</span> if our step size is small enough. Now do it again, <span class="math display">\[x_2 = x_1 - \alpha \frac{dy}{dx}\bigg|_{x_1}.\]</span> And again, <span class="math display">\[x_3 = x_2 - \alpha \frac{dy}{dx}\bigg|_{x_2}.\]</span> Keep doing this over and over. Stop when the points aren’t changing much anymore, i.e.&nbsp;when <span class="math inline">\(|x_{n+1}-x_n|&lt;\varepsilon\)</span> for some small tolerance <span class="math inline">\(\varepsilon\)</span>. Then we can say that the argmin is <span class="math inline">\(x^* \approx x_n\)</span>, and the minimum is <span class="math inline">\(y^* \approx f(x_n)\)</span>. Done.</p>
<p>This simple algorithm to find the (local) minimum by starting at a random point and steadily marching in the direction of the derivative is called <strong>gradient descent</strong>. With some relatively minor modifications here and there, gradient descent is how many machine learning algorithms are trained, including essentially all deep learning algorithms. It’s very possibly the most important algorithm in machine learning.</p>
<p>In machine learning, running an optimizer like gradient descent is usually called <strong>training</strong>. You can kind of imagine optimization as trying to teach something to a model. The condition of being at the minimum is analogous to the model learning whatever task it is you’re trying to teach it. The thing we’re minimizing in this case is the loss function, which is hand-picked essentially to measure how well the model is learning the given task.</p>
<p>The step size <span class="math inline">\(\alpha\)</span> is so important in machine learning that it’s given a special name, the <strong>learning rate</strong>. It in essence controls how quickly a model learns, or trains. I’ll use this terminology for <span class="math inline">\(\alpha\)</span> going forward.</p>
<p>Here’s what the algorithm looks like as a python function <code>gradient_descent</code>. It will take as arguments the function <code>f</code> we’re trying to minimize, the function for its derivative or gradient <code>grad_fn</code>, the initial point <code>x0</code>, the learning rate <code>alpha</code>. I’ll also pass in two optional arguments, <code>max_iter</code> and <code>eps</code>, where <code>max_iter</code> is how many iterations to run gradient descent in the worst case, and <code>eps</code> is the tolerance parameter to indicate when to stop.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gradient_descent(f, grad_fn, x0, alpha, max_iter<span class="op">=</span><span class="dv">1000</span>, eps<span class="op">=</span><span class="fl">1e-5</span>):</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    x_prev <span class="op">=</span> x0  <span class="co"># initialize the algorithm</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(max_iter):</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>        x_curr <span class="op">=</span> x_prev <span class="op">-</span> alpha <span class="op">*</span> grad_fn(x_prev)  <span class="co"># gradient descent step</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> np.<span class="bu">abs</span>(x_curr <span class="op">-</span> x_prev) <span class="op">&lt;</span> eps:  <span class="co"># if changes are smaller than eps we're done, return x*</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f'converged after </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss"> iterations'</span>)</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> x_curr</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>        x_prev <span class="op">=</span> x_curr</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'failed to converge in </span><span class="sc">{</span>max_iter<span class="sc">}</span><span class="ss"> iterations'</span>)  <span class="co"># else warn and return x* anyway</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x_curr</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Let’s run this algorithm on our simple example <span class="math inline">\(y=x^2\)</span>. Recall its derivative function is <span class="math inline">\(\frac{dy}{dx}=2x\)</span>. I’ll choose an initial point <span class="math inline">\(x_0=5\)</span> and a learning rate of <span class="math inline">\(\alpha=0.8\)</span>. The optional arguments won’t change.</p>
<p>We can see that gradient descent in this case converges (i.e.&nbsp;finishes) after only 27 iterations. It predicts an argmin of about <span class="math inline">\(x^* \approx 3 \cdot 10^{-6}\)</span> and a minimum of about <span class="math inline">\(y^* \approx 9 \cdot 10^{12}\)</span>. Since both are basically <span class="math inline">\(0\)</span> (the true value for both) to within one part in <span class="math inline">\(10^{-5}\)</span> we seem to have done pretty well here.</p>
<p>Feel free to play around with different choices of the learning rate <code>alpha</code> to see how that affects training time and convergence. Getting a good feel for gradient descent is essential for a machine learning practitioner.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>f <span class="op">=</span> <span class="kw">lambda</span> x: x <span class="op">**</span> <span class="dv">2</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>grad_fn <span class="op">=</span> <span class="kw">lambda</span> x: <span class="dv">2</span> <span class="op">*</span> x</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>x0 <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">0.8</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>x_min <span class="op">=</span> gradient_descent(f, grad_fn, x0, alpha)</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>y_min <span class="op">=</span> f(x_min)</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'estimated argmin: </span><span class="sc">{</span>x_min<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'estimated min: </span><span class="sc">{</span>y_min<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>While I’ve shown the math and code for gradient descent, we’ve still yet to get a good intuition for what the algorithm is doing. For this I’ll turn to a visualization. What I’m going to do is plot the function curve in black, and on top of it show each step of gradient descent. Each red dot on the curve of the function will indicate the point <span class="math inline">\((x_n,y_n)\)</span> at step <span class="math inline">\(n\)</span> of the algorithm. Successive steps will be connected by a red line. Each red line will show which points the algorithm jumps from and to at each step. Starting and ending points will be annotated as well.</p>
<p>To do this I’ll use a helper function <code>plot_gradient_descent</code>, which takes in the same arguments as <code>gradient_descent</code> as well as a few more arguments that do some styling of the plot. Internally, all this function is doing is running gradient descent on the given arguments, then plotting the functions, dots, and line segments described.</p>
<p>I’ll start by showing what gradient descent is doing on the exact same example as above. The curve of course is just a parabola sloping upward from the origin. The starting point is just <span class="math inline">\((x_0,f(x_0))=(5,25)\)</span>. After running for <span class="math inline">\(N=30\)</span> iterations the algorithm basically settles down to <span class="math inline">\((x_N,f(x_N)) \approx (0,0)\)</span>. Notice what’s happening in between though. Imagine you dropped a marble into a bowl at the starting point. After landing, the marble bounces across the bowl several times as it settles down around the origin, where it rolls around less and less until it eventually dissipates all its kinetic energy and settles down at the bottom of the bowl.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>plot_gradient_descent(f<span class="op">=</span>f, grad_fn<span class="op">=</span>grad_fn, x0<span class="op">=</span>x0, alpha<span class="op">=</span>alpha, n_iters<span class="op">=</span><span class="dv">30</span>, </span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>                      title<span class="op">=</span><span class="ss">f'$y=x^2$,  $</span><span class="ch">\\</span><span class="ss">alpha=</span><span class="sc">{</span>alpha<span class="sc">}</span><span class="ss">$,  $N=</span><span class="sc">{</span><span class="dv">30</span><span class="sc">}</span><span class="ss">$,  $x_0=</span><span class="sc">{</span>x0<span class="sc">}</span><span class="ss">$'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>To illustrate what the learning rate is doing, and how important it is to tune it well, let’s try the same problem in two other cases: a really high learning rate, and a really low learning rate. I’ll start with a high learning rate of <span class="math inline">\(\alpha=1.1\)</span>. I’ll run the algorithm this time for <span class="math inline">\(N=20\)</span> iterations.</p>
<p>Pay particular attention in this case to the start and end labels. Evidently choosing a high learning rate caused the algorithm not to spiral down towards the minimum, but to spiral up away from the minimum! This is the hallmark of choosing too large a learning rate. The algorithm won’t converge at all. It’ll just keep shooting further and further away from the minimum.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">1.1</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> <span class="dv">20</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>plot_gradient_descent(f<span class="op">=</span>f, grad_fn<span class="op">=</span>grad_fn, x0<span class="op">=</span>x0, alpha<span class="op">=</span>alpha, n_iters<span class="op">=</span><span class="dv">10</span>, </span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>                      title<span class="op">=</span><span class="ss">f'$y=x^2$,  $</span><span class="ch">\\</span><span class="ss">alpha=</span><span class="sc">{</span>alpha<span class="sc">}</span><span class="ss">$,  $N=</span><span class="sc">{</span><span class="dv">30</span><span class="sc">}</span><span class="ss">$,  $x_0=</span><span class="sc">{</span>x0<span class="sc">}</span><span class="ss">$'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Let’s now look at a low learning rate of <span class="math inline">\(\alpha=0.01\)</span>. I’ll run this one for <span class="math inline">\(N=150\)</span> iterations. Notice now that the algorithm is indeed converging towards the minimum, but it’s doing it really, really slowly. It’s not bouncing around the bowl at all, but rather slowly crawling down in small steps. This is the hallmark of using too low a learning rate. The algorithm will converge, but it’ll do so really, really slowly, and you’ll need to train for a lot of iterations.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> <span class="dv">150</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>plot_gradient_descent(f<span class="op">=</span>f, grad_fn<span class="op">=</span>grad_fn, x0<span class="op">=</span>x0, alpha<span class="op">=</span>alpha, n_iters<span class="op">=</span>N,</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>                      title<span class="op">=</span><span class="ss">f'$y=x^2$,  $</span><span class="ch">\\</span><span class="ss">alpha=</span><span class="sc">{</span>alpha<span class="sc">}</span><span class="ss">$,  $N=</span><span class="sc">{</span>N<span class="sc">}</span><span class="ss">$,  $x_0=</span><span class="sc">{</span>x0<span class="sc">}</span><span class="ss">$'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Things may seem all fine and good. We have an algorithm that seems like it can reliably find the minimum of whatever function we give it, at least in the univariate case. Unfortunately, there are a few subtleties involved that I’ve yet to mention. It turns out that the function I picked, <span class="math inline">\(y=x^2\)</span> is a particularly easy function to minimize. It’s a convex function. Not all functions behave that nicely. Practically no loss function in deep learning does.</p>
<p>If a function is non-convex (i.e.&nbsp;not bowl-shaped) it can have multiple minima. This means that you can’t be sure gradient descent will pick out the global minimum if you run it. Which minimum it settles in will depend on your choice of initial point <span class="math inline">\(x_0\)</span>, the learning rate <span class="math inline">\(\alpha\)</span>, and perhaps even the number of iterations <span class="math inline">\(N\)</span> you run the algorithm.</p>
<p>This isn’t the only problem, or even the worst problem. Perhaps the worst problem is saddle points. If there are saddle points in the function, gradient descent may well settle down on one of those instead of any of the minima. Here’s an example of this. Let’s look at the function <span class="math inline">\(y=x^3 + (x+1)^4\)</span>. Its derivative function turns out to be <span class="math inline">\(\frac{dy}{dx}=3x^2 + 4(x+1)^3\)</span>. Check WolframAlpha if you don’t believe me.</p>
<p>Now, suppose we want to find the minimum of this function. Not knowing any better, we pick an initial point <span class="math inline">\(x_0=3\)</span>, and just to be safe we pick a small learning rate <span class="math inline">\(\alpha=0.001\)</span>. Let’s run gradient descent now for <span class="math inline">\(N=500\)</span> iterations. Surely that’s enough to find the minimum, right?</p>
<p>Evidently not. The true minimum seems to be somewhere around the point <span class="math inline">\((-2.8, -12)\)</span>. The algorithm didn’t settle down anywhere near this point. It settled around the origin <span class="math inline">\((0,0)\)</span>. So what happened? If you look closely, you’ll see it got stuck in a flat spot, i.e.&nbsp;a saddle point. No matter how many iterations you run gradient descent with this learning rate, it will never leave this flat spot. It’s stuck.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>f <span class="op">=</span> <span class="kw">lambda</span> x: x <span class="op">**</span> <span class="dv">3</span> <span class="op">+</span> (x <span class="op">+</span> <span class="dv">1</span>) <span class="op">**</span> <span class="dv">4</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>grad_fn <span class="op">=</span> <span class="kw">lambda</span> x: <span class="dv">3</span> <span class="op">*</span> x <span class="op">**</span> <span class="dv">2</span> <span class="op">+</span> <span class="dv">4</span> <span class="op">*</span> (x <span class="op">+</span> <span class="dv">1</span>) <span class="op">**</span> <span class="dv">3</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>x0 <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> <span class="dv">500</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>alpha<span class="op">=</span><span class="fl">0.001</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>plot_gradient_descent(f, grad_fn, x0, alpha<span class="op">=</span>alpha, n_iters<span class="op">=</span>N, xlim<span class="op">=</span>(<span class="op">-</span><span class="dv">4</span>, <span class="dv">2</span>), ylim<span class="op">=</span>(<span class="op">-</span><span class="dv">15</span>, <span class="dv">50</span>), </span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>                      title<span class="op">=</span><span class="ss">f'$y=x^3 + (x-1)^4$,  $</span><span class="ch">\\</span><span class="ss">alpha=</span><span class="sc">{</span>alpha<span class="sc">}</span><span class="ss">$,  $N=</span><span class="sc">{</span>N<span class="sc">}</span><span class="ss">$,  $x_0=</span><span class="sc">{</span>x0<span class="sc">}</span><span class="ss">$'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>All isn’t necessarily lost. What happens if we pick a higher learning rate to let the algorithm bounce around the function a little bit before slowing down? Let’s pick <span class="math inline">\(\alpha=0.03\)</span> now and run for the same number of iterations. Now it looks like we’re doing just fine. Gradient descent was able to bounce across the flat spot and settle down at the other side.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>alpha<span class="op">=</span><span class="fl">0.03</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>plot_gradient_descent(f, grad_fn, x0, alpha<span class="op">=</span>alpha, n_iters<span class="op">=</span>N, xlim<span class="op">=</span>(<span class="op">-</span><span class="dv">6</span>, <span class="dv">4</span>), ylim<span class="op">=</span>(<span class="op">-</span><span class="dv">15</span>, f(<span class="dv">3</span>) <span class="op">+</span> <span class="dv">20</span>), </span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>                      annotate_start_end<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>                      title<span class="op">=</span><span class="ss">f'$y=x^3 + (x-1)^4$,  $</span><span class="ch">\\</span><span class="ss">alpha=</span><span class="sc">{</span>alpha<span class="sc">}</span><span class="ss">$,  $N=</span><span class="sc">{</span>N<span class="sc">}</span><span class="ss">$,  $x_0=</span><span class="sc">{</span>x0<span class="sc">}</span><span class="ss">$'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>This example was meant to show that saddle points can be a real issue. Gradient descent will not tell you if the point it found is a minimum or a saddle point, it’ll just stop running and spit out a value. You thus need to be careful about things like this when running gradient descent on real-life functions. It’s even worse in higher dimensions, where it turns out that almost all stationary points will be saddle points, and very few will be minima or maxima.</p>
<p>For these reasons, it’s common in machine learning to not use a tolerance condition like <span class="math inline">\(|x_{n}-x_{n-1}| &lt; \varepsilon\)</span>. Instead we just specify some number of iterations <span class="math inline">\(N\)</span> and run the algorithm <span class="math inline">\(N\)</span> times. Basically, we want to give the algorithm a chance to get out of a flat spot if it gets stuck in one for some reason. Said differently, if a function is not convex, and most in machine learning are not convex, the notion of convergence doesn’t necessarily mean that much since we don’t even know if we’re at a minimum or not.</p>
<p>The gradient descent algorithm works exactly the same as in the univariate case, except we now use the gradient vector instead of the derivative at each step. Here’s the algorithm in steps: 1. Initialize a starting vector <span class="math inline">\(\mathbf{x}_0\)</span>. 2. For <span class="math inline">\(N\)</span> iterations, perform the gradient descent update <span class="math display">\[\mathbf{x}_n = \mathbf{x}_{n-1} - \alpha \frac{dz}{d\mathbf{x}}\bigg|_{\mathbf{x}=\mathbf{x}_{n-1}}.\]</span> 3. Converge either when some convergence criterion is satisfied, <span class="math inline">\(||\mathbf{x}_n-\mathbf{x}_{n-1}||_2 \leq \varepsilon\)</span>, or when some maximum number of iterations <span class="math inline">\(N\)</span> is reached. 4. Return <span class="math inline">\(\mathbf{x}_N\)</span>. The best guess for the argmin is <span class="math inline">\(\mathbf{x}^* \approx \mathbf{x}_N\)</span>, and for the minimum is <span class="math inline">\(z^* \approx f(\mathbf{x}_N)\)</span>.</p>
<p><strong>Aside:</strong> I’ll quickly note that gradient descent isn’t the only minimization algorithm. Some other algorithms worth noting use not just the first derivative in their updates, but also the second derivative. Examples include algorithms like Newton’s Method and LBFGS. The second derivative provides information about the curvature of the function, which can speed up convergence by making the learning rate adaptive. While these <em>second-order</em> algorithms are useful in some areas of machine learning, it usually turns out to be far too computationally expensive to calculate the second derivative (also called the Hessian) of a function in high dimensions. Perhaps the main reason gradient descent is used in machine learning is because it provides a good tradeoff between its speed of convergence and computational performance.</p>
<p>This pretty much covers everything I wanted to talk about regarding optimization, the most important application of calculus to machine learning. In future lessons we’ll spend more time talking about gradient descent as well as its more modern variants like SGD and Adam.</p>
</section>
</section>
<section id="moments-of-a-distribution" class="level2" data-number="13.3">
<h2 data-number="13.3" class="anchored" data-anchor-id="moments-of-a-distribution"><span class="header-section-number">13.3</span> Moments of a Distribution</h2>
<p>Probability distributions have special quantities that are worth keeping track of, called moments. The most important moments for practical purposes are the mean and variance, but there are higher-order moments as well like skewness and kurtosis that sometimes become important.</p>
<section id="univariate-moments" class="level3" data-number="13.3.1">
<h3 data-number="13.3.1" class="anchored" data-anchor-id="univariate-moments"><span class="header-section-number">13.3.1</span> Univariate Moments</h3>
<p>Let’s start with a boring term that I’ll call the “zeroth moment”. It’s just a restatement of the fact that probabilities sum to one from before,</p>
<p><span class="math display">\[
1 =
\begin{cases}
\sum_{j=0}^{k-1} p(x_k), &amp; x \text{ is discrete}, \\ \\
\int_{-\infty}^\infty p(x) dx, &amp; x \text{ is continuous}.
\end{cases}
\]</span></p>
<p>Moving onto the first moment. Define the <strong>mean</strong> (or <strong>expected value</strong>) of a distribution <span class="math inline">\(p(x)\)</span>, usually denoted by symbols like <span class="math inline">\(\langle x \rangle\)</span> or <span class="math inline">\(\mathbb{E}(x)\)</span> or <span class="math inline">\(\mu\)</span>, by</p>
<p><span class="math display">\[
\langle x \rangle = \mathbb{E}(x) =
\begin{cases}
\sum_{j=0}^{k-1} x_k p(x_k), &amp; x \text{ is discrete}, \\ \\
\int_{-\infty}^\infty x p(x) dx, &amp; x \text{ is continuous}.
\end{cases}
\]</span></p>
<p>The mean of a univariate distribution <span class="math inline">\(p(x)\)</span> is an estimate of the “center of mass” or the “balancing point” of the distribution. It will be a single number.</p>
<p>Moving onto the second moment. Let <span class="math inline">\(\mu = \langle x \rangle\)</span> for convenience. Define the <strong>variance</strong> (or <strong>mean square</strong>) of a distribution <span class="math inline">\(p(x)\)</span>, usually denoted <span class="math inline">\(\text{Var}(x)\)</span> or <span class="math inline">\(\sigma^2\)</span>, by the mean of the squared difference <span class="math inline">\((x-\mu)^2\)</span>, that is</p>
<p><span class="math display">\[
\text{Var}(x) = \big\langle (x-\mu)^2 \big\rangle =
\begin{cases}
\sum_{j=0}^{k-1} (x_k-\mu)^2 p(x_k), &amp; x \text{ is discrete}, \\ \\
\int_{-\infty}^\infty (x-\mu)^2 p(x) dx, &amp; x \text{ is continuous}.
\end{cases}
\]</span></p>
<p>Similar to the mean, the variance of a univariate distribution will also be a single number indicating the “spread” of the distribution. More commonly, when talking about the “spread” of a distribution, we like to talk about the square root of the variance, called the <strong>standard deviation</strong> (or <strong>root mean square</strong>) and usually denoted <span class="math inline">\(\sigma\)</span>, <span class="math display">\[\sigma = \sqrt{\text{Var}(x)}.\]</span></p>
<p>The standard deviation has the advantage that it’s on the same scale of <span class="math inline">\(x\)</span>, and has the same units.</p>
<p>I won’t prove it, but this pattern is very general. We can talk about taking the mean of any function of a random variable just as well. Suppose <span class="math inline">\(f(x)\)</span> is some reasonably well-behaved function. Then we can talk about the mean of <span class="math inline">\(f(x)\)</span> by using</p>
<p><span class="math display">\[
\langle f(x) \rangle = \mathbb{E}f(x) =
\begin{cases}
\sum_{j=0}^{k-1} f(x_j) p(x_j), &amp; x \text{ is discrete}, \\ \\
\int_{-\infty}^\infty f(x) p(x) dx, &amp; x \text{ is continuous}.
\end{cases}
\]</span></p>
<p>This trick is sometimes called the Law of the Unconscious Statistician.</p>
<p>The variance can be written in a different form by doing some algebraic manipulations, <span class="math display">\[\text{Var}(x) = \langle x^2 \rangle - \mu^2.\]</span> You can pretty easily derive this formula if you like, but I won’t do so here. This form is sometimes easier to use to do calculations if we’ve already calculated the mean <span class="math inline">\(\mu\)</span>, since we only need to calculate <span class="math inline">\(\langle x^2 \rangle\)</span>, which is often easier to do.</p>
<p>It’s worth noting that the mean operation is <em>linear</em>, which means two things, - if <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> are two random variables, then <span class="math inline">\(\langle x + y \rangle = \langle x \rangle + \langle y \rangle\)</span>, - if <span class="math inline">\(x\)</span> is a random variable and <span class="math inline">\(c\)</span> is some constant, then <span class="math inline">\(\langle cx \rangle = c\langle x \rangle\)</span>.</p>
<p>This fact follows immediately from the fact that the mean is just a sum (or integral), and sums are linear. This will always be true, not just for any two random variables, but any number of them. It <em>won’t</em>, however, be true for the variance. That is, <span class="math display">\[\text{Var}(x+y) \neq \text{Var}(x) + \text{Var}(y), \qquad \text{Var}(cx) = c^2\text{Var}(x).\]</span></p>
</section>
<section id="multivariate-moments" class="level3" data-number="13.3.2">
<h3 data-number="13.3.2" class="anchored" data-anchor-id="multivariate-moments"><span class="header-section-number">13.3.2</span> Multivariate Moments</h3>
<p>Moments also extend to multivariate distributions, but it becomes more complicated. The <em>mean</em> for univariate distributions becomes the <em>mean vector</em> for multivariate distributions, while <em>variance</em> becomes the <em>covariance matrix</em>. Other moments can be even more complicated in the multivariate case by becoming higher-rank tensors. I’ll briefly spell out the definition of mean and covariance below and stop there.</p>
<p>The <strong>mean vector</strong> of an <span class="math inline">\(n\)</span> dimensional multivariate distribution <span class="math inline">\(p(\mathbf{x})\)</span>, denoted <span class="math inline">\(\langle \mathbf{x} \rangle\)</span> or <span class="math inline">\(\mathbb{E}(\mathbf{x})\)</span> or <span class="math inline">\(\boldsymbol{\mu}\)</span>, is defined by</p>
<p><span class="math display">\[
\langle \mathbf{x} \rangle = \mathbb{E}(\mathbf{x}) =
\begin{cases}
\sum_{j=0}^{k-1} \mathbf{x}_j p(\mathbf{x}_j), &amp; \mathbf{x} \text{ is discrete}, \\ \\
\int_{\mathbb{R}^n} \mathbf{x} p(\mathbf{x}) dA_{n-1}, &amp; \mathbf{x} \text{ is continuous}.
\end{cases}
\]</span></p>
<p>If you look carefully, you’ll see the mean vector is just the vector whose components are the ordinary means, <span class="math inline">\(\mu_i = \langle x_i \rangle\)</span>. Just as in the univariate case, the mean vector again represents the “center of mass” of the distribution, except now in <span class="math inline">\(n\)</span> dimensional space.</p>
<p>The <strong>covariance matrix</strong> of an <span class="math inline">\(n\)</span> dimensional multivariate distribution <span class="math inline">\(p(\mathbf{x})\)</span>, denoted either <span class="math inline">\(\text{Cov}(\mathbf{x})\)</span> or <span class="math inline">\(\boldsymbol{\Sigma}\)</span>, is defined as the mean square of the outer product of the difference <span class="math inline">\(\mathbf{x} - \boldsymbol{\mu}\)</span>, i.e. <span class="math display">\[\boldsymbol{\Sigma} = \text{Cov}(\mathbf{x}) = \big\langle (\mathbf{x}-\boldsymbol{\mu}) (\mathbf{x}-\boldsymbol{\mu})^\top \big\rangle \qquad \Rightarrow \qquad
\sigma_{i,j} = \langle (x_i-\mu_i)(x_j-\mu_j) \rangle.\]</span></p>
<p>This gives a symmetric, positive <span class="math inline">\((n,n)\)</span> matrix whose elements <span class="math inline">\(\sigma_{i,j}\)</span> represent how much any two variables “depend on each other”. The diagonal elements <span class="math inline">\(\sigma_{i,i}\)</span> represent the variances of each <span class="math inline">\(x_i\)</span>, while the off-diagonal elements <span class="math inline">\(\sigma_{i,j}\)</span>, <span class="math inline">\(i \neq j\)</span> represent the correlation of <span class="math inline">\(x_i\)</span> with <span class="math inline">\(x_j\)</span>. The higher <span class="math inline">\(\sigma_{i,j}\)</span>, the higher the two variables are said to correlate. If <span class="math inline">\(\sigma_{i,j}=0\)</span> the two variables are said to be uncorrelated.</p>
</section>
<section id="example-bernoulli-distribution" class="level3" data-number="13.3.3">
<h3 data-number="13.3.3" class="anchored" data-anchor-id="example-bernoulli-distribution"><span class="header-section-number">13.3.3</span> Example: Bernoulli Distribution</h3>
<p>Just so you can see how this process is done, I’ll work one simple example, the Bernoulli distribution. This one should be easy since it only takes on two values <span class="math inline">\(x=0,1\)</span>. The other ones, not so much.</p>
<p>Recall for Bernoulli we have the probability function</p>
<p><span class="math display">\[
p(x) =
\begin{cases}
p_0 &amp; x = 1, \\
(1-p_0) &amp; x = 0.
\end{cases}
\]</span></p>
<p>For the mean, we thus have</p>
<p><span class="math display">\[
\begin{align}
\langle x \rangle &amp;= \sum_{x=0}^{1} x p(x) \\
&amp;= (0 \cdot (1-p_0)) + (1 \cdot p_0) \\
&amp;= p_0.
\end{align}
\]</span></p>
<p>That is, <span class="math inline">\(\mu = \langle x \rangle = p_0\)</span>. The mean of Bernoulli is just the probability that <span class="math inline">\(x=1\)</span>.</p>
<p>For the variance, we have</p>
<p><span class="math display">\[
\begin{align}
\text{Var}(x) &amp;= \langle (x-\mu)^2 \rangle \\
&amp;= \sum_{x=0}^{1} (x - \mu)^2 p(x) \\
&amp;= ((0-p_0)^2 \cdot (1-p_0)) + ((1-p_0)^2 \cdot p_0) \\
&amp;= p_0^2 \cdot (1-p_0) + (1-p_0)^2 \cdot p_0 \\
&amp;= p_0 (1 - p_0).
\end{align}
\]</span></p>
<p>Thus, the variance of Bernoulli is <span class="math inline">\(\text{Var}(x) = p_0(1-p_0)\)</span>, i.e.&nbsp;the probability <span class="math inline">\(x=1\)</span> times the probability <span class="math inline">\(x=0\)</span>. Taking the square root then gives the standard deviation, <span class="math inline">\(\sigma = \sqrt{p_0(1-p_0)}\)</span>.</p>
</section>
<section id="list-of-means-and-variances" class="level3" data-number="13.3.4">
<h3 data-number="13.3.4" class="anchored" data-anchor-id="list-of-means-and-variances"><span class="header-section-number">13.3.4</span> List of Means and Variances</h3>
<p>Rather than put you through the tedium of anymore boring and messy calculations, I’ll just list the mean and variance of several common distributions below for reference. Notice they won’t be a function of the random variable, but they <em>will</em> be a function of the distribution’s parameters.</p>

<table>
<tbody><tr>
<th style="text-align: left;">
Distribution
</th>
<th style="text-align: left;">
Support
</th>
<th>
Mean
</th>
<th>
Variance
</th>
</tr>
<tr>
<td style="width: 200px; height: 40px; text-align: left;">
Discrete Uniform <span class="math inline">\(\text{DU}(a,b)\)</span>
</td>
<td style="width: 150px; height: 40px; text-align: left;">
<span class="math inline">\(\{a,a+1,\cdots,b-1\}\)</span>
</td>
<td style="width: 150px; height: 40px;">
<span class="math inline">\(\frac{1}{2}(a+b-1)\)</span>
</td>
<td style="width: 150px; height: 40px;">
<span class="math inline">\(\frac{1}{12}((b-a)^2-1)\)</span>
</td>
</tr>
<tr>
<td style="width: 200px; height: 40px; text-align: left;">
Bernoulli <span class="math inline">\(\text{Ber}(\text{p})\)</span>
</td>
<td style="width: 150px; height: 40px; text-align: left;">
<span class="math inline">\(\{0,1\}\)</span>
</td>
<td style="width: 150px; height: 40px;">
<span class="math inline">\(\text{p}\)</span>
</td>
<td style="width: 150px; height: 40px;">
<span class="math inline">\(\text{p}(1-\text{p})\)</span>
</td>
</tr>
<tr>
<td style="width: 200px; height: 40px; text-align: left;">
Binomial <span class="math inline">\(\text{Bin}(n, \text{p})\)</span>
</td>
<td style="width: 150px; height: 40px; text-align: left;">
<span class="math inline">\(\{0,1,\cdots,n\}\)</span>
</td>
<td style="width: 150px; height: 40px;">
<span class="math inline">\(n\text{p}\)</span>
</td>
<td style="width: 150px; height: 40px;">
<span class="math inline">\(n\text{p}(1-\text{p})\)</span>
</td>
</tr>
<tr>
<td style="width: 200px; height: 40px; text-align: left;">
Categorical <span class="math inline">\(\text{Cat}(\mathbf{p})\)</span>
</td>
<td style="width: 150px; height: 40px; text-align: left;">
<span class="math inline">\(\{0,1,\cdots,k-1\}\)</span>
</td>
<td style="width: 150px; height: 40px;">
<span class="math inline">\(p_j\)</span> for all <span class="math inline">\(j\)</span>
</td>
<td style="width: 150px; height: 40px;">
<span class="math inline">\(p_j (1 - p_j)\)</span> for all <span class="math inline">\(j\)</span>
</td>
</tr>
<tr>
<td style="width: 200px; height: 40px; text-align: left;">
Uniform <span class="math inline">\(U(a,b)\)</span>
</td>
<td style="width: 150px; height: 40px; text-align: left;">
<span class="math inline">\([a,b]\)</span>
</td>
<td style="width: 150px; height: 40px;">
<span class="math inline">\(\frac{1}{2}(a+b)\)</span>
</td>
<td style="width: 150px; height: 40px;">
<span class="math inline">\(\frac{1}{12}(b-a)^2\)</span>
</td>
</tr>
<tr>
<td style="width: 200px; height: 40px; text-align: left;">
Gaussian <span class="math inline">\(\mathcal{N}(\mu,\sigma^2)\)</span>
</td>
<td style="width: 150px; height: 40px; text-align: left;">
<span class="math inline">\(\mathbb{R}\)</span>
</td>
<td style="width: 150px; height: 40px;">
<span class="math inline">\(\mu\)</span>
</td>
<td style="width: 150px; height: 40px;">
<span class="math inline">\(\sigma^2\)</span>
</td>
</tr>
<tr>
<td style="width: 200px; height: 40px; text-align: left;">
Laplace <span class="math inline">\(L(\mu,\sigma)\)</span>
</td>
<td style="width: 150px; height: 40px; text-align: left;">
<span class="math inline">\(\mathbb{R}\)</span>
</td>
<td style="width: 150px; height: 40px;">
<span class="math inline">\(\mu\)</span>
</td>
<td style="width: 150px; height: 40px;">
<span class="math inline">\(2\sigma\)</span>
</td>
</tr>
<tr>
<td style="width: 200px; height: 40px; text-align: left;">
Multivariate Gaussian <span class="math inline">\(\mathcal{N}(\boldsymbol{\mu},\boldsymbol{\Sigma})\)</span>
</td>
<td style="width: 150px; height: 40px; text-align: left;">
<span class="math inline">\(\mathbb{R}^n\)</span>
</td>
<td style="width: 150px; height: 40px;">
<span class="math inline">\(\boldsymbol{\mu}\)</span>
</td>
<td style="width: 150px; height: 40px;">
<span class="math inline">\(\boldsymbol{\Sigma}\)</span>
</td>
</tr>

</tbody></table>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation column-page-right">
  <div class="nav-page nav-page-previous">
      <a href="../notebooks/multivariate-probability.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Multivariate Distributions</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
  </div>
</nav>
</div> <!-- /content -->



</body></html>