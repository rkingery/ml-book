[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Mathematics for Machine Learning",
    "section": "",
    "text": "Preface\nThis is my book Mathematics for Machine Learning."
  },
  {
    "objectID": "notebooks/basic-math.html#elementary-math",
    "href": "notebooks/basic-math.html#elementary-math",
    "title": "1  Basic Math",
    "section": "1.1 Elementary Math",
    "text": "1.1 Elementary Math\nIt’s useful in machine learning to be able to read and manipulate basic arithmetic and algebraic equations, particularly when reading research papers, blog posts, or documentation. I won’t go into depth on the basics of high school arithmetic and algebra. I do have to assume some mathematical maturity of the reader, and this seems like a good place to draw the line. I’ll just mention a few key points.\n\n1.1.0.1 Numbers\nRecall that numbers can come in several forms. We can have,\n\nNatural Numbers: These are positive whole numbers \\(0, 1, 2, 3, 4, \\cdots\\). Note the inclusion of \\(0\\) in this group. Following the computer science convention I’ll tend to do that. The set of all natural numbers is denoted by the symbol \\(\\mathbb{N}\\).\nIntegers: These are any whole numbers \\(\\cdots, -2, -1, 0, 1, 2, \\cdots\\), positive, negative, and zero. The set of all integers is denoted by the symbol \\(\\mathbb{Z}\\).\nRational Numbers: These are any ratios of integers, for example \\[\\frac{1}{2}, \\frac{5}{4}, -\\frac{3}{4}, \\frac{1000}{999}, \\cdots.\\] Any ratio will do, so long as the denominator (the bottom number) is not zero. The set of all rational numbers is denoted by the symbol \\(\\mathbb{Q}\\).\nReal Numbers: These are any arbitrary decimal numbers on the number line, for example \\[1.00, \\ 5.07956, \\ -0.99999\\dots, \\ \\pi=3.1415\\dots, \\ e=2.718\\dots, \\ \\cdots.\\] They include as a special case both the integers and the rational numbers, but also include numbers that can’t be represented as fractions, like \\(\\pi\\) and \\(e\\). The set of all real numbers is denoted by the symbol \\(\\mathbb{R}\\).\nComplex numbers: These are numbers with both real and imaginary parts, like \\(1 + 2i\\) where \\(i=\\sqrt{-1}\\). Complex numbers include the real numbers as a special case. Since they don’t really show up in machine learning we won’t deal with these after this. The set of all complex numbers is denoted by the symbol \\(\\mathbb{C}\\).\n\n\n\n1.1.0.2 Basic Algebra\nYou should be familiar with the usual arithmetic operations defined on these systems of numbers. Things like addition, subtraction, multiplication, and division. You should also at least vaguely recall the order of operations, which defines the order in which complex arithmetic operations with parenthesis are carried out. For example,\n\\[(5+1) \\cdot \\frac{(7-3)^2}{2} = 6 \\cdot \\frac{4^2}{2} = 6 \\cdot \\frac{16}{2} = 6 \\cdot 8 = 48.\\]\nYou should be able to manipulate and simplify simple fractions by hand. For example,\n\\[\\frac{3}{7} + \\frac{1}{5} = \\frac{3 \\cdot 5 + 1 \\cdot 7}{7 \\cdot 5} = \\frac{22}{35} \\approx 0.62857.\\]\nAs far as basic algebra goes, you should be familiar with algebraic expressions like \\(x+5=7\\) and be able to solve for the unknown variable \\(x\\),\n\\[x=7-5=2.\\]\nYou should be able to take an equation like \\(ax + b = c\\) and solve it for \\(x\\) in terms of coefficients \\(a, b, c\\),\n\\[\\begin{align*}\nax + b &= c \\\\\nax &= c - b \\\\\nx &= \\frac{c - b}{a}.\n\\end{align*}\\]\nYou should also be able to expand simple expressions like this,\n\\[\\begin{align*}\n(ax - b)^2 &= (ax - b)(ax - b) \\\\\n&= (ax)^2 - (ax)b - b(ax) + b^2 \\\\\n&= a^2x^2 - abx - abx + b^2 \\\\\n&= a^2x^2 - 2abx + b^2.\n\\end{align*}\\]\n\n\n1.1.0.3 Sets and Intervals\nIt’s also worth recalling what a set is. Briefly, a set is a collection of unique elements. Usually those elements are numbers. To say that an element \\(x\\) is an element of a set \\(S\\), we’d write \\(x \\in S\\), read “\\(x\\) is in \\(S\\)”. If \\(x\\) is not in the set, we’d write \\(x \\notin S\\). For example, the set of elements \\(1, 2, 3\\) can be denoted \\(S = \\{1, 2, 3\\}\\). Then \\(1 \\in S\\), but \\(5 \\notin S\\).\nI’ve already mentioned the most common sets we’ll care about, namely the natural numbers \\(\\mathbb{N}\\), integers \\(\\mathbb{Z}\\), rational numbers \\(\\mathbb{Q}\\), and real numbers \\(\\mathbb{R}\\). Also of interest will be the intervals,\n\nOpen interval: \\((a, b) = \\{x: a < x < b \\}\\).\nHalf-open left interval: \\((a, b] = \\{x: a < x \\leq b \\}\\).\nHalf-open right interval: \\([a, b) = \\{x: a \\leq x < b \\}\\).\nClosed interval: \\([a, b] = \\{x: a \\leq x \\leq b \\}\\).\n\nThink of intervals as representing line segments on the real line, connecting \\(a\\) to \\(b\\). I’ll touch on sets more in coming lessons. I just want you to be familiar with the notation, since I’ll occasionally use it.\n\n\n1.1.1 Symbolic vs Numerical Computation\nThere are two fundamental ways to perform mathematical computations: numerical computation, and symbolic computation. You’re familiar with both even though you may not realize it. Numerical computation involves crunching numbers. You plug in numbers, and get out numbers. When you type something like 10.5 / 12.4 in python, it will return a number, like 0.8467741935483871. This is numerical computation.\n\n\nCode\n10.5 / 12.4\n\n\n0.8467741935483871\n\n\nThis contrasts with a way of doing computations that you learned in math class, where you manipulate symbols. This is called symbolic computation. Expanding an equation like \\((ax-b)^2\\) to get \\(a^2x^2 - 2abx + b^2\\) is an example of a symbolic computation. You see the presence of abstract variables like \\(x\\) that don’t have a set numeric value.\nUsually in practice we’re interested in numerical computations. We’ll mostly be doing that in this book. But sometimes, when working with equations, we’ll need to do symbolic computations as well. Fortunately, python has a library called SymPy, or sympy, that can do symbolic computation automatically. I won’t use it a whole lot in this book, but it will be convenient in a few places to show you that you don’t need to manipulate mathematical expressions by hand all the time.\nTo use sympy, I’ll import sympy with the alias sp. Before defining a function to operate on, we first have to encode all the symbols in the problem as sympy Symbol objects. Once that’s done, we can create equations out of them and perform mathematical operations.\nHere’s an example of using sympy to expand the equation above, \\((ax-b)^2\\).\n\n\nCode\nimport sympy as sp\n\n\n\n\nCode\na = sp.Symbol('a')\nb = sp.Symbol('b')\nx = sp.Symbol('x')\na, b, x\n\n\n(a, b, x)\n\n\n\n\nCode\nequation = (a * x - b) ** 2\nexpanded = sp.expand(equation, x)\nprint(f'expanded equation: {expanded}')\n\n\nexpanded equation: a**2*x**2 - 2*a*b*x + b**2\n\n\nWe can also use sympy to solve equations. Here’s an example of solving the quadratic equation \\(x^2 = 6\\) for its two roots, \\(x = \\pm \\sqrt{6}\\).\n\n\nCode\nequation = x**2 - 6\nsolutions = sp.solve(equation, x)\nprint(f'solutions = {solutions}')\n\n\nsolutions = [-sqrt(6), sqrt(6)]\n\n\nSympy has a lot of functionality, and it can be a very difficult library to learn due to its often strange syntax for things. Since we won’t really need it all that often I’ll skip the in depth tutorial. See the documentation if you’re interested."
  },
  {
    "objectID": "notebooks/basic-math.html#univariate-functions",
    "href": "notebooks/basic-math.html#univariate-functions",
    "title": "1  Basic Math",
    "section": "1.2 Univariate Functions",
    "text": "1.2 Univariate Functions\nAs I’m sure you’ve seen before, a mathematical function is a way to map inputs \\(x\\) to outputs \\(y\\). That is, a function \\(f(x)\\) is a mapping that takes in a value \\(x\\) and maps it to a unique value \\(y=f(x)\\). These values can be either single numbers (called scalars), or multiple numbers (vectors or tensors). When \\(x\\) and \\(y\\) are both scalars, \\(f(x)\\) is called a univariate function.\nLet’s quickly cover some of the common functions you’d have seen before in a math class, focusing mainly on the ones that show up in machine learning. I’ll also cover a couple machine-learning specific functions you perhaps haven’t seen before.\n\n1.2.1 Affine Functions\nThe most basic functions to be aware of are the straight-line functions: constant functions, linear functions, and affine functions:\n\nConstant functions: \\(y=c\\) or \\(x=c\\)\n\nExamples: \\(y=2\\), \\(x=1\\)\n\nLinear functions: \\(y=ax\\)\n\nExamples: \\(y=-x\\), \\(y=5x\\)\n\nAffine functions: \\(y=ax+b\\)\n\nExamples: \\(y=-x+1\\), \\(y=5x-4\\)\n\n\nAll constant functions are linear functions, and all linear functions are affine functions. In the case of affine functions, the value \\(b\\) is called the intercept. It corresponds to the value where the function crosses the y-axis. The value \\(a\\) is called the slope. It corresponds to the steepness of the curve, i.e. its height over its width (or “rise” over “run”). Notice linear functions are the special case where the intercept is always the origin \\(x=0, y=0\\).\n\n1.2.1.1 Plotting\nWe can plot these and any other univariate function \\(y=f(x)\\) in the usual way you learned about in school. We sample a lot of \\((x,y)\\) pairs from the function, and plot them on a grid with a horizontal x-axis and vertical y-axis.\nBefore plotting some examples I need to mention that plotting in python is usually done with the matplotlib library. Typically what we’d do to get a very simple plot is:\n\nImport plt, which is the alias to the submodule matplotlib.pyplot\nGet a grid of x values we want to plot, e.g. using np.linspace or np.arange\nGet a grid of y values either directly, or by first defining a python function f(x)\nPlot x vs y by calling plt.(x, y), followed by plt.show().\n\nNote step (2) requires another library called numpy to create the grid of points. You don’t have to use numpy for this, but it’s typically easiest. Usually numpy is imported with the alias np. Numpy is python’s main library for working with numerical arrays. We’ll cover it in much more detail in future lessons.\nLet me go ahead and load these libraries. I’ll also show a simple example of a plot. What I’ll do is define a grid x of 100 equally spaced points between -10 and 10, and plot the function \\(y=x-1\\) using the method described above.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\n\nCode\nx = np.linspace(-10, 10, 100)\ny = x - 1\nplt.plot(x, y)\nplt.show()\n\n\n\n\n\n\n\n\n\nThis plot is pretty ugly. It’s too big, arbitrarily scaled, and doesn’t include any information about what’s being plotted against what. In matplotlib if you want to include all these things to make nice plots you have to include a bunch of extra style commands.\nFor this reason, for the rest of the plotting in this lesson I’m going to use a helper function plot_function, which takes in x and y, the range of x values we want to plot, and an optional title. I didn’t think the details of this helper function were worth going into now, so I abstracted it away into the file utils.py in this same directory. It uses matplotlib like I described, but with a good bit of styling to make the plot more readable. If you really want to see the details perhaps the easiest thing to do is create a cell below using ESC-B and type the command ??plot_function, which will print the code inside the function as the output.\nBack to it, let’s plot one example each of a constant function \\(y=2\\), a linear function \\(y=2x\\), and an affine function \\(2x-1\\).\n\n\nCode\nx = np.arange(-10, 10, 0.1)\nf = lambda x: 2 * np.ones(len(x))\nplot_function(x, f, xlim=(-5, 5), ylim=(-5, 5), ticks_every=[1, 1], \n              title='Constant Function: $y=2$')\n\n\n\n\n\n\n\n\n\n\n\nCode\nx = np.arange(-10, 10, 0.1)\nf = lambda x: 2 * x\nplot_function(x, f, xlim=(-5, 5), ylim=(-5, 5), ticks_every=[1, 1], \n              title='Linear Function: $y=2x$')\n\n\n\n\n\n\n\n\n\n\n\nCode\nx = np.arange(-10, 10, 0.1)\nf = lambda x: 2 * x - 1\nplot_function(x, f, xlim=(-5, 5), ylim=(-5, 5), ticks_every=[1, 1], \n              title='Affine Function: $y=2x-1$')\n\n\n\n\n\n\n\n\n\n\n\n\n1.2.2 Polynomial Functions\nPolynomial functions are just sums of positive integer powers of \\(x\\), e.g. something like \\(y=3x^2+5x+1\\) or \\(y=x^{10}-x^{3}+4\\). The highest power that shows up in the function is called the degree of the polynomial. For example, the above examples have degrees 2 and 10 respectively. Polynomial functions tend to look like lines, bowls, or roller coasters that turn up and down some number of times.\nA major example is the quadratic function \\(y=x^2\\), which is just an upward-shaped bowl. Its bowl-shaped curve is called a parabola. We can get a downward-shaped bowl by flipping the sign to \\(y=-x^2\\).\n\n\nCode\nx = np.arange(-10, 10, 0.1)\nf = lambda x: x ** 2\nplot_function(x, f, xlim=(-5, 5), ylim=(0, 10), ticks_every=[1, 1], \n              title='Quadratic Function: $y=x^2$')\n\n\n\n\n\n\n\n\n\nThe next one up is the cubic function \\(y=x^3\\). The cubic looks completely different from the bowl-shaped parabola.\n\n\nCode\nx = np.arange(-10, 10, 0.1)\nf = lambda x: x ** 3\nplot_function(x, f, xlim=(-5, 5), ylim=(-5, 5), ticks_every=[1, 1], \n              title='Cubic Function: $y=x^3$')\n\n\n\n\n\n\n\n\n\nPolynomials can take on much more interesting shapes than this. Here’s a more interesting polynomial degree 10,\n\\[y = (x^2 - 1)^5 - 5(x^2 - 1)^4 + 10(x^2 - 1)^3 - 10(x^2 - 1)^2 + 5(x^2 - 1) - 1.\\]\n\n\nCode\nx = np.arange(-10, 10, 0.1)\ndef f(x): \n    y = (x**2 - 1)**5 - 5 * (x**2 - 1)**4 + 10 * (x**2 - 1)**3 - \n    10 * (x**2 - 1)**2 + 5 * (x**2 - 1) - 1\nplot_function(x, f, xlim=(-3, 3), ylim=(-40, 40), ticks_every=[1, 10], \n              title='Arbitrary Polynomial')\n\n\n\n\n\n\n\n\n\n\n\n1.2.3 Rational Functions\nRational functions are functions that are ratios of polynomial functions. Examples might be \\(y=\\frac{1}{x}\\), or\n\\[y=\\frac{x^3+x+1}{x^2-1}.\\]\nThese functions typically look kind of like polynomial functions, but have points where the curve “blows up” to positive or negative infinity. The points where the function blows up are called poles or asymptotes.\nHere’s a plot of the function\n\\[y=\\frac{x^3+x+1}{x^2-1}.\\]\nNotice how weird it looks. There are asymptotes (the vertical lines) where the function blows up at \\(\\pm 1\\), which is where the denominator \\(x^2-1=0\\).\n\n\nCode\nx = np.arange(-10, 10, 0.01)\nf = lambda x: (x ** 3 + x + 1) / (x ** 2 - 1)\nplot_function(x, f, xlim=(-5, 5), ylim=(-5, 5), ticks_every=[1, 1], \n              title='Rational Function')\n\n\n\n\n\n\n\n\n\nHere’s a plot of \\(y=\\frac{1}{x}\\). There’s an asymptote at \\(x=0\\). When \\(x > 0\\) it starts at \\(+\\infty\\) and tapers down to \\(0\\) as \\(x\\) gets large. When \\(x < 0\\) it does the same thing, except flipped across the origin \\(x=y=0\\). This is an example of an odd function, a function that looks like \\(f(x)=-f(x)\\), which is clear in this case since \\(1/(-x)=-1/x\\). Functions like the linear function \\(y=x\\) and the cubic function \\(y=x^3\\) are also odd functions.\n\n\nCode\nx = np.arange(-10, 10, 0.1)\nf = lambda x: 1 / x\nplot_function(x, f, xlim=(-5, 5), ylim=(-5, 5), ticks_every=[1, 1], \n              title='Odd Function: $y=1/x$')\n\n\n\n\n\n\n\n\n\nA related function is \\(y=\\frac{1}{|x|}\\). The difference here is that \\(|x|\\) can never be negative. This means \\(f(x)=f(-x)\\). This is called an even function. Functions like this are symmetric across the y-axis. The quadratic function \\(y=x^2\\) is also an even function.\n\n\nCode\nx = np.arange(-10, 10, 0.1)\nf = lambda x: 1 / np.abs(x)\nplot_function(x, f, xlim=(-5, 5), ylim=(-1, 5), ticks_every=[1, 1], \n              title='Even Function: $y=1/|x|$')\n\n\n\n\n\n\n\n\n\n\n\n1.2.4 Power Functions\nFunctions that look like \\(y=\\frac{1}{x^n}\\) for some \\(n\\) are sometimes called inverse, hyperbolic. These can be represented more easily by using a negative power like \\(y=x^{-n}\\), which means the exact same thing as \\(y=\\frac{1}{x^n}\\).\nWe can extend \\(n\\) to deal with things like square roots or cube roots or any kind of root as well by allowing \\(n\\) to be non-integer. For example, we can represent the square root function \\(y=\\sqrt{x}\\) as \\(y=x^{1/2}\\), and the cube root \\(y=\\sqrt[3]{x}\\) as \\(y=x^{1/3}\\). Roots like these are only defined when \\(x \\geq 0\\).\nThe general class of functions of the form \\(y=x^p\\) for some arbitrary real number \\(p\\) are often called power functions.\nHere’s a plot of what the square root function looks like. Here \\(y\\) grows slower than a linear function, but still grows arbitrarily large with \\(x\\).\n\n\nCode\nx = np.arange(0, 10, 0.1)\nf = lambda x: np.sqrt(x)\nplot_function(x, f, xlim=(0, 5), ylim=(-2, 4), ticks_every=[1, 1], \n              title='Square Root: $y=\\sqrt{x}=x^{1/2}$')\n\n\n\n\n\n\n\n\n\nPower functions obey the following rules:\n\n\n\n\n\n\n\nRule\nExample\n\n\n\\(x^0 = 1\\)\n\\(2^0 = 1\\)\n\n\n\\(x^{m+n} = x^m x^n\\)\n\\(3^{2+5} = 3^2 3^5 = 3^8 = 6561\\)\n\n\n\\(x^{m-n} = \\frac{x^m}{x^n}\\)\n\\(3^{2-5} = \\frac{3^2}{3^5} = 3^{-3} \\approx 0.037\\)\n\n\n\\(x^{mn} = (x^m)^n\\)\n\\(2^{2 \\cdot 5} = (2^2)^5 = 2^{10} = 1024\\)\n\n\n\\((xy)^n = x^n y^n\\)\n\\((2 \\cdot 2)^3 = 2^3 2^3 = 4^3 = 2^6 = 64\\)\n\n\n\\(\\big(\\frac{x}{y}\\big)^n = \\frac{x^n}{y^n}\\)\n\\(\\big(\\frac{2}{4}\\big)^3 = \\frac{2^3}{4^3} = \\frac{1}{8}\\)\n\n\n\\(\\big(\\frac{x}{y}\\big)^{-n} = \\frac{y^n}{x^n}\\)\n\\(\\big(\\frac{2}{4}\\big)^{-3} = \\frac{4^3}{2^3} = 2^3 = 8\\)\n\n\n\\(x^{1/2} = \\sqrt{x} = \\sqrt[2]{x}\\)\n\\(4^{1/2} = \\sqrt{4} = 2\\)\n\n\n\\(x^{1/n} = \\sqrt[n]{x}\\)\n\\(3^{1/4} = \\sqrt[4]{3} \\approx 1.316\\)\n\n\n\\(x^{m/n} = \\sqrt[n]{x^m}\\)\n\\(3^{3/4} = \\sqrt[4]{3^3} = \\sqrt[4]{9} \\approx 1.732\\)\n\n\n\\(\\sqrt[n]{xy} = \\sqrt[n]{x} \\sqrt[n]{y}\\)\n\\(\\sqrt[4]{3 \\cdot 2} = \\sqrt[4]{3} \\sqrt[4]{2} \\approx 1.565\\)\n\n\n\\(\\sqrt[n]{\\frac{x}{y}} = \\frac{\\sqrt[n]{x}}{\\sqrt[n]{y}}\\)\n\\(\\sqrt[4]{\\frac{3}{2}} = \\frac{\\sqrt[4]{3}}{\\sqrt[4]{2}} \\approx 1.107\\)\n\n\n\nIt’s important to remember that power functions do not distribute over addition, i.e.\n\\[(x+y)^n \\neq x^n + y^n,\\]\nand by extension nor do roots,\n\\[\\sqrt[n]{x+y} \\neq \\sqrt[n]{x} + \\sqrt[n]{y}.\\]\n\n\n1.2.5 Exponentials and Logarithms\nTwo very important functions are the exponential function \\(y=\\exp(x)\\) and the logarithm function \\(y=\\log(x)\\). They show up surprisingly often in machine learning and the sciences, certainly more than most other special functions do.\nThe exponential function can be written as a power by defining a number \\(e\\) called Euler’s number, given by \\(e = 2.71828\\dots\\) . Like \\(\\pi\\), \\(e\\) is an example of an irrational number, i.e. a number that can’t be represented as a ratio of integers. Using \\(e\\), we can write the exponential function in the more usual form \\(y=e^x\\), where it’s roughly speaking understood that we mean “multiply \\(e\\) by itself \\(x\\) times”. For example, \\(\\exp(2) = e^2 = e \\cdot e\\).\nThe logarithm is defined as the inverse of the exponential function. It’s the unique function satisfying \\(\\log(\\exp(x)) = x\\). The opposite is also true since the exponential must then be the inverse of the logarithm function, \\(\\exp(\\log(x)) = x\\). This gives a way of mapping between the two functions,\n\\[\\log(a) = b \\quad \\Longleftrightarrow \\quad \\exp(b) = a.\\]\nHere are some plots of what the exponential and logarithm functions look like. The exponential function is a function that blows up very, very quickly. The log function grows very, very slowly (much more slowly than the square root does).\nNote the log function is only defined for positive-valued numbers \\(x \\geq 0\\), with \\(\\log(+0)=-\\infty\\). This is dual to the exponential function only taking on \\(y \\geq 0\\).\n\n\nCode\nx = np.arange(-5, 5, 0.1)\nf = lambda x: np.exp(x)\nplot_function(x, f, xlim=(-5, 5), ylim=(-1, 10), ticks_every=[1, 2], \n              title='Exponential Function: $y=\\exp(x)$')\n\n\n\n\n\n\n\n\n\n\n\nCode\nx = np.arange(0.01, 5, 0.1)\nf = lambda x: np.log(x)\nplot_function(x, f, xlim=(-1, 5), ylim=(-5, 2), ticks_every=[1, 1], \n              title='Logarithm Function: $y=\\log(x)$')\n\n\n\n\n\n\n\n\n\nThe exponential and logarithm functions I defined are the “natural” way to define these functions. We can also have exponential functions in other bases, \\(y=a^x\\) for any positive number \\(a\\). Each \\(a\\) has an equivalent logarithm, written \\(y = \\log_{a}(x)\\). The two functions \\(y=a^x\\) and \\(y=\\log_{a}(x)\\) are inverses of each other. When I leave off the \\(a\\), it’s assumed that all logs are the natural base \\(a=e\\), sometimes also written \\(\\ln(x)\\).\nTwo common examples of other bases that show up sometimes are the base-2 functions \\(2^x\\) and \\(\\log_{2}(x)\\), and the base-10 functions \\(10^x\\) and \\(\\log_{10}(x)\\). Base-2 functions in particular show up often in computer science because of the tendency to think in bits. Base-10 functions show up when we want to think about how many digits a number has.\nHere are some rules that exponentials and logs obey:\n\n\n\n\n\n\n\nRule\nExample\n\n\n\\(e^0 = 1\\)\n\n\n\n\\(\\log(1) = 0\\)\n\n\n\n\\(\\log(e) = 1\\)\n\n\n\n\\(e^{a+b} = e^a e^b\\)\n\\(e^{2+5} = e^2 e^5 = e^8 \\approx 2980.96\\)\n\n\n\\(e^{a-b} = \\frac{e^a}{e^b}\\)\n\\(e^{2-5} = \\frac{e^2}{e^5} = e^{-3} \\approx 0.0498\\)\n\n\n\\(e^{ab} = (e^a)^b\\)\n\\(e^{2 \\cdot 5} = (e^2)^5 = e^{10} \\approx 22026.47\\)\n\n\n\\(a^b = e^{b \\log(a)}\\)\n\\(2^3 = e^{3 \\log(2)} = 8\\)\n\n\n\\(\\log(ab) = \\log(a) + \\log(b)\\)\n\\(\\log(2 \\cdot 5) = \\log(2) + \\log(5) = \\log(10) \\approx 2.303\\)\n\n\n\\(\\log\\big(\\frac{a}{b}\\big) = \\log(a) - \\log(b)\\)\n\\(\\log\\big(\\frac{2}{5}\\big) = \\log(2) - \\log(5) \\approx -0.916\\)\n\n\n\\(\\log(a^b) = b\\log(a)\\)\n\\(\\log(5^2) = 2\\log(5) \\approx 3.219\\)\n\n\n\\(\\log_a(x) = \\frac{\\log(x)}{\\log(a)}\\)\n\\(\\log_2(5) = \\frac{\\log(5)}{\\log(2)} \\approx 2.322\\)\n\n\n\nHere’s an example of an equation involving exponentials and logs. Suppose you have \\(n\\) bits of numbers (perhaps it’s the precision in some float) and you want to know how many digits this number takes up in decimal form (what you’re used to). This would be equivalent to solving the following equation for \\(x\\),\n\\[\\begin{align*}\n2^n &= 10^{x} \\\\\n\\log(2^n) &= \\log(10^{x}) \\\\\nn\\log(2) &= x\\log(10) \\\\\nx &= \\frac{\\log(2)}{\\log(10)} \\cdot n \\\\\nx &\\approx 0.3 \\cdot n. \\\\\n\\end{align*}\\]\nFor example, you can use this formula to show that 52 bits of floating point precision translates to about 15 to 16 digits of precision. In numpy, the function np.log function calculates the (base-\\(e\\)) log of a number.\n\n\nCode\nn = 52\nx = np.log(2) / np.log(10) * n\nprint(f'x = {x}')\n\n\nx = 15.65355977452702\n\n\n\n\n1.2.6 Trigonometric Functions\nOther textbook functions typically covered in math courses are the trig functions: sine, cosine, tangent, cosine, cosecant, and cotangent. Of these functions, the most important to know are the sine function \\(y=\\sin x\\), the cosine function \\(y = \\cos x\\), and sometimes the tangent function \\(y = \\tan x\\).\nHere’s what their plots look like. They’re both waves that repeat themselves, in the sense \\(f(x + 2\\pi) = f(x)\\). The length for the function to repeat itself is called the period, in this case \\(2\\pi \\approx 6.28\\). Note that the cosine is just a sine function that’s shifted right by \\(\\frac{\\pi}{2} \\approx 1.57\\).\n\n\nCode\nx = np.arange(-10, 10, 0.1)\nf = lambda x: np.sin(x)\nplot_function(x, f, xlim=(-6, 6), ylim=(-2, 2),  ticks_every=[1, 0.5], \n              title='Sine Function: $y=\\sin(x)$')\n\n\n\n\n\n\n\n\n\n\n\nCode\nx = np.arange(-10, 10, 0.1)\nf = lambda x: np.cos(x)\nplot_function(x, f, xlim=(-6, 6), ylim=(-2, 2), ticks_every=[1, 0.5], \n              title='Cosine Function: $y=\\cos(x)$')\n\n\n\n\n\n\n\n\n\nTrig functions don’t really show up that much in machine learning, so I won’t remind you of all those obscure trig rules you’ve forgotten. I’ll just mention that we can define all the other trig functions using the sine and cosine as follows,\n\\[\\begin{align*}\n&\\tan x = \\frac{\\sin x}{\\cos x}, \\\\\n&\\csc x = \\frac{1}{\\sin x}, \\\\\n&\\sec x = \\frac{1}{\\cos x}, \\\\\n&\\cot x = \\frac{1}{\\tan x} = \\frac{\\cos x}{\\sin x}.\n\\end{align*}\\]\nWe can talk about the inverse of trig functions as well. These are just the functions that undo the trig operations and give you back the angle (in radians). Since none of the trig functions are monotonic, we can’t invert them on the whole real line, but only on a given range.\nBelow I’ll just list the inverse sine, cosine, and tangent functions and their defined input and output ranges. Note by historical convention, these inverse functions are usually called the arcsine, arccosine, and arctangent respectfully.\n\n\n\n\n\n\n\n\nInverse Function\nInput Range\nOutput Range\n\n\n\\(y = \\arcsin x = \\sin^{-1} x\\)\n\\(-1 \\leq x \\leq 1\\)\n\\(-90^\\circ \\leq y \\leq 90^\\circ\\)\n\n\n\\(y = \\arccos x = \\cos^{-1} x\\)\n\\(-1 \\leq x \\leq 1\\)\n\\(0^\\circ \\leq y \\leq 180^\\circ\\)\n\n\n\\(y = \\arctan x = \\tan^{-1} x\\)\n\\(-\\infty < x < \\infty\\)\n\\(-90^\\circ \\leq y \\leq 90^\\circ\\)\n\n\n\n\n\n1.2.7 Piecewise Functions\nThe functions covered so far are examples of continuous functions. Their graphs don’t have jumps or holes in them anywhere. Continuous functions we can often write using a single equation, like \\(y=x^2\\) or \\(y=1 + \\sin(x)\\). We can also have functions that require more than one equation to write. These are called piecewise functions. Piecewise functions usually aren’t continuous, but sometimes can be.\nAn example of a discontinuous piecewise function is the unit step function \\(y=u(x)\\) given by\n\\[\ny =\n\\begin{cases}\n0 & x < 0, \\\\\n1 & x \\geq 0.\n\\end{cases}\n\\]\nThis expression means \\(y=0\\) whenever \\(x < 0\\), but \\(y=1\\) whenever \\(x \\geq 0\\). It breaks up into two pieces, one horizontal line \\(y=0\\) when \\(x\\) is negative, and another horizontal line \\(y=1\\) when \\(x\\) is positive.\nUsing Boolean expressions, we can also write this function in a more economical way by agreeing to identify \\(x=1\\) with \\(\\text{TRUE}\\) and \\(x=0\\) with \\(\\text{FALSE}\\), which python does by default. In this notation, we can write\n\\[u(x) = [x \\geq 0],\\]\nwhich means exactly the same thing as the piecewise definition, since \\(x \\geq 0\\) is only true when (you guessed it), \\(x \\geq 0\\).\nHere’s a plot of this function. Note the discontinuous jump at \\(x=0\\).\n\n\nCode\nx = np.arange(-10, 10, 0.01)\nf = lambda x:  (x >= 0)\nplot_function(x, f, xlim=(-3, 3), ylim=(-1, 2), ticks_every=[1, 0.5], \n              title='Unit Step Function: $y=u(x)$')\n\n\n\n\n\n\n\n\n\nAn example of a piecewise function that’s continuous is the ramp function, defined by\n\\[\ny =\n\\begin{cases}\n0 & x < 0, \\\\\nx & x \\geq 0.\n\\end{cases}\n\\]\nThis function gives a horizontal line \\(y=0\\) when \\(x\\) is negative, and a \\(45^\\circ\\) line \\(y=x\\) when \\(x\\) is positive. Both lines connect at \\(x=0\\), but leave a kink in the graph.\nAnother way to write the same thing using Boolean expressions is \\(y = x \\cdot [x \\geq 0]\\), which is of course just \\(y = x \\cdot u(x)\\).\nIn machine learning it’s more common to write the ramp function using the \\(\\max\\) function as \\(y = \\max(0,x)\\). This means, for each \\(x\\), take that value and compare it with \\(0\\), and take the maximum of those two. That is, if \\(x\\) is negative take \\(y=0\\), otherwise take \\(y=x\\). It’s also more common to call this function a rectified linear unit, or ReLU for short. It’s an ugly, unintuitive name, but unfortunately it’s stuck in the field.\nHere’s a plot of the ramp or ReLU function. Notice how it stays at \\(y=0\\) for a while, then suddenly “ramps upward” at \\(x=0\\).\n\n\nCode\nx = np.arange(-10, 10, 0.1)\nf = lambda x:  x * (x >= 0)\nplot_function(x, f, xlim=(-3, 3), ylim=(-3, 3), ticks_every=[1, 1], \n              title='ReLU Function')\n\n\n\n\n\n\n\n\n\nLast, I’ll mention here the absolute value function \\(y = |x|\\), defined by the piecewise function\n\\[\ny = \\begin{cases}\nx & \\text{if } x \\ge 0 \\\\\n-x & \\text{if } x < 0.\n\\end{cases}\n\\]\nThe absolute value just ignores negative signs and makes everything positive. The function looks like the usual line \\(y=x\\) when positive, but like the negative-sloped line \\(y=-x\\) when negative. At \\(x=0\\) the two lines meet, creating a distinctive v-shape. To get the absolute value function in python, use abs or np.abs.\n\n\nCode\nx = np.arange(-5, 5, 0.1)\nf = lambda x: abs(x)\nplot_function(x, f, xlim=(-5, 5), ylim=(0, 5), ticks_every=[1, 1], \n              title='Absolute Value Function: $y=|x|$')\n\n\n\n\n\n\n\n\n\n\n\n1.2.8 Composite Functions\nWe can also have any arbitrary hybrid of the above functions. We can apply exponentials to affine functions, logs to sine functions, sines to exponential functions. In essence, this kind of layered composition of functions is what a neural network is as we’ll see later on.\nMath folks often write an abstract compositional function as a function applied to another function, like \\(y=f(g(x))\\) or \\(y=(f \\circ g)(x)\\). These can be chained arbitrarily many times, not just two. Neural networks do just that, often hundreds or thousands of times.\nConsider, for example, the function composition done by applying the following functions in sequence:\n\nan affine function \\(f(x) = wx+b\\)\nfollowed by a linear function \\(g(x) = -x\\)\nfollowed by an exponential function \\(h(x)=e^x\\)\nfollowed by a rational function \\(r(x)=\\frac{1}{x}\\)\n\nto get the full function \\[y = r(h(g(x))) = \\frac{1}{1 + e^{-(wx+b)}}.\\]\nHere’s a plot of what this function looks like for the “standard form” where \\(w=1, b=0\\). Notice that \\(0 \\leq y \\leq 1\\). The values of \\(x\\) get “squashed” to values between 0 and 1 after the function is applied.\n\n\nCode\nx = np.arange(-10, 10, 0.1)\nf = lambda x:  1 / (1 + np.exp(-x))\nplot_function(x, f, xlim=(-6, 6), ylim=(-0.2, 1.2), ticks_every=[2, 0.2], \n              title='Sigmoid Function')\n\n\n\n\n\n\n\n\n\nThis function is called the sigmoid function. The sigmoid is very important in machine learning since it in essence creates probabilities. We’ll see it a lot more. The standard form sigmoid function, usually written \\(\\sigma(x)\\), is given by\n\\[\\sigma(x) = \\frac{1}{1 + e^{-x}}.\\]\nArbitrary affine transformations of the standard form would then be written as \\(\\sigma(wx+b)\\).\nA similar looking function shows up sometimes as well called the hyperbolic tangent or tanh function, which has the (standard) form\n\\[\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}.\\]\nThe tanh function looks pretty much the same as the sigmoid except it’s rescaled vertically so that \\(-1 \\leq y \\leq 1\\).\nHere’s a plot of the tanh function. Notice how similar it looks to the sigmoid with the exception of the scale of the y-axis.\n\n\nCode\nx = np.arange(-10, 10, 0.1)\nf = lambda x: (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\nplot_function(x, f, xlim=(-5, 5), ylim=(-2, 2), ticks_every=[1, 0.5], \n              title='Tanh Function')\n\n\n\n\n\n\n\n\n\n\n\n1.2.9 Function Transformations\nSuppose we have some arbitrary function \\(f(x)\\) and we apply a series of compositions to get a new function \\[g(x)=a \\cdot f(b \\cdot (x + c)) + d.\\] We can regard each parameter \\(a,b,c,d\\) as doing some kind of geometric transformation to the graph of the original function \\(f(x)\\). Namely,\n\n\\(a\\) re-scales the function vertically (if \\(a\\) is negative it also flips \\(f(x)\\) upside down)\n\\(b\\) re-scales the function horizontally (if \\(b\\) is negative it also flips \\(f(x)\\) left to right)\n\\(c\\) shifts the function horizontally (left if \\(c\\) is positive, right if \\(c\\) is negative)\n\\(d\\) shifts the function vertically (up if \\(d\\) is positive, down if \\(d\\) is negative)\n\nHere’s an example of how these work. Consider the function \\(f(x)=x^2\\). We’re going to apply each of these transformations one by one to show what they do to the graph of \\(f(x)\\).\nFirst, let’s look at the transformation \\(g(x) = \\frac{1}{2} f(x) = \\frac{1}{2} x^2\\). Here \\(a=\\frac{1}{2}\\) and the rest are zero. I’ll plot it along side the original graph (the blue curve). Notice the graph gets flattened vertically by a factor of two (the orange curve).\n\n\nCode\nx = np.arange(-10, 10, 0.1)\nf = lambda x: x ** 2\n\n\n\n\nCode\na = 1 / 2\ng = lambda x: a * x ** 2\nplot_function(x, [f, g], xlim=(-3, 3), ylim=(-2, 10), ticks_every=[1, 2], \n              title=f'$a={a}$')\n\n\n\n\n\n\n\n\n\nNow consider at the transformation\n\\[g(x) = f\\big(\\frac{1}{2} x\\big) = \\bigg(\\frac{1}{2} x \\bigg)^2.\\]\nHere \\(b=\\frac{1}{2}\\) and the rest are zero. Notice the graph again gets flattened but in a slightly different way.\n\n\nCode\nb = 1 / 2\ng = lambda x: (b * x) ** 2\nplot_function(x, [f, g], xlim=(-3, 3), ylim=(-2, 10), ticks_every=[1, 2], \n              title=f'$b={b}$')\n\n\n\n\n\n\n\n\n\nNext, consider the transformation \\(g(x) = f(x-1) = (x-1)^2.\\) Here \\(c=1\\) and the rest are zero. Notice the graph’s shape doesn’t change. It just gets shifted right by \\(c=1\\) since \\(c\\) is negative.\n\n\nCode\nc = -1\ng = lambda x: (x + c) ** 2\nplot_function(x, [f, g], xlim=(-3, 3), ylim=(-7, 7), ticks_every=[1, 2], \n              title=f'$c={c}$')\n\n\n\n\n\n\n\n\n\nFinally, let’s look at the transformation \\(g(x) = f(x) + 2 = x^2 + 2\\). Here \\(d=2\\) and the rest are zero. Notice again the graph’s shape doesn’t change. It just gets shifted up by \\(d=2\\) units.\n\n\nCode\nd = 2\ng = lambda x: x ** 2 + d\nplot_function(x, [f, g], xlim=(-3, 3), ylim=(-1, 8), ticks_every=[1, 2], \n              title=f'$d={d}$')\n\n\n\n\n\n\n\n\n\nLet’s now put them all together to see what happens. We should see rescaling in both directions and shifts in both directions. It’s hard to see in the plot, but it’s all there if you zoom in. The vertex of the parabola is at the point \\(x=c=1, y=d=2\\). And the stretching factors due to \\(a=b=1/2\\) are both acting to flatten the parabola.\n\n\nCode\ng = lambda x: a * (b * (x + c)) ** 2 + d\nplot_function(x, [f, g], xlim=(-8, 8), ylim=(-2, 10), ticks_every=[2, 2], \n              title=f'$a={a}, b={b}, c={c}, d={d}$')"
  },
  {
    "objectID": "notebooks/basic-math.html#multivariate-functions",
    "href": "notebooks/basic-math.html#multivariate-functions",
    "title": "1  Basic Math",
    "section": "1.3 Multivariate Functions",
    "text": "1.3 Multivariate Functions\nWhat we’ve covered thus far only deals with univariate functions, functions where \\(y=f(x)\\), but \\(x\\) and \\(y\\) are just single numbers, i.e. scalars. In machine learning we’re almost always dealing with multivariate functions with lots of variables, sometimes billions of them. It turns out that most of what I’ve covered so far extends straight forwardly to multivariate functions with some small caveats, which I’ll cover below.\nSimply put, a multivariate function is a function of multiple variables. Instead of a single variable \\(x\\), we might have several variables, e.g. \\(x_0, x_1, x_2, x_3, x_4, x_5\\),\n\\[y = f(x_0, x_1, x_2, x_3, x_4, x_5).\\]\nIf you think about mathematical functions analogously to python functions it shouldn’t be surprising functions can have multiple arguments. They usually do, in fact.\nHere’s an example of a function that takes two arguments \\(x\\) and \\(y\\) and produces a single output \\(z\\), more often written as a bivariate function \\(z=f(x,y)\\). The example I’ll look at is \\(z = x^2 + y^2\\). I’ll evaluate the function at three points:\n\n\\(x=0\\), \\(y=0\\),\n\\(x=1\\), \\(y=-1\\),\n\\(x=0\\), \\(y=1\\).\n\nThe main thing to notice is the function does exactly what you think it does. If you plug in 2 values, you get out 1 value.\n\n\nCode\nf = lambda x, y: x ** 2 + y ** 2\nprint(f'z = f{(0, 0)} = {f(0, 0)}')\nprint(f'z = f{(1, -1)} = {f(1, -1)}')\nprint(f'z = f{(0, 1)} = {f(0, 1)}')\n\n\nz = f(0, 0) = 0\nz = f(1, -1) = 2\nz = f(0, 1) = 1\n\n\nWe can also have functions that map multiple inputs to multiple outputs. Suppose we have a function that takes in 2 values \\(x_0, x_1\\) and outputs 2 values \\(y_0, y_1\\). We’d write this as \\((y_0, y_1) = f(x_0, x_1)\\).\nConsider the following example,\n\\[(y_0, y_1) = f(x_0, x_1) = (x_0+x_1, x_0-x_1).\\]\nThis is really just two functions, both functions of \\(x_0\\) and \\(x_1\\). We can completely equivalently write this function as\n\\[y_0 = f_1(x_0, x_1) = x_0+x_1,\\] \\[y_1 = f_2(x_0, x_1) = x_0-x_1.\\]\nHere’s this function defined and evaluated at the point \\(x_0=1\\), \\(x_1=1\\).\n\n\nCode\nf = lambda x0, x1: (x0 + x1, x0 - x1)\nprint(f'(y0, y1) = {f(1, 1)}')\n\n\n(y0, y1) = (2, 0)\n\n\nFor now I’ll just focus on the case of multiple inputs, single output like the first example. These are usually called scalar-valued functions. We can also have vector-valued functions, which are functions whose outputs can have multiple values as well. I’ll focus on scalar-valued functions here.\nA scalar-valued function of \\(n\\) variables \\(x_0, x_1, \\cdots, x_{n-1}\\) has the form\n\\[y = f(x_0, x_1, \\cdots, x_{n-1}).\\]\nNote \\(n\\) can be as large as we want it to be. When working with deep neural networks (which are just multivariate functions of a certain form) \\(n\\) can be huge. For example, if the input is a \\(256 \\times 256\\) image, the input might be \\(256^2=65536\\) pixels. For a 10 second audio clip that’s sampled at 44 kHz, the input might be \\(10*44k=440k\\) amplitudes. Large numbers indeed.\nCalculating the output of multivariate functions is just as straight-forward as for univariate functions pretty much. Unfortunately, visualizing them is much harder. The human eye can’t see 65536 dimensions, only 3 dimensions. This in some sense means we need to give up on the ability to “graph” a function and instead find other ways to visualize it.\nOne thing that sometimes help to visualize high dimension functions is to pretend they’re functions of two variables, like \\(z=f(x,y)\\). In this special case we can visualize the inputs as an xy-plane, and the output as a third axis sticking out perpendicular to the xy-plane from the origin. Each \\(x,y\\) pair will map to one unique \\(z\\) value. Done this way, we won’t get a graph of a curve as before, but a surface.\nHere’s an example of what this might look like for the simple function \\(z=x^2+y^2\\). I’ll plot the function on the domain \\(-10 \\leq x \\leq 10\\) and \\(-10 \\leq y \\leq 10\\) using the helper function plot_3d. It takes in two lists of values x and y. I’ll use np.linspace to sample 100 points from -10 to 10 for each. Then I’ll define a lambda function that maps x and y to the output z. Passing these three arguments into the helper function gives us our 3D plot.\n\n\nCode\nx = np.linspace(-10, 10, 100)\ny = np.linspace(-10, 10, 100)\nf = lambda x, y: x**2 + y**2\n\n\n\n\nCode\nplot_function_3d(x, y, f, title='3D Plot: $z=x^2+y^2$', \n                 ticks_every=[5, 5, 50], labelpad=5, dist=12)\n\n\n\n\n\n\n\n\n\nNotice how the plot looks like an upward facing bowl. Imagine a bowl lying on a table. The table is the xy-plane. The bowl is the surface \\(z=x^2+y^2\\) we’re plotting. While the plot shows the general idea what’s going on, 3D plots can often be difficult to look at. They’re often slanted at funny angles and hide important details.\nHere’s another way we can visualize the same function: Rather than create a third axis for \\(z\\), we can plot it directly on the xy-plane as a 2D plot. Since we’re dealing with a surface, not a curve, we have to do this for lots of different \\(z\\) values, which will give a family of curves. For example, we might plot all of the following curves corresponding to different values of \\(z\\) in the xy-plane,\n\\[\\begin{align}\n25 &= x^2 + y^2, \\\\\n50 &= x^2 + y^2, \\\\\n75 &= x^2 + y^2, \\\\\n100 &= x^2 + y^2, \\\\\n125 &= x^2 + y^2, \\\\\n150 &= x^2 + y^2.\n\\end{align}\\]\nDoing this will give a family of curves on one 2D plot, with each curve representing some value of \\(z\\). In our example, these curves are all circles of radius \\(z^2\\). Each curve is called a level curve or level set.\nThese kinds of plots are called contour plots. A contour map can be thought of as looking at the surface from the top down, where each level set corresponds to slicing the function \\(z=f(x,y)\\) horizontally for different values of \\(z\\). This trick is often used in topographical maps to visualize 3D terrain on a 2D sheet of paper. Here is a contour plot for \\(z=x^2+y^2\\) using the above level curves.\n\n\nCode\nplot_countour(x, y, f, title='Countour Plot: $z=x^2+y^2$')\n\n\n\n\n\n\n\n\n\nNotice how we get a bunch of concentric rings in the contour plot, each labeled by some value (their \\(z\\) values). These rings correspond to the circles I was talking about. You can visually imagine this plot as looking down from the top of the bowl. In the middle you see the bottom. The rings get closer together the further out you go, which indicates that the bowl is sloping steeper the further out we get.\nWe’ll see more examples of multivariate functions in the coming lessons."
  },
  {
    "objectID": "notebooks/basic-math.html#systems-of-equations",
    "href": "notebooks/basic-math.html#systems-of-equations",
    "title": "1  Basic Math",
    "section": "1.4 Systems of Equations",
    "text": "1.4 Systems of Equations\nIn machine learning we’ll find ourselves frequently interested not just with single equations, but multiple equations each with many variables. One thing we might seek to do is solve these coupled systems, which means finding a solution that satisfies every equation simultaneously. Consider the following example,\n\\[\\begin{alignat*}{3}\n   x & {}+{} &  y & {}={} & 2  \\\\\n   2x & {}-{} &  3y & {}={} & 7.\n\\end{alignat*}\\]\nThis system consists of two equations, \\(x + y = 2\\), and \\(2x - 3y = 7\\). Each equation contains two unknown variables, \\(x\\) and \\(y\\). We need to find a solution for both \\(x\\) and \\(y\\) that satisfies both of these equations.\nUsually the easiest and most general way to solve simple coupled systems like this is the method of substitution. The idea is to solve one equation for one variable in terms of the other, then plug that solution into the second equation to solve for the other variable. Once the second variable is solved for, we can go back and solve for the first variable explicitly. Let’s start by solving the first equation for \\(x\\) in terms of \\(y\\). This is pretty easy,\n\\[x = 2 - y.\\]\nNow we can take this solution for \\(x\\) and plug it into the second equation to solve for \\(y\\),\n\\[\\begin{align*}\n2x - 3y &= 7 \\\\\n2(2 - y) - 3y &= 7 \\\\\n4 - 5y &= 7 \\\\\n5y &= -3 \\\\\ny &= -\\frac{3}{5}.\n\\end{align*}\\]\nWith \\(y\\) in hand, we can now solve for \\(x\\), \\(x = 2 - y = 2 + \\frac{3}{5} = \\frac{13}{5}\\). Thus, the pair \\(x=\\frac{13}{5}\\), \\(y=-\\frac{3}{5}\\) is the solution that solves both of these coupled equations simultaneously.\nHere’s sympy’s solution to the same system. It should of course agree with what I just got, which it does.\n\n\nCode\nx, y = sp.symbols('x y')\neq1 = sp.Eq(x + y, 2)\neq2 = sp.Eq(2 * x - 3 * y, 7)\nsol = sp.solve((eq1, eq2), (x, y))\nprint(f'x = {sol[x]}')\nprint(f'y = {sol[y]}')\n\n\nx = 13/5\ny = -3/5\n\n\nNotice that both of the equations in this example are linear, since each term only contains terms proportional to \\(x\\) and \\(y\\). There are no terms like \\(x^2\\) or \\(\\sin y\\) or whatever. Linear systems of equations are special because they can always be solved as long as there are enough variables. I’ll spend a lot more time on these when I get to linear algebra.\nWe can also imagine one or more equations being nonlinear. Provided we can solve each equation one-by-one, we can apply the method of substitution to solve these too. Here’s an example. Consider the nonlinear system\n\\[\\begin{align*}\ne^{x + y} &= 10  \\\\\nxy &= 1.\n\\end{align*}\\]\nLet’s solve the second equation first since it’s easier. Solving for \\(y\\) gives \\(y = \\frac{1}{x}\\). Now plug this into the first equation and solve for \\(x\\),\n\\[\\begin{align*}\ne^{x + y} &= 10  \\\\\ne^{x + 1/x} &= 10  \\\\\n\\log \\big(e^{x + 1/x}\\big) &= \\log 10 \\\\\nx + \\frac{1}{x} &= \\log 10 \\\\\nx^2 - \\log 10 \\cdot x + 1 &= 0 \\\\\nx &= \\frac{1}{2} \\bigg(\\log 10 \\pm \\sqrt{(\\log 10)^2 - 4}\\bigg) \\\\\nx &\\approx 0.581, \\ 1.722.\n\\end{align*}\\]\nNote here I had to use the quadratic formula, which I’ll assume you’ve forgotten. If you have a quadratic equation of the form \\(ax^2 + bx + c = 0\\), then it will (usually) have exactly two solutions given by the formula\n\\[x = \\frac{1}{2a} \\bigg(-b \\pm \\sqrt{b^2 - 4ac}\\bigg).\\]\nThis means we have two different possible solutions for \\(x\\), which thus means we’ll also have two possible solutions to \\(y\\) since \\(y=\\frac{1}{x}\\). Thus, this system has two possible solutions,\n\\[\\text{Solution 1: }x \\approx 0.581, \\ y \\approx 1.722,\\] \\[\\text{Solution 2: }x \\approx 1.722, \\ y \\approx 0.581.\\]\nIt’s interesting how symmetric these two solutions are. They’re basically the same with \\(x\\) and \\(y\\) swapped. This is because the system has symmetry. You can swap \\(x\\) and \\(y\\) in the system above and not change the equation, which means the solutions must be the same up to permutation of \\(x\\) and \\(y\\)!\nHere’s sympy’s attempt to solve this system.\n\n\nCode\nx, y = sp.symbols('x y')\neq1 = sp.Eq(sp.exp(x + y), 10)\neq2 = sp.Eq(x * y, 1)\nsol = sp.solve((eq1, eq2), (x, y))\nprint(f'x1 = {sol[0][0].round(5)} \\t y1 = {sol[0][1].round(5)}')\nprint(f'x2 = {sol[1][0].round(5)} \\t y2 = {sol[1][1].round(5)}')\n\n\nx1 = 0.58079     y1 = 1.72180\nx2 = 1.72180     y2 = 0.58079\n\n\nIn general, it’s not even possible to solve a system of nonlinear equations except using numerical methods. The example I gave was rigged so I could solve it by hand. General purpose root-finding algorithms exist that can solve arbitrary systems of equations like this numerically, no matter how nonlinear they are.\nTo solve a nonlinear system like this numerically, you can use the scipy function scipy.optimize.fsolve. Scipy is an extension of numpy that includes a lot of algorithms for working with non-linear functions. To use fsolve, you have to define the system as a function mapping a list of variables to a list of equations. You also have to specify a starting point x0 for the root finder. This tells it where to start looking for the root. Since nonlinear equations have multiple solutions, picking a different x0 can and will often give you a different root. I won’t dwell on all this since we don’t really need to deal with root finding much in machine learning.\n\n\nCode\nfrom scipy.optimize import fsolve\n\nsystem = lambda xy: [np.exp(xy[0] + xy[1]) - 10, xy[0] * xy[1] - 1]\nsolution = fsolve(system, x0=(1, 1))\nprint(f'solution = {solution}')\n\n\nsolution = [0.5807888 1.7217963]"
  },
  {
    "objectID": "notebooks/basic-math.html#sums-and-products",
    "href": "notebooks/basic-math.html#sums-and-products",
    "title": "1  Basic Math",
    "section": "1.5 Sums and Products",
    "text": "1.5 Sums and Products\n\n1.5.1 Sums\nWe typically find ourselves performing operations on large numbers of numbers at a time. By far the most common operation is adding up a bunch of numbers, or summation. Suppose we have some sequence of \\(n\\) numbers \\(x_0,x_1,x_2,\\cdots,x_{n-1}\\). They could be anything, related by a function, or not. If we wanted to sum them together to get a new number \\(x\\) we could write\n\\[x = x_0 + x_1 + x_2 + \\cdots + x_{n-1}.\\]\nBut it’s kind of cumbersome to always write like this. For this reason in math there’s a more compact notation to write sums called summation notation. We introduce the symbol \\(\\sum\\) for “sum”, and write \\[x = \\sum_{i=0}^{n-1} x_i.\\]\nRead this as “the sum of all \\(x_i\\) for \\(i=0,1,\\cdots,n-1\\) is \\(x\\)”. The index \\(i\\) being summed over is called a dummy index. It can be whatever we want since it never appears on the left-hand side. It gets summed over and then disappears. The lower and upper values \\(i=0\\) and \\(i=n-1\\) are the limits of the summation. The limits need not always be \\(i=0\\) and \\(i=n-1\\). We can choose them to be whatever we like as a matter of convenience.\nFrequently summation notation is paired with some kind of generating function \\(f(i) = x_i\\) that generates the sequence. For example, suppose our sequence is generated by the function \\(f(i) = i\\), and we want to sum from \\(i=1\\) to \\(i=n\\). We’d have\n\\[x = \\sum_{i=1}^n x_i = \\sum_{i=1}^n i = 1 + 2 + \\cdots + n = \\frac{1}{2} n(n+1).\\]\nThe right-hand term \\(\\frac{1}{2} n(n-1)\\) is not obvious, and only applies to this particular sum. I just wrote it down since it’s sometimes useful to remember. This is a special kind of sum called an arithmetic series. Here’s a “proof” of this relationship using sympy.\n\n\nCode\ni, n = sp.symbols('i n')\nsummation = sp.Sum(i, (i, 1, n)).doit()\nprint(f'sum i for i=1,...,n = {summation}')\n\n\nsum i for i=1,...,n = n**2/2 + n/2\n\n\nIn the general case when we don’t have nice rules like this we’d have to loop over the entire sum and do the sum incrementally.\nIn python, the equivalent of summation notation is the sum function, where we pass in the sequence we want to sum as a list. Here’s the arithmetic sum up to \\(n=10\\), which should be \\(\\frac{1}{2} 10 \\cdot (10+1) = 55\\).\n\n\nCode\nsum([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n\n\n55\n\n\nAnother useful sum to be aware of is the geometric series. A geometric series is a sum over a sequence whose generating function is \\(f(i) = r^i\\) for some real number \\(r \\neq 1\\). Its rule is\n\\[x = \\sum_{i=0}^{n-1} r^i = r^0 + r^1 + \\cdots + r^{n-1} = \\frac{1-r^n}{1-r}.\\]\nFor example, if \\(n=10\\) and \\(r=\\frac{1}{2}\\), we have\n\\[x = \\sum_{i=0}^{9} \\bigg(\\frac{1}{2}\\bigg)^i = \\frac{1-\\big(\\frac{1}{2}\\big)^{10}}{1-\\big(\\frac{1}{2}\\big)} = 2\\bigg(1-\\frac{1}{2^{10}}\\bigg) \\approx 1.998.\\]\n\n\nCode\nr = 1 / 2\nn = 10\nsum([r ** i for i in range(n)])\n\n\n1.998046875\n\n\nNotice how the term \\(\\big(\\frac{1}{2}\\big)^{10} \\approx 0.00098\\) is really small. We can practically ignore it. In fact, as \\(n \\rightarrow \\infty\\) we can completely ignore it, in which case\n\\[x = \\sum_{i=0}^{\\infty} \\bigg(\\frac{1}{2}\\bigg)^i = \\frac{1}{1-\\big(\\frac{1}{2}\\big)} = 2.\\]\nThis is an example of the infinite version of the geometric series. If \\(0 \\leq r \\leq 1\\), then\n\\[x = \\sum_{i=0}^{\\infty} r^i = r^0 + r^1 + r^2 + \\cdots = \\frac{1}{1-r}.\\]\nWhat happens when \\(r=1\\)? Clearly the rule breaks down at this point, since the denominator becomes infinite. But it’s easy enough to see what it is by writing out the sum,\n\\[x = \\sum_{i=0}^{n-1} 1^i = 1^0 + 1^1 + \\cdots + 1^{n-1} = \\underbrace{1 + 1 + \\cdots + 1}_{\\text{n times}} = n.\\]\nIn this case, if we send \\(n \\rightarrow \\infty\\), then \\(x\\) clearly blows up to \\(\\infty\\) too. You can see this by plotting the function \\(y = \\frac{1}{1-x}\\) and observing it asymptotes at \\(x=1\\).\n\n\nCode\nx = np.arange(0, 1, 0.01)\nf = lambda x: 1 / (1 - x)\nplot_function(x, f, xlim=(0, 1), ylim=(0, 100), ticks_every=[0.2, 20], \n              title='$y=1/(1-x)$')\n\n\n\n\n\nWe can always factor constants \\(c\\) out of sums. This follows naturally from just expanding the sum out,\n\\[\\sum_{i=0}^{n-1} c x_i = cx_0 + cx_1 + \\cdots + cx_{n-1} = c(x_0 + x_1 + \\cdots + x_{n-1}) = c\\sum_{i=0}^{n-1} x_i.\\]\nSimilarly, we can break sums up into pieces (or join sums together) as long as we’re careful to get the index limits right,\n\\[\\sum_{i=0}^{n-1} x_i = \\sum_{i=0}^{k} x_i + \\sum_{i=k+1}^{n-1} x_i.\\]\nWe can have double sums (sums of sums) as well. If \\(x_{i,j}\\) is some 2-index variable where \\(i=0,\\cdots,n-1\\) and \\(j=0,\\cdots,m-1\\), we can sum over both sets of indices to get \\(n \\cdot m\\) total terms,\n\\[\\sum_{i=0}^{n-1} \\sum_{j=0}^{m-1} x_{i,j} = \\sum_{j=0}^{m-1} \\sum_{i=0}^{n-1} x_{i,j} = x_{0,0} + x_{0,1} + \\cdots x_{0,m-1} + \\cdots + x_{n-1,0} + x_{n-1,1} + \\cdots x_{n-1,m-1}.\\]\nNotice the two sums can swap, or commute, with each other, \\(\\sum_i \\sum_j = \\sum_j \\sum_i\\). This follows by expanding the terms out like on the right-hand side and noting the must be equal in both cases.\n\n\n1.5.2 Products\nThe notation I’ve covered for sums has an analogue for products, called product notation. Suppose we want to multiply \\(n\\) numbers \\(x_0,x_1,\\cdots,x_{n-1}\\) together to get some number \\(x\\). We could write\n\\[x = x_0 \\cdot x_1 \\cdots x_{n-1},\\]\nbut we have a more compact notation for this as well. Using the symbol \\(\\prod\\) in analogy to \\(\\sum\\), we can write\n\\[x = \\prod_{i=0}^{n-1} x_i.\\]\nRead this as “the product of all \\(x_i\\) for \\(i=0,1,\\cdots,n-1\\) is \\(x\\)”.\nUnlike sums, python doesn’t have a native function to calculate products of elements in a sequence, but numpy has one called np.prod. Here’s an example. I’ll calculate the product of all integers between one and ten.\n\\[x = \\prod_{i=1}^{10} i = 1 \\cdot 2 \\cdot 3 \\cdot 4 \\cdot 5 \\cdot 6 \\cdot 7 \\cdot 8 \\cdot 9 \\cdot 10 = 3628800.\\]\n\n\nCode\nnp.prod([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n\n\n3628800\n\n\nLuckily, there aren’t any common products to remember. It’s just worth being familiar with the notation, since we’ll occasionally use it.\nProducts don’t obey quite the same properties sums do, so you have to be careful. When in doubt, just write out the product the long way and make sure what you’re doing makes sense. For example, pulling a factor \\(c\\) out of a product gives a factor of \\(c^n\\), not \\(c\\), since there are \\(c\\) total products multiplied together,\n\\[x = \\prod_{i=0}^{n-1} cx_i = cx_0 \\cdot cx_1 \\cdots cx_{n-1} = c^n(x_0 \\cdot x_1 \\cdots x_{n-1}) = c^n \\prod_{i=0}^{n-1} x_i.\\]\nIt’s worth noting (because we’ll use this fact), that we can turn products into sums by taking the log of the product,\n\\[\\log \\bigg(\\prod_{i=0}^{n-1} x_i \\bigg) = \\sum_{i=0}^{n-1} \\log x_i.\\]\nThis follows from the rule \\(\\log(x \\cdot y) = \\log x + \\log y\\), which extends to arbitrarily many products too."
  },
  {
    "objectID": "notebooks/basic-math.html#greek-alphabet",
    "href": "notebooks/basic-math.html#greek-alphabet",
    "title": "1  Basic Math",
    "section": "1.6 Greek Alphabet",
    "text": "1.6 Greek Alphabet\nLike many other technical fields, machine learning makes heavy use of the Greek alphabet to represent variable names in mathematical equations. While not all Greek characters are used, certain ones are worth being aware of. Below is a table of the Greek letters. You don’t need to memorize all of these letters, but it’s worth referencing this table whenever you encounter a symbol you don’t recognize."
  },
  {
    "objectID": "notebooks/numerical-computing.html#integers",
    "href": "notebooks/numerical-computing.html#integers",
    "title": "2  Numerical Computation",
    "section": "2.1 Integers",
    "text": "2.1 Integers\n\n2.1.1 Basics\nRecall the integers are whole numbers that can be positive, negative, or zero. Examples are 5, 100151, 0, -72, etc. The set of all integers is commonly denoted by the symbol \\(\\mathbb{Z}\\).\nIn python, integers (ints for short) are builtin objects of type int that more or less follow the rules that integers in math follow.\nAmong other things, the following operations can be performed with integers:\n\nAddition: \\(2 + 2 = 4\\).\nSubtraction: \\(2 - 5 = -3\\).\nMultiplication: \\(3 \\cdot 3 = 9\\)\n\nIn python this is the * operator, e.g. 3 * 3 = 9\n\nExponentiation: \\(2^3 = 2 \\times 2 \\times 2 = 8\\)\n\nIn python this is the ** operator, e.g. 2 ** 3 = 8.\n\nRemainder (or Modulo): the remainder of 10 when divided by 3 is 1, written \\(10 \\text{ mod } 3 = 1\\)\n\nIn python this is the % operator, e.g. 10 % 3 = 1.\n\n\nIf any of these operations are applied to two integers, the output will itself always be an integer.\nHere are a few examples.\n\n\nCode\n2 + 2\n\n\n4\n\n\n\n\nCode\n2 - 5\n\n\n-3\n\n\n\n\nCode\n3 * 3\n\n\n9\n\n\n\n\nCode\n10 % 3\n\n\n1\n\n\n\n\nCode\n2 ** 3\n\n\n8\n\n\nWhat about division? You can’t always divide two integers and get another integer. What you have to do instead is called integer division. Here you divide the two numbers and then round the answer down to the nearest whole number. Since \\(5 \\div 2 = 2.5\\), the nearest rounded down integer is 2.\nIn math, this “nearest rounded down integer” 2 is usually called the floor of 2.5, and represented with the funny symbol \\(\\lfloor 2.5 \\rfloor.\\) Using this notation we can write the above integer division as \\[\\big\\lfloor \\frac{5}{2} \\big\\rfloor = 2.\\]\nIn python, integer division is done using the // operator, e.g. 5 // 2 = 2. I’ll usually write \\(5 \\ // \\ 2\\) instead of \\(\\big\\lfloor \\frac{5}{2} \\big\\rfloor\\) when it makes sense, \\[5 \\ // \\ 2 = \\big\\lfloor \\frac{5}{2} \\big\\rfloor = 2.\\]\n\n\nCode\n5 // 2\n\n\n2\n\n\nWe can also do regular division / with ints, but the output will not be an integer even if the answer should be, e.g. 4 / 2. Only integer division is guaranteed to return an integer. I’ll get to this shortly.\n\n\nCode\n4 / 2\ntype(4 / 2)\n\n\n2.0\n\n\nfloat\n\n\n\n\nCode\n4 // 2\ntype (4 // 2)\n\n\n2\n\n\nint\n\n\nDivision by zero is of course undefined for both division and integer division. In python it will always raise a ZeroDivisionError like so.\n\n\nCode\n4 / 0\n\n\nZeroDivisionError: division by zero\n\n\n\n\nCode\n4 // 0\n\n\nZeroDivisionError: integer division or modulo by zero\n\n\n\n\n2.1.2 Representing Integers\nJust like every other data type, on a computer integers are actually represented internally as a sequence of bits. A bit is a “binary digit”, 0 or 1. A sequence of bits is just a sequence of zeros and ones, e.g. 0011001010 or 1001001.\nThe number of bits used to represent a piece of data is called its word size. If we use a word size of \\(n\\) bits to represent an integer, then there are \\(2^n\\) possible integer values we can represent.\nIf integers could only be positive or zero, representing them with bits would be easy. We could just convert them to binary and that’s it. To convert a non-negative integer to binary, we just need to keep dividing it by 2 and recording its remainder (0 or 1) at each step. The binary form is then just the sequence of remainders, written right to left. More generally, the binary sequence of some arbitrary number \\(x\\) is the sequence of coefficients \\(b_k=0,1\\) in the sum\n\\[x = \\sum_{k=-\\infty}^\\infty b_k 2^k = \\cdots + b_2 2^2 + b_1 2^1 + b_0 2^0 + b_{-1} 2^{-1} + b_{-2} 2^{-2} + \\cdots.\\]\nHere’s an example. Suppose we wanted to represent the number \\(12\\) in binary.\n\n\\(12 \\ // \\ 2 = 6\\) with a remainder of \\(0 = 12 \\text{ mod } 2\\), so the first bit from the right is then \\(0\\).\n\\(6 \\ // \\ 2 = 3\\) with a remainder of \\(0 = 6 \\text{ mod } 2\\), so the next bit is \\(0\\).\n\\(3 \\ // \\ 2 = 1\\) with a remainder of \\(1 = 3 \\text{ mod } 2\\), so the next bit is \\(1\\).\n\\(1 \\ // \\ 2 = 0\\) with a remainder of \\(1 = 1 \\text{ mod } 2\\), so the next bit is \\(1\\).\n\nSo the binary representation of \\(12\\) is \\(1100\\), which is the sequence of coefficients in the sum\n\\[12 = 1 \\cdot 2^{3} + 1 \\cdot 2^{2} + 0 \\cdot 2^{1} + 0 \\cdot 2^{0}.\\]\nRather than keep doing these by hand, you can quickly convert a number to binary in python by using bin. It’ll return a string representing the binary sequence of that number, prepended with the special prefix 0b. To get back to the integer from, use int, passing in a base of 2.\n\n\nCode\nbin(12)\n\n\n'0b1100'\n\n\n\n\nCode\nint('0b110', 2)\n\n\n6\n\n\nThis representation works fine for non-negative integers, also called the unsigned integers in computer science. To represent an unsigned integer with \\(n\\) bits, just get its binary form and prepend it with enough zeros on the left until all \\(n\\) bits are used. For example, if we used 8-bit unsigned integers then \\(n=8\\), hence representing the number \\(12\\) would look like \\(00000110\\). Simple, right?\nUnsigned ints work fine if we never have to worry about negative numbers. But in general we do. These are called the signed integers in computer science. To represent signed ints, we need to use one of the bits to represent the sign. What we can do is reserve the left-most bit for the sign, \\(0\\) if the integers is positive or zero, \\(1\\) if the integer is negative.\nFor example, if we used 8-bit signed integers to represent \\(12\\), we’d again write \\(00000110\\), exactly as before. But this time it’s understood that left-most \\(0\\) is encoding the fact that \\(12\\) is positive. If instead we wanted to represent the number \\(-12\\) we’d need to flip that bit to a \\(1\\), so we’d get \\(10000110\\).\nLet’s now do an example of a simple integer system. Consider the system of 4-bit signed ints. In this simple system, \\(n=4\\) is the word size, and an integer \\(x\\) is represented with the sequence of bits\n\\[x \\equiv sb_1b_2b_3,\\]\nwhere \\(s\\) is the sign bit and \\(b_1b_2b_3\\) are the remaining 3 bits allowed to represent the numerical digits. This system can represent \\(2^4=16\\) possible values in the range \\([-2^3+1,2^3-1] = [-8,7]\\), given in the following table:\n\n\n\nInteger\nRepresentation\nInteger\nRepresentation\n\n\n\n\n-0\n1000\n+0\n0000\n\n\n-1\n1001\n1\n0001\n\n\n-2\n1010\n2\n0010\n\n\n-3\n1011\n3\n0011\n\n\n-4\n1100\n4\n0100\n\n\n-5\n1101\n5\n0101\n\n\n-6\n1110\n6\n0110\n\n\n-7\n1111\n7\n0111\n\n\n\nNote the presence of \\(-0 \\equiv 1110\\) in the upper left. This is because the system as I’ve defined it leaves open the possibility of two zeros, \\(+0\\) and \\(-0\\), since for zero the sign bit is redundant. A way to get around this is to encode the negative numbers slightly differently, by not just setting the sign bit to one, but also inverting the remaining bits and subtracting one from them. This is called the two’s complement representation. It’s how most languages, including python, actually represent integers. I won’t go into this representation in any depth, except to say that it gets rid of the need for \\(-0\\) and replaces it with \\(-2^{n-1}\\).\nHere’s what that table looks like for 4-bit integers. It’s almost the same, except there’s no \\(-0\\), instead a \\(-8\\). Notice the positive integers look exactly the same. It’s only the negative integers that look different. For them, the right three bits get inverted and added with a one.\n\n\n\nInteger\nTwo’s Complement\nInteger\nTwo’s Complement\n\n\n\n\n-1\n1111\n0\n0000\n\n\n-2\n1110\n1\n0001\n\n\n-3\n1101\n2\n0010\n\n\n-4\n1100\n3\n0011\n\n\n-5\n1011\n4\n0100\n\n\n-6\n1010\n5\n0101\n\n\n-7\n1001\n6\n0110\n\n\n-8\n1000\n7\n0111\n\n\n\nIt’s worth visualizing what integers look like on the number line, if for nothing else than to compare it with what floats look like later on. Below I’ll plot what a 6-bit signed integer system would look like. Such a system should go from -32 to 31. As you’d expect, we get a bunch of equally spaced points from -32 to 31.\n\n\nCode\nn = 6\nsix_bit_ints = range(-2**(n-1), 2**(n-1))\nplot_number_dist(six_bit_ints, title=f'Distribution of {n}-bit Signed Ints')\n\n\n\n\n\n\n\n\n\nIn python, integers are represented by default using a much bigger word size of \\(n=64\\) bits, called long integers, or int64 for short. This means (using two’s complement) we can represent \\(2^{64}=18446744073709551616\\) possible integer values in the range \\([-2^{63}, 2^{63}-1]\\).\nYou can see from this that 64-bit integers have a minimum integer allowed and a maximum integer allowed, which are\n\\[\\text{min\\_int}=-2^{63}=-9223372036854775808, \\qquad \\text{max\\_int}=2^{63}-1=9223372036854775807.\\]\nWhat I’ve said is technically only exactly true in older versions of pythons as well as other programming languages like C. It turns out newer versions of python have a few added tricks that allow you to represent essentially arbitrarily large integers. You can see this by comparing it to numpy’s internal int64 representation, which uses the C version. A numpy int64 outside the valid range will throw an overflow error.\n\n\nCode\nmin_int = -2 ** 63\nmax_int = 2 ** 63 - 1\n\n\n\n\nCode\nmin_int - 1\nnp.int64(min_int - 1)\n\n\n-9223372036854775809\n\n\nOverflowError: Python int too large to convert to C long\n\n\n\n\nCode\nmax_int + 1\nnp.int64(max_int + 1)\n\n\n9223372036854775808\n\n\nOverflowError: Python int too large to convert to C long"
  },
  {
    "objectID": "notebooks/numerical-computing.html#floats",
    "href": "notebooks/numerical-computing.html#floats",
    "title": "2  Numerical Computation",
    "section": "2.2 Floats",
    "text": "2.2 Floats\n\n2.2.1 Basics\nWhat if we want to represent decimal numbers or fractions instead of whole numbers, like \\(1.2\\) or \\(0.99999\\), or even irrational numbers like \\(\\pi=3.1415926\\dots\\)? To do this we need a new system of numbers that I’ll call floating point numbers, or floats, for reasons I’ll explain soon. Floats will be a computer’s best attempt to represent the real numbers \\(\\mathbb{R}\\). They’ll represent real numbers only approximately with some specified precision.\nIn python, floats are builtin objects of type float. Floats obey pretty much the same operations that integers do with some minor exceptions:\n\nAddition: \\(1.2 + 4.3 = 5.5\\).\nSubtraction: \\(1.2 - 4.3 = -3.1\\).\nMultiplication: \\(1.2 \\times 4.3 = 5.16\\).\nExponentiation: \\(4.3^2 = 18.49\\).\nRemainder (or Modulo): \\(4.3 \\text{ mod } 1.2 = 0.7\\).\nInteger Division: \\(4.3 \\ // \\ 1.2 = 3.0\\).\nDivision: \\(4.3 \\div 1.2\\).\n\nLet’s verify the first few of these to see what’s going on.\n\n\nCode\n1.2 + 4.3\n\n\n5.5\n\n\n\n\nCode\n1.2 - 4.3\n\n\n-3.0999999999999996\n\n\n\n\nCode\n1.2 * 4.3\n\n\n5.159999999999999\n\n\nMost of them look right. But what the heck is going on with \\(1.2 - 4.3\\) and \\(1.2 \\times 4.3\\)? We’re getting some weird trailing nines that shouldn’t be there. This gets to how floats are actually represented on a computer.\n\n\n2.2.2 Representing Floats\nRepresenting real numbers on a computer is a lot more subtle than representing integers. Since a computer can only have a finite number of bits, they can’t represent infinitely many digits, e.g. in irrational numbers like \\(\\pi\\). Using finite word sizes will necessarily have to truncate real numbers to some number of decimal places. This truncation will create an error in the calculation called numerical roundoff.\nSo how should we represent a decimal number using \\(n\\) bits? As an example, let’s imagine we’re trying to represent the number \\(x=157.208\\). Perhaps the first thing you might think of is to use some number of those bits to represent the integer part, and some number to represent the fractional part. Suppose you have \\(n=16\\) bits available to represent \\(x\\). Then maybe you can use 8 bits for the integer part \\(157\\), and 8 bits for the fractional part \\(0.208\\). Converting both halves to binary, you’d get \\[157 \\equiv 10011101, \\quad 0.208 \\equiv 0011010100111111.\\]\nTruncating both sequences to 8 bits (from the left), you could thus adopt a convention that \\(157.208 \\equiv 10011101 \\ 00110101\\).\nThis system is an example of a fixed point representation. This has to do with the fact that we’re always using a fixed number of bits for the integer part, and a fixed number for the fractional part. The decimal point isn’t allowed to float, or move around to allocate more bits to the integer or fractional part depending which needs more precision. The decimal point is fixed.\nAs I’ve suggested, the fixed point representation seems to be limited and not terribly useful. If you need really high precision in the fractional part, your only option is to use a larger word size. If you’re dealing with really big numbers and don’t care much about the fractional part, you also need a larger word size so you don’t run out of numbers. A solution to this problem is to allow the decimal point to float. We won’t allocate a fixed number of bits to represent the integer or fractional parts. We’ll design it in such a way that larger numbers give the integer part more bits, and smaller numbers give the fractional part more bits.\nThe trick to allowing the decimal point to float is to represent not just the digits of a number but also its exponent. Think about scientific notation, where if you have a number like say \\(x=1015.23\\), you can write it as \\(1.01523 \\cdot 10^3\\), or 1.01523e3. That \\(3\\) is the exponent. It says something about how big the number is. What we can do is convert a number to scientific notation. Then use some number of bits to represent the exponent \\(3\\) and some to represent the remaining part \\(1.01523\\). This is essentially the whole idea behind floating point.\nIn floating point representation, instead of using scientific notation with powers of ten, it’s more typical to use powers of two. When using powers of two, the decimal part can always be scaled to be between 1 and 2, so they look like \\(1.567\\) or something like that. Since the \\(1.\\) part is always there, we can agree it’s always there, and only worry about representing the fractional part \\(0.567\\). We’ll call this term the mantissa. Denoting the sign bit as \\(s\\), the exponent as \\(e\\), and the mantissa as \\(m\\), we can thus right any decimal number \\(x\\) in a modified scientific notation of the form \\[x = (-1)^s \\cdot (1+m) \\cdot 2^{e}.\\] Once we’ve converted \\(x\\) to this form, all we need to do is to figure out how to represent \\(s\\), \\(m\\), and \\(e\\) using some number of bits of \\(n\\), called the floating point precision. Assume the \\(n\\) bits of precision allocate \\(1\\) bit for the sign, \\(n_e\\) bits for the exponent, and \\(n_m\\) bits for the mantissa, so \\(n=1+n_e+n_m\\).\nHere are the steps to convert a number \\(x\\) into its \\(n\\)-bit floating point representation.\n\nGiven some number \\(x\\), get its modified scientific notation form \\(x = (-1)^s \\cdot (1+m) \\cdot 2^e\\).\n\nDetermine the sign of \\(x\\). If negative, set the sign bit to \\(s=1\\), else default to \\(s=0\\). Set \\(x = |x|\\).\nKeep performing the operation \\(x = x \\ // \\ 2\\) until \\(1 \\leq x \\leq 2\\). Keep track of the number of times you’re dividing, which is the exponent \\(e\\).\nThe remaining part will be some \\(1 \\leq x \\leq 2\\). Write it in the form \\(x = 1 + m\\), where \\(m\\) is the mantissa.\n\nConvert the scientific notation form into a sequence of \\(n\\) bits, truncating where necessary.\n\nFor reasons I’ll describe in a second, it’s good to add a bias term \\(b\\) to the exponent \\(e\\) before converting the exponent to binary. Let \\(e'=e+b\\) be this modified exponent.\nConvert each of \\(e'\\) and \\(m\\) into binary sequences, truncated to sizes \\(n_e\\), and \\(n_m\\) respectively.\nConcatenate these binary sequences together to get a sequence of \\(n=1+n_e+n_m\\) total bits. By convention, assume the order of bit concatenation is the sign bit, then exponent bits, then the mantissa bits.\n\n\nThere are of course other ways you could do it, for example by storing the sequences in a different order. I’m just stating one common way it’s done.\nSince all of this must seem like Greek, here’s a quick example. Let’s consider the number \\(x=15.25\\). We’ll represent it using \\(n=8\\) bits of precision, where \\(n_e=4\\) is the number of exponent bits, \\(n_m=3\\) is the number of precision bits, and \\(b=10\\) is the bias.\n\nConvert \\(x=15.25\\) to its modified scientific notation.\n\nSince \\(x \\geq 0\\) the sign is positive, so \\(s=0\\).\nKeep integer dividing \\(x\\) by \\(2\\) until it’s less than \\(2\\). It takes \\(e=3\\) divisions before \\(x<2\\).\nWe now have \\(x = 1.90625 \\cdot 2^3\\). The mantissa is then \\(m = (1.90625-1) = 0.90625\\).\nIn modified scientific notation form we now have \\(x=(-1)^0 \\cdot (1 + 0.90625) \\cdot 2^3\\).\n\nConvert everything to binary.\n\nAdding the bias to the exponent gives \\(e'=3+10=13\\).\nConverting each piece to binary we get \\(e' = 13 \\equiv 1101\\), \\(m = 0.90625 \\equiv 11101\\).\nSince \\(m\\) requires more than \\(n_m=3\\) bits to represent, truncate off the two right bits to get \\(m \\equiv 111\\).\n\nThis truncation will cause numerical roundoff, since \\(0.90625\\) truncates to \\(0.875\\). That’s an error of \\(0.03125\\) that gets permanently lost.\n\nThe final representation is thus \\(x \\equiv 0 \\ 1101 \\ 111\\).\n\n\nBelow I show the example I just calculated. I print out both the scientific notation form and its binary representation.\n\n\nCode\nrepresent_as_float(15.25, n=8, n_exp=4, n_man=3, bias=10)\n\n\nscientific notation: (-1)^0 * (1 + 0.90625) * 2^3\n8-bit floating point representation: 0 1101 111\n\n\nSo what’s going on with the bias term \\(b\\)? Why do we need it? The easiest answer to give is that without it, we can’t have negative exponents without having to use another sign bit for them. Consider a number like \\(x=0.5\\). In modified scientific notation this would look like \\(x=(-1)^0 \\cdot (1+0) \\cdot 2^{-1} = 2^{-1}\\), meaning its exponent would be \\(e=-1\\). Rather than have to keep yet another sign bit for the exponent, it’s easier to just add a bias term \\(b\\) that ensures the exponent \\(e'=e+b\\) is always non-negative. The higher the bias, the more precision we can show in the range \\(-1 < x < 1\\). The trade-off is that we lose precision for large values of \\(x\\).\nOn top of floats defined the way I mentioned, we also have some special numbers that get defined in a floating point system. These are \\(\\pm 0\\), \\(\\pm \\infty\\), and \\(\\text{NaN}\\) or “not a number”. Each of these numbers is allocated its own special sequence of bits, depending on the precision.\n\n\\(+0\\) and \\(-0\\): These numbers are typically represented using a biased exponent \\(e'=0\\) (all zero bits) and a mantissa \\(m=0\\) (all zero bits). The sign bit is used to distinguish between \\(+0\\) and \\(-0\\). In our example, these would be \\(+0 \\equiv 0 \\ 0000 \\ 000\\) and \\(-0 \\equiv 1 \\ 0000 \\ 000\\).\n\\(+\\infty\\) and \\(-\\infty\\): These numbers are typically represented using the max allowed exponent (all one bits) and a mantissa \\(m=0\\) (all zero bits). The sign bit is used to distinguish between \\(+\\infty\\) and \\(-\\infty\\). In our example, these would be \\(+\\infty \\equiv 0 \\ 1111 \\ 000\\) and \\(-\\infty \\equiv 1 \\ 1111 \\ 000\\).\n\\(\\text{NaN}\\): This value is typically represented using the max allowed exponent (all one bits) and a non-zero \\(m \\neq 0\\). The sign bit is usually not used for \\(\\text{NaN}\\) values. Note this means we can have many different sequences that all represent \\(\\text{NaN}\\). In our example, any number of the form \\(\\text{NaN} \\equiv \\text{x} \\ 1111 \\ \\text{xxx}\\) would work.\n\nSo I can illustrate some points about how floating point numbers behave, I’m going to generate all possible \\(8\\)-bit floats (excluding the special numbers) and plot them on a number line, similar to what I did above with the \\(8\\)-bit signed integers. I’ll generate the floats using the using the helper function gen_all_floats, passing in the number of mantissa bits n_man=3, the number of exponent bits n_exp=4, and a bias of bias=10.\nFirst, I’ll use these numbers to print out some interesting statistics of this 8-bit floating point system.\n\n\nCode\neight_bit_floats = gen_all_floats(n=8, n_man=3, n_exp=4, bias=10)\nprint(f'Total number of 8-bit floats: {len(eight_bit_floats)}')\nprint(f'Most negative float: {min(eight_bit_floats)}')\nprint(f'Most positive float: {max(eight_bit_floats)}')\nprint(f'Smallest nonzero float: {min([x for x in eight_bit_floats if x > 0])}')\nprint(f'Machine Epsilon: {min([x for x in eight_bit_floats if x > 1]) - 1}')\n\n\nTotal number of 8-bit floats: 120\nMost negative float: -56.0\nMost positive float: 56.0\nSmallest nonzero float: 0.001953125\nMachine Epsilon: 0.25\n\n\nWe can see that this 8-bit system only contains 120 unique floats. We could practically list them all out. Just like with the integers, we see there’s a most negative float, \\(-56.0\\), and a most positive float, \\(56.0\\). The smallest float, i.e. the one closest to \\(0\\), is \\(0.001953125\\). Notice how much more precision the smallest float has than the largest ones do. The largest ones are basically whole numbers, while the smallest one has nine digits of precision. Evidently, floating point representations give much higher precision to numbers close to zero than to numbers far away from zero.\nWhat happens if you try to input a float larger than the max, in this case \\(56.0\\)? Typically it will overflow. This will result in either the system raising an error, or the number getting set to \\(+\\infty\\), in a sense getting “rounded up”. Similarly, for numbers more negative than the min, in this case \\(-56.0\\), either an overflow error will be raised, or the number will get “rounded down” to \\(-\\infty\\).\nYou have to be careful in overflow situations like this, especially when you don’t know for sure which of these your particular system will do. It’s amusing to note that python will raise an overflow error, but numpy will round to \\(\\pm \\infty\\). Two different conventions to worry about. Just as amusing, when dealing with signed integers, it’s numpy that will raise an error if you overflow, while python won’t care. One of those things…\nWhat happens when you try to input a float smaller than the smallest value, in this case \\(0.001953125\\)? In this case, the number is said to undeflow. Usually underflow won’t raise an error. The number will pretty much always just get set to \\(+0\\) (or \\(-0\\)). This is again something you have to worry about, especially if you’re dealing with small numbers in denominators, where they can lead to division by zero errors which do get raised.\nOverflow and underflow errors are some of the most common numerical bugs that occur in deep learning, and usually result from not handling floats correctly to begin with.\nI also printed out a special value called the machine epsilon. The machine epsilon, denoted \\(\\varepsilon_m\\), is defined as the smallest value in a floating point system that’s larger than \\(1\\). In some sense, \\(\\varepsilon_m\\) is a proxy for how finely you can represent numbers in a given \\(n\\)-bit floating point system. The smaller \\(\\varepsilon_m\\) the more precisely you can represent numbers, i.e. the more decimal places of precision you get access to. In our case, we get \\(\\varepsilon_m=0.25\\). This means numbers in 8-bit floating point tend to be \\(0.25\\) apart from each other on average, which means we can represent numbers in this system only with a measly 2-3 digits of precision.\nWith these numbers in hand let’s now plot their distribution on the number line. Compare with the plot of the signed integers I did above.\n\n\nCode\nplot_number_dist(eight_bit_floats, title='Distribution of 8-bit Floats')\n\n\n\n\n\n\n\n\n\nNotice how different this plot is from the ones for the signed integers. With the integers, the points were equally spaced. Now points close to \\(0\\) are getting represented much closer together than points far from \\(0\\). There are \\(74\\) of the \\(120\\) total points showing up just in the range \\([-1,1]\\). That’s over half!. Meanwhile, only \\(22\\) points total show up in the combined ranges of \\([-60,-10]\\) and \\([10,60]\\). Very strange.\nFeel free to play around with different floating point systems by using different choices for n, n_man, n_exp, and bias. Be careful, however, not to make n_exp too large or you may crash the kernel…\n\n\n2.2.3 Double Precision\nSo how does python represent floats? Python by default uses what’s called double precision to represent floats, also called float64. This means \\(n=64\\) total bits of precision are used, with \\(n_e=11\\), \\(n_m=52\\), and bias \\(b=1023=2^{10}-1\\). Double precision allows for a much larger range of numbers than 8-bit precision does:\n\nThe max value allowed is \\(2^{2^{n_e}-b} = 2^{1025} \\approx 10^{308}\\).\nThe min value allowed is \\(-2^{2^{n_e}-b} = -2^{1025} \\approx -10^{308}\\).\nNumbers outside the range of about \\([-10^{308}, 10^{308}]\\) will overflow.\nThe smallest values allowed are (plus or minus) \\(2^{-b+1} = 2^{-1022} \\approx 10^{-308}\\).\n\nUsing subordinal numbers, the smallest values are (plus or minus) \\(2^{-b-n_m+1} = 2^{-1074} \\approx 10^{-324}\\).\n\nNumbers inside the range of about \\([-10^{-308}, 10^{-308}]\\) will underflow.\n\nUsing subordinal numbers, this range is around \\([-10^{-324}, 10^{-324}]\\).\n\nThe machine epsilon is \\(\\varepsilon_m = 2^{-53} \\approx 10^{-16}\\).\nNumbers requiring more than about 15-16 digits of precision will get truncated, resulting in numerical roundoff.\nThe special numbers \\(\\pm \\infty\\), \\(\\pm 0\\), and \\(\\text{NaN}\\) are represented similarly as before, except using 64 bits.\n\nTo illustrate the point regarding numerical roundoff, here’s what happens if we try to use double precision floating point to define the constant \\(\\pi\\) to its first 100 digits? Notice it just gets truncated to its first 15 digits. Double precision is unable to keep track of the other 85 digits. They just get lost to numerical roundoff.\n\n\nCode\npi = 3.141592653589793238462643383279502884197169399375105820974944592307816406286208998628034825342117068\npi\n\n\n3.141592653589793\n\n\nAnother thing to worry about is adding small numbers to medium to large sized numbers, e.g. \\(10 + 10^{-16}\\), which will just get rounded down to \\(10.0\\).\n\n\nCode\n10.0 + 1e-16\n\n\n10.0\n\n\nNumerical roundoff is often an issue when subtracting two floats. Here’s what happens when we try to subtract two numbers that should be equal, \\(x=0.1+0.2\\) and \\(y=0.3\\). Instead of \\(y-x=0\\), we get \\(y-x \\approx -5.55 \\cdot 10^{-17}\\). The problem comes from the calculation \\(x=0.1+0.2\\), which caused a slight loss of precision in \\(x\\).\n\n\nCode\nx = 0.1 + 0.2\ny = 0.3\ny - x\n\n\n-5.551115123125783e-17\n\n\nA major implication of these calculations is that you should never test floating points for exact equality because numerical roundoff can mess it up. If you’d tried to test something like (y - x) == 0.0, you’d have gotten the wrong answer. Instead, you want to test that y - x is less than some small number tol, called a tolerance, i.e. abs(y - x) < tol.\n\n\nCode\ny - x == 0.0\n\n\nFalse\n\n\n\n\nCode\ntol = 1e-5\nabs(y - x) < tol\n\n\nTrue\n\n\nNumerical roundoff explains why we got the weird results above when subtracting \\(1.2 - 4.3\\). The imperfect precision in the two numbers resulted in a numerical roundoff error, leading in the trailing \\(9\\)s that should’ve rounded up to \\(-3.1\\) exactly. In general, subtracting floats is one of the most dangerous operations to do, as it tends to lead to the highest loss of precision in calculations. The closer two numbers are to being equal the worse this loss of precision tends to get.\nI mentioned that double precision has a smallest number of \\(2^{-1022} \\approx 10^{-308}\\), but caveated that by saying that, by using a trick called subordinal numbers, we can get the smallest number down to about \\(10^{-324}\\). What did I mean by this? It turns out that the bits where the biased exponent \\(e'=0\\) (i.e. all exponent bits are zero) go mostly unused in the standard version of double precision. By using this zero exponent and allowing the mantissa \\(m\\) to take on all its possible values, we can get about \\(2^{52}\\) more values (since the mantissa has 52 bits). This lets us get all the way down to \\(2^{-1022} \\cdot 2^{-52} = 2^{-1074} \\approx 10^{-324}\\).\nPython (and numpy) by default implements double precision with subordinal numbers, as we can see.\n\n\nCode\n2 ** (-1074)\n\n\n5e-324\n\n\n\n\nCode\n2 ** (-1075)\n\n\n0.0\n\n\nThe special numbers \\(\\pm \\infty\\), \\(\\pm 0\\), and \\(\\text{NaN}\\) are also defined in double precision. In python (and numpy) they’re given by the following commands,\n\n\\(\\infty\\): float('inf') or np.inf,\n\\(-\\infty\\): float('-inf') or -np.inf,\n\\(\\pm 0\\): 0,\n\\(\\text{NaN}\\): float('nan') or np.nan.\n\n\n\nCode\nfloat('inf')\nnp.inf\n\n\ninf\n\n\ninf\n\n\n\n\nCode\nfloat('-inf')\n-np.inf\n\n\n-inf\n\n\n-inf\n\n\n\n\nCode\n0\n-0\n\n\n0\n\n\n0\n\n\n\n\nCode\nfloat('nan')\nnp.nan\n\n\nnan\n\n\nnan\n\n\nYou may be curious what exactly \\(\\text{NaN}\\) (“not a number”) is and where it might show up. Basically, NaNs are used wherever values are undefined. Anytime an operation doesn’t return a sensible value it risks getting converted to NaN. One example is the operation \\(\\infty - \\infty = \\infty + (-\\infty)\\), which mathematically doesn’t make sense. No, it’s not zero…\n\n\nCode\nfloat('inf') + float('-inf')\nnp.inf - np.inf\n\n\nnan\n\n\nnan\n\n\nI’ll finish this section by mentioning that there are two other floating point representations worth being aware of: single precision (or float32), and half precision (or float16). Single precision uses 32 bits to represent a floating point number. Half precision uses 16 bits. It may seem strange to even bother having these less-precise precisions lying around, but they do have their uses. For example, half precision shows up in deep learning as a more efficient way to represent the weights of a neural network. Since half precision floats only take up 25% as many bits as default double precision floats do, using them can yield a 4x reduction in model memory sizes. We’ll see more on this later.\n\n\n2.2.4 Common Floating Point Pitfalls\nTo cap this long section on floats, here’s a list of common pitfalls people run into when working with floating point numbers, and some ways to avoid each one. This is probably the most important thing to take away from this section. You may find it helpful to reference later on. See this post for more information.\n\nNumerical overflow: Letting a number blow up to infinity (or negative infinity)\n\nClip numbers from above to keep them from being too large\nWork with the log of the number instead\nMake sure you’re not dividing by zero or a really small number\nNormalize numbers so they’re all on the same scale\n\nNumerical underflow: Letting a number spiral down to zero\n\nClip numbers from below to keep them from being too small\nWork with the exp of the number instead\nNormalize numbers so they’re all on the same scale\n\nSubtracting floats: Avoid subtracting two numbers that are approximately equal\n\nReorder operations so approximately equal numbers aren’t nearby to each other\nUse some algebraic manipulation to recast the problem into a different form\nAvoid differencing squares (e.g. when calculating the standard deviation)\n\nTesting for equality: Trying to test exact equality of two floats\n\nInstead of testing x == y, test for approximate equality with something like abs(x - y) <= tol\nUse functions like np.allclose(x, y), which will do this for you\n\nUnstable functions: Defining some functions in the naive way instead of in a stable way\n\nExamples: factorials, softmax, logsumexp\nUse a more stable library implementation of these functions\nLook for the same function but in log form, e.g. log_factorial or log_softmax\n\nBeware of NaNs: Once a number becomes NaN it’ll always be a NaN from then on\n\nPrevent underflow and overflow\nRemove missing values or replace them with finite values"
  },
  {
    "objectID": "notebooks/numerical-computing.html#array-computing",
    "href": "notebooks/numerical-computing.html#array-computing",
    "title": "2  Numerical Computation",
    "section": "2.3 Array Computing",
    "text": "2.3 Array Computing\nIn machine learning and most of scientific computing we’re not interested in operating on just single numbers at a time, but many numbers at a time. This is done using array operations. The most popular library in python for doing numerical computation on arrays is numpy.\nWhy not just do numerical computations in base python? After all, if we have large arrays of data we can just put them in a list. Consider the following example. Suppose we have two tables of data, \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\). Each table has \\(m=5\\) rows and \\(n=3\\) columns. The rows represent samples, e.g. measured in a lab, and the columns represent the variables, or features, being measured, call them \\(x\\), \\(y\\), and \\(z\\), if you like. I’ll define these two tables using python lists A and B below.\n\n\nCode\nA = [[3.5, 18.1, 0.3],\n     [-8.7, 3.2, 0.5],\n     [-1.3, 8.4, 0.2],\n     [5.6, 12.9, 0.9],\n     [-6.8, 19.7, 0.7]]\n\nB = [[-9.7, 12.5, 0.1],\n     [-5.1, 14.1, 0.6],\n     [-1.6, 3.7, 0.7],\n     [2.3, 19.3, 0.9],\n     [8.2, 9.7, 0.2]]\n\n\nSuppose we wanted to add the elements in these two tables together, index by index, like this,\n\\[\n\\begin{bmatrix}\nA[0][0] + B[0][0], & A[0][1] + B[0][1], & A[0][2] + B[0][2] \\\\\nA[1][0] + B[1][0], & A[1][1] + B[1][1], & A[1][2] + B[1][2] \\\\\nA[2][0] + B[2][0], & A[2][1] + B[2][1], & A[2][2] + B[2][2] \\\\\nA[3][0] + B[3][0], & A[3][1] + B[3][1], & A[3][2] + B[3][2] \\\\\nA[4][0] + B[4][0], & A[4][1] + B[4][1], & A[4][2] + B[4][2] \\\\\n\\end{bmatrix}.\n\\]\nIf we wanted to do this in python, we’d have to loop over all rows and columns and place the sums one-by-one inside an array \\(\\mathbf{C}\\), like this.\n\n\nCode\ndef add_arrays(A, B):\n    n_rows, n_cols = len(A), len(A[0])\n    C = []\n    for i in range(n_rows):\n        row = []\n        for j in range(n_cols):\n            x = A[i][j] + B[i][j]\n            row.append(x)\n        C.append(row)\n    return C\n\nC = add_arrays(A, B)\nprint(f'C = {np.array(C).round(2).tolist()}')\n\n\nC = [[-6.2, 30.6, 0.4], [-13.8, 17.3, 1.1], [-2.9, 12.1, 0.9], [7.9, 32.2, 1.8], [1.4, 29.4, 0.9]]\n\n\nNumpy makes this far easier to do. It implements element-wise array operatations, which allow us to operate on arrays with far fewer lines of code. In numpy, to perform the same adding operation we just did, we’d just add the two arrays together directly, \\(\\mathbf{A}+\\mathbf{B}\\).\nTo use numpy operations we have to convert data into the native numpy data type, the numpy array. Do this by wrapping lists inside the function np.array. Once we’ve done this, we can just add them together in one line. This will simultaneously element-wise add the elements in the array so we don’t have to loop over anything.\n\n\nCode\nA = np.array(A)\nB = np.array(B)\nprint(f'C = \\n{A+B}')\n\n\nC = \n[[ -6.2  30.6   0.4]\n [-13.8  17.3   1.1]\n [ -2.9  12.1   0.9]\n [  7.9  32.2   1.8]\n [  1.4  29.4   0.9]]\n\n\nThis is really nice. We’ve managed to reduce a double foor loop of 8 lines of code down to just 1 line with no loops at all. Of course, there are loops happening in the background inside the numpy code, we just don’t see them.\nNumpy lets us do this with pretty much any arithmetic operation we can think of. We can element-wise add, subtract, multiply, or divide the two arrays. We can raise them to powers, exponentiate them, take their logarithms, etc. Just like we would do so with single numbers. In numpy, arrays become first class citizens, treated on the same footing as the simpler numerical data types int and float. This is called vectorization.\nHere are a few examples of different vectorized functions we can call on A and B. All of these functions are done element-wise.\n\n\nCode\nA - B\n\n\narray([[ 13.2,   5.6,   0.2],\n       [ -3.6, -10.9,  -0.1],\n       [  0.3,   4.7,  -0.5],\n       [  3.3,  -6.4,   0. ],\n       [-15. ,  10. ,   0.5]])\n\n\n\n\nCode\nA / B\n\n\narray([[-0.36082474,  1.448     ,  3.        ],\n       [ 1.70588235,  0.22695035,  0.83333333],\n       [ 0.8125    ,  2.27027027,  0.28571429],\n       [ 2.43478261,  0.66839378,  1.        ],\n       [-0.82926829,  2.03092784,  3.5       ]])\n\n\n\n\nCode\nA ** B\n\n\narray([[5.27885788e-06, 5.25995690e+15, 8.86568151e-01],\n       [           nan, 1.32621732e+07, 6.59753955e-01],\n       [           nan, 2.62925893e+03, 3.24131319e-01],\n       [5.25814384e+01, 2.71882596e+21, 9.09532576e-01],\n       [           nan, 3.60016490e+12, 9.31149915e-01]])\n\n\n\n\nCode\nnp.sin(A)\n\n\narray([[-0.35078323, -0.68131377,  0.29552021],\n       [-0.66296923, -0.05837414,  0.47942554],\n       [-0.96355819,  0.85459891,  0.19866933],\n       [-0.63126664,  0.32747444,  0.78332691],\n       [-0.49411335,  0.75157342,  0.64421769]])\n\n\nIf vectorization just made code easier to read it would be a nice to have. But it’s more than this. In fact, vectorization also makes your code run much faster in many cases. Let’s see an example of this. I’ll again run the same operations above to add two arrays, but this time I’m going to profile the code in each case. That is, I’m going to time each operation over several runs and average the times. The ones with the lowest average time is faster than the slower one, obviously. To profile in a notebook, the easiest way is to use the %timeit magic command, which will do all this for you.\n\n\nCode\nA = A.tolist()\nB = B.tolist()\n%timeit C = add_arrays(A, B)\n\n\n2.61 µs ± 8.21 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\n\n\nCode\nA = np.array(A)\nB = np.array(B)\n%timeit C = A + B\n\n\n411 ns ± 0.75 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n\n\nEven with these small arrays the numpy vectorized array addition is almost 10 times faster than the python loop array addition. This difference becomes much more pronounced when arrays are larger. The arrays just considered are only of shape \\((10,3)\\). We can easily confirm this in numpy using the methods A.shape and B.shape.\n\n\nCode\nprint(f'A.shape = {A.shape}')\nprint(f'B.shape = {B.shape}')\n\n\nA.shape = (5, 3)\nB.shape = (5, 3)\n\n\nLet’s try to run the add operations on much larger arrays of shape \\((10000,100)\\). To do this quickly I’ll use np.random.rand(shape), which will sample an array with shape shape whose values are uniformly between 0 and 1. More on sampling in a future lesson. Running the profiling, we’re now running about 100 times faster using numpy vectorization compared to python loops.\n\n\nCode\nD = np.random.rand(10000, 100)\nE = np.random.rand(10000, 100)\n\n\n\n\nCode\nD = D.tolist()\nE = E.tolist()\n%timeit F = add_arrays(D, E)\n\n\n119 ms ± 517 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\n\n\nCode\nD = np.array(D)\nE = np.array(E)\n%timeit F = D + E\n\n\n1.35 ms ± 60.2 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n\n\nSo why is numpy vectorization so much faster than using native python loops? Because it turns out that numpy by and large doesn’t actually perform array operations in python! When array operations are done, numpy compiles them down to low-level C code and runs the operations there, where things are much faster.\nNot only that, numpy takes advantage of very efficient linear algebra functions written over the course of decades by smart people. These functions come from low-level FORTRAN and C libraries like BLAS and LAPACK. They’re hand-designed to take maximum advantage of computational speed-ups where available. These include things like parallelization, caching, and hardware vectorization operations. Native python doesn’t take advantage of any of these nice things. The moral is, if you want to run array operations efficiently, you need to use a numerical library like numpy or modern variants like pytorch.\n\n2.3.1 Higher-Dimensional Arrays\nThe number of different dimensions an array has is called its dimension or rank. Equivalently, the rank or dimension of an array is just the length of its shape tuple. The arrays I showed above are examples of rank-2 or 2-dimensional arrays. We can define arrays with any number of dimensions we like. These arrays of different rank sometimes have special names:\n\nA 0-dimensional (rank-0) array is called a scalar. These are single numbers.\nA 1-dimensional (rank-1) array is called a vector. These are arrays with only one row.\nA 2-dimensional (rank-2) array is called a matrix. These are arrays with multiple rows.\nAn array of dimension or rank 3 or higher is called a tensor. These are arrays with multiple matrices.\n\nMore on these when we get to linear algebra. Here are some examples so you can see what they look like. Note I’m using dtype=np.float64 to explicitly cast the values as float64 when defining the arrays. Numpy’s vectorization operations work for all of these arrays regardless of their shape.\n\n\nCode\nscalar = np.float64(5)\nscalar # 0-dimensional\n\n\n5.0\n\n\n\n\nCode\nvector = np.array([1, 2, 3], dtype=np.float64)\nprint(f'vector.shape = {vector.shape}')\nprint(f'vector = {vector}')\n\n\nvector.shape = (3,)\nvector = [1. 2. 3.]\n\n\n\n\nCode\nmatrix = np.array([[1, 2, 3], [4, 5, 6]], dtype=np.float64)\nprint(f'matrix.shape = {matrix.shape}')\nprint(f'matrix = \\n{matrix}')\n\n\nmatrix.shape = (2, 3)\nmatrix = \n[[1. 2. 3.]\n [4. 5. 6.]]\n\n\n\n\nCode\ntensor = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]], dtype=np.float64)\nprint(f'tensor.shape = {tensor.shape}')\nprint(f'tensor = \\n{tensor}')\n\n\ntensor.shape = (2, 2, 2)\ntensor = \n[[[1. 2.]\n  [3. 4.]]\n\n [[5. 6.]\n  [7. 8.]]]\n\n\nNumpy also supports array aggregation operations as well. Suppose you have a matrix A and want to get the sum of the values in each row of A. To do this, you could use np.sum(A, axis=1), where axis is the index of the dimension you want to sum over (the columns in this case). This will return a vector where the value at index \\(i\\) is the sum of elements in row \\(i\\). To sum over all elements in the array, don’t pass anything to axis.\n\n\nCode\nA = np.array([[1, 2, 3], [-1, -2, -3], [1, 0, -1]], dtype=np.float64)\nprint(f'A = \\n{A}')\nprint(f'sum over all A = {np.sum(A)}')\nprint(f'row sums of A = {np.sum(A, axis=1)}')\n\n\nA = \n[[ 1.  2.  3.]\n [-1. -2. -3.]\n [ 1.  0. -1.]]\nsum over all A = 0.0\nrow sums of A = [ 6. -6.  0.]\n\n\nIndexing into numpy arrays like A is more powerful than with python lists. Instead of having to awkwardly index like A[1][0], write A[1, 0]. To get all values in column index 1, write A[:, 1]. To get just the first and last row, we could just pass the index we want in as a list like this, A[[0, -1], :].\n\n\nCode\nprint(f'A[1, 0] = {A[1][0]} = {A[1, 0]}')\n\n\nA[1, 0] = -1.0 = -1.0\n\n\n\n\nCode\nprint(f'col 1 of A = {A[:, 1]}')\nprint(f'rows 0 and -1 of A = \\n{A[[0, -1], :]}')\n\n\ncol 1 of A = [ 2. -2.  0.]\nrows 0 and -1 of A = \n[[ 1.  2.  3.]\n [ 1.  0. -1.]]\n\n\nNumpy also supports Boolean masks as indexes. Suppose we want to get all the positive elements x >= 0 in A. We could create a mask A > 0, and pass that into A as an index to pick out the positive elements only.\n\n\nCode\nprint(f'mask of (A >= 0) = \\n{(A >= 0)}')\nprint(f'elements of (A >= 0) = \\n{A[A >= 0]}')\n\n\nmask of (A >= 0) = \n[[ True  True  True]\n [False False False]\n [ True  True False]]\nelements of (A >= 0) = \n[1. 2. 3. 1. 0.]"
  },
  {
    "objectID": "notebooks/numerical-computing.html#broadcasting",
    "href": "notebooks/numerical-computing.html#broadcasting",
    "title": "2  Numerical Computation",
    "section": "2.4 Broadcasting",
    "text": "2.4 Broadcasting\nBroadcasting is a set of conventions for doing array operations on arrays with incompatible shapes. This may seem like a strange thing to do, but it turns out knowing how and when to broadcast can make your code much shorter, more readable, and efficient. All modern-day numerical libraries in python support broadcasting, including numpy, pytorch, tensorflow, etc. So it’s a useful thing to learn.\n\n2.4.1 Motivation\nLet’s start with a simple example. Suppose we have an array of floats defined below. We’d like to add 1 to every number in the array. How can we do it? One “pythonic” way might be to use a list comprehension like so. This will work just fine, but it requires going back and forth between arrays and lists.\n\n\nCode\nx = np.array([1., 2., 3., 4., 5.])\nprint(f'x = {x}')\n\nx_plus_1 = np.array([val + 1 for val in x])\nprint(f'x + 1 = {x_plus_1}')\n\n\nx = [1. 2. 3. 4. 5.]\nx + 1 = [2. 3. 4. 5. 6.]\n\n\nWhat if we didn’t want to go back and forth like that? It is slow after all. Anytime numpy has to handoff back to python or vice versa it’s going to slow things down. Another thing we could try is to make a vector of ones of the same size as x, then add it to x. This is also fine, but it requires defining this extra array of ones just to add 1 to the original array.\n\n\nCode\nones = np.ones(len(x))\nx_plus_1 = x + ones\nprint(f'x + 1 = {x_plus_1}')\n\n\nx + 1 = [2. 3. 4. 5. 6.]\n\n\nWe’d like to be able to just add 1 to the array like we would with numbers. If x were a single number we’d just write x + 1 to add one to it, right? But technically we can’t do this if x is an array, since x has shape (5,) and 1 is just a number with no shape. This is where broadcasting comes in. Broadcasting says let’s define the operation x + 1 so that it means add 1 to every element of x.\n\n\nCode\nx_plus_1 = x + 1\nprint(f'x + 1 = {x_plus_1}')\n\n\nx + 1 = [2. 3. 4. 5. 6.]\n\n\nThis notation has the advantage of keeping array equations simple, while at the same time keeping all operations in numpy so that they run fast.\n\n\n2.4.2 Broadcasting Rules\nSuppose now that we have two arrays A and B of arbitrary shape and we want to operate on them, e.g. via the operations +, -, *, /, //, **. Here are the general broadcasting rules, quoted directly from the numpy documentation.\n\nNumpy DocumentationWhen operating on two arrays, numpy compares their shapes element-wise. It starts with the trailing (i.e. rightmost) dimensions and works its way left. Two dimensions are compatible when 1. they are equal, or2. one of them is 1 If these conditions are not met, a ValueError: operands could not be broadcast together exception is thrown, indicating that the arrays have incompatible shapes. The size of the resulting array is the size that is not 1 along each axis of the inputs.\n\nLet’s look at an example. First, suppose A has shape (2, 2, 3) and B has shape (3,). Suppose for simplicity that they’re both arrays of all ones. Here’s what this looks like, with B aligned to the right.\n\\[\\begin{align*}\nA &:& 2, & & 2, & & 3 \\\\\nB &:&   & &   & & 3 \\\\\n\\hline\nC &:& 2, & & 2, & & 3 \\\\\n\\end{align*}\\]\nHere are the broadcasting steps that will take place. Note that only B will change in this example. A will stay fixed.\n\nNumpy will start in the rightmost dimension, checking if they’re equal.\nBegin with A of shape (2, 2, 3) and B of shape (3,).\nIn this case, the rightmost dimension is 3 in both arrays, so we have a match.\nMoving left by one, B no longer has anymore dimensions, but A has two, each 2. These arrays are thus compatible.\nNumpy will now copy B to the left in these new dimensions until it has the same shape as A.\n\nCopy values of B twice to get B = [[1, 1, 1], [1, 1, 1]] with shape (2, 3).\nCopy values of B twice again to get B = [[[1, 1, 1], [1, 1, 1]], [[1, 1, 1], [1, 1, 1]]] with shape (2, 2, 3).\n\nThe shapes of A and B are now equal. The output array C will have shape (2, 2, 3).\n\nLet’s verify this is true on two simple arrays of ones. Let’s also print out what C looks like. Since only copying is taking place we should just be adding 2 arrays of ones, hence the output should sum 2 arrays of ones, giving one array C of twos.\n\n\nCode\nA = np.ones((2, 2, 3))\nB = np.ones(3,)\nprint(f'A.shape = {A.shape}')\nprint(f'B.shape = {B.shape}')\n\nC = A + B\nprint(f'C.shape = {C.shape}')\nprint(f'C = \\n{C}')\n\n\nA.shape = (2, 2, 3)\nB.shape = (3,)\nC.shape = (2, 2, 3)\nC = \n[[[2. 2. 2.]\n  [2. 2. 2.]]\n\n [[2. 2. 2.]\n  [2. 2. 2.]]]\n\n\nLet’s do one more example. Suppose now that A has shape (8, 1, 6, 1) and B has shape (7, 1, 5). Here’s a table of this case, again with B aligned to the right since it has the fewest dimensions.\n\\[\\begin{align*}\nA &:& 8, & & 1, & & 6, & & 1 \\\\\nB &:&    & & 7, & & 1, & & 5 \\\\\n\\hline\nC &:& 8, & & 7, & & 6, & & 5 \\\\\n\\end{align*}\\]\nHere are the broadcasting steps that will take place.\n\nStarting again from the right, dimensions 1 and 5 don’t match. But since A has a 1 rule (2) applies, so A will broadcast itself (i.e. copy its values) 5 times in this dimension to match B.\nMoving left by one we get 6 and 1. Now B will broadcast itself in this dimension 6 times to match A.\nMoving left again we get 1 and 7. Now A will broadcast itself in this dimension 7 times to match B.\nLast, we get 8 in A and B is out of dimensions, so B will broadcast itself 8 times to match A.\nThe shapes of A and B are now equal. The output C thus has shape (8, 7, 6, 5).\n\nHere again is an example on two arrays of ones. Verify that the shapes come out right.\n\n\nCode\nA = np.ones((8, 1, 6, 1))\nB = np.ones((7, 1, 5))\nprint(f'A.shape = {A.shape}')\nprint(f'B.shape = {B.shape}')\n\nC = A / B\nprint(f'C.shape = {C.shape}')\n\n\nA.shape = (8, 1, 6, 1)\nB.shape = (7, 1, 5)\nC.shape = (8, 7, 6, 5)\n\n\nThat’s pretty much all there is to broadcasting. It’s a systematic way of trying to copy the dimensions in each array until they both have the same shape. All this broadcasting is done under the hood for you when you try to operate on two arrays of different shapes. You don’t need to do anything but understand how the arrays get broadcast together so you can avoid errors in your calculations, sometimes very subtle errors.\nThis can be a bit confusing to understand if you’re not used to it. We’ll practice broadcasting a good bit so you can get the hang of it."
  },
  {
    "objectID": "notebooks/calculus.html#infinitesimal-calculus",
    "href": "notebooks/calculus.html#infinitesimal-calculus",
    "title": "3  Calculus",
    "section": "3.1 Infinitesimal Calculus",
    "text": "3.1 Infinitesimal Calculus\n\n3.1.1 Infinitesimals\nPerhaps most fundamental to what calculus is is the idea of an infinitely small number, usually called an infinitesimal. A positive infinitesimal is a hyperreal number \\(\\varepsilon\\) that’s much smaller than any positive real number \\(x\\),\n\\[0 < \\varepsilon \\ll x.\\]\nA negative infinitesimal is defined similarly, except with the signs all flipped, \\(x \\ll \\varepsilon < 0\\). I say “hyperreal number” because infinitesimals aren’t technically real numbers. They extend the number line in a sense. Thankfully we won’t really have to worry about this technical distinction in this book.\nFor practical purposes, think of infinitesimals as a really, really small numbers. When we say infinitesimal in practice, we usually mean a really small number like \\(\\varepsilon = 10^{-10}\\). This means \\(\\varepsilon^2 = 10^{-20}\\). If we take say \\(x=1\\), then \\(\\varepsilon=10^{-10}\\) is really small compared to \\(x\\), but \\(\\varepsilon^2 = 10^{-20}\\) is really really small compared to \\(x\\), so small that it might as well be zero,\n\\[\\varepsilon^2 \\approx 0.\\]\nA computer wouldn’t even be able to tell the difference really between \\(x\\) and \\(x + \\varepsilon^2\\).\n\n\nCode\nx = 1\nepsilon = np.float64(1e-10)\nprint(f'x = {x}')\nprint(f'epsilon = {epsilon}')\nprint(f'x + epsilon = {1 - epsilon}')\nprint(f'epsilon^2 = {epsilon ** 2}')\nprint(f'x + epsilon^2 = {1 - epsilon ** 2}')\n\n\nx = 1\nepsilon = 1e-10\nx + epsilon = 0.9999999999\nepsilon^2 = 1.0000000000000001e-20\nx + epsilon^2 = 1.0\n\n\n\n\n3.1.2 Infinitely Large Numbers\nThe inverse of an infinitesimal must evidently be a really big number \\(N = \\frac{1}{\\varepsilon}\\). These are called infinitely large numbers. Technically speaking, a positive infinitely large number is a hyperreal number that’s much larger than any positive real number \\(x\\),\n\\[0 \\leq x \\ll N.\\]\nSimilarly for infinitely large negative numbers, we’d have \\(N \\ll x < 0\\).\nIf in practice we’d think of an infinitesimal as a tiny number like \\(\\varepsilon = 10^{-10}\\), we’d think of an infinitely large number a really big number like \\(N=10^{10}\\). If \\(N\\) is really big, then \\(N^2\\) is really really big, so big that it might as well be infinity,\n\\[N^2 \\approx \\infty.\\]\nIf \\(x=1\\), then \\(x + N \\approx N\\). As far as a computer is concerned you can’t really even tell the difference.\n\n\nCode\nx = 1\nN = np.float64(1e10)\nprint(f'x = {x}')\nprint(f'N = {N}')\nprint(f'x + N = {1 + N}')\nprint(f'N^2 = {N ** 2}')\nprint(f'x + N^2 = {1 + N ** 2}')\n\n\nx = 1\nN = 10000000000.0\nx + N = 10000000001.0\nN^2 = 1e+20\nx + N^2 = 1e+20\n\n\n\n\n3.1.3 Limits\nFor historical reasons, most modern calculus books don’t talk about infinitesimals or infinitely large numbers at all. It’s not because they’re not rigorous, at least anymore. It mostly has to do with the history of calculus. For a long time people had a hard time rigorously pinning down what exactly these infinitely small numbers were. To get around this, in the 19th century many mathematicians started to think instead in terms of limits.\nIn the language of infinitesimals, we’d say that the limit of a function \\(y=f(x)\\) as \\(x\\) approaches some value \\(a\\) is the value \\(L\\) such that\n\\[L \\approx f(a + \\varepsilon),\\]\nwhere \\(\\varepsilon\\) is any infinitesimal added to \\(a\\). Said differently, \\(f(x) \\approx L\\) whenever \\(x \\approx a\\). Limits are usually written with the notation\n\\[\\lim_{x \\rightarrow a} f(x) = L.\\]\nIn the language of limits, we’d think of \\(x\\) being some value that gets closer and closer to \\(a\\), which then causes \\(f(x)\\) to get closer and closer to \\(L\\).\nI personally find that the language of limits tends to complicate understanding of calculus. Anything phrased in terms of limits pretty much can be more easily phrased in terms of infinitesimals. As such, you’ll see we don’t even really need limits anywhere in this book."
  },
  {
    "objectID": "notebooks/calculus.html#differential-calculus",
    "href": "notebooks/calculus.html#differential-calculus",
    "title": "3  Calculus",
    "section": "3.2 Differential Calculus",
    "text": "3.2 Differential Calculus\n\n3.2.1 Differentials and Derivatives\nTo make it easier to tell what’s an infinitesimal with respect to what, we often use the notation of differentials. If \\(x\\) is some variable, we’d denote any infinitesimal added to it by the symbol \\(dx\\), called the differential of \\(x\\). If \\(y=f(x)\\) is some function of \\(x\\), then the differential with respect to \\(y\\) is the amount that \\(y\\) changes if we change \\(x\\) to \\(x+dx\\),\n\\[dy = f(x+dx) - f(x).\\]\nIf it helps, you can think of \\(dx\\) as meaning “a little bit of \\(x\\)”, and \\(dy\\) as meaning “a little bit of \\(y\\)”.\nLet’s look at an example. Suppose \\(y=x^2\\). What is \\(dy\\)? Evidently, we’d have\n\\[dy = f(x + dx) - f(x) = (x+dx)^2 - x^2 = (x^2 + 2xdx + dx^2) - x^2 = 2xdx + dx^2.\\]\nThis is how much \\(y\\) changes if we change \\(x\\) by \\(dx\\). Now, if \\(dx\\) is infinitesimal, \\(dx^2 \\approx 0\\), which means\n\\[dy \\approx 2xdx.\\]\nNotice how if \\(dx\\) is infinitesimal, then \\(dy\\) will evidently be an infinitesimal too. If we change \\(x=1\\) by a really small amount \\(dx=10^{-10}\\), then we’d change \\(y=x^2=1\\) by \\(dy=2xdx=2 \\cdot 10^{-10}\\), which is also a really small change.\nThe ratio of these differentials says something about the rate that \\(y\\) changes if we change \\(x\\). It’s called the derivative of \\(y=x^2\\),\n\\[\\frac{dy}{dx} = 2x.\\]\nNotice that the derivative is not itself infinitesimal since all the differentials are on the left-hand side. It’s a real number, on the same scale as \\(x\\) and \\(y\\).\nOf course, we can calculate these things numerically too. We just have to be careful about floating point roundoff. Since we’re subtracting two numbers that are almost equal we’ll inevitably get a lot of roundoff if we make \\(dx\\) too small. When coding these things you have to be careful about how small you choose \\(dx\\). I’ll take \\(dx=10^{-5}\\) here. If the calculation were exact, we’d expect to get\n\\[dy=2xdx = 2 \\cdot 10^{-5} = 0.00002.\\]\nBut evidently we don’t get this exactly. In fact, since \\(dx^2 = 10^{-10}\\), we should expect to start seeing errors around 10 decimal places due to \\(dx\\) not being exactly infinitesimal, which is what we’re seeing here.\n\n\nCode\ndx = 1e-5\nx = 1\ny = 1 ** 2\ndy = (1 + dx) ** 2 - y\nprint(f'dy = {dy}')\nprint(f'dy/dx = {dy / dx}')\n\n\ndy = 2.0000100000139298e-05\ndy/dx = 2.00001000001393\n\n\nLet’s do another example. Suppose we have the function \\(y = x^3\\). Then we’d have\n\\[dy = f(x+dx) - f(x) = (x+dx)^3 - x^3 = (x^3 + 3x^2 dx + 3 x dx^2 + dx^3) - x^3 = 3x^2 dx + 3 x dx^2 + dx^3.\\]\nIf \\(dx\\) is infinitesimal, then \\(dx^2 \\approx 0\\), which also means \\(dx^3 \\approx 0\\). Thus, if \\(x\\) changes by a small amount \\(dx\\), then \\(y=x^3\\) changes by an amount\n\\[dy = 3x^2 dx.\\]\nThe derivative of \\(y=x^3\\) is again just the ratio of differentials,\n\\[\\frac{dy}{dx} = 3x^2.\\]\nHere’s a numerical calculation to verify this fact. I’ll again choose \\(x=1\\) and \\(dx=10^{-5}\\). We should expect to get \\(dy =3 \\cdot 10^{-5}\\) and \\(\\frac{dy}{dx} = 3\\). To within an error of about \\(dx^2=10^{-10}\\) it seems we do.\n\n\nCode\ndx = 1e-5\nx = 1\ny = 1 ** 3\ndy = (1 + dx) ** 3 - y\nprint(f'dy = {dy}')\nprint(f'dy/dx = {dy / dx}')\n\n\ndy = 3.0000300001109537e-05\ndy/dx = 3.000030000110953\n\n\nIn both examples, the amount that \\(y\\) changes if \\(x\\) changes by \\(dx\\) is evidently just the derivative times \\(dx\\),\n\\[dy = \\frac{dy}{dx} dx.\\]\nThe notation makes this fact look pretty trivial, since we can imagine canceling the \\(dx\\) terms on the right to get \\(dy\\). In fact, that’s exactly what we’re doing.\nIn general, if \\(y=f(x)\\) is some function that’s reasonably well-behaved at a point \\(x\\), then the differential of \\(y\\) at that point is\n\\[dy = f(x+dx) - f(x),\\]\nand the derivative of \\(y=f(x)\\) at that point is given by\n\\[\\frac{dy}{dx} = \\frac{f(x+dx) - f(x)}{dx}.\\]\nThis will only be exact when \\(dx\\) is infinitesimal, otherwise we’d have an error on the order of \\(dx^2\\). Notice that both the differential and the derivative are themselves functions of \\(x\\). For this reason, it’s common to think of \\(\\frac{d}{dx}\\) is some kind of derivative operator and write\n\\[\\frac{dy}{dx} = \\frac{d}{dx} f(x), \\quad \\text{or} \\quad \\frac{dy}{dx} = f'(x).\\]\nWhat exactly did I mean when I said the function \\(f(x)\\) needs to be “reasonably well behaved” at the point \\(x\\)? For one thing, the function needs to be continuous. Informally, you can think of \\(f(x)\\) as being continuous if you can draw its graph on a piece of paper without lifting your pen. There are no jumps or holes anywhere in the function’s graph. More formally, a function is continuous at \\(x\\) if \\(dy = f(x+dx) - f(x)\\) is infinitesimal whenever \\(dx\\) is infinitesimal.\nContinuity is just one condition necessary for \\(f(x)\\) to be differentiable. It also can’t be too jagged in some sense. Derivatives don’t make sense at points where there are kinks in the graph. In practice, however, this isn’t a huge problem. We can just extend the derivative to be what’s called a subderivative. With a subderivative, you can roughly speaking take whatever value for the derivative you want at these kinks and it won’t make a difference.\nHere’s a quick python function diff that can numerically calculate the derivative of some function \\(f(x)\\) at a point \\(x\\).\n\n\nCode\ndef diff(f, x, dx=1e-5):\n    dy = f(x + dx) - f(x)\n    return dy / dx\n\nf = lambda x: x ** 2\ndydx = diff(f, 1)\nprint(f'dy/dx = {dydx}')\n\n\ndy/dx = 2.00001000001393\n\n\nCalculating the derivative this way is called numerical differentiation. It turns out, you can reduce the error in numerical differentiation calculations a lot by centering the difference estimate for \\(dy\\),\n\\[dy=\\frac{f\\big(x+\\frac{dx}{2}\\big)-f\\big(x-\\frac{dx}{2}\\big)}{2}.\\]\nIt’s equivalent to the above function when \\(dx\\) is infinitesimal, but when it’s not it reduces the error by a factor of \\(dx\\). For more details, see here.\n\n\n3.2.2 Interpreting Derivatives\nSince \\(dy\\) is the change in \\(y\\) in response to the change in \\(x\\) by \\(dx\\), the derivative \\(\\frac{dy}{dx}\\) evidently represents some kind of a rate. It’s the rate that \\(y\\) changes in response to small changes in \\(x\\).\nDepending on what exactly \\(y\\) and \\(x\\) are we can think of the derivative as representing many different things. Here are some examples:\n\nIf \\(x\\) is the position of some particle and \\(t\\) is time, \\(\\frac{dx}{dt}\\) represents the speed or velocity of that particle at time \\(t\\).\nIf \\(p\\) is the pressure in the atmosphere at a height \\(z\\) from the Earth’s surface, then \\(\\frac{dp}{dz}\\) represents the pressure gradient, a measure of how much pressure varies with altitude.\nIf \\(q\\) is the quantity of some economic good and \\(p\\) is the price of that good, then \\(\\frac{dq}{dp}\\) represents the elasticity of that good, i.e. how responsive the good’s quantity is to changes in its price.\nIf \\(C\\) is the cost of some good and \\(q\\) is the amount of that good we have, then \\(\\frac{dC}{dq}\\) represents the marginal cost of that good. It’s the amount we’d have to pay to get one more unit of that good.\nIf \\(L\\) is the loss function of some machine learning algorithm and \\(\\theta\\) is some parameter of the model, then \\(\\frac{dL}{d\\theta}\\) represents the loss gradient, a measure of how sensitive the loss is to changes in that particular parameter.\n\nThe derivative also has a useful interpretation when thinking about a function graphically. It represents the slope of a function \\(y=f(x)\\) at the point \\(x\\). This follows from the fact that\n\\[dy = \\frac{dy}{dx} dx.\\]\nIf we relax the requirement that \\(dx\\) be infinitesimal, then it’s just the difference between two finite points \\(x - a\\), which means \\(dy\\) is also the difference between two points \\(f(x) - f(a)\\). If we plug these into the above formula and rearrange, it says\n\\[f(x) \\approx f(a) + \\frac{d}{dx} f(a) (x - a).\\]\nIf we think of \\(a\\) as some fixed point and \\(x\\) as a variable, this is just the equation for a line. It’s a line passing through the point \\((a, f(a))\\) with a slope \\(\\frac{d}{dx} f(a)\\). This line is called the tangent line of the function at the point \\(a\\). To see why let’s do a quick example.\nSuppose again that \\(y=x^2\\). If we take \\(a=1\\), then \\(f(a) = 1\\) and \\(\\frac{d}{dx} f(a) = 2a = 2\\), so we have an equation\n\\[y \\approx 1 + 2(x - 1) = 2x - 1.\\]\nLet’s plot this line along with the function itself to see what’s going on. Notice the tangent line is hugging the function at the red point \\(a=1, f(a)=1\\), and that the slope of the line seems to hug the curve at that point. This is what it means to say that the derivative is the slope of a function at a point. It’s the slope of the tangent line passing through that point.\n\n\nCode\nf = lambda x: x ** 2\ndfdx = lambda x: 2 * x\n\na = 1\nx = np.arange(-3, 3, 0.1)\nf_line = lambda x: f(a) + dfdx(a) * (x - a)\n\nplot_function(x, [f, f_line], points=[[a, f(a)]], xlim=(-3, 3), ylim=(-2, 4),\n              title=f'Tangent to $y=x^2$ at $a$={a}')\n\n\n\n\n\n\n\n\n\nNotice a curious fact from this slope interpretation. At the minimum value of the function, in this case at \\(x=0\\), the slope is flat, which evidently implies that the derivative at the minimum is zero. We’ll exploit this fact in a future lesson when we talk about optimization.\nAnother way to interpret the equation\n\\[f(x) \\approx f(a) + \\frac{d}{dx} f(a) (x - a)\\]\nis that the right-hand side is the best linear approximation of the function at the point \\(x=a\\). Provided \\(x \\approx a\\), we can well-approximate \\(f(x)\\) by its tangent line. Another way of saying the same thing is that if we zoom in enough on the plot we can’t tell that the function isn’t linear.\n\n\nCode\nplot_function(x, [f, f_line], points=[[a, f(a)]], xlim=(0.7, 1.3), ylim=(0.7, 1.3),\n              title=f'$y=x^2$ near $a=1$')\n\n\n\n\n\n\n\n\n\n\n\n3.2.3 Second Derivatives\nDifferentials and derivatives cover the situation when we’re interested in changes in a function that are first-order in \\(dx\\). But suppose we’re interested in changes that are second-order in \\(dx\\)? This leads us to the notion of second differentials. If \\(dx^2\\) is a second-order differential change in \\(x\\), then \\(d^2y = d(dy)\\) is a second-order differential change in \\(y=f(x)\\). Using the fact that \\(dy=f(x+dx)-f(x)\\), it’s not too hard to show that\n\\[d^2 y  = f(x+dx)-2f(x)+f(x-dx).\\]\nThe ratio of second differentials is called the second derivative of \\(y\\) with respect to \\(x\\),\n\\[\\frac{d^2 y}{dx^2} = \\frac{f(x+dx)-2f(x)+f(x-dx)}{dx^2}.\\]\nAs with the first derivative \\(\\frac{dy}{dx}\\), this equality will only be exact when \\(dx\\) is infinitesimal. Otherwise there will be an error of order \\(dx^3\\).\nLet’s do a quick example. Consider again the cubic function \\(y=x^3\\). I already showed its first derivative is the function \\(\\frac{dy}{dx} = 3x^2\\). Let’s see what its second derivative is. Evidently, for \\(d^2y\\) we have\n\\[\\begin{align*}\nd^2 y &= f(x+dx)-2f(x)+f(x-dx) \\\\\n&= (x+dx)^3 - 2x^3 + (x-dx)^3 \\\\\n&= (x^3 + 3x^2dx + 3xdx^2 + dx^3) - 2x^3 + (x^3 - 3x^2dx + 3xdx^2 - dx^3) \\\\\n&= 6xdx^2.\n\\end{align*}\\]\nDividing both sides by \\(dx^2\\), the second derivative of \\(y=x^3\\) is evidently just \\(\\frac{d^2 y}{dx^2} = 6x\\). We can actually see this in a simpler way. Notice that\n\\[\\frac{d^2 y}{dx^2} = \\frac{d}{dx} \\frac{dy}{dx}.\\]\nThat is, it’s just the derivative of the derivative. Since we already knew \\(dy = 3x^2dx\\) and \\(d(x^2)=2xdx\\), we could just have done\n\\[\\begin{align*}\nd^2 y = d(dy) &= d(3x^2dx) \\\\\n&= 3d(x^2)dx \\\\\n&= 3(2xdx)dx \\\\\n&= 6xdx^2. \\\\\n\\end{align*}\\]\nJust like the first derivative is a function of \\(x\\), so is the second derivative. For this reason we’d also often write it as\n\\[\\frac{d^2 y}{dx^2} = \\frac{d^2}{dx^2} f(x) = f''(x).\\]\nTo see how the second derivative arises we need to ask what happens if we want to approximate a function \\(y=f(x)\\) not just with a line, but with a parabola. It turns out that if we want to approximate the function with a parabola about some point \\(x=a\\) we’d use something like this,\n\\[f(x) \\approx f(a) + \\frac{d}{dx} f(a) (x - a) + \\color{red}{\\frac{1}{2} \\frac{d^2}{dx^2} f(a) (x - a)^2}.\\]\nThe new term is shown in red. It’s quadratic in \\(x-a\\) and proportional to the second derivative of \\(y=f(x)\\) at the point \\(a\\). We can imagine writing it in a form\n\\[y = A(x-a)^2 + B(x-a) + C.\\]\nThis gives the equation of a parabola with vertex at \\(\\big(a-\\frac{B}{2A}, \\big(a-\\frac{B}{2A}\\big)^2\\big)\\). Evidently, the curvature of the parabola is determined by the coefficient \\(A = \\frac{1}{2} \\frac{d^2}{dx^2} f(a)\\). This means that the second derivative is a measure of the function’s curvature around a point. The larger the second derivative is, the steeper the parabola at that point will be, and hence the steeper the function’s curvature will be around \\(x=a\\).\nHere’s an example. Suppose we have the function \\(y = x^3 + 5x^2\\), and we wanted to approximate the function with a parabola about the point \\(x=1\\). The approximating parabola turns out to be\n\\[y = 8(x - 1)^2 + 13(x - 1) + 6.\\]\nThe plot of the function and the approximating parabola is shown below.\n\n\nCode\nf = lambda x: x ** 3 + 5 * x ** 2\ndfdx = lambda x: 3 * x ** 2 + 10 * x\nd2fdx2 = lambda x: 6 * x + 10\n\na = 1\nx = np.arange(-50, 50, 0.1)\n\nf_parabola = lambda x: f(a) + dfdx(a) * (x - a) + 1/2 * d2fdx2(a) * (x - a) ** 2\n\nplot_function(x, [f, f_parabola], points=[[a, f(a)]], xlim=(-10, 10), ylim=(-100, 100),\n              title=f'')\n\n\n\n\n\n\n\n\n\nWhile this looks like a bad approximation when \\(x\\) is far away from \\(x=1\\), especially if \\(x\\) is negative, if we zoom in closer to \\(x=1\\) we see that it actually does a very good job of approximating the function locally. In the range \\([0.5, 1.5]\\) you can’t even really tell the function isn’t parabolic.\n\n\nCode\nplot_function(x, [f, f_parabola], points=[[a, f(a)]], xlim=(-1, 2), ylim=(-5, 30),\n              title=f'')\n\n\n\n\n\n\n\n\n\nThe sign of the second derivative at \\(x=a\\) says something about which way the function is curving. If the second derivative is positive, the function is curving upward around \\(x=a\\). If the second derivative is negative, the function is curving downward at \\(x=a\\). In the edge case where the second derivative is zero you can’t tell. You have to go to higher order terms and look at the sign of those.\nIf we interpret the first derivative as some kind of rate, then we’d have to interpret the second derivative as a rate of a rate, or an acceleration. For example, if \\(y\\) was the position of an object, then the first derivative \\(\\frac{dy}{dx}\\) would represent its speed or velocity, while the second derivative \\(\\frac{d^2y}{dx^2}\\) would represent the object’s acceleration at a given point in time.\nThere are higher-order differentials and derivatives as well. We can take third differentials, fourth differentials, and so on. Fortunately, these higher terms don’t really seem to show up in machine learning, so it’s not worth going into them.\nI’ll just briefly mention a nice fact. For many functions in practice, we can keep doing the kind of approximation I’ve been doing indefinitely for higher and higher derivatives. If we take infinitely many terms we get what’s called the Taylor Series Expansion of \\(y=f(x)\\) about \\(x=a\\),\n\\[f(x) = \\sum_{n=0}^\\infty \\frac{1}{n!} \\frac{d^n}{dx^n} f(a) (x-a)^n = f(a) + \\frac{d}{dx} f(a) (x - a) + \\frac{1}{2} \\frac{d^2}{dx^2} f(a) (x - a)^2 + \\frac{1}{6} \\frac{d^3}{dx^3} f(a) (x - a)^3 + \\cdots\\]\nBy taking more and more terms in the Taylor Series we can get a better and better approximation of a function about a point \\(x=a\\) by using higher and higher degree polynomials. Note that when infinitely many terms are used the equality becomes exact. It’s only approximate if we ignore higher-order terms.\n\n\n3.2.4 Differentiation Rules\nIt would be useful to have a systematic way to calculate differentials and derivatives without having to go back to the definition each time or always do it numerically. In this section I’ll show some rules that makes this kind of thing much easier.\nTo start with, I’ll list the differentials and derivatives of some common functions. I won’t torture you by deriving all of these one-by-one.\n\n\n\n\n\n\n\n\nFunction\nDifferential\nDerivative\n\n\n\\(y = 0\\)\n\\(dy = 0\\)\n\\(\\frac{dy}{dx} = 0\\)\n\n\n\\(y = 1\\)\n\\(dy = 0\\)\n\\(\\frac{dy}{dx} = 0\\)\n\n\n\\(y = x\\)\n\\(dy = dx\\)\n\\(\\frac{dy}{dx} = 1\\)\n\n\n\\(y = x^n\\)\n\\(dy = nx^{n-1}dx\\)\n\\(\\frac{dy}{dx} = nx^{n-1}\\)\n\n\n\\(y = \\sqrt{x}\\)\n\\(dy = \\frac{dx}{2\\sqrt{x}}\\)\n\\(\\frac{dy}{dx} = \\frac{1}{2\\sqrt{x}}\\)\n\n\n\\(y = \\frac{1}{x}\\)\n\\(dy = -\\frac{dx}{x^2}\\)\n\\(\\frac{dy}{dx} = -\\frac{1}{x^2}\\)\n\n\n\\(y = e^x\\)\n\\(dy = e^xdx\\)\n\\(\\frac{dy}{dx} = e^x\\)\n\n\n\\(y = \\log{x}\\)\n\\(dy = \\frac{dx}{x}\\)\n\\(\\frac{dy}{dx} = \\frac{1}{x}\\)\n\n\n\\(y = \\sin{x}\\)\n\\(dy = \\cos{x}dx\\)\n\\(\\frac{dy}{dx} = \\cos{x}\\)\n\n\n\\(y = \\cos{x}\\)\n\\(dy = -\\sin{x}dx\\)\n\\(\\frac{dy}{dx} = -\\sin{x}\\)\n\n\n\\(y = \\sigma(x)\\)\n\\(dy = \\sigma(x)\\big(1-\\sigma(x)\\big)dx\\)\n\\(\\frac{dy}{dx} = \\sigma(x)\\big(1-\\sigma(x)\\big)\\)\n\n\n\\(y = \\tanh(x)\\)\n\\(dy = \\big(1 - \\tanh^2(x)\\big)dx\\)\n\\(\\frac{dy}{dx} = 1 - \\tanh^2(x)\\)\n\n\n\\(y = \\text{ReLU}(x)\\)\n\\(dy = u(x)dx = [x \\geq 0]dx\\)\n\\(\\frac{dy}{dx} = u(x) = [x \\geq 0]\\)\n\n\n\nLet’s now look at some general rules for differentials and derivatives. The most fundamental rule is that these things are linear. That is, if \\(a, b\\) are constant real numbers and \\(u, v\\) are functions,\n\\[d(au + bv) = adu + bdv, \\quad \\text{and} \\quad \\frac{d(au+bv)}{dx} = a \\frac{du}{dx} + b \\frac{dv}{dx}.\\]\nIt’s pretty easy to see this. Suppose \\(f(x) = au(x) + bv(x)\\). Then we’d have\n\\[\\begin{align*}\nd(au + bv) &= f(x + dx) - f(x) \\\\\n&= \\big(au(x+dx) + bv(x+dx)\\big) - \\big(au(x) + bv(x)\\big) \\\\\n&= a \\big(u(x+dx) - u(x) \\big) + b \\big(v(x+dx) - v(x) \\big) \\\\\n&= adu + bdv.\n\\end{align*}\\]\nFor example, that if we have the function \\(y = 5\\sin x + 10x^2 - 4\\), then we can write\n\\[dy = d(5\\sin x + 10x^2 - 4) = 5d(\\sin x) + 10d(x^2) - 4d(1) = 5\\cos x dx + 20xdx + 0dx,\\]\nor dividing both sides by \\(dx\\),\n\\[\\frac{dy}{dx} = 5\\cos x + 20x.\\]\nIf we wanted, we could take the second derivative just as easily,\n\\[d^2y = d(5\\cos x \\cdot dx + 20xdx) = 5dx \\cdot d(\\cos x) + 20dx^2 = -5\\sin x \\cdot dx^2 + 20 dx^2,\\]\nwhich means\n\\[\\frac{d^2y}{dx^2} = -5\\sin x + 20.\\]\nSympy can do all this for you. You’ll first need to define a variable x, then define y as a function of x. Once you’ve done this, you can get the first derivative with y.diff(x), and the second derivative with y.diff(x).diff(x).\n\n\nCode\nx = sp.Symbol('x')\ny = 5 * sp.sin(x) + 10 * x ** 2  - 4\ndydx = y.diff(x)\nd2dx2 = y.diff(x).diff(x)\nprint(f'y = {y}')\nprint(f'dy/dx = {dydx}')\nprint(f'd2y/dx2 = {d2dx2}')\n\n\ny = 10*x**2 + 5*sin(x) - 4\ndy/dx = 20*x + 5*cos(x)\nd2y/dx2 = 20 - 5*sin(x)\n\n\nHere’s a more interesting example that shows we can take differentials of any equation. It need not even be a single-valued function \\(y=f(x)\\). Any function of the form \\(g(x,y)=0\\) will do. Suppose we have the equation \\(x^2+y^2=1\\). This is the equation for a circle of radius 1. Taking the differential of both sides and solving for \\(dy\\), we have\n\\[\\begin{align*}\nd(x^2 + y^2) &= d(1) \\\\\nd(x^2) + d(y^2) &= 0 \\\\\n2xdx + 2ydy &= 0 \\\\\ndy &= -\\frac{x}{y}dx.\n\\end{align*}\\]\nEvidently the slope of a circle changes like \\(\\frac{dy}{dx} = -\\frac{x}{y}\\). Since the differential of a constant is always zero, this will be true for a circle of any radius.\n\n\nCode\nf1 = lambda x: np.sqrt(1 - x ** 2)\nf2 = lambda x: -np.sqrt(1 - x ** 2)\ndfdx1 = lambda x: -x / f1(x)\ndfdx2 = lambda x: -x / f2(x)\n\na = 0.5\nya = f1(a)\nx = np.arange(-2, 2, 0.001)\nif ya >= 0:\n    f_line = lambda x: ya + dfdx1(a) * (x - a)\nelse:\n    f_line = lambda x: ya + dfdx2(a) * (x - a)\n\nplot_function(x, [f1, f2, f_line], points=[[a, ya]], xlim=(-2, 2), ylim=(-2, 2),\n              colors=['steelblue', 'steelblue', 'orange'], \n              title=f'Tangent to $x^2+y^2=1$ at ({round(a, 3)},{round(ya, 3)})')\n\n\n\n\n\n\n\n\n\nAnother useful rule that differentials and derivatives follow is the product rule. If \\(u\\) and \\(v\\) are two functions, then the differential of their product \\(y=uv\\) is\n\\[d(uv) = udv + vdu.\\]\nIt’s pretty easy to see that this is true as well. If we nudge \\(y\\) a little bit to \\(y+dy\\), then we necessarily must nudge \\(u\\) to some \\(u+du\\) and \\(v\\) to some \\(v+dv\\). This means we have\n\\[\\begin{align*}\ndy = d(uv) &=  (u+du)(v+dv) - uv \\\\\n&=  (uv + udv + vdu + dudv) - uv \\\\\n&\\approx  udv + vdu, \\\\\n\\end{align*}\\]\nwhere the last step follows provided \\(dudv \\approx 0\\), which will be true if both \\(du\\) and \\(dv\\) are infinitesimal. Dividing both sides by \\(dx\\) gives the derivative equivalent,\n\\[\\frac{d(uv)}{dx} = u\\frac{dv}{dx} + v\\frac{du}{dx}.\\]\nAs an example, suppose we had the function \\(y=x^2e^x\\). Setting \\(u=x^2\\) and \\(v=e^x\\), we’d have\n\\[dy = d(uv) = udv + vdu = x^2 d(e^x) + e^x d(x^2) = x^2 e^x dx + 2xe^x dx.\\]\nDividing both sides by \\(dx\\) then gives the derivative, \\(\\frac{dy}{dx} = x^2 e^x + 2xe^x\\).\n\n\nCode\nx = sp.Symbol('x')\ny = x ** 2 * sp.exp(x)\ndydx = y.diff(x)\nprint(f'y = {y}')\nprint(f'dy/dx = {dydx}')\n\n\ny = x**2*exp(x)\ndy/dx = x**2*exp(x) + 2*x*exp(x)\n\n\nSuppose we have a function \\(y=\\frac{u}{v}\\) that’s a quotient of two functions \\(u\\) and \\(v\\). To find \\(dy\\), can write this as \\(y=uv^{-1}\\) and use the product rule,\n\\[dy = d(uv^{-1}) = ud(v^{-1}) + v^{-1}du = -\\frac{udv}{v^2} + \\frac{du}{v} = \\frac{vdu - udv}{v^2}.\\]\nThis is called the quotient rule,\n\\[d\\bigg(\\frac{u}{v}\\bigg) = \\frac{vdu - udv}{v^2}.\\]\nAgain, dividing both sides by \\(dx\\) gives the quotient rule for derivatives,\n\\[\\frac{dy}{dx} = \\frac{v\\frac{du}{dx} - u\\frac{dv}{dx}}{v^2}.\\]\nAs an example, suppose we wanted to differentiate \\(y=\\frac{12e^x + 1}{x^2 + 1}\\). Taking \\(u=12e^x + 1\\) and \\(v=x^2 + 1\\), we’d have\n\\[dy = \\frac{vdu - udv}{v^2} = \\frac{(x^2 + 1)d(12e^x + 1) - (12e^x + 1)d(x^2 + 1)}{(x^2 + 1)^2} = \\frac{12e^x(x^2 + 1)dx - 2x(12e^x + 1)dx}{(x^2 + 1)^2}.\\]\nOr dividing both sides by \\(dx\\),\n\\[\\frac{dy}{dx} = \\frac{12e^x(x^2 + 1) - 2x(12e^x + 1)}{(x^2 + 1)^2}.\\]\n\n\nCode\nx = sp.Symbol('x')\ny = (12 * sp.exp(x) + 1) / (x ** 2 + 1)\ndydx = y.diff(x)\nprint(f'y = {y}')\nprint(f'dy/dx = {dydx}')\n\n\ny = (12*exp(x) + 1)/(x**2 + 1)\ndy/dx = -2*x*(12*exp(x) + 1)/(x**2 + 1)**2 + 12*exp(x)/(x**2 + 1)\n\n\nThe last rule I’ll mention is by far the most important one to know for machine learning purposes. It’s called the chain rule. Suppose we had a composite function \\(y = f(g(x))\\), or equivalently \\(y=g(z)\\) and \\(z=f(x)\\). Then taking differentials we’d get\n\\[dy = \\frac{dy}{dz} dz = \\frac{dy}{dz} \\frac{dz}{dx} dx.\\]\nDividing both sides by \\(dx\\) gives the derivative version,\n\\[\\frac{dy}{dx} = \\frac{dy}{dz} \\frac{dz}{dx}.\\]\nThe notation makes this look trivial since it seems like all we’re doing is multiplying and dividing by \\(dz\\). But the chain rule is a pretty deep fact. It says if we have a complicated function we can always break it up into easier composite functions, differentiate those, then multiply the results together.\nHere’s an example. Suppose we had a function \\(y = e^{-\\frac{1}{2} x^2}\\). This looks pretty complicated but it’s not. Notice we can think about this as a composition of the form \\(y = e^z\\) where \\(z = -\\frac{1}{2} x^2\\). Differentiating each of these on their is easy,\n\\[\\begin{align*}\ndy &= d(e^z) = e^z dz, \\\\\ndz &= d\\bigg(-\\frac{1}{2} x^2\\bigg) = -xdx. \\\\\n\\end{align*}\\]\nIf we want to differentiate \\(y\\) with respect to \\(x\\) we just need to multiply these two together and substitute in for \\(z\\),\n\\[dy = \\frac{dy}{dz} \\frac{dz}{dx} dx = e^z (-x) dx = -x e^{-\\frac{1}{2} x^2} dx,\\]\nor again dividing both sides by \\(dx\\),\n\\[\\frac{dy}{dx} = -x e^{-\\frac{1}{2} x^2}.\\]\n\n\nCode\nx = sp.Symbol('x')\ny = sp.exp(-sp.Rational(1, 2) * x ** 2)\ndydx = y.diff(x)\nprint(f'y = {y}')\nprint(f'dy/dx = {dydx}')\n\n\ny = exp(-x**2/2)\ndy/dx = -x*exp(-x**2/2)\n\n\nWhat really makes the chain rule powerful for machine learning is that we can use it on arbitrarily many composition chains. For example, if we have a composition of three functions \\(y=f(u)\\), \\(u=g(z)\\), \\(z=h(x)\\), we’d have\n\\[\\frac{dy}{dx} = \\frac{dy}{du} \\frac{du}{dz} \\frac{dz}{dx}.\\]\nMore generally, if we had a complicated function of \\(n\\) compositions\n\\[\\begin{align*}\ny_0 &= x, \\\\\ny_1 &= f_1(y_0), \\\\\ny_2 &= f_2(y_1), \\\\\n\\vdots & \\qquad \\vdots \\\\\ny_n &= f_n(y_{n-1}), \\\\\ny &= y_n, \\\\\n\\end{align*}\\]\nthe chain rule would say\n\\[\\frac{dy}{dx} = \\prod_{i=1}^n \\frac{dy_i}{dy_{i-1}} = \\frac{dy}{dy_{n-1}} \\frac{dy_{n-1}}{dy_{n-2}} \\cdots \\frac{dy_2}{dy_1} \\frac{dy_1}{dx}.\\]\nThis fact is fundamental to how a neural network is trained. At each step of training a neural network, we have to backpropagate through the neural network to calculate \\(\\frac{dy}{dx}\\). All backpropogation is is the chain rule, where each term in the chain is a layer in the neural network. It’s probably no exaggeration to say that deep learning works as well as it does because of the ability of the chain rule to make calculating the derivative of complicated composite functions easy.\nThe last rule I’ll mention is a method to calculate the derivative of an inverse function. Suppose \\(y=f(x)\\). If the function is invertible, we can solve for \\(x\\) and get an inverse function \\(x = f^{-1}(y)\\). The inverse rule says if we know the derivative of one of these we can easily get the derivative of the other by inverting it,\n\\[\\frac{dx}{dy} = \\frac{1}{\\frac{dy}{dx}}.\\]\nAgain, the notation makes this look pretty trivial. What’s not said is that in going from one to the other we have to be careful what’s a function of what. On the left-hand side, \\(x\\) is a function of \\(y\\), but on the right-hand side \\(y\\) is a function of \\(x\\).\nFor example, suppose we had the function \\(y=e^x\\) and for some reason wanted to know \\(\\frac{dx}{dy}\\). What we can do is calculate \\(\\frac{dy}{dx} = e^x\\), and invert it to get \\(\\frac{dx}{dy}\\),\n\\[\\frac{dx}{dy} = \\frac{1}{\\frac{dy}{dx}} = \\frac{1}{e^x} = \\frac{1}{y}.\\]\nOf course, this is just the derivative of the logarithm function \\(x = \\log y\\), which makes sense since \\(x=\\log y\\) is the inverse of \\(y=e^x\\).\nHere’s a summary table of all these rules for reference. I’ll state them in derivative form and provide an example of each one for later reference.\n\n\n\n\n\n\n\n\nName\nRule\nExample\n\n\nLinear Rule\n\\(\\frac{d}{dx}(au + bv) = a\\frac{du}{dx} + b\\frac{dv}{dx}\\)\n\\(\\frac{d}{dx}(2x^2 + 5\\log x) = 2\\frac{d}{dx}x^2 + 5\\frac{d}{dx}\\log x = 4x + \\frac{5}{x}\\)\n\n\nProduct Rule\n\\(\\frac{d}{dx}(uv)=u\\frac{dv}{dx} + v\\frac{du}{dx}\\)\n\\(\\frac{d}{dx}(x e^x) = x \\frac{d}{dx}e^x + e^x \\frac{d}{dx} x = xe^x + e^x\\)\n\n\nQuotient Rule\n\\(\\frac{d}{dx}\\big(\\frac{u}{v}\\big) = \\frac{v\\frac{du}{dx}-u\\frac{dv}{dx}}{v^2}\\)\n\\(\\frac{d}{dx} \\frac{\\cos x}{x^2} = \\frac{x^2\\frac{d}{dx}\\cos x-\\cos x\\frac{d}{dx}x^2}{(x^2)^2} = \\frac{-x^2 \\sin x - 2x \\cos x}{x^4}\\)\n\n\nChain Rule\n\\(\\frac{dy}{dx} = \\frac{dy}{dz}\\frac{dz}{dx}\\)\n\\(\\frac{d}{dx} e^{\\sin x} = \\frac{d}{dy} e^y \\frac{d}{dx}\\sin x = e^{\\sin x} \\cos x\\)\n\n\nInverse Rule\n\\(\\frac{dx}{dy} = \\frac{1}{\\frac{dy}{dx}}\\)\n\\(y = 5x + 1 \\quad \\Longrightarrow \\quad \\frac{dx}{dy} = \\frac{1}{\\frac{dy}{dx}} = \\frac{1}{5}\\)"
  },
  {
    "objectID": "notebooks/calculus.html#integral-calculus",
    "href": "notebooks/calculus.html#integral-calculus",
    "title": "3  Calculus",
    "section": "3.3 Integral Calculus",
    "text": "3.3 Integral Calculus\nBy far, differential calculus is the most important area of calculus to know for machine learning purposes. The main reason for this is that in machine learning we seek to train models by optimizing functions, and (as I’ll cover in a future lesson) optimization is all about taking derivatives.\nNevertheless, integral calculus is the giant other half of calculus, so I owe it to at least briefly cover the subject. Knowing a little about integration will also make it easier to understand the later lessons on probability theory and distributions.\nWe can think about integral calculus in two distinct ways that turn out to be linked together by an important theorem. One way of thinking about an integral is as an inverse operation that undoes differentiation. The other way of thinking about an integral is as sum of a bunch of infinitesimal segments.\n\n3.3.1 Indefinite Integration\nSuppose we knew that some function \\(F(x)\\) had a derivative of \\(f(x)\\), i.e.\n\\[f(x) = \\frac{d}{dx} F(x).\\]\nIf we only knew \\(f(x)\\), how would we go about “undoing” the differentiation to get back \\(F(x)\\)? Essentially what we’d need to do is work backward.\nFor example, suppose we knew \\(f(x)=x\\). How could we find \\(F(x)\\)? Well, we already know that \\(\\frac{d}{dx} x^2 = 2x\\). If we divide both sides of this equation by \\(2\\) we’ve evidently got \\(F(x)\\), i.e. \\(F(x) = \\frac{1}{2} x^2\\). Strictly speaking, this is only one possible solution. We could add any constant to \\(F(x)\\) and not change the answer since the derivative of a constant is zero.\nThe function \\(F(x)\\) is called the indefinite integral of \\(f(x)\\). In textbooks it’s sometimes also called an antiderivative since it undoes the derivative operation. For reasons I’ll explain soon, we usually write the indefinite integral using a funny notation,\n\\[F(x) = \\int f(x) dx.\\]\nFor example, if \\(f(x) = x\\), we’d write\n\\[\\int x dx = \\frac{1}{2} x^2.\\]\nThe function on the right is the \\(F(x)=\\frac{1}{2} x^2\\) that undoes the function inside the integral, namely \\(f(x)=x\\).\nAs far as interpreting what an indefinite integral is, if you think of \\(f(x)\\) as the rate of some thing, then \\(F(x)\\) tells you the aggregate total of that thing. For example, if \\(f(x)\\) was the speed of an object as a function of time, then its integral \\(F(x)\\) would tell you how far that object has moved as a function of time. If \\(f(x)\\) was the demand of some good as a function of price, then \\(F(x)\\) would be the consumer surplus of that good, or how much consumers benefit from being able to buy that good for cheaper than they were willing to pay.\nWe could go through and figure out what the indefinite integral is one-by-one for the common functions we’ve seen so far, but I’ll spare you the detail. They’re given in the table below.\n\n\n\n\n\n\n\nFunction\nIntegral\n\n\n\\(y = 0\\)\n\\(\\int y dx = 0\\)\n\n\n\\(y = 1\\)\n\\(\\int y dx = x\\)\n\n\n\\(y = x\\)\n\\(\\int y dx = \\frac{1}{2}x^2\\)\n\n\n\\(y = \\sqrt{x}\\)\n\\(\\int y dx = \\frac{2}{3} x^{3/2}\\)\n\n\n\\(y = \\frac{1}{x}\\)\n\\(\\int y dx = \\log{x}\\)\n\n\n\\(y = x^n\\) where \\(n \\neq -1\\)\n\\(\\int y dx = \\frac{1}{n+1}x^{n+1}\\)\n\n\n\\(y = e^x\\)\n\\(\\int y dx = e^x\\)\n\n\n\\(y = \\log{x}\\)\n\\(\\int y dx = x \\log{x} - x\\)\n\n\n\\(y = \\sin{x}\\)\n\\(\\int y dx = -\\cos{x}\\)\n\n\n\\(y = \\cos{x}\\)\n\\(\\int y dx = \\sin{x}\\)\n\n\n\nBy construction, the indefinite integral is the inverse operation of the derivative operation. This means they cancel each other,\n\\[d\\bigg(\\int f(x) dx\\bigg) = f(x) dx, \\quad \\text{and} \\quad \\int dF(x) = \\int \\frac{d}{dx} F(x) dx = F(x).\\]\nThis fact is sometimes called the Leibniz Rule. It means that the indefinite integral inherits many of the properties of the derivative. Most importantly, it’s linear. If \\(a\\) and \\(b\\) are constants, and \\(u\\) and \\(v\\) are two functions, then\n\\[\\int \\big(au + bv\\big) dx = a \\int u dx + b \\int v dx.\\]\nFor example, if \\(y = -x^3 + 7\\sin x\\), we’d have\n\\[\\int y dx = \\int (-x^3 + 7\\sin x) dx = - \\int x^3 dx + 6 \\int \\sin x dx = -\\frac{1}{4} x^4 - 6 \\cos x.\\]\nAgain, you could technically add a constant to this answer and not change the result, but it’s not typical to do this, at least outside of calculus textbooks.\nAs with differentiation, sympy can calculate these things for you. Just define x and y, then calculate the indefinite integral with y.integrate(x).\n\n\nCode\nx = sp.Symbol('x')\ny = -x ** 3 + 7 * sp.sin(x)\nint_y = y.integrate(x)\nprint(f'y = {y}')\nprint(f'int y dx = {int_y}')\n\n\ny = -x**3 + 7*sin(x)\nint y dx = -x**4/4 - 7*cos(x)\n\n\nThe integral version of the product rule is called integration by parts. If we start with the product rule,\n\\[d(uv) = udv + vdu,\\]\nand integrate both sides, we get\n\\[\\int d(uv) = \\int udv + \\int vdu.\\]\nSince \\(uv = \\int d(uv)\\), we can re-arrange terms to get the rule for integration by parts,\n\\[\\int u dv = uv - \\int v du.\\]\nActually using integration by parts to calculate things isn’t often as straight forward as the product rule. To make integration by parts work in practice, you want to try to choose \\(u\\) and \\(v\\) so the right-hand side is easier to integrate than the left-hand side.\nAs a quick example, suppose we wanted to integrate \\(y = x e^x\\). What we could do is take \\(u = x\\) and \\(v=e^x\\), so \\(du=dx\\) and \\(dv = e^x dx\\). Then we’d have\n\\[\\int x e^x dx = x e^x - \\int e^x dx = x e^x - e^x.\\]\n\n\nCode\nx = sp.Symbol('x')\ny = x * sp.exp(x)\nint_y = y.integrate(x)\nprint(f'y = {y}')\nprint(f'int y dx = {int_y}')\n\n\ny = x*exp(x)\nint y dx = (x - 1)*exp(x)\n\n\nThough less widely known and rarely covered in textbooks for some reason, there’s also an integration by parts for the quotient rule. Using the quotient rule\n\\[d\\bigg( \\frac{u}{v} \\bigg) = \\frac{vdu - udv}{v^2},\\]\nand integrating both sides, we get\n\\[\\int d\\bigg( \\frac{u}{v} \\bigg) = \\int \\frac{vdu - udv}{v^2} = \\int \\frac{1}{v}du + \\int \\frac{u}{v^2}dv.\\]\nSince \\(\\frac{u}{v} = \\int d\\big(\\frac{u}{v}\\big)\\) we can again rearrange terms to get\n\\[\\int \\frac{1}{v} du = \\frac{u}{v} + \\int \\frac{u}{v^2} du.\\]\nJust as with the product integration by parts, using this in practice often involves reducing a complicated integral on the left-hand side to a less complicated integral on the right-hand side.\nThe integral version of the chain rule is called change of variables or substitution. Suppose we had a function \\(y=f(x)\\) and wanted to integrate it. One thing we could do is make a change of variable by letting \\(x\\) be a function of some other variable \\(u\\), say \\(x = g(u)\\). If we multiply and divide \\(f(x)dx\\) by \\(du\\), then\n\\[f(x) dx = f(g(u)) \\frac{dx}{du} du.\\]\nIntegrating both sides gives the change of variables formula,\n\\[\\int f(x) dx = \\int f(g(u)) \\frac{dx}{du} du.\\]\nThe left-hand side is integrated with respect to \\(x\\), while the right-hand side is integrated with respect to \\(u\\). Just like with integration by parts, using the change of variables formula in practice is more an art than a science. The tricky part is figuring out what \\(x=g(u)\\) should be to make the integral easier to compute.\nFor example, suppose we wanted to integrate the function \\(y = x e^{x^2}\\). What we could do is let \\(u=x^2\\). Then \\(du=2xdx\\), and so we’d have\n\\[\\int x e^{x^2} dx = \\int x e^{u} \\frac{1}{2x} du = \\frac{1}{2} \\int e^u du = \\frac{1}{2} e^u = \\frac{1}{2} e^{x^2}.\\]\n\n\nCode\nx = sp.Symbol('x')\ny = x * sp.exp(x ** 2)\nint_y = y.integrate(x)\nprint(f'y = {y}')\nprint(f'int y dx = {int_y}')\n\n\ny = x*exp(x**2)\nint y dx = exp(x**2)/2\n\n\nHere’s a summary table of the integral rules I’ve covered so far.\n\n\n\n\n\n\n\n\nName\nRule\nExample\n\n\nLeibniz Rule\n\\(\\frac{d}{dx} \\int y dx = y\\)\n\\(\\frac{d}{dx} \\int \\sin t dt = \\sin x\\)\n\n\nLinear Rule\n\\(\\int (au + bv) dx = a\\int u dx + b\\int v dx\\)\n\\(\\int (-1 + 5e^x) dx = -\\int 1 dx + 5\\int e^x dx = -x + 5e^x\\)\n\n\nIntegration By Parts (product version)\n\\(\\int u dv = uv - \\int v du\\)\n\\(\\int x e^x dx = \\int x d(e^x) = x e^x - \\int e^x dx = x e^x - e^x\\)\n\n\nIntegration By Parts (quotient version)\n\\(\\int \\frac{1}{v} du = \\frac{u}{v} + \\int \\frac{u}{v^2} dv\\)\n\\(\\int \\frac{\\sin \\sqrt{x}}{x^2} dx = \\frac{2\\cos x^{-1/2}}{x^{1/2}} + \\int x^{3/2} \\cos x^{-1/2}dx = \\frac{2\\cos x^{-1/2}}{x^{1/2}} - 2 \\sin x^{-1/2}\\)\n\n\nChange of Variables\n\\(\\int f(x) dx = \\int f(g(u)) \\frac{dx}{du} dx\\)\n\\(\\int x e^{x^2} dx = \\int e^{x^2} d\\big(\\frac{1}{2}x^2\\big) = \\frac{1}{2} \\int e^u du = \\frac{1}{2} e^u = \\frac{1}{2} e^{x^2}\\)\n\n\n\nBefore moving on, I’ll mention an important fact. While we can always take the derivative of a function and get a symbolic answer in terms of functions we’re used to, this is not always possible with integrals. In fact, most integrals we can write down we can’t evaluate using the methods covered here. There’s no way to find an explicit \\(F(x)\\) whose derivative is \\(f(x)\\) in terms of simple functions. A classic example of this is the Gaussian function\n\\[y = e^{-x^2}.\\]\nThere’s no way you’d be able to integrate this symbolically and get a closed form solution in terms of simple functions. Unfortunately, this is the rule, not the exception. This fact often makes integration a lot harder than differentiation in practice. Fortunately, we can always evaluate an integral numerically, but I’ll need to discuss the definite integral first so we can understand what that even means.\n\n\n3.3.2 Definite Integration\nI said that integral calculus essentially consists of two things, finding functions that undo a derivative, and summing up infinitesimals. The first part I just covered, the indefinite integral. I’ll now discuss the second part, and how it can be linked with the definite integral via the Fundamental Theorem of Calculus.\nSuppose now we had a function \\(y=f(x)\\) and we wanted to compute the area under this curve between two endpoints \\(x=a\\) and \\(x=b\\). So we have a working example, suppose the function is \\(y=\\sqrt{x}\\) and we want to find the area under its curve from \\(x=0\\) to \\(x=10\\).\n\n\nCode\nf = lambda x: np.sqrt(x)\nx = np.linspace(0, 10, 100)\na, b = 0, 10\nplot_function_with_area(x, f, a=a, b=b, title='$y=\\sqrt{x}$ on $[0,10]$')\n\n\n\n\n\n\n\n\n\nIf you had to estimate the area of something like this with no prior knowledge, what you might do is try to approximate the area by using a shape you already know how to find the area of. Perhaps the easiest thing to try would be a rectangle. Suppose we took a rectangle whose base was length \\(10\\) and whose height was \\(\\sqrt{10}\\). Then very roughly speaking we could claim the area \\(A\\) under the curve is\n\\[A \\approx 10 \\cdot \\sqrt{10} \\approx 31.62.\\]\nBut this actually isn’t a great estimate. Here’s what we just calculated the area of. Notice how much area above the curve we’re taking with this estimate. We’re over-estimating the true area by a good bit it seems. How can we improve this estimate?\n\n\nCode\nplot_approximating_rectangles(x, f, dx=10, title='$A \\\\approx$ 1 Rectangle', alpha=0.7)\n\n\nApproximate Area: 31.622776601683796\n\n\n\n\n\n\n\n\n\nOne thing we could do is use not one rectangle, but two rectangles. Suppose we took each rectangle to have half the width of the curve, say \\(dx=5\\). The left rectangle can have height \\(\\sqrt{5}\\), and the right can have height \\(\\sqrt{10}\\). If we sum up these two areas, we’d get\n\\[A \\approx 5 \\cdot \\sqrt{5} + 5 \\cdot \\sqrt{10} \\approx 26.99\\]\nHere’s what this approximation looks like. It’s clearly a lot better, but we’re still over-estimating the area by a decent bit.\n\n\nCode\nplot_approximating_rectangles(x, f, dx=5, title='$A \\\\approx $ 2 Rectangles', alpha=0.7)\n\n\nApproximate Area: 26.991728188340847\n\n\n\n\n\n\n\n\n\nIt seems like we can keep going with this strategy though. Let’s try \\(N=10\\) rectangles each of width \\(dx=1\\). Call the areas of these \\(N\\) rectangles in order \\(A_0, A_1, \\cdots, A_{N-1}\\). Each area will be a width \\(dx\\) times a height \\(y_i=\\sqrt{i+1}\\). Thus, we have\n\\[\\begin{align*}\nA \\approx \\sum_{i=0}^{N-1} A_i &= \\sum_{i=0}^{N-1} y_i dx \\\\\n&= y_0dx + y_1dx + y_2dx + \\cdots + y_9dx \\\\\n&= \\big(\\sqrt{1} + \\sqrt{2} + \\sqrt{3} + \\cdots + \\sqrt{10}\\big)\\cdot 1 \\\\\n&\\approx 22.468\n\\end{align*}\\]\nHere’s what this approximation looks like. It’s clearly a whole lot better. Notice how little extra area is left above the curve.\n\n\nCode\nplot_approximating_rectangles(x, f, dx=1, title='$A \\\\approx $ 10 Rectangles', alpha=0.7)\n\n\nApproximate Area: 22.468278186204103\n\n\n\n\n\n\n\n\n\nNow, it’s fair to ask if this approximation will ever become exact. Can we eventually take enough rectangles such that \\(A = \\sum y_i dx\\) exactly? In fact we can. The trick is to allow the rectangle widths \\(dx\\) to become infinitesimally thin. If we do that, while taking \\(N = \\lfloor \\frac{b-a}{dx} \\rfloor\\) rectangles, then \\(N\\) will become infinitely large and we’ll have an exact equality\n\\[A = \\sum_{i=0}^{N-1} y_i dx.\\]\nHere’s what this might look like. Notice how for all practical purposes it looks like we’re calculating the area of the curve exactly now.\n\n\nCode\nplot_approximating_rectangles(x, f, dx=0.1, title='$A \\\\approx N$ Rectangles', alpha=0.7, print_area=False)\n\n\n\n\n\n\n\n\n\nWhat I’ve just shown is that the exact area under the curve of a function \\(y=f(x)\\) is the sum of an infinitely large number of rectangles of height \\(y_i=f(x_i)\\) and infinitesimal width \\(dx\\). For historical reasons we usually write a sum of infinitely many infinitesimals using a special notation,\n\\[\\int_a^b y dx = \\sum_{i=0}^{N-1} y_i dx.\\]\nThis is called the definite integral of the function \\(y=f(x)\\) from \\(x=a\\) to \\(x=b\\). Historically, the reason the \\(\\int\\) was chosen is because it looks kind of like the S in “summa”, the Latin word for “sum”. This makes it clear that an integral is just a sum, a sum of infinitely many infinitesimal quantities \\(ydx\\).\nOf course, this says nothing about how to actually calculate these things. We’ve got nothing to go on but the definition. For numerical purposes that’s good enough. If we wanted to calculate a definite integral numerically, we could just take a bunch of really small rectangles like this and sum them up.\nBelow I’ll define a function called integrate that will take in the function f and the endpoints a and b, and return the definite integral. Since it’s just an area, the definite integral will always be a numerical value. I’ll default dx = 1e-4 to give a reasonable tradeoff between accuracy and speed.\nI’ll use this function to estimate the area of our running example, namely\n\\[A = \\int_0^{10} \\sqrt{x} dx.\\]\nEvidently, the exact area in this case seems to be about \\(A \\approx 21.082\\).\n\n\nCode\ndef integrate(f, a, b, dx=1e-4):\n    N = int((b - a) / dx)\n    interval = np.cumsum(dx * np.ones(N))\n    rectangles = np.array([f(x - dx/2) * dx for x in interval])\n    integral = np.sum(rectangles)\n    return integral\n\nf = lambda x: np.sqrt(x)\na, b = 0, 10\narea = integrate(f, a, b)\nprint(f'A = {area}')\n\n\nA = 21.081851128609895\n\n\nThis method of computing an integral is called numerical integration. We can actually improve the integrate function a lot by using \\(f\\big(x - \\frac{dx}{2}\\big) dx\\) for the rectangle areas instead of \\(f(x)dx\\). This is sometimes called the midpoint rule. It’s similar to differentiation, in that we improve convergence a lot by taking the midpoint function value instead of the right-most function value. As with differentiation, this centering trick reduces the error estimate by a factor of \\(dx\\). Doing this will allow you to use a much larger \\(dx\\) value to get the same desired accuracy.\nWhile it may not be obvious, the formula I gave to calculate the area under the curve actually calculates what’s called a signed area. That is, it counts area as positive when the curve is above the x-axis, and as negative when the curve is below the x-axis. To see this, notice if we took \\(y=-\\sqrt{x}\\), we’d have\n\\[\\int_0^{10} y dx = - \\int_0^{10} \\sqrt{x} dx \\approx -21.082.\\]\nWhy? Because this curve lies below the x-axis on the interval \\(0 \\leq x \\leq 10\\). This means the definite integral doesn’t calculate the area per se. It subtracts the area above the x-axis with the area below the x-axis.\nWhile we can always calculate definite integrals numerically, and in practice this is what you’d usually do, we can also derive a formula to calculate them exactly in many cases. This formula is important because it’s what links together the indefinite integral with the definite integral, i.e. undoing the derivative with summing up infinitesimals areas. It’s called the Fundamental Theorem of Calculus.\nSuppose we had a definite integral \\(F(x) = \\int f(x) dx\\) whose derivative is \\(f(x)\\). If we wanted to evaluate the definite integral of \\(f(x)\\) from \\(x=a\\) to \\(x=b\\), we could use the formula\n\\[\\int_a^b f(x) dx = F(b) - F(a) = F(x) \\ \\bigg|_{x=a}^{x=b}.\\]\nThe notation \\(F(x) \\big|_{x=a}^{x=b}\\) is just a short-hand way of writing \\(F(b)-F(a)\\). This formula says that all we’d need to do is calculate the definite integral at the endpoints and subtract them to get the area under the curve of \\(f(x)\\) in the region \\(a \\leq x \\leq b\\).\nSince I’m already covering more than I need to for machine learning purposes, I won’t prove this fact. I’ll just show that it’s true by using it to calculate the previous example and verifying that it agrees with the numerical answer. In that case, we have\n\\[\\begin{align*}\n\\int_0^{10} \\sqrt{x} dx &= \\int_0^{10} x^{1/2} dx \\\\\n&= \\frac{x^{3/2}}{3/2} \\ \\bigg|_{x=0}^{x=10} \\\\\n&= \\frac{2}{3}x^{3/2} \\ \\bigg|_{x=0}^{x=10} \\\\\n&= \\frac{2}{3} 10^{3/2} - \\frac{2}{3} 0^{3/2} \\approx 21.082. \\\\\n\\end{align*}\\]\nYou can also use sympy to calculate definite integrals by using the method y.integrate((x, a, b)).\n\n\nCode\nx = sp.Symbol('x')\ny = sp.sqrt(x)\nA = y.integrate((x, 0, 10))\nprint(f'A = {A} ≈ {A.round(3)}')\n\n\nA = 20*sqrt(10)/3 ≈ 21.082\n\n\nA couple more rules for the definite integral I’ll mention involve the endpoints \\(a\\) and \\(b\\), called the limits of integration. Since all we’re doing is summing up rectangles, we can always split up a sum over the interval \\([a,b]\\) into a sum over two subintervals \\([a,c]\\) and \\([c,b]\\). That is, if \\(y=f(x)\\), then\n\\[\\int_a^b y dx = \\int_a^c y dx + \\int_c^d y dx.\\]\nWe can also swap the limits of integration if we agree on the convention that doing so flips the sign of the integral,\n\\[\\int_a^b y dx = -\\int_b^a y dx.\\]\nWe can even have either limit be infinite, provided that it makes sense to integrate the function over an infinite interval. We can do so as long as the output isn’t a NaN value like \\(\\infty - \\infty\\). For technical reasons these types of integrals are sometimes called improper integrals.\nFor example, suppose we had the function \\(y=e^{-x}\\). We can integrate this function from \\(a=0\\) to \\(b=\\infty\\) just fine. First, notice that taking a change of variables \\(u=-x\\) we get \\(du=-dx\\), so\n\\[\\int e^{-x} dx = -\\int e^u du = -e^{u} = -e^{-x}.\\]\nThen the definite integral is just\n\\[\\int_0^\\infty e^{-x} dx = -e^{-x} \\ \\bigg |_{x=0}^{x=\\infty} = -e^{-\\infty} + e^{-0} = 1.\\]\nThe last equality follows from the fact that \\(e^{-N} \\approx 0\\) when \\(N\\) is infinitely large. Strangely, the area under the curve of this infinite graph is exactly one. Curves with area one are called probability density functions. They can be used to create probability distributions. I’ll talk more on that in a few lessons.\n\n\nCode\nf = lambda x: np.exp(-x)\nx = np.linspace(0, 10, 100)\nplot_function_with_area(x, f, a=0, b=10, title='$y=e^{-x}$', alpha=0.7)"
  },
  {
    "objectID": "notebooks/linear-systems.html#linear-functions",
    "href": "notebooks/linear-systems.html#linear-functions",
    "title": "4  Linear Systems",
    "section": "4.1 Linear Functions",
    "text": "4.1 Linear Functions\nWe’ve already seen scalar linear functions, which have the form \\(y = ax\\). Linear functions, like the name suggests, represent lines in the plane. Since \\(y=0\\) if \\(x=0\\), those lines must always pass through the origin.\nThe coefficient \\(a\\) is called the slope of the function. It determines the steepness of the line and whether the line slants to the left or to the right. The slope also represents the derivative of the function, since\n\\[\\frac{dy}{dx} = a.\\]\nThe fact that the derivative is the slope tells us something about what \\(a\\) means practically speaking. It’s the amount that \\(y\\) changes in response to changes in \\(x\\). If we increase \\(x\\) by one unit, then \\(y\\) changes by \\(a\\) units. In this sense, you can also think of \\(a\\) as a weight or a gain that tells how much \\(x\\) influences \\(y\\).\nFor example, suppose you’re on a road trip, say from San Francisco to Los Angeles. You look at your speedometer and reason that you’re averaging a speed of about \\(a=60\\) miles per hour. If you’ve already driven for \\(x=5\\) hours and covered a distance of \\(y=300\\) miles, how much more distance will you cover if you drive for \\(dx=1\\) more hour? Clearly it’s \\(a=60\\) miles, which will bring your distance traveled to \\(y+dy=360\\) miles. That’s all the slope is saying.\nThe above example corresponds to the linear equation \\(y=60x\\). Here’s a plot of what this looks like. Nothing special, just a line with slope \\(60\\).\n\n\nCode\na = 60\nx = np.linspace(-3, 3, 100)\nf = lambda x: a * x\nplot_function(x, f, xlim=(-3, 3), ylim=(-100, 100), title=f'$y={a}x$')\n\n\n\n\n\n\n\n\n\nIf there are two inputs \\(x_0\\) and \\(x_1\\), a linear function would look like\n\\[y = a_0 x_0 + a_1 x_1.\\]\nThis defines a plane in 3-dimensional space that passes through the origin. Each coefficient again tells you something about how the output changes if that input is changed. If \\(x_0\\) is changed by one unit, holding \\(x_1\\) fixed, then \\(y\\) changes by \\(a_0\\) units. Similarly, if \\(x_1\\) is changed by one unit, holding \\(x_0\\) fixed, then \\(y\\) changes by \\(a_1\\) units.\nHere’s an example. Take \\(y = 5x_0 - 2x_1\\). It will look like the plane shown below. Changing \\(x_0\\) by one unit while holding \\(x_1\\) fixed will cause \\(y\\) to increase by \\(5\\) units. Changing \\(x_1\\) by one unit while holding \\(x_0\\) fixed will cause \\(y\\) to decrease by \\(2\\) units.\n\n\nCode\na0, a1 = 5, -2\nx0 = np.linspace(-3, 3, 100)\nx1 = np.linspace(-3, 3, 100)\nf = lambda x0, x1: a0 * x0 + a1 * x1\nplot_function_3d(x0, x1, f, azim=80, elev=25, ticks_every=[1, 1, 10],\n                 titlepad=6, labelpad=3, title=f'$y=5x_0 - 2x_1$')\n\n\n\n\n\n\n\n\n\nThis idea readily extends to \\(n\\) variables too, though you can’t visualize it anymore. If there are \\(n\\) variables \\(x_0, x_1, \\cdots, x_{n-1}\\), a linear function has the form\n\\[y = a_0 x_0 + a_1 x_1 + \\cdots a_{n-1} x_{n-1}.\\]\nThis equation now defines a hyperplane in \\(n\\)-dimensional space that passes through the origin. Each coefficient \\(a_i\\) represents how much \\(y\\) changes if \\(x_i\\) is increased by one unit, while holding all the other \\(x_j\\) fixed.\nWe can also think about systems of linear equations. For example, we can have 2 outputs \\(y_0, y_1\\), each of which is its own linear function of 3 input variables \\(x_0, x_1, x_2\\). It might look like this\n\\[\\begin{alignat*}{5}\ny_0 & {}={}   a_{0,0} x_0 & {}+{} &  a_{0,1} x_1 & {}+{} & a_{0,2} x_2 \\\\\ny_1 & {}={}   a_{1,0} x_0 & {}+{} &  a_{1,1} x_1 & {}+{} & a_{1,2} x_2. \\\\\n\\end{alignat*}\\]\nThe most general situation we’ll consider is a system of \\(m\\) linear equations with \\(n\\) inputs,\n\\[\n\\begin{array}{c<{x_0} c c<{x_1} c c<{\\cdots} c c<{x_{n-1}} c l}\ny_0 & = & a_{0,0}x_0 & + & a_{0,1}x_1 & + & \\cdots & + & a_{0,n-1}x_{n-1} \\\\\ny_1 & = & a_{1,0}x_0 & + & a_{1,1}x_1 & + & \\cdots & + & a_{1,n-1}x_{n-1} \\\\\n\\vdots & & \\vdots    &   & \\vdots    &   &  \\ddots  &   & \\quad \\vdots\\\\\ny_{m-1} & = & a_{m-1,0}x_0 & + & a_{m-1,1}x_1 & + & \\cdots & + & a_{m-1,n-1}x_{n-1}. \\\\\n\\end{array}\n\\]\nThis kind of linear system is often called an \\(m \\times n\\) linear system, or a system of \\(m\\) linear equations with \\(n\\) unknowns. There are \\(m \\cdot n\\) coefficients in this system, namely \\(a_{0,0}, \\cdots, a_{m-1,n-1}\\). Each \\(a_{i,j}\\) would tell you how much the output \\(y_i\\) would change if the input \\(x_j\\) was increased by one unit. Visually, you can think of an \\(m \\times n\\) linear system as corresponding to a set of \\(m\\) \\(n\\)-dimensional hyperplanes."
  },
  {
    "objectID": "notebooks/linear-systems.html#matrix-vector-notation",
    "href": "notebooks/linear-systems.html#matrix-vector-notation",
    "title": "4  Linear Systems",
    "section": "4.2 Matrix-Vector Notation",
    "text": "4.2 Matrix-Vector Notation\nLinear systems of equations are incredibly cumbersome to work with in all but the simplest cases of like 2 or 3 equations. There’s a much cleaner notation for working with these linear systems. Here’s what we can do. Notice we seem to have three separate types of objects showing up in these equations:\n\nThe \\(m\\) output variables \\(y_0, y_1, \\cdots, y_{m-1}\\).\nThe \\(m \\cdot n\\) coefficients \\(a_{0,0}, \\ a_{0,1}, \\ \\cdots, \\ a_{m-1,n-1}\\).\nThe \\(n\\) input variables \\(x_0, x_1, \\cdots, x_{n-1}\\).\n\nLet’s put each of these sets into their own array, and define an \\(m \\times n\\) linear system of equations to mean the same thing as the following expression,\n\\[\n\\begin{pmatrix}\ny_0 \\\\ y_1 \\\\ \\vdots \\\\ y_{m-1}\n\\end{pmatrix} =\n\\begin{pmatrix}\na_{0,0} & a_{0,1} & \\cdots & a_{0,n-1} \\\\\na_{1,0} & a_{1,1} & \\cdots & a_{1,n-1} \\\\\n\\vdots  & \\vdots  & \\ddots & \\vdots    \\\\\na_{m-1,0} & a_{m-1,1} & \\cdots & a_{m-1,n-1}\n\\end{pmatrix}\n\\begin{pmatrix}\nx_0 \\\\ x_1 \\\\ \\vdots \\\\ x_{n-1}\n\\end{pmatrix}.\n\\]\nEach of these arrays is a 2-dimensional array. The left-most array is a shape \\((m, 1)\\) array of outputs, and the right-most array is a shape \\((n, 1)\\) array of inputs. These are both called column vectors, or when we’re being sufficiently lazy just vectors. Even though they’re not technically 1-dimensional arrays, and hence not technically vectors, they’re close enough that they might as well be. They’re isomorphic to vectors. The middle array is a shape \\((m, n)\\) array of coefficients. We’ll call this array an \\(m \\times n\\) matrix.\nHere’s a couple of examples of going back and forth between equation notation and matrix-vector notation so you get the idea. It’s good to be able to do this kind of thing without thinking. I’ll frequently go back and forth from now on depending on which notation is most convenient.\n\\[\\begin{gather*}\n\\begin{alignedat}{3}\n   y_0 & {}={} & 3x_0 & {}+{} &  x_1  \\\\\n   y_1 & {}={} & x_0 & {}-{} &  2x_1\n\\end{alignedat}\n\\quad \\Longleftrightarrow \\quad\n\\begin{pmatrix}\ny_0 \\\\\ny_1\n\\end{pmatrix} =\n\\begin{pmatrix}\n3 & 1 \\\\\n1 & -2\n\\end{pmatrix}\n\\begin{pmatrix}\nx_0 \\\\\nx_1\n\\end{pmatrix},\n\\end{gather*}\\]\n\\[\\begin{gather*}\n\\begin{alignedat}{5}\n   y_0 & {}={} & x_0 & {}+{} &  2x_1 & {}+{} & 3x_2  \\\\\n   y_1 & {}={} & 4x_0 & {}-{} &  5x_1 & {} {} &\n\\end{alignedat}\n\\quad \\Longleftrightarrow \\quad\n\\begin{pmatrix}\ny_0 \\\\\ny_1 \\\\\n\\end{pmatrix} =\n\\begin{pmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 0\n\\end{pmatrix}\n\\begin{pmatrix}\nx_0 \\\\\nx_1 \\\\\nx_2\n\\end{pmatrix}.\n\\end{gather*}\\]\nIt’s convenient to use an abstract notation to express vectors and matrices so we can more easily manipulate them. If we define the symbol \\(\\mathbf{x}\\) to represent the column vector of inputs, the symbol \\(\\mathbf{y}\\) to represent the column vector of outputs, and the symbol \\(\\mathbf{A}\\) to represent the matrix of coefficients, we can write the same \\(m \\times n\\) linear system in the much simpler form\n\\[\\mathbf{y} = \\mathbf{A} \\mathbf{x}.\\]\nThis looks almost just like the simple one-dimensional linear equation \\(y=ax\\), except it’s packing a lot more into it that we’ll have to analyze. By convention, I’ll use bold-face characters to represent vectors and matrices (and tensors) in this book. For this most part, I’ll try to use lower-case letters for vectors, and upper-case letters for matrices. This is an almost universally followed convention, but it’s not unanimous.\nTo index into these arrays, I’ll mostly use subscript notation. For example, the element of \\(\\mathbf{x}\\) at index \\(i\\) will be denoted \\(x_i\\). The element of \\(\\mathbf{A}\\) at index \\((i,j)\\) will be denoted \\(A_{i,j}\\). Sometimes I’ll also use the code equivalent of \\(x[i]\\) or \\(A[i,j]\\) when it’s more clear. Following the python convention, I’ll always index starting from \\(0\\), so that an array of \\(n\\) elements goes from \\(0, 1, \\cdots, n-1\\), not from \\(1, 2, \\cdots, n\\) as is more typical in math books. I do this mainly to make going between math and code easier, as index errors can be a pain to deal with.\nIt may not be at all obvious, but having written a linear system as a matrix-vector equation, I’ve implicitly defined a new kind of array multiplication. To see this, I’ll define a new column vector that I’ll call \\(\\mathbf{A} \\mathbf{x}\\) whose elements are just the right-hand side of the linear system when written out,\n\\[\n\\mathbf{A} \\mathbf{x} =\n\\begin{pmatrix}\na_{0,0}x_0 & + & a_{0,1}x_1 & + & \\cdots & + & a_{0,n-1}x_{n-1} \\\\\na_{1,0}x_0 & + & a_{1,1}x_1 & + & \\cdots & + & a_{1,n-1}x_{n-1} \\\\\n\\vdots    &   & \\vdots    &   &  \\ddots  &   & \\quad \\vdots     \\\\\na_{m-1,0}x_0 & + & a_{m-1,1}x_1 & + & \\cdots & + & a_{m-1,n-1}x_{n-1} \\\\\n\\end{pmatrix}.\n\\]\nSetting the \\(i\\)th row of \\(\\mathbf{A} \\mathbf{x}\\) equal to the \\(i\\)th row of \\(\\mathbf{y}\\) must imply that each element \\(y_i\\) can be written\n\\[y_i = a_{i,0}x_0 + a_{i,1}x_1 + \\cdots + a_{i,n-1}x_{n-1} = \\sum_{k=0}^{n-1} a_{i,k}x_k.\\]\nThat is, each constant term \\(y_i\\) is the sum of the products of the \\(i\\)th row of the matrix \\(\\mathbf{A}\\) with the vector \\(\\mathbf{x}\\). This is matrix-vector multiplication, a special case of matrix multiplication, which I’ll get to shortly. Note that this operation is only defined when the number columns of \\(\\mathbf{A}\\) matches the size of \\(\\mathbf{x}\\). We say in this case that \\(\\mathbf{A}\\) and \\(\\mathbf{x}\\) are compatible.\nHere’s a quick example, where a \\(2 \\times 3\\) matrix \\(\\mathbf{A}\\) is matrix multiplied with a size \\(3\\) vector \\(\\mathbf{x}\\). For each row we’re element-wise multiplying that row of \\(\\mathbf{A}\\) with the vector \\(\\mathbf{x}\\) and then summing up the terms. The output will be the vector \\(\\mathbf{y}\\) of size \\(2\\).\n\\[\n\\mathbf{A} \\mathbf{x} =\n\\begin{pmatrix}\n\\color{red}{1} & \\color{red}{2} & \\color{red}{3} \\\\\n\\color{blue}{4} & \\color{blue}{5} & \\color{blue}{6}\n\\end{pmatrix}\n\\begin{pmatrix}\n1 \\\\\n1 \\\\\n1\n\\end{pmatrix} =\n\\begin{pmatrix}\n\\color{red}{1} \\cdot 1 + \\color{red}{2} \\cdot 1 + \\color{red}{3} \\cdot 1 \\\\\n\\color{blue}{4} \\cdot 1 + \\color{blue}{5} \\cdot 1 + \\color{blue}{6} \\cdot 1 \\\\\n\\end{pmatrix} =\n\\begin{pmatrix}\n6 \\\\\n15\n\\end{pmatrix} = \\mathbf{y}.\n\\]\nI used the color coding to help illustrate a point. Notice that each element of \\(\\mathbf{y}\\) is just that row of \\(\\mathbf{A}\\) being element-wise multiplied by \\(\\mathbf{x}\\) and summed over. The fancy term for this operation is a dot product. I’ll get into that more in the next lesson."
  },
  {
    "objectID": "notebooks/linear-systems.html#matrix-multiplication",
    "href": "notebooks/linear-systems.html#matrix-multiplication",
    "title": "4  Linear Systems",
    "section": "4.3 Matrix Multiplication",
    "text": "4.3 Matrix Multiplication\nMatrix-vector multiplication is just a special case of the more general matrix multiplication. If \\(\\mathbf{A}\\) is an \\(m \\times n\\) matrix and \\(\\mathbf{B}\\) is an \\(n \\times p\\) matrix, we’ll define their matrix multiplication as a new \\(m \\times p\\) matrix \\(\\mathbf{C}\\) whose elements are given by\n\\[C_{i,j} = \\sum_{k=0}^{n-1} A_{i,k} B_{k,j} = A_{i,0} B_{0,j} + A_{i,1} B_{1,j} + \\cdots + A_{i,n-1} B_{n-1,j}.\\]\nMatrix multiplication is always expressed symbolically by directly concatenating the two matrix symbols next to each other like \\(\\mathbf{A}\\mathbf{B}\\). We’d never use a multiplication symbol between them since those are often used to represent other kinds of multiplication schemes like element-wise multiplication or convolutions. Further, matrix multiplication is only defined when the numbers of columns in \\(\\mathbf{A}\\) equals the number of rows of \\(\\mathbf{B}\\). We say matrices satisfying this condition are compatible. If they can’t be multiplied, they’re called incompatible.\nIn words, matrix multiplication is the process where you take a row \\(i\\) of the left matrix \\(\\mathbf{A}\\), element-wise multiply it with a column \\(j\\) of the right matrix \\(\\mathbf{B}\\), and then sum up the results to get the entry \\(C_{i,j}\\) of the output matrix \\(\\mathbf{C}\\). Doing this for all pairs of rows and columns will fill in \\(\\mathbf{C}\\).\nHere’s an example where \\(\\mathbf{A}\\) is \\(3 \\times 3\\) and \\(\\mathbf{B}\\) is \\(3 \\times 2\\). The output matrix \\(\\mathbf{C}\\) will be \\(3 \\times 2\\).\n\\[\n\\begin{pmatrix}\n    \\color{red}{1} & \\color{red}{2} & \\color{red}{3} \\\\\n    \\color{blue}{4} & \\color{blue}{5} & \\color{blue}{6} \\\\\n    \\color{green}{7} & \\color{green}{8} & \\color{green}{9} \\\\\n\\end{pmatrix}\n\\begin{pmatrix}\n    \\color{orange}{6} & \\color{purple}{5} \\\\\n    \\color{orange}{4} & \\color{purple}{3} \\\\\n    \\color{orange}{2} & \\color{purple}{1} \\\\\n\\end{pmatrix} =\n\\begin{pmatrix}\n   \\color{red}{1} \\cdot \\color{orange}{6} + \\color{red}{2} \\cdot \\color{orange}{4} + \\color{red}{3} \\cdot \\color{orange}{2} & \\color{red}{1} \\cdot \\color{purple}{5} + \\color{red}{2} \\cdot \\color{purple}{3} + \\color{red}{3} \\cdot \\color{purple}{1} \\\\\n   \\color{blue}{4} \\cdot \\color{orange}{6} + \\color{blue}{5} \\cdot \\color{orange}{4} + \\color{blue}{6} \\cdot \\color{orange}{2} & \\color{blue}{4} \\cdot \\color{purple}{5} + \\color{blue}{5} \\cdot \\color{purple}{3} + \\color{blue}{6} \\cdot \\color{purple}{1} \\\\\n   \\color{green}{7} \\cdot \\color{orange}{6} + \\color{green}{8} \\cdot \\color{orange}{4} + \\color{green}{9} \\cdot \\color{orange}{2} & \\color{green}{7} \\cdot \\color{purple}{5} + \\color{green}{8} \\cdot \\color{purple}{3} + \\color{green}{9} \\cdot \\color{purple}{1} \\\\\n\\end{pmatrix} =\n\\begin{pmatrix}\n   20 & 14 \\\\\n   56 & 41 \\\\\n   92 & 68 \\\\\n\\end{pmatrix}.\n\\]\nAside: If you’re still having a hard time picturing what matrix multiplication is doing, you may find this online visualization tool useful.\nNote that matrix multiplication does not commute. That is, we can’t swap the order of the two matrices being multiplied, \\(\\mathbf{A}\\mathbf{B} \\neq \\mathbf{B}\\mathbf{A}\\). Try to multiply the previous example in the opposite order and see what happens. The matrices won’t even be compatible anymore.\nHowever, matrix multiplication is associative, which means you can group parentheses just like you ordinarily would. For example, multiplying three matrices \\(\\mathbf{A}, \\mathbf{B}, \\mathbf{C}\\) could be done by multiplying either the first two, and then the last; or the last two, and then the first. That is,\n\\[\\mathbf{A}\\mathbf{B}\\mathbf{C} = \\mathbf{A}(\\mathbf{B}\\mathbf{C}) = (\\mathbf{A}\\mathbf{B})\\mathbf{C}.\\]\n\n4.3.1 Matrix Multiplication Algorithm\nMatrix multiplication is perhaps the single most important mathematical operation in machine learning. It’s so important I’m going to write a function to code it from scratch before showing how to do it in numpy. I’ll also analyze the speed of the algorithm in FLOPS and the memory in terms of word size. Algorithmically, all matrix multiplication is doing is looping over every single element of \\(\\mathbf{C}\\) and performing the sum-product calculation above for each \\(C_{i,j}\\). I’ll define a function called matmul that takes in two numpy arrays A and B and multiplies them, returning the product C if the dimensions are compatible.\n\n\nCode\ndef matmul(A, B):\n    assert A.shape[1] == B.shape[0]\n    m, n, p = A.shape[0], A.shape[1], B.shape[1]\n    C = np.zeros((m, p))\n    for i in range(m):\n        for j in range(p):\n            for k in range(n):\n                C[i, j] += A[i, k] * B[k, j]\n    return C\n\n\n\n\nCode\nA = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]); print(f'A = \\n{A}')\nB = np.array([[6, 5], [4, 3], [2, 1]]); print(f'B = \\n{B}')\nC = matmul(A, B); print(f'C = AB = \\n{C.astype(A.dtype)}')\n\n\nA = \n[[1 2 3]\n [4 5 6]\n [7 8 9]]\nB = \n[[6 5]\n [4 3]\n [2 1]]\nC = AB = \n[[20 14]\n [56 41]\n [92 68]]\n\n\nLet’s take a quick look at what this function is doing complexity wise. First off, we’re pre-computing the output matrix \\(\\mathbf{C}\\). That’ll contribute \\(O(mp)\\) memory since \\(\\mathbf{C}\\) is \\(m \\times p\\). All of the FLOPS are happening inside the double loop over \\(m\\) and \\(p\\). For each \\(i,j\\) pair, the function is doing \\(n\\) total multiplications and \\(n-1\\) total additions, which means there’s \\(2n-1\\) FLOPs per \\(i,j\\) pair. Since we’re doing this operation \\(m \\cdot p\\) times, we’re thus doing \\(m \\cdot p \\cdot (2n-1)\\) total FLOPS in the matrix multiply. This gives us an \\(O(nmp)\\) algorithm in general. Matrix multiplication is an example of a cubic time algorithm since if \\(n=m=p\\) we’d have a \\(O(n^3)\\) FLOPS operation.\nAside: People have found algorithms that can matrix multiply somewhat faster than cubic time. For example, Strassen’s algorithm can matrix multiply in about \\(O(n^{2.8})\\) time. In practice, though, these algorithms tend to have large constants out front, which means they’re not that useful unless \\(n\\) is huge. If the matrices have special structure, e.g. banded matrices or sparse matrices, they have special algorithms that can multiply them even faster, for example by using the Fast Fourier Transform, which can matrix multiply as fast as \\(O(n^2 \\log^2 n)\\). This is what, for example, convolutional neural networks use.\nCubic time may seem fast since it’s polynomial time, but it’s really not that great unless the matrices are relatively small (say \\(n \\leq 1000\\) or so). For this reason, a lot of effort has instead gone into pushing down the algorithmic constant out front, e.g. by doing special hardware optimizations like SIMD, optimizing matrix blocks to take advantage of memory efficiency, or heavily parallelizing the operation by doing the inner loops in parallel. It’s also a good idea to push operations down to low-level compiled code written in FORTRAN or C, which is what numpy essentially does.\nIn the age of deep learning we’re finding ourselves needing to multiply a lot of matrices and needing to do it quickly. This has been enabled largely through the use of GPUs to do array computations. GPUs are essentially specially built hardware just to do array operations like matrix multiplication efficiently. It’s no exaggeration in fact to say that the recent deep learning revolution happened precisely because of GPUs.\nAnyway, we’d never want to implement matrix multiplication natively in python like this. It’s far too most of the time. In practice we’d use something like np.matmul(A, B). Numpy also supports a cleaner syntax using the @ operator. This means we can also express the matrix multiply as A @ B, which means exactly the same thing as np.matmul(A, B), just with cleaner syntax. This syntax is what I’ll typically use in this book.\nHere’s an example. I’ll multiply the same two matrices from before, but this time using numpy. To show it’s faster than native python matmul, I’ll run a quick profiler as well. You can see that even with these small matrices we’re still getting a 10x speedup using numpy over base python. The speedup can get up to 100x and higher for much larger matrices.\n\n\nCode\nC = A @ B\nprint(f'C = \\n{C.astype(A.dtype)}')\n\n\nC = \n[[20 14]\n [56 41]\n [92 68]]\n\n\n\n\nCode\n%timeit matmul(A, B)\n\n\n11.2 µs ± 14 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\n\n\nCode\n%timeit A @ B\n\n\n892 ns ± 4.44 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n\n\n\n\n4.3.2 Chained Matrix Multiplication\nWhat if we’d like to multiply three or more matrices together. I already said matrix multiplication is associative, so in theory we should be able to matrix multiply in any order and get the same answer. However, there are often computational advantages to multiplying them together in some particular sequence. For example, suppose we wanted to multiply \\(\\mathbf{D} = \\mathbf{A}\\mathbf{B}\\mathbf{C}\\). Suppose, \\(\\mathbf{A}\\) is \\(m \\times n\\), \\(\\mathbf{B}\\) is \\(n \\times p\\), and \\(\\mathbf{C}\\) is \\(p \\times q\\). No matter which order we do it, the output \\(\\mathbf{D}\\) will have size \\(m \\times q\\). But there are two ways we could do this multiplication.\n\n\\(\\mathbf{D} = \\mathbf{A}(\\mathbf{B}\\mathbf{C})\\): In this case, the \\(\\mathbf{E}=\\mathbf{B}\\mathbf{C}\\) computation requires \\(nq(2p-1)\\) FLOPS, and then the \\(\\mathbf{A}\\mathbf{E}\\) computation requires \\(mq(2n-1)\\) FLOPS. The total is thus the sum of these two, i.e. \\[nq(2p-1) + mq(2n-1) = O(npq+mnq) \\ \\ \\text{FLOPS}.\\]\n\\(\\mathbf{D} = (\\mathbf{A}\\mathbf{B})\\mathbf{C}\\): In this case, the \\(\\mathbf{F}=\\mathbf{A}\\mathbf{B}\\) computation requires \\(mp(2n-1)\\) FLOPS, and then the \\(\\mathbf{F}\\mathbf{C}\\) computation requires \\(mq(2n-1)\\) FLOPS. The total is thus the sum of these two, i.e. \\[mq(2p-1) + mp(2n-1) = O(mpq+mnp) \\ \\ \\text{FLOPS}.\\]\n\nLet’s put some numbers in to make it clear what’s going on. Suppose \\(m=1000\\), \\(n=2\\), \\(p=100\\), and \\(q=100\\). Then the first case takes\n\\[nq(2p-1) + mq(2n-1) = 339800 \\ \\ \\text{FLOPS},\\]\nwhile the second case takes a staggering\n\\[mq(2p-1) + mp(2n-1) = 20200000 \\ \\ \\text{FLOPS}.\\]\nIt would thus behoove us in this case to multiply the matrices in the first order to save on computation, \\(\\mathbf{D} = \\mathbf{A}(\\mathbf{B}\\mathbf{C})\\). Here’s a programmatic way to see this.\n\n\nCode\nm = 1000\nn = 2\np = 100\nq = 100\n\nprint(f'A(BC): {m * q * (2 * n - 1) + n * q * (2 * p - 1)} FLOPS')\nprint(f'(AB)C: {m * p * (2 * n - 1) + m * q * (2 * p - 1)} FLOPS')\n\n\nA(BC): 339800 FLOPS\n(AB)C: 20200000 FLOPS\n\n\nThe same issues extend to multiplying together arbitrarily many matrices. You can save a lot of compute by first taking time to find the optimal order to multiply them together before doing the computation. Don’t just naively multiply them in order. Numpy has a function np.linalg.multi_dot that can do this for you. If you pass in a list of matrices, it’ll multiply them together in the most efficient order to help save on computation. Here’s an example. I’ll profile the different ways we can do the \\(\\mathbf{A}\\mathbf{B}\\mathbf{C}\\) example above. Notice that indeed \\(\\mathbf{A}(\\mathbf{B}\\mathbf{C})\\) is much faster than \\((\\mathbf{A}\\mathbf{B})\\mathbf{C}\\). The multi_dot solution is roughly as fast as the \\(\\mathbf{A}(\\mathbf{B}\\mathbf{C})\\) solution, but it does take slightly longer because it first calculates the optimal ordering, which adds a little bit of time.\n\n\nCode\nA = np.random.rand(m, n)\nB = np.random.rand(n, p)\nC = np.random.rand(p, q)\n\n\n\n\nCode\n%timeit A @ (B @ C)\n\n\n64.8 µs ± 37.2 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n\n\n\n\nCode\n%timeit (A @ B) @ C\n\n\n526 µs ± 14 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n\n\n\n\nCode\n%timeit np.linalg.multi_dot([A, B, C])\n\n\n78.2 µs ± 223 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n\n\n\n\n4.3.3 Matrix Multiplication vs Element-Wise Multiplication\nWe’ve already seen a different way we can multiply two matrices (or any array), namely element-wise multiplication. For matrices, element-wise multiplication is sometimes called the Hadamard product. I’ll denote element-wise multiplication as \\(\\mathbf{A} \\circ \\mathbf{B}\\). Note that element-wise multiplication is only defined when the shapes of \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) are exactly equal (or can be broadcasted to be equal).\nIt’s important to mind the difference between matrix multiplication and element-wise multiplication of matrices. In general \\(\\mathbf{A} \\circ \\mathbf{B} \\neq \\mathbf{A} \\mathbf{B}\\). They’re defined completely differently,\n\\[\\begin{align*}\n(A \\circ B)_{i,j} &= A_{i,j} \\cdot B_{i,j} \\\\\n(AB)_{i,j} &= \\sum_k A_{i,k}B_{k,j}.\n\\end{align*}\\]\nIn numpy use A * B for element-wise multiplication and A @ B for matrix multiplication. To make it clear the two kinds of multiplication aren’t the same thing here’s an example.\n\n\nCode\nA = np.array([[1, 2], [3, 4]])\nB = np.array([[1, 0], [0, 1]])\nprint(f'A*B = \\n{A * B}')\nprint(f'AB = \\n{A @ B}')\n\n\nA*B = \n[[1 0]\n [0 4]]\nAB = \n[[1 2]\n [3 4]]\n\n\n\n\n4.3.4 Interpreting Matrix Multiplication\nThis stuff might seem kind of abstract so far. Why should we care about multiplying matrices? I’ll say more about that in the next lesson, but for now I want to mention a useful way to interpret matrix-vector multiplication and matrix multiplication, a way that you almost certainly never learned in school when you were taught about matrices.\nWe can think about a matrix in a few different ways. One way is just as a 2-dimensional array of numbers. We can also think of it as a stack of vectors. If \\(\\mathbf{A}\\) is an \\(m \\times n\\), we can think of each column of \\(\\mathbf{A}\\) as being a vector of size \\(m \\times 1\\). These are called the column vectors of \\(\\mathbf{A}\\). We can also think about each row of \\(\\mathbf{A}\\) as being a vector of size \\(1 \\times n\\). These are called the row vectors of \\(\\mathbf{A}\\). In keeping with the python convention, I’ll denote the column vectors of \\(\\mathbf{A}\\) by \\(\\mathbf{A}_{:, i}\\), and the row vectors of \\(\\mathbf{A}\\) by \\(\\mathbf{A}_{i, :}\\). Notice the use of the slice operator \\(:\\) here, which means “take everything in that dimension”.\nHere’s an example. Take \\(\\mathbf{A}\\) to be the following \\(2 \\times 3\\) matrix,\n\\[\n\\mathbf{A} =\n\\begin{pmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6\n\\end{pmatrix}.\n\\]\nThe column vectors of \\(\\mathbf{A}\\) are just\n\\[\\mathbf{A}_{:, 0} = \\begin{pmatrix} 1 \\\\ 4 \\end{pmatrix}, \\quad \\mathbf{A}_{:, 1} = \\begin{pmatrix} 2 \\\\ 5 \\end{pmatrix}, \\quad \\mathbf{A}_{:, 2} = \\begin{pmatrix} 3 \\\\ 6 \\end{pmatrix},\\]\nAnd the row vectors of \\(\\mathbf{A}\\) are just\n\\[\\mathbf{A}_{0, :} = \\begin{pmatrix} 1 & 2 & 3 \\end{pmatrix}, \\quad \\mathbf{A}_{1, :} = \\begin{pmatrix} 4 & 5 & 6 \\end{pmatrix}.\\]\nUsing the column vectors of \\(\\mathbf{A}\\) we can think about the matrix multiplication \\(\\mathbf{A} \\mathbf{x}\\) in an interesting way. If there are \\(n\\) total column vectors, we can write\n\\[\n\\mathbf{A} \\mathbf{x} =\n\\begin{pmatrix}\n\\mathbf{A}_{:, 0} & \\mathbf{A}_{:, 1} & \\cdots & \\mathbf{A}_{:, n-1}\n\\end{pmatrix}\n\\begin{pmatrix}\nx_0 \\\\\nx_1 \\\\\n\\vdots \\\\\nx_{n-1}\n\\end{pmatrix} =\nx_0 \\mathbf{A}_{:, 0} + x_1 \\mathbf{A}_{:, 1} + \\cdots x_{n-1} \\mathbf{A}_{:, n-1}.\n\\]\nEvidently, we can think of \\(\\mathbf{A} \\mathbf{x}\\) as some kind of mixture of the columns of \\(\\mathbf{A}\\), weighted by the elements of \\(\\mathbf{x}\\). Such a mixture is called a linear combination. In this terminology, we’d say that the matrix-vector multiplication \\(\\mathbf{A} \\mathbf{x}\\) is a linear combination of the columns of \\(\\mathbf{A}\\).\nFor example, if \\(\\mathbf{A}\\) is the \\(2 \\times 3\\) matrix from the previous example and \\(\\mathbf{x} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}\\), we’d have\n\\[\n\\mathbf{A} \\mathbf{x} =\n\\begin{pmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6\n\\end{pmatrix}\n\\begin{pmatrix}\n1 \\\\\n1 \\\\\n1\n\\end{pmatrix} = 1 \\cdot \\binom{1}{4} + 1 \\cdot \\binom{2}{5} + 1 \\cdot \\binom{3}{6} = \\binom{1+2+3}{4+5+6} = \\binom{6}{15}.\n\\]\nWe can think of matrix multiplication in a similar way. Suppose we want to multiply two matrices \\(\\mathbf{A} \\mathbf{X}\\). You can think of \\(\\mathbf{X}\\) as itself being a bunch of different column vectors \\(\\mathbf{X}_{:, i}\\), where for each of those column vectors we’re doing a matrix-vector multiplication \\(\\mathbf{A}\\mathbf{X}_{:, i}\\). That is, matrix multiplication is just a batch of weighted linear combinations of the columns of \\(\\mathbf{A}\\),\n\\[\n\\begin{array}{c<{x_0} c c<{x_1} c c<{\\cdots} c c<{x_{n-1}} c l}\n\\mathbf{A} \\mathbf{X}_{:, 0} & = & X_{0,0} \\mathbf{A}_{:, 0} & + & X_{1,0} \\mathbf{A}_{:, 1} & + & \\cdots & + & X_{m-1,0} \\mathbf{A}_{:, n-1} \\\\\n\\mathbf{A} \\mathbf{X}_{:, 1} & = & X_{0,1} \\mathbf{A}_{:, 0} & + & X_{1,1} \\mathbf{A}_{:, 1} & + & \\cdots & + & X_{m-1,1} \\mathbf{A}_{:, n-1} \\\\\n\\vdots & & \\vdots    &   & \\vdots    &   &  \\ddots  &   & \\quad \\vdots\\\\\n\\mathbf{A} \\mathbf{X}_{:, n-1} & = & X_{0,n-1} \\mathbf{A}_{:, 0} & + & X_{1,n-1} \\mathbf{A}_{:, 1} & + & \\cdots & + & X_{m-1,n-1} \\mathbf{A}_{:, n-1}. \\\\\n\\end{array}\n\\]\nThese aren’t the only ways to interpret what matrix multiplication is doing. I’ll cover a more geometric interpretation later, where it’ll turn out that matrix multiplication is the same thing as the composition of linear maps."
  },
  {
    "objectID": "notebooks/linear-systems.html#solving-linear-systems",
    "href": "notebooks/linear-systems.html#solving-linear-systems",
    "title": "4  Linear Systems",
    "section": "4.4 Solving Linear Systems",
    "text": "4.4 Solving Linear Systems\nOne of the most important things we’d like to do with linear systems is solve for their inputs. Suppose we have a linear equation of the form \\(ax=b\\) and we wanted to solve for \\(x\\). It’s clear in this case what we’d do. Provided \\(a \\neq 0\\), we’d divide both sides by \\(a\\) to get\n\\[x = \\frac{b}{a} = a^{-1} b.\\]\nWe’d like to be able to do something like this for an \\(m \\times n\\) linear system \\(\\mathbf{Ax} = \\mathbf{b}\\). But dividing by a matrix doesn’t really make sense. We need to figure out another way to proceed.\n\n4.4.1 Square Linear Systems\nPerhaps it would help to recall how we’d solve a system of equations. Suppose for example we have the following system of 2 linear equations with 2 unknowns \\(x_0\\) and \\(x_1\\),\n\\[\\begin{alignat*}{3}\n   x_0 & {}+{} &  x_1 & {}={} & 2  \\\\\n   x_0 & {}-{} &  x_1 & {}={} & 0. \\\\\n\\end{alignat*}\\]\nUsing the method that pretty much always works, substitution, we can solve these one at a time. The second equation says \\(x_0 = x_1\\). Plugging this into the first equation then says \\(x_0=x_1=1\\), which is our solution. This is an example of the more general \\(2 \\times 2\\) linear system\n\\[\\begin{alignat*}{3}\n   ax_0 & {}+{} &  bx_1 & {}={} & e \\\\\n   cx_0 & {}+{} &  dx_1 & {}={} & f.\n\\end{alignat*}\\]\nThis can be solved by substitution too, but I’ll spare you the details and use sympy to get the answer. It’s given by\n\\[\\begin{align*}\nx_0 &= \\frac{de-bf}{ad-bc} \\\\\nx_1 &= \\frac{af-ce}{ad-bc}.\n\\end{align*}\\]\n\n\nCode\nx0, x1 = sp.symbols('x_0 x_1')\na, b, c, d, e, f = sp.symbols('a b c d e f')\neq1 = sp.Eq(a * x0 + b * x1, e)\neq2 = sp.Eq(c * x0 + d * x1, f)\nsol = sp.solve((eq1, eq2), (x0, x1))\nprint(f'x0 = {sol[x0]}')\nprint(f'x1 = {sol[x1]}')\n\n\nx0 = (-b*f + d*e)/(a*d - b*c)\nx1 = (a*f - c*e)/(a*d - b*c)\n\n\nIt’s worth plotting what these equations look like to try to visualize what’s going on. Let’s look again at the specific set of equations\n\\[\\begin{alignat*}{3}\n   x_0 & {}+{} &  x_1 & {}={} & 2  \\\\\n   x_0 & {}-{} &  x_1 & {}={} & 0. \\\\\n\\end{alignat*}\\]\nEach of these equations corresponds to a line in the plane, namely\n\\[y = 2 - x, \\quad y = x.\\]\nIf we plot these two lines, the point where they intersect is \\((1,1)\\), which is the solution to the linear system.\n\n\nCode\nx = np.linspace(-3, 3, 100)\nf0 = lambda x: 2 - x\nf1 = lambda x: x\nplot_function(x, [f0, f1], xlim=(0, 3), ylim=(0, 3), title='2 Linear Equations, 2 Unknowns',\n              labels=[f'$y=2-x$', f'$y=x$'])\n\n\n\n\n\n\n\n\n\nMore generally, the coefficients \\(a,b,c,d,e,f\\) represent the slopes and intercepts of the two lines. Changing any of these will change the point of intersection, and hence also the solution to the \\(2 \\times 2\\) system. Notice there is one edge case, namely when \\(ad=bc\\). This is when the two lines are parallel. Since parallel lines don’t intersect, such a system would have no solution. You can also see this by noticing that the denominator for \\(x_0\\) and \\(x_1\\) blows up, since \\(D=ad-bc=0\\). These denominators are special. They essentially say whether or not a solution to a given linear system will even exist.\nLet’s look now at the general \\(3 \\times 3\\) linear system\n\\[\\begin{alignat*}{5}\n   ax_0 & {}+{} &  bx_1 & {}+{} & cx_2 {}={} & j \\\\\n   dx_0 & {}+{} &  ex_1 & {}+{} & fx_2 {}={} & k \\\\\n   gx_0 & {}+{} &  hx_1 & {}+{} & ix_2 {}={} & l.\n\\end{alignat*}\\]\nAccording to sympy, the solution to this system is evidently this monstrosity,\n\\[\\begin{align*}\nx_0 &= \\frac{bfl - bik - cel + chk + eij - fhj}{aei - afh - bdi + bfg + cdh - ceg} \\\\\nx_1 &= \\frac{-afl + aik + cdl - cgk - dij + fgj}{aei - afh - bdi + bfg + cdh - ceg} \\\\\nx_2 &= \\frac{ael - ahk - bdl + bgk + dhj - egj}{aei - afh - bdi + bfg + cdh - ceg}. \\\\\n\\end{align*}\\]\n\n\nCode\nx0, x1, x2 = sp.symbols('x_0 x_1 x_2')\na, b, c, d, e, f, g, h, i, j, k, l = sp.symbols('a b c d e f g h i j k l')\neq1 = sp.Eq(a * x0 + b * x1 + c * x2, j)\neq2 = sp.Eq(d * x0 + e * x1 + f * x2, k)\neq3 = sp.Eq(g * x0 + h * x1 + i * x2, l)\nsol = sp.solve((eq1, eq2, eq3), (x0, x1, x2))\nprint(f'x0 = {sol[x0]}')\nprint(f'x1 = {sol[x1]}')\nprint(f'x2 = {sol[x2]}')\n\n\nx0 = (b*f*l - b*i*k - c*e*l + c*h*k + e*i*j - f*h*j)/(a*e*i - a*f*h - b*d*i + b*f*g + c*d*h - c*e*g)\nx1 = (-a*f*l + a*i*k + c*d*l - c*g*k - d*i*j + f*g*j)/(a*e*i - a*f*h - b*d*i + b*f*g + c*d*h - c*e*g)\nx2 = (a*e*l - a*h*k - b*d*l + b*g*k + d*h*j - e*g*j)/(a*e*i - a*f*h - b*d*i + b*f*g + c*d*h - c*e*g)\n\n\nIgnore the details of this thing. Just notice the fact that all three unknowns seem to again have the same denominator, in this case\n\\[D = aei - afh - bdi + bfg + cdh - ceg.\\]\nIf \\(D=0\\), the \\(3 \\times 3\\) system will have no solution. I can keep going, next to \\(4 \\times 4\\) systems, then \\(5 \\times 5\\) systems, but hopefully you get the point. There will always be a common denominator \\(D\\) in the solutions that can’t be zero. These denominators have a name. They’re called determinants. If \\(\\mathbf{A}\\) is the \\(n \\times n\\) matrix of coefficients, we’ll denote its determinant by \\(\\det(\\mathbf{A})\\) or sometimes just by \\(|\\mathbf{A}|\\). We’ve thus stumbled on a general fact.\nFact: An \\(n \\times n\\) system of linear equations \\(\\mathbf{Ax}=\\mathbf{b}\\) where \\(\\mathbf{b} \\neq \\mathbf{0}\\) has a solution if and only if \\(\\det(\\mathbf{A}) \\neq 0\\). In fact, this solution is unique.\nNote there’s an edge case when \\(\\mathbf{b} = \\mathbf{0}\\) and \\(\\det(\\mathbf{A}) \\neq 0\\). In that one case, there will be infinitely many solutions. You can think of this as the situation where the solutions have a \\(\\frac{0}{0}\\), which is the one case where dividing by \\(0\\) could still give a finite number.\nBefore moving on, let’s visualize what the \\(3 \\times 3\\) linear system looks like. Consider the following specific example,\n\\[\\begin{alignat*}{5}\n   3x_0 & {}+{} &  2x_1 & {}+{} & x_2 {}={} & 0 \\\\\n   x_0 & {}+{} &  x_1 & {}-{} & x_2 {}={} & 1 \\\\\n   x_0 & {}-{} &  3x_1 & {}-{} & x_3 {}={} & -3. \\\\\n\\end{alignat*}\\]\nAgain using substitution, you can solve each equation one by one to check that this system has a solution at \\(x_0=-\\frac{1}{2}\\), \\(x_1=1\\), and \\(x_2=-\\frac{1}{2}\\). The three equations above form a set of three planes given by\n\\[\\begin{align*}\nz &= -3x - 2y + 0, \\\\\nz &= x + y - 1, \\\\\nz &= 4 . \\\\\n\\end{align*}\\]\nThe solution to this \\(3 \\times 3\\) system will be the point where all three of these planes intersect. It’s a perhaps a little hard to see in the plot, but hopefully you get the point. In general, the solution of an \\(n \\times n\\) linear system will occur at the point where the set of \\(n\\) hyperplanes all intersect. If any two of the hyperplanes are parallel, the determinant will be zero, and there won’t be a solution.\n\n\nCode\nx = np.linspace(-2.5, 1.5, 100)\ny = np.linspace(0, 2, 100)\nf1 = lambda x, y: -3 * x - 2 * y + 0\nf2 = lambda x, y: x + y - 1\nf3 = lambda x, y: x - 3 * y + 3\nplot_function_3d(x, y, [f1, f2, f3], azim=65, elev=25, ticks_every=[1, 1, 3], figsize=(5, 5), zorders=[0, 2, 1],\n                 colors=['steelblue', 'salmon', 'limegreen'], points=[[-0.5, 1, -0.5]], alpha=0.6, labelpad=3, \n                 dist=11, title='3 Equations, 3 Unknowns')\n\n\n\n\n\n\n\n\n\nLet’s now try to see if we can figure out a pattern, a way to systematically solve these linear systems. Let’s start by going back to the easy \\(2 \\times 2\\) case. Recall that the linear system\n\\[\\begin{alignat*}{3}\n   ax_0 & {}+{} &  bx_1 & {}={} & e \\\\\n   cx_0 & {}+{} &  dx_1 & {}={} & f. \\\\\n\\end{alignat*}\\]\nhas solutions given by\n\\[\\begin{align*}\nx_0 &= \\frac{de-bf}{ad-bc} \\\\\nx_1 &= \\frac{af-ce}{ad-bc}. \\\\\n\\end{align*}\\]\nNow, if we write this \\(2 \\times 2\\) linear system in matrix-vector notation, we’d have\n\\[\n\\mathbf{A}\\mathbf{x} =\n\\begin{pmatrix}\na & b \\\\\nc & d\n\\end{pmatrix}\n\\begin{pmatrix}\nx_0 \\\\\nx_1\n\\end{pmatrix} =\n\\begin{pmatrix}\ne \\\\\nf\n\\end{pmatrix}\n= \\mathbf{b},\n\\]\nand the solutions would look like\n\\[\n\\mathbf{x} =\n\\begin{pmatrix}\nx_0 \\\\\nx_1\n\\end{pmatrix} =\n\\begin{pmatrix}\n\\frac{de-bf}{ad-bc} \\\\\n\\frac{af-ce}{ad-bc}\n\\end{pmatrix}.\n\\]\nI’m going to manipulate the solutions so they have a suggestible form. Observe that we can write\n\\[\n\\mathbf{x} =\n\\begin{pmatrix}\nx_0 \\\\\nx_1\n\\end{pmatrix} =\n\\begin{pmatrix}\n\\frac{de-bf}{ad-bc} \\\\\n\\frac{af-ce}{ad-bc}\n\\end{pmatrix} =\n\\begin{pmatrix}\n\\frac{d}{ad-bc} & -\\frac{b}{ad-bc} \\\\\n-\\frac{c}{ad-bc} & \\frac{a}{ad-bc}\n\\end{pmatrix}\n\\begin{pmatrix}\ne \\\\\nf\n\\end{pmatrix} =\n\\frac{1}{ad-bc}\n\\begin{pmatrix}\nd & -b \\\\\n-c & a\n\\end{pmatrix}\n\\begin{pmatrix}\ne \\\\\nf\n\\end{pmatrix}.\n\\]\nOn the right-hand side, we seem to have some kind of matrix times the vector \\(\\mathbf{b}\\). Whatever that matrix is, it seems to “undo” \\(\\mathbf{A}\\). Let’s call that matrix \\(\\mathbf{A}^{-1}\\). Then the solution of the linear system in abstract notation would just be\n\\[\\mathbf{x} = \\mathbf{A}^{-1} \\mathbf{b}.\\]\nThis is the most general kind of solution we could write for a square linear system. Of course, the real hard part in solving a general \\(n \\times n\\) system is finding what exactly \\(\\mathbf{A}^{-1}\\) is.\nBut why did I use the notation \\(\\mathbf{A}^{-1}\\) for this matrix? Because it’s in some sense a way to “divide” by a matrix. Recall in the \\(1 \\times 1\\) case where \\(ax=b\\) the solution looked like \\(x=a^{-1}b\\). In that case, \\(a^{-1}\\) was literally the inverse of the number \\(a\\), since \\(aa^{-1} = a^{-1}a = 1\\). It turns out the matrix \\(\\mathbf{A}^{-1}\\) is the higher-dimensional generalization of \\(a^{-1}\\). It’s called the inverse of \\(\\mathbf{A}\\). To see why, notice in the \\(2 \\times 2\\) case if we multiply \\(\\mathbf{A}\\mathbf{A}^{-1}\\), we’d have\n\\[\n\\mathbf{A}\\mathbf{A}^{-1} =\n\\begin{pmatrix}\na & b \\\\\nc & d\n\\end{pmatrix}\n\\begin{pmatrix}\n\\frac{d}{ad-bc} & -\\frac{b}{ad-bc} \\\\\n-\\frac{c}{ad-bc} & \\frac{a}{ad-bc}\n\\end{pmatrix} =\n\\begin{pmatrix}\n\\frac{ad}{ad-bc}-\\frac{bc}{ad-bc} & -\\frac{ab}{ad-bc}+\\frac{ab}{ad-bc} \\\\\n\\frac{cd}{ad-bc}-\\frac{dc}{ad-bc} & -\\frac{cb}{ad-bc}+\\frac{da}{ad-bc}\n\\end{pmatrix} =\n\\begin{pmatrix}\n\\frac{ad-bc}{ad-bc} & \\frac{ab-ab}{ad-bc} \\\\\n\\frac{cd-cd}{ad-bc} & \\frac{ad-bc}{ad-bc}\n\\end{pmatrix} =\n\\begin{pmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{pmatrix}.\n\\]\nI’ll write the matrix on the right as \\(\\mathbf{I}\\). It’s called the identity matrix. It’s evidently the matrix generalization of the number \\(1\\). What I’ve just shown is that \\(\\mathbf{A}^{-1}\\) “undoes” \\(\\mathbf{A}\\) in the sense that \\(\\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{I}\\). Of course, since matrix multiplication doesn’t commute, this says nothing about what the reverse product \\(\\mathbf{A}^{-1}\\mathbf{A}\\) is. You can check in the \\(2 \\times 2\\) case that indeed we’d get \\(\\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{I}\\) as well. That is, \\(\\mathbf{A}^{-1}\\) is a two-sided inverse. A matrix and its inverse always commute.\nNotice that in the \\(2 \\times 2\\) case, the inverse matrix \\(\\mathbf{A}^{-1}\\) includes a division by the determinant \\(\\det(\\mathbf{A})\\),\n\\[\n\\mathbf{A}^{-1} =\n\\frac{1}{ad-bc}\n\\begin{pmatrix}\nd & -b \\\\\n-c & a\n\\end{pmatrix} =\n\\frac{1}{\\det(\\mathbf{A})}\n\\begin{pmatrix}\nd & -b \\\\\n-c & a\n\\end{pmatrix}.\n\\]\nEvidently, the inverse matrix only exists when \\(\\det(\\mathbf{A}) \\neq 0\\), since otherwise the matrix would blow up due to division by zero. This is a general statement. For any \\(n \\times n\\) matrix, its inverse exists if and only if its determinant is non-zero. For this reason, we say a square matrix with a non-zero determinant is invertible. If the determinant is zero, we call the matrix singular.\nOf course, it’s no longer obvious at all how to even find \\(\\mathbf{A}^{-1}\\) or \\(\\text{det}(\\mathbf{A})\\) when \\(n\\) is greater than \\(2\\) or \\(3\\). Thankfully, we don’t really need to know the gritty details of how to find these things. Just know that algorithms exist to calculate them. I’ll talk at a high level about how those algorithms work in a future lesson.\nIn numpy, you can solve a square linear system \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\) by using the command np.linalg.solve(A, b). While you could also solve a system by first calculating the inverse and then taking \\(\\mathbf{x} = \\mathbf{A}^{-1} \\mathbf{b}\\), this turns out to be a bad idea to do numerically. It’s actually better to avoid explicitly calculating \\(\\mathbf{A}^{-1}\\) unless you absolutely need it. The main reason is the inverse computation turns out to be highly prone to numerical loss of precision. Nevertheless, if you do need the inverse for some reason, you can get it with np.linalg.inv(A). Just like matrix multiplication, both of these functions are cubic time algorithms.\nHere’s an example. I’ll solve the \\(3 \\times 3\\) linear system below using np.solve. To do this, I’ll first need to convert everything to matrix-vector notation,\n\\[\\begin{gather*}\n\\begin{alignedat}{5}\n   x_0 & {}+{} &  2x_1 & {}+{} & 3x_2 {}={} & 1 \\\\\n   4x_0 & {}+{} &  5x_1 & {}-{} & 6x_2 {}={} & 1 \\\\\n   7x_0 & {}-{} &  8x_1 & {}-{} & 9x_3 {}={} & 1. \\\\\n\\end{alignedat}\n\\quad \\Longrightarrow \\quad\n\\begin{pmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6 \\\\\n7 & 8 & 9 \\\\\n\\end{pmatrix}\n\\begin{pmatrix}\nx_0 \\\\\nx_1 \\\\\nx_2 \\\\\n\\end{pmatrix} =\n\\begin{pmatrix}\n1 \\\\\n1 \\\\\n1 \\\\\n\\end{pmatrix}.\n\\end{gather*}\\]\n\n\nCode\nA = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]); print(f'A = \\n{A}')\nb = np.array([[1], [1], [1]]); print(f'b = \\n{b}')\nx = np.linalg.solve(A, b); print(f'x = \\n{x}')\n\n\nA = \n[[1 2 3]\n [4 5 6]\n [7 8 9]]\nb = \n[[1]\n [1]\n [1]]\nx = \n[[ 0.2]\n [-1.4]\n [ 1.2]]\n\n\nHere’s the determinant and inverse of this matrix. Notice how close it is to being non-singular, since \\(\\det(\\mathbf{A}) \\approx -10^{-15}\\) is tiny. This small determinant causes the inverse to be huge, with terms on the order of \\(10^{15}\\). This is where you can start to see the loss of precision creeping in. If we calculate \\(\\mathbf{A}\\mathbf{A}^{-1}\\) we won’t get anything looking like the identity matrix. Yet, using np.solve worked just fine. Multiply \\(\\mathbf{A}\\mathbf{x}\\) and you’ll get exactly \\(\\mathbf{b}\\) back.\n\n\nCode\nprint(f'det(A) = {np.linalg.det(A)}')\nprint(f'A^(-1) = \\n{np.linalg.inv(A)}')\nprint(f'A A^(-1) = \\n{A @ np.linalg.inv(A)}')\n\n\ndet(A) = -9.51619735392994e-16\nA^(-1) = \n[[ 3.15251974e+15 -6.30503948e+15  3.15251974e+15]\n [-6.30503948e+15  1.26100790e+16 -6.30503948e+15]\n [ 3.15251974e+15 -6.30503948e+15  3.15251974e+15]]\nA A^(-1) = \n[[ 0.  1.  0.]\n [ 0.  2.  0.]\n [-4.  3.  2.]]\n\n\n\n\n4.4.2 Rectangular Systems\nEverything I just covered applies only to square linear systems, where there are exactly as many equations as there are unknowns. In real life, the systems of equations we care about solving are rarely square. For example, in machine learning we’re usually dealing with matrices of data, where the rows represent the number of samples and the columns represent the number of features in the data. It’ll almost never be the case that we have exactly the same number of samples as we have features.\nA rectangular system is an \\(m \\times n\\) linear system \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\) where \\(m \\neq n\\). That is, the number of equations is different from the number of unknowns. Evidently there are two distinct cases to consider here:\n\nMore equations than unknowns (\\(m > n\\)): These are called over-determined systems. In an over-determined system, we have too many equations. It’ll usually be impossible to solve them all exactly.\nMore unknowns than equations (\\(m < n\\)): These are called under-determined systems. In an under-determined system, we don’t have enough equations. There will always be infinitely many ways to solve these kinds of systems.\n\n\n4.4.2.1 Over-Determined Systems\nIn either case, \\(\\mathbf{A}\\) won’t have a two-sided inverse anymore, nor will it have a determinant. What do we do? Let’s again start small. Let’s first look at a simple over-determined system, a \\(3 \\times 2\\) system. Consider the following example.\n\\[\\begin{gather*}\n\\begin{alignedat}{3}\n   2x_0 & {}+{} &  x_1 {}={} & -1 \\\\\n   -3x_0 & {}+{} &  x_1 {}={} & -2 \\\\\n   -x_0 & {}-{} &  x_1 {}={} & 1. \\\\\n\\end{alignedat}\n\\quad \\Longrightarrow \\quad\n\\begin{pmatrix}\n2 & 1 \\\\\n-3 & 1 \\\\\n-1 & 1 \\\\\n\\end{pmatrix}\n\\begin{pmatrix}\nx_0 \\\\\nx_1 \\\\\n\\end{pmatrix} =\n\\begin{pmatrix}\n-1 \\\\\n-2 \\\\\n1 \\\\\n\\end{pmatrix}.\n\\end{gather*}\\]\nGraphically, this system corresponds to 3 lines in the plane. Let’s plot them and see what’s going on. The equations for the lines are,\n\\[\\begin{align*}\ny &= -1 - 2x, \\\\\ny &= -2 + 3x, \\\\\ny &= 1 + x. \\\\\n\\end{align*}\\]\n\n\nCode\nx = np.linspace(-10, 10, 100)\nf0 = lambda x: -1 - 2 * x\nf1 = lambda x: -2 + 3 * x\nf2 = lambda x: 1 + x\nplot_function(x, [f0, f1, f2], xlim=(-6, 6), ylim=(-6, 6), \n              title='3 Linear Equations, 2 Unknowns',\n              labels=[f'$y=-1-2x$', f'$y=-2-3x$', f'$y=1-x$'], \n              legend_fontsize=9.5, legend_loc='upper left')\n\n\n\n\n\n\n\n\n\nFrom the plot, we can see that the three lines don’t all intersect at the same point, which means there’s no single solution that satisfies this particular system. In fact, this is general. It’s very unlikely that more than two lines will intersect at the same point, or more than three planes will intersect at the same point, etc.\nSo what do we do? If we can’t find an exact solution, can we at least find an approximately good solution? Yes we can. Let’s look at the situation abstractly for a minute. Suppose \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\) describes the over-determined example given above. Then \\(\\mathbf{A}\\) is a \\(3 \\times 2\\) matrix. We can’t invert it, nor can we take its determinant. But we can find a way “squarify it” somehow. To do that, I’ll need to introduce the transpose operation.\nIf \\(\\mathbf{A}\\) is some \\(m \\times n\\) matrix, either square or rectangular, we can swap its rows and columns to get an \\(n \\times m\\) matrix that’s somehow related to \\(\\mathbf{A}\\). This swapped matrix is called the transpose of \\(\\mathbf{A}\\). It’s usually denoted by the symbol \\(\\mathbf{A}^\\top\\), read “A transpose”. Formally, it’s defined by\n\\[A_{i,j}^\\top = A_{j,i}.\\]\nIn the above example, we’d have\n\\[\n\\mathbf{A} =\n\\begin{pmatrix}\n2 & 1 \\\\\n-3 & 1 \\\\\n-1 & 1 \\\\\n\\end{pmatrix} \\quad \\Longrightarrow \\quad\n\\mathbf{A}^\\top =\n\\begin{pmatrix}\n2 & -3 & 1 \\\\\n1 & 1 & 1 \\\\\n\\end{pmatrix}.\n\\]\nAll I did was swap the rows and columns. That’s all the transpose operation is doing. Since \\(\\mathbf{A}\\) in this example is \\(3 \\times 2\\), \\(\\mathbf{A}^\\top\\) must be \\(2 \\times 3\\).\nThe transpose gives us an interesting and sensible way to “squarify” a matrix. Consider what happens when we left multiply an \\(m \\times n\\) matrix \\(\\mathbf{A}\\) by its transpose. Evidently the product \\(\\mathbf{A}^\\top \\mathbf{A}\\) would have to be an \\(n \\times n\\) matrix. That is, it’s square. In the above example, we’d get the \\(2 \\times 2\\) matrix\n\\[\n\\mathbf{A}^\\top \\mathbf{A} =\n\\begin{pmatrix}\n14 & -2 \\\\\n-2 & 3 \\\\\n\\end{pmatrix}.\n\\]\nHere’s what this looks like in numpy. We can get the transpose of a matrix A by using either the method A.T or the function np.transpose(A).\n\n\nCode\nA = np.array([\n    [2, 1], \n    [-3, 1], \n    [-1, 1]])\nAt = A.T\nAtA = At @ A\nprint(f'A = \\n{A}')\nprint(f'A.T = \\n{At}')\nprint(f'A.T A = \\n{AtA}')\n\n\nA = \n[[ 2  1]\n [-3  1]\n [-1  1]]\nA.T = \n[[ 2 -3 -1]\n [ 1  1  1]]\nA.T A = \n[[14 -2]\n [-2  3]]\n\n\nNow, let’s go back to the over-determined system \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\). If we left-multiply both sides by \\(\\mathbf{A}^\\top\\), we’d get\n\\[\\mathbf{A}^\\top \\mathbf{A}\\mathbf{x} = \\mathbf{A}^\\top \\mathbf{b}.\\]\nMost of the time, the square matrix \\(\\mathbf{A}^\\top \\mathbf{A}\\) will be invertible. Provided that’s the case, we can write\n\\[\\mathbf{x} \\approx (\\mathbf{A}^\\top \\mathbf{A})^{-1} \\mathbf{A}^\\top \\mathbf{b}.\\]\nI use approximately equals here because this won’t usually give the exact solution to \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\). But it does give in some sense the best approximate solution you can get. For reasons I won’t go into much right this moment, this kind of approximate solution is called the least squares solution to the linear system. It’s the solution that minimizes the weird looking term \\((\\mathbf{A}\\mathbf{x} - \\mathbf{b})^\\top (\\mathbf{A}\\mathbf{x} - \\mathbf{b})\\), whatever that means.\nIn numpy, we can’t use np.linalg.solve(A, b) when a linear system isn’t square. If we want to find the least squares solution, we’ll need to use the function np.linalg.lstsq(A, b) instead. This function actually returns a lot more stuff than just the x we seek. For now I’ll ignore those other objects and show you what the least squares solution to the above \\(3 \\times 2\\) system looks like. Evidently, it’s\n\\[\n\\mathbf{x} \\approx\n\\begin{pmatrix}\n0.136 \\\\\n-0.578 \\\\\n\\end{pmatrix}.\n\\]\nIf you go back to the previous plot, you’ll see this point seems to lie close to the point where the blue and orange lines intersect. That’s interesting.\n\n\nCode\nb = np.array([[-1], [-2], [1]])\nx, _, _, _ = np.linalg.lstsq(A, b)\nprint(f'x ≈ \\n{x}')\n\n\nx ≈ \n[[ 0.13157895]\n [-0.57894737]]\n\n\nLet’s look again at the least squares solution \\(\\mathbf{x} \\approx (\\mathbf{A}^\\top \\mathbf{A})^{-1} \\mathbf{A}^\\top \\mathbf{b}\\). Notice that the matrix \\((\\mathbf{A}^\\top \\mathbf{A})^{-1} \\mathbf{A}^\\top\\) seems to function kind of like \\(\\mathbf{A}^{-1}\\), if it existed. For this reason, it’s called the pseudoinverse of \\(\\mathbf{A}\\), usually denoted by the special symbol \\(\\mathbf{A}^+\\). The pseudoinverse is in some sense the closest we can get to inverting the matrix of an over-determined system. Evidently, it satisfies the property that it’s a left inverse of \\(\\mathbf{A}\\),\n\\[\\mathbf{A}^+ \\mathbf{A} = \\mathbf{I}.\\]\n\n\n4.4.2.2 Under-Determined Systems\nI’ll come back to this more later. Let’s briefly take a look at the other type of rectangular system, the over-determined system where \\(m < n\\). In this case there are too many unknowns and not enough equations. As an example, consider the following \\(2 \\times 3\\) linear system,\n\\[\\begin{gather*}\n\\begin{alignedat}{3}\n   x_0 & {}+{} &  x_1 & {}+{} & x_2 {}={} & 2  \\\\\n   x_0 & {}-{} &  x_1 & {}+{} & x_2 {}={} & 0 \\\\\n\\end{alignedat}\n\\quad \\Longrightarrow \\quad\n\\begin{pmatrix}\n1 & 1 & 1 \\\\\n1 & -1 & 1 \\\\\n\\end{pmatrix}\n\\begin{pmatrix}\nx_0 \\\\\nx_1 \\\\\nx_2 \\\\\n\\end{pmatrix} =\n\\begin{pmatrix}\n2 \\\\\n0 \\\\\n\\end{pmatrix}.\n\\end{gather*}\\]\nGraphically, this system will look like two planes in 3D space,\n\\[\\begin{align*}\nz &= 2 - x - y, \\\\\nz &= -x + y. \\\\\n\\end{align*}\\]\nSince there are two planes, they’ll intersect not at a point, but at a line. Any point on this line will be a solution to the linear system.\n\n\nCode\nx = np.linspace(-3, 3, 100)\ny = np.linspace(-3, 3, 100)\nt = np.linspace(-1.9, 3.9, 100)\nf1 = lambda x, y: 2 - x - y\nf2 = lambda x, y: y - x\nplot_function_3d(x, y, [f1, f2], azim=30, elev=20, ticks_every=[1, 1, 2], figsize=(5, 5), zorders=[0, 1], dist=12,\n        colors=['steelblue', 'limegreen'], alpha=0.6, titlepad=-5, labelpad=2, title='2 Equations, 3 Unknowns',\n        lines=[[1 - t, np.full(len(t), 1), t]])\n\n\n\n\n\n\n\n\n\nTo see why this system has infinitely many solutions, let’s try to solve it. It’s easy enough using substitution. The second equation says \\(x_1 = x_0 + x_2\\). Plugging this into the first equation then says \\(x_2 = 1 - x_0\\). There’s no way to solve for \\(x_0\\) because we don’t have enough equations. We thus have to conclude that the solutions to this system look like\n\\[x_0 = x_0, \\quad x_1 = 1, \\quad x_2 = 1 - x_0.\\]\nAny choice of \\(x_0\\) will satisfy this linear system, which means it’ll have infinitely many solutions, which are of course just the points on the line above.\nIn general, we can almost exactly the same trick to “solve” these linear systems as we did with the over-determined systems. This time, instead of multiplying \\(\\mathbf{A}\\) on the left by \\(\\mathbf{A}^\\top\\), we’ll instead multiply on the right by \\(\\mathbf{A}^\\top\\),\n\\[\\mathbf{A}\\mathbf{A}^\\top \\mathbf{x} = \\mathbf{A}^\\top \\mathbf{b}.\\]\nProvided we can invert \\(\\mathbf{A}\\mathbf{A}^\\top\\), and usually we can, we’ll get a solution of the form\n\\[\\mathbf{x} = (\\mathbf{A}\\mathbf{A}^\\top)^{-1} \\mathbf{A}^\\top \\mathbf{b}.\\]\nNote that this gives only one of the infinitely many possible solutions to an under-determined linear system. For reasons I won’t go into now, it turns out the solution it gives is called the least norm solution. In a sense, this means it gives you the “smallest” vector \\(\\mathbf{x}\\) that satisfies \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\). By smallest, I mean it’s the vector such that the \\(1 \\times 1\\) matrix \\(\\mathbf{x}^\\top \\mathbf{x}\\) is minimized.\nIt turns out the matrix \\((\\mathbf{A}\\mathbf{A}^\\top)^{-1} \\mathbf{A}^\\top\\) on the right is also a pseudoinverse. It satisfies the property that it’s a right inverse of \\(\\mathbf{A}\\), in the sense that\n\\[\\mathbf{A} \\mathbf{A}^+ = \\mathbf{I}.\\]\nIn fact, there are many different kinds of pseudoinverses. The two I covered here are the most practical ones.\nIn numpy, you can solve an under-determined system by using the same np.linalg.lstsq(A, b) function from before. It’s able to tell which case you want by looking at the shape of the A you pass in. Here’s the least norm solution for the above example,\n\\[\n\\mathbf{x} =\n\\begin{pmatrix}\n\\frac{1}{2} \\\\\n1 \\\\\n\\frac{1}{2} \\\\\n\\end{pmatrix}.\n\\]\nYou can check it satisfies the linear system with the choice of \\(x_0 = \\frac{1}{2}\\).\n\n\nCode\nA = np.array([\n    [1, 1, 1], \n    [1, -1, 1]])\nb = np.array([[2], [0]])\nx, _, _, _ = np.linalg.lstsq(A, b)\nprint(f'x ≈ \\n{x}')\n\n\nx ≈ \n[[0.5]\n [1. ]\n [0.5]]\n\n\nI’ll come back to this stuff more in later lessons and fill in some of these missing pieces. I just want to close by mentioning that I’ve essentially just derived much of the ideas behind linear regression in this section. In fact, training a linear regression model is completely equivalent to finding either a least squares solution or a least norm solution to \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\). In that case, \\(\\mathbf{A}\\) represents the matrix of data, \\(\\mathbf{x}\\) represents the parameters the model needs to learn, and \\(\\mathbf{b}\\) represents the target values. Using what I’ve covered in this lesson, you could completely solve any linear regression problem you wanted from scratch just by using something like x = np.linalg.lstsq(A, b)."
  },
  {
    "objectID": "notebooks/vector-spaces.html#visualizing-vectors",
    "href": "notebooks/vector-spaces.html#visualizing-vectors",
    "title": "5  Vector Spaces",
    "section": "5.1 Visualizing Vectors",
    "text": "5.1 Visualizing Vectors\nLet’s go back to the simple 2-dimensional case. Imagine you have a point in the xy-plane, call it \\((x,y)\\). Now, we can think of this as a single point, but we can also imagine it differently. Suppose there was an arrow pointing from the origin \\((0,0)\\) to the point \\((x,y)\\). For example, if the point was \\((1,1)\\), this arrow might look like this.\n\n\nCode\npoint = np.array([1, 1])\nplot_vectors(point, title=f'Arrow From $(0,0)$ To $(1,1)$', ticks_every=0.5)\n\n\n\n\n\n\n\n\n\nUnlike the point \\((x,y)\\), the arrow \\((x,y)\\) has both a length and a direction. Its length is given by the Pythagorean Theorem. If the triangle has base \\(x\\) and height \\(y\\), then the length of the arrow is just its hypotenuse, i.e. \\(r = \\sqrt{x^2 + y^2}\\). The direction of the arrow is its angle \\(\\theta\\) with respect to the x-axis. This angle is just given by the inverse tangent of height over base, i.e. \\(\\theta = \\tan^{-1}\\big(\\frac{y}{x}\\big)\\).\nIn the example plotted, the length is \\(r=\\sqrt{1+1}=\\sqrt{2}\\), and the angle is \\(\\theta = \\tan^{-1}(1) = 45^\\circ\\). These two values uniquely specify the arrow, assuming it starts at the origin. If we know the length and direction, we know exactly which arrow we’re talking about.\nWhat I’ve just shown is another way to define a vector. A vector is an arrow in the plane. Said differently, a vector is just a point that’s also been endowed with a length (or magnitude) and a direction. The x and y values are called components of a vector. Usually we’ll write a vector in bold-face and its components in regular type but with subscripts indicating which component. For example, \\(\\mathbf{v}=(v_x,v_y)\\). Here’s the same arrow I plotted above, but explicitly labeled as a vector \\(\\mathbf{v}=(1,1)\\). Its components are \\(v_x=1\\) and \\(v_y=1\\).\n\n\nCode\nv = np.array([1, 1])\nplot_vectors(v, title='$\\mathbf{v}=(1,1)$', labels=['$\\mathbf{v}$'], ticks_every=0.5)\n\n\n\n\n\n\n\n\n\nNotation: It’s common to represent vectors in a few different ways depending on the situation. One way to represent a vector is as a column vector. This is what I did when doing matrix-vector multiplication. Another way, what I just introduced, is a flat vector, or a 1-dimensional array. This is more common when thinking about a vector geometrically. Yet another way is to think of a vector as a row vector, which is the transpose of a column vector. All of these representations conceptually represent the same object, but their shapes are different. Here’s an example: The size-2 vector \\(\\mathbf{v}=(1,1)\\) can be written in 3 different but all equivalent ways:\n\\[\\begin{align*}\n&\\text{Flat vector of shape } (2,): \\mathbf{v} = (1,1), \\\\\n&\\text{Column vector of shape } (2,1): \\mathbf{v} = \\begin{pmatrix}\n1 \\\\\n1 \\\\\n\\end{pmatrix}, \\\\\n&\\text{Row vector of shape } (1,2): \\mathbf{v}^\\top = \\begin{pmatrix}\n1 & 1 \\\\\n\\end{pmatrix}.\n\\end{align*}\\]\nBe careful when working with vectors in code to make sure you’re using the right shapes for the right situation or you’ll get shape mismatch errors (or worse a silent bug)."
  },
  {
    "objectID": "notebooks/vector-spaces.html#vector-operations",
    "href": "notebooks/vector-spaces.html#vector-operations",
    "title": "5  Vector Spaces",
    "section": "5.2 Vector Operations",
    "text": "5.2 Vector Operations\nThe magnitude, or length, of \\(\\mathbf{v}\\) is typically denoted by the symbol \\(||\\mathbf{v}||\\), called a norm,\n\\[||\\mathbf{v}|| = \\sqrt{v_x^2 + v_y^2}.\\]\nIn the above example with \\(\\mathbf{v}=(1,1)\\), its norm is \\(||\\mathbf{v}||=\\sqrt{1+1}=\\sqrt{2} \\approx 1.414\\).\nNotice that the norm must be non-negative since it’s the square root of a sum of squares, i.e. \\(||\\mathbf{v}|| \\geq 0\\). This should sound right, after all negative lengths don’t make any sense.\nWhat happens if we scale a vector \\(\\mathbf{v}\\) by some scalar \\(c\\)? By the rules of scalar-vector multiplication, the new vector should be \\(c\\mathbf{v}=(cx,cy)\\). Since the new vector has length \\(||c\\mathbf{v}||\\), a little math shows that\n\\[||c\\mathbf{v}|| = \\sqrt{(cv_x)^2 + (cv_y)^2} = \\sqrt{c^2(v_x^2 + v_y^2)} = |c| \\sqrt{v_x^2 + v_y^2} = |c| \\cdot ||\\mathbf{v}||.\\]\nThat is, the re-scaled vector \\(c\\mathbf{v}\\) just gets its length re-scaled by \\(c\\). That’s why \\(c\\) is called a scalar. It rescales vectors. Notice if \\(c\\) is negative, the length stays the same, but the direction gets reversed \\(180^\\circ\\) since in that case \\(c\\mathbf{v} = c(v_x, v_y) = -|c|(v_x,v_y)\\).\nHere’s what vector scaling looks like geometrically. I’ll plot the vector \\(\\mathbf{v}=(1,1)\\) again, but scaled by two numbers, one \\(c=2\\), the other \\(c=-1\\). When \\(c=2\\), the vector just doubles its length. That’s the light blue arrow. When \\(c=-1\\), the vector reverses its direction \\(180^\\circ\\), but maintains its length since \\(|c|=1\\). That’s the light orange arrow.\n\n\nCode\nv = np.array([1, 1])\nplot_vectors([v, -v, 2*v], xlim=(-2,3), ylim=(-2,3), title=f'Scaling Vectors', headwidth=7, ticks_every=1,\n             labels=['$\\mathbf{v}$', '$-\\mathbf{v}$', '$2\\mathbf{v}$'], \n             colors=['black', 'salmon', 'steelblue'],\n             text_offsets=[[-0.2, 0.2], [-0.2, 0.4], [-0.2, 0.2]])\n\n\n\n\n\n\n\n\n\nWhat does adding two vectors do? Let \\(\\mathbf{v}=(v_x,v_y)\\) and \\(\\mathbf{w}=(w_x,w_y)\\) be two vectors in the plane. Then their sum is \\(\\mathbf{v}+\\mathbf{w} = (v_x+w_x,v_y+w_y)\\). I’ll plot an example below with \\(\\mathbf{v}=(1,1)\\) and \\(\\mathbf{w}=(1,3)\\). Their sum should be\n\\[\\mathbf{v}+\\mathbf{w}=(1+1,1+3)=(2,4).\\]\n\n\nCode\nv = np.array([1, 1])\nw = np.array([1, 3])\nplot_vectors([v, w, v + w], xlim=(0, 3), ylim=(0, 5), title=f'Adding Two Vectors', ticks_every=1,\n             labels=['$\\mathbf{v}$', '$\\mathbf{w}$', '$\\mathbf{v}+\\mathbf{w}$'], \n             colors=['salmon', 'steelblue', 'black'])\n\n\n\n\n\n\n\n\n\nIt may not be obvious yet what vector addition is doing geometrically. Let me plot it slightly differently. What I’ll do is plot the vectors “head to tail” by taking the tail of \\(\\mathbf{w}\\) and placing it at the head of \\(\\mathbf{v}\\). Then the head of this translated \\(\\mathbf{w}\\) vector points at the head of the sum \\(\\mathbf{v}+\\mathbf{w}\\). We can do this “head to tail” stuff since the base of a vector is irrelevant. We can place the arrow wherever we want as long as we maintain its length and direction.\nInformally speaking, to add two vectors, just stack them on top of each other head to tail, and draw an arrow from the starting point to the ending point. You can geometrically add arbitrarily many vectors this way, not just two. Just keep stacking them.\n\n\nCode\nplot_vectors([v, w, v + w], xlim=(0, 3), ylim=(0, 5), title=f'Adding Two Vectors (Head to Tail)',\n             colors=['salmon', 'steelblue', 'black'],\n             tails=[[0, 0], [v[0], v[1]], [0, 0]], text_offsets=[[-0.5, -0.85], [0.5, -0.8], [-1.4, -1.6]],\n             labels=['$\\mathbf{v}$', '$\\mathbf{w}$', '$\\mathbf{v}+\\mathbf{w}$'],\n             zorders = [0, 1, 2], ticks_every=1)\n\n\n\n\n\n\n\n\n\nThe norm satisfies what’s known as the triangle inequality: If \\(\\mathbf{v}\\) and \\(\\mathbf{w}\\) are two vectors, then the length of their sum is less than the sum of their individual lengths, i.e.\n\\[||\\mathbf{v}+\\mathbf{w}|| \\leq ||\\mathbf{v}|| + ||\\mathbf{w}||.\\]\nYou can see this by staring at the plot above. The added lengths of \\(\\mathbf{v}\\) and \\(\\mathbf{w}\\) is larger than the length of their sum \\(\\mathbf{v}+\\mathbf{w}\\). In fact, the only time the lengths will be equal is if \\(\\mathbf{v}\\) and \\(\\mathbf{w}\\) are parallel to each other.\nWhat about subtracting two vectors? By combining the rules for scalar multiplication and vector addition, you can convince yourself that the difference of two vectors is also element-wise,\n\\[\\mathbf{v}-\\mathbf{w} = (v_x-w_x,v_y-w_y).\\]\nTo visualize what subtracting two vectors looks like, notice we can write subtraction as a sum like this, \\(\\mathbf{w} + (\\mathbf{v}-\\mathbf{w}) = \\mathbf{v}\\). Now use the same trick for adding vectors, only this time placing \\((\\mathbf{v}-\\mathbf{w})\\) at the head of \\(\\mathbf{w}\\), and noticing that it points to the sum of the two, which is \\(\\mathbf{v}\\).\nAn easy way to remember what subtracting two vectors looks like is to connect the two vectors you’re subtracting with a line segment, and place the head on the first vector. This trick will never fail you.\n\n\nCode\nv = np.array([1, 1])\nw = np.array([1, 3])\nplot_vectors([v, w, v - w], xlim=(-0.5, 1.5), ylim=(-0.5, 3.5), title=f'Subtracting Two Vectors', headwidth=4,\n             ticks_every=1, colors=['salmon', 'steelblue', 'black'],\n             tails=[[0, 0], [0, 0], [w[0], w[1]]], text_offsets=[[-0.5, -0.8], [-0.5, -1], [1.05, 3.8]],\n             labels=['$\\mathbf{v}$', '$\\mathbf{w}$', '$\\mathbf{v}-\\mathbf{w}$'])"
  },
  {
    "objectID": "notebooks/vector-spaces.html#the-dot-product",
    "href": "notebooks/vector-spaces.html#the-dot-product",
    "title": "5  Vector Spaces",
    "section": "5.3 The Dot Product",
    "text": "5.3 The Dot Product\nIt turns out we can understand both the lengths and angles of vectors in terms of a single operation called the dot product, also called the inner or scalar product. The dot product is a kind of multiplication between two vectors that returns a scalar. If \\(\\mathbf{v}=(v_x,v_y)\\) and \\(\\mathbf{w}=(w_x,w_y)\\) are two vectors in the plane, their dot product is defined as\n\\[\\mathbf{v} \\cdot \\mathbf{w} = v_x w_x + v_y w_y.\\]\nThat is, the dot product is just the sum of the element-wise products of the two vectors.\nIn terms of vectorized numpy code, the dot product is just the operation np.sum(v * w). Numpy also has a convenience function np.dot(v, w) that calculates it directly. Here’s the calculation of the dot product between the two vectors \\(\\mathbf{v}=(5,-1)\\) and \\(\\mathbf{w}=(2,4)\\). The answer should be\n\\[\\mathbf{v} \\cdot \\mathbf{w} = 5 \\cdot 2 + (-1) \\cdot 4 = 10 - 4 = 6.\\]\n\n\nCode\nv = np.array([5, -1])\nw = np.array([2, 4])\nprint(f'v . w = {np.dot(v, w)}')\nnp.sum(v * w) == np.dot(v, w)\n\n\nv . w = 6\n\n\nTrue\n\n\nAlgorithm Analysis: Evaluating the dot product uses \\(2n-1\\) or \\(O(n)\\) total FLOPS, since for a vector of size \\(n\\) there are \\(n\\) multiplications and \\(n-1\\) additions.\nHere are some fairly trivial properties the dot product satisfies. These follow straight from the definition.\n\nThe dot product of a vector with itself is nonnegative: \\(\\mathbf{v} \\cdot \\mathbf{v} \\geq 0\\).\nIt commutes: \\(\\mathbf{v} \\cdot \\mathbf{w} = \\mathbf{w} \\cdot \\mathbf{v}\\).\nIt distributes over scalar multiplication: \\(c\\mathbf{v} \\cdot \\mathbf{w} = \\mathbf{v} \\cdot c\\mathbf{w} = c(\\mathbf{v} \\cdot \\mathbf{w})\\).\nIt distributes over vector addition: \\((\\mathbf{u} + \\mathbf{v}) \\cdot \\mathbf{w} = \\mathbf{u} \\cdot \\mathbf{w} + \\mathbf{v} \\cdot \\mathbf{w}\\) and \\(\\mathbf{v} \\cdot (\\mathbf{u}+\\mathbf{w}) = \\mathbf{v} \\cdot \\mathbf{u} + \\mathbf{v} \\cdot \\mathbf{w}\\).\n\nNotation: The dot product is often written in several different ways in different fields. Another notation arises by thinking of the dot product as the matrix multiplication of a row vector \\(\\mathbf{v}^\\top = \\begin{pmatrix}v_x & v_y \\end{pmatrix}\\) with a column vector \\(\\mathbf{w} = \\begin{pmatrix} w_x \\\\ w_y \\end{pmatrix}\\). In that case,\n\\[\n\\mathbf{v}^\\top \\mathbf{w} =\n\\begin{pmatrix} v_x & v_y \\end{pmatrix}\n\\begin{pmatrix} w_x \\\\ w_y \\end{pmatrix}\n= v_x w_x + v_y w_y = \\mathbf{v} \\cdot \\mathbf{w}.\n\\]\nThis is the most commonly used notation for the dot product in machine learning. I’ll use it more frequently after this lesson.\nWe can write the norm or length of a vector in terms of the dot product. Observe that by dotting \\(\\mathbf{v}\\) with itself, I get\n\\[\\mathbf{v} \\cdot \\mathbf{v} = v_x^2 + v_y^2 = ||\\mathbf{v}||^2.\\]\nTaking the square root of both sides, you can see that the norm or length of a vector is just the square root of its dot product with itself,\n\\[||\\mathbf{v}|| = \\sqrt{\\mathbf{v} \\cdot \\mathbf{v}}.\\]\nWe can also talk about the distance between any two vectors \\(\\mathbf{v}\\) and \\(\\mathbf{w}\\). Denote the distance between these two vectors as \\(d(\\mathbf{v}, \\mathbf{w})\\). Since the difference vector is \\(\\mathbf{v} - \\mathbf{w}\\), the distance between the two vectors is evidently just the length of the difference vector,\n\\[d(\\mathbf{v}, \\mathbf{w}) = ||\\mathbf{v} - \\mathbf{w}|| = \\sqrt{(\\mathbf{v} - \\mathbf{w}) \\cdot (\\mathbf{v} - \\mathbf{w})} = \\sqrt{(v_x-w_x)^2 - (v_y-w_y)^2}.\\]\nFor example, the distance between the two vectors \\(\\mathbf{v}=(1,1)\\) and \\(\\mathbf{w}=(1, 0)\\) is\n\\[d(\\mathbf{v}, \\mathbf{w}) = ||\\mathbf{v} - \\mathbf{w}|| = \\sqrt{(1-1)^2 + (1-0)^2} = 1.\\]\nIf a vector \\(\\mathbf{e}\\) has norm \\(||\\mathbf{e}||=1\\) it’s called a unit vector. We can convert any non-zero vector \\(\\mathbf{v}\\) into a unit vector by dividing by its norm, which is called normalizing \\(\\mathbf{v}\\). The unit vector gotten from normalizing \\(\\mathbf{v}\\) I’ll call \\(\\mathbf{e_v}\\). It’s given by\n\\[\\mathbf{e_v} = \\frac{\\mathbf{v}}{||\\mathbf{v}||}.\\]\nFor example, if \\(\\mathbf{v}=(1,1)\\), its norm is \\(||\\mathbf{v}||=\\sqrt{2}\\), so if we wanted to normalize it into a new unit vector \\(\\mathbf{e_v}\\), we’d have\n\\[\\mathbf{e_v} = \\frac{\\mathbf{v}}{||\\mathbf{v}||} = \\frac{\\mathbf{v}}{\\sqrt{2}} = \\frac{1}{\\sqrt{2}}(1,1) \\approx (0.707, 0.707).\\]\nUnit vectors will always point in the same direction as the vector used to normalize them. The only difference is they’ll have length one. In the plane, unit vectors will always lie along the unit circle. Here’s a plot of this idea using the previous example.\n\n\nCode\nv = np.array([1, 2])\nev = v / np.sqrt(2)\nplot_vectors([v, ev], title='$\\mathbf{e}_v = ||\\mathbf{v}||^{-1} \\mathbf{v}$', ticks_every=0.5, zorders=[0, 1],\n             text_offsets=[[0.01, 0.05], [-0.2, 0.2]], colors=['steelblue', 'red'],\n             labels=['$\\mathbf{v}$', '$\\mathbf{e}_v$'], headwidth=6)"
  },
  {
    "objectID": "notebooks/vector-spaces.html#projections",
    "href": "notebooks/vector-spaces.html#projections",
    "title": "5  Vector Spaces",
    "section": "5.4 Projections",
    "text": "5.4 Projections\nLet \\(\\mathbf{e}_x=(1,0)\\). It’s the unit vector pointing along the positive x-axis. Notice the dot product between \\(\\mathbf{v}=(v_x, v_y)\\) and \\(\\mathbf{e}_x\\) is just\n\\[\\mathbf{v} \\cdot \\mathbf{e}_x = v_x \\cdot 1 + v_y \\cdot 0 = v_x.\\]\nEvidently the dot product \\(\\mathbf{v} \\cdot \\mathbf{e}_x\\) “picks” out the x-component of \\(\\mathbf{v}\\), namely \\(v_x\\). The vector \\(v_x \\mathbf{e}_x = (v_x,0)\\) gotten by rescaling \\(\\mathbf{e}_x\\) by \\(v_x\\) is called the projection of \\(\\mathbf{v}\\) onto the x-axis. It’s the vector you’d get by dropping \\(\\mathbf{v}\\) perpendicular to the x-axis.\nSimilarly, if \\(\\mathbf{e}_y = (0,1)\\) is the unit vector along the positive y-axis, we can “pick out” the y-component of \\(\\mathbf{v}\\) by taking the dot product of \\(\\mathbf{v}\\) with \\(\\mathbf{e}_y\\), i.e. \\(v_y = \\mathbf{v} \\cdot \\mathbf{e}_y\\). The vector \\(v_y\\mathbf{e}_y\\) is the projection of \\(\\mathbf{v}\\) onto the y-axis.\nEvidently, then, \\(\\mathbf{v}\\) is just the sum of projections of \\(\\mathbf{v}\\) onto all of the axes,\n\\[\\mathbf{v} = v_x \\mathbf{e_x} + v_y \\mathbf{e_y}.\\]\nThis is yet another way to express a vector in terms of its components. Just project down onto the axes and sum up the linear combination.\nHere’s what this looks like when \\(\\mathbf{v}=(0.5,1)\\). In this example, the projection onto the x-axis is just \\(v_x \\mathbf{e}_x=(0.5, 0)\\), and the projection onto the y-axis is just \\(v_y \\mathbf{e_y}=(0,1)\\). Using these projections, we can write \\(\\mathbf{v}=(0.5,1)\\) as \\(\\mathbf{v} = 0.5 \\mathbf{e}_x + \\mathbf{e}_y\\).\n\n\nCode\nv = np.array([1, 2])\nex = np.array([1, 0])\ney = np.array([0, 1])\nplot_vectors([v, v[0] * ex, v[1] * ey], title='Projections Of $\\mathbf{v}$', ticks_every=0.5,\n             text_offsets=[[0.02, 0.1], [-0.1, 0.2], [0.05, 0.05]], colors=['red', 'steelblue', 'steelblue'],\n             labels=['$\\mathbf{v}$', '$v_x \\mathbf{e}_x$', '$v_y \\mathbf{e}_y$'], headwidth=4,\n             xlim=(-0.5, 2.5), ylim=(-0.5, 2.5))"
  },
  {
    "objectID": "notebooks/vector-spaces.html#linear-independence",
    "href": "notebooks/vector-spaces.html#linear-independence",
    "title": "5  Vector Spaces",
    "section": "5.5 Linear Independence",
    "text": "5.5 Linear Independence\nI just showed we can decompose any vector \\(\\mathbf{v} \\in \\mathbb{R}^2\\) into its projections \\(\\mathbf{v} = v_x \\mathbf{e}_x + v_y \\mathbf{e}_y\\). The fact we can do this is because the unit vectors \\(\\mathbf{e}_x\\) and \\(\\mathbf{e}_y\\) are special, for a few reasons.\nThe first reason these vectors are special is that they don’t lie along the same line in the plane. Said differently, we can’t write one vector as a scalar multiple of the other, \\(\\mathbf{e}_x \\neq c \\mathbf{e}_y\\) for any scalar \\(c\\). Vectors with this property are called linearly independent.\nMore generally, a set of \\(k\\) vectors \\(\\mathbf{v}_0, \\mathbf{v}_1, \\cdots, \\mathbf{v}_{k-1}\\) is called linearly independent if no one vector \\(\\mathbf{v}_j\\) in the set can be written as a linear combination of the rest. That is, for any choice of scalars \\(c_0, c_1, \\cdots, c_{k-1}\\),\n\\[\\mathbf{v}_j \\neq \\sum_{i \\neq j} c_i \\mathbf{v}_i.\\]\nA set of vectors that isn’t linearly independent is called linearly dependent. In a linearly dependent set, you can always express at least one vector as a linear combination of the rest, for example by finding a choice of scalars \\(c_0, c_1, \\cdots, c_{k-1}\\), you could write \\(\\mathbf{v}_0\\) as\n\\[\\mathbf{v}_0 = \\sum_{i=1}^{k-1} c_i \\mathbf{v}_i = c_1 \\mathbf{v}_1 + c_2 \\mathbf{v}_2 + \\cdots + c_{k-1} \\mathbf{v}_{k-1}.\\]\nLinearly dependent sets of vectors are redundant in a sense. We have more than we need. We can always keep dropping vectors from the set until the ones remaining are linearly independent.\nThe vector space spanned by all linear combinations of a set of vectors is called the span of that set. The span of a single vector will always be a line, since a linear combination of any one vector is just the scalar multiples of that vector. The span of any two linearly independent vectors will always be a plane. The span of \\(k\\) linearly independent vectors will form a \\(k\\)-dimensional hyperplane.\nAs a simple example, consider the following set of vectors in the plane,\n\\[\\begin{align*}\n\\mathbf{v}_0 &= (1, 0), \\\\\n\\mathbf{v}_1 &= (0, 1), \\\\\n\\mathbf{v}_2 &= (1, 1).\n\\end{align*}\\]\nIf you stare at these for a second, you’ll see that \\(\\mathbf{v}_2 = \\mathbf{v}_0 + \\mathbf{v}_1\\), so this set can’t be linearly independent. The third vector is redundant. Any two vectors in this set span the exact same plane \\(\\mathbb{R}^2\\). In fact, you’ll never have more than 2 linearly independent vectors of size 2. Why?\n\n\nCode\nv0 = np.array([1, 0])\nv1 = np.array([0, 1])\nv2 = np.array([1, 1])\nplot_vectors(\n    [v2, v0, v1], colors=['salmon', 'steelblue', 'limegreen'], xlim=(-0.5, 1.5), ylim=(-0.5, 1.5),\n    ticks_every=0.5, zorders=[0, 1, 2, 3], headwidth=5, text_offsets=[[0.03, 0.05], [0.03,0.05], [0.03,0.05]],\n    title='$\\mathbf{v}_0$, $\\mathbf{v}_1$, $\\mathbf{v}_2=\\mathbf{v}_0+\\mathbf{v}_1$', \n    labels=['$\\mathbf{v}_2$', '$\\mathbf{v}_0$', '$\\mathbf{v}_1$'])\n\n\n\n\n\n\n\n\n\nFor vectors in \\(\\mathbb{R}^2\\), there are only two possibilities, they either lie on the same line, or they span the whole plane. This follows from the fact that any vector \\(\\mathbf{v}\\) can be decomposed as \\(\\mathbf{v} = v_x \\mathbf{e}_x + v_y \\mathbf{e}_y\\). An implication of this fact is that a set of vectors in \\(\\mathbb{R}^2\\) can only be linearly independent if it contains only one or two vectors. If it contains a third vector, that vector must be a linear combination of the other two. The maximum number of linearly independent vectors in a set is the dimension of the vector space. Since \\(\\mathbb{R}^2\\) is 2-dimensional, it can only sustain 2 linearly independent vectors at a time."
  },
  {
    "objectID": "notebooks/vector-spaces.html#basis-vectors",
    "href": "notebooks/vector-spaces.html#basis-vectors",
    "title": "5  Vector Spaces",
    "section": "5.6 Basis Vectors",
    "text": "5.6 Basis Vectors\nIn \\(\\mathbb{R}^2\\), if we can find any two vectors \\(\\mathbf{a}\\) and \\(\\mathbf{b}\\) that are linearly independent, then we can write any other vector \\(\\mathbf{v}\\) as a linear combination of those two vectors,\n\\[\\mathbf{v} = v_a \\mathbf{a} + v_b \\mathbf{b}.\\]\nThe set \\(\\{\\mathbf{a}, \\mathbf{b}\\}\\) is called a basis. We can use vectors in this set as a “basis” to write any other vector.\nMore generally, a set of \\(k\\) vectors \\(\\mathbf{v}_0, \\mathbf{v}_1, \\cdots, \\mathbf{v}_{k-1}\\) form a basis for a vector space if the following two conditions hold,\n\nThe vectors are all linearly independent,\nThe vectors span the full vector space.\n\nAnother way of saying the same thing is that a basis is a set of exactly \\(n\\) linearly independent vectors, where \\(n\\) is the dimension of the vector space. A basis contains the minimal number of vectors needed to span the vector space.\nThe special vectors \\(\\mathbf{e}_x\\) and \\(\\mathbf{e}_y\\) form a basis for \\(\\mathbb{R}^2\\), since we can write any other vector as a linear combination of those two. Not only are these two vectors a basis, however. They satisfy two other useful properties,\n\nThey’re both unit vectors, \\(||\\mathbf{e}_x|| = ||\\mathbf{e}_y|| = 1\\).\nThey’re orthogonal to each other, that is, \\(\\mathbf{e}_x \\cdot \\mathbf{e}_y = 0\\).\n\nA basis satisfying these two properties is called an orthonormal basis. An orthonormal basis is special in that it allows us to pick out the components of a vector directly by just taking dot products with the basis vectors. It’s only true in an orthonormal basis that we can write the components of a vector \\(\\mathbf{v}\\) as,\n\\[\\begin{align*}\nv_x &= \\mathbf{v} \\cdot \\mathbf{e}_x, \\\\\nv_y &= \\mathbf{v} \\cdot \\mathbf{e}_y.\n\\end{align*}\\]\nThe set \\(\\{\\mathbf{e}_x, \\mathbf{e}_y\\}\\) is only one example of an orthonormal basis for \\(\\mathbb{R}^2\\). It’s called the standard basis, since it’s the basis whose vectors point along the usual positive x and y axes.\nExpressing any vector in terms of its basis is just projecting the vector down onto each of the basis axes. Let’s do a quick example. Let \\(\\mathbf{v}=(1.25,2)\\) be a vector. Decomposed into the standard basis we just get\n\\[\\mathbf{v} = 1.25 \\mathbf{e}_x + 2 \\mathbf{e}_y.\\]\nGraphically this just looks as follows. We’ve already seen a plot like this, except this time I’m including the basis vectors \\(\\mathbf{e}_x\\) and \\(\\mathbf{e}_y\\) explicitly. Notice that the two basis vectors form a \\(90^\\circ\\) angle, i.e. they’re perpendicular. I’ll show in a moment that this is implied by the fact that \\(\\mathbf{e}_x \\cdot \\mathbf{e}_y = 0\\).\n\n\nCode\nv = np.array([1.25, 2])\nex = np.array([1, 0])\ney = np.array([0, 1])\nplot_vectors(\n    [v, v[0] * ex, v[1] * ey, ex, ey], colors=['red', 'steelblue', 'steelblue', 'black', 'black'], \n    ticks_every=1, zorders=[0, 1, 2, 3, 4, 5], headwidth=5,\n    text_offsets=[[0,0], [0,0.2], [0.05,0], [-0.2,0.2], [0.05,0]],\n    title='$\\mathbf{v}=v_x \\mathbf{e}_x + v_y \\mathbf{e}_y$', \n    labels=['$\\mathbf{v}$', '$v_x \\mathbf{e}_x$', '$v_y \\mathbf{e}_y$', '$\\mathbf{e}_x$', '$\\mathbf{e}_y$'])\n\n\n\n\n\n\n\n\n\nOf course, I already said the standard basis isn’t the only orthonormal basis for \\(\\mathbb{R}^2\\) we could choose. Here’s an example of another one that would work equally well. Let \\(\\mathbf{e}_a=\\frac{1}{\\sqrt{2}} (1,1)\\) and \\(\\mathbf{e}_b=\\frac{1}{\\sqrt{2}} (-1,1)\\). Notice that both vectors have unit length, and they’re orthogonal since \\(\\mathbf{e}_a \\cdot \\mathbf{e}_b = 0\\). Thus, they form an orthonormal basis for \\(\\mathbb{R}^2\\). In this basis, \\(\\mathbf{v}=(1.25, 2)\\) would be written\n\\[\\mathbf{v} = (\\mathbf{v} \\cdot \\mathbf{e}_a) \\mathbf{e}_a + (\\mathbf{v} \\cdot \\mathbf{e}_b) \\mathbf{e}_b \\approx 2.298 \\mathbf{e}_a + 0.530 \\mathbf{e}_b.\\]\nThis is a very different representation for \\(\\mathbf{v}\\). Nevertheless, the two basis vectors are still perpendicular to each other. You can see a plot of this below.\nThere are infinitely many orthonormal bases for \\(\\mathbb{R}^2\\). Just take any two perpendicular vectors in the plane and normalize them to unit length and they’ll form a valid orthonormal basis.\n\n\nCode\nv = np.array([1.25, 2])\nea = np.array([1, 1]) / np.sqrt(2)\neb = np.array([-1, 1]) / np.sqrt(2)\nvectors = [v, np.dot(v, ea) * ea, np.dot(v, eb) * eb, ea, eb]\nplot_vectors(\n    vectors, ticks_every=1, zorders=[0, 1, 5, 3, 4, 2], headwidth=7,\n    colors=['red', 'steelblue', 'steelblue', 'black', 'black'],\n    text_offsets=[[0, 0], [0.03, 0], [-0.3, -0.65], [-0.1, -0.48], [-0.2, 0.15]],\n    title='$\\mathbf{v}=v_a \\mathbf{e}_a + v_b \\mathbf{e}_b$', \n    labels=['$\\mathbf{v}$', '$v_a \\mathbf{e}_a$', '$v_b \\mathbf{e}_b$', '$\\mathbf{e}_a$', '$\\mathbf{e}_b$'])"
  },
  {
    "objectID": "notebooks/vector-spaces.html#cosine-similarity",
    "href": "notebooks/vector-spaces.html#cosine-similarity",
    "title": "5  Vector Spaces",
    "section": "5.7 Cosine Similarity",
    "text": "5.7 Cosine Similarity\nJust like we can express the length of a vector using the dot product, it turns out we can also express the angle between any two vectors in the plane using the dot product. If \\(\\theta\\) is the angle between two vectors \\(\\mathbf{v}\\) and \\(\\mathbf{w}\\), it turns out the dot product is given by\n\\[\\mathbf{v} \\cdot \\mathbf{w} = ||\\mathbf{v}|| \\cdot ||\\mathbf{w}|| \\cos \\theta.\\]\nNote that both sides of this equation are scalars since the dot product is a scalar and the product of norms is a scalar. If you’re good at trigonometry, you can convince yourself this formula must be true by projecting \\(\\mathbf{v}\\) onto \\(\\mathbf{w}\\) similar to the way we did projections onto the x and y axes before. The difference this time is that the component of \\(\\mathbf{v}\\) in the direction of \\(\\mathbf{w}\\) is not \\(v_x\\) or \\(v_y\\) anymore, but instead \\(||\\mathbf{v}|| \\cos \\theta\\).\nYou can see two special cases of this formula by looking at what happens when the two vectors are parallel or perpendicular. If the two vectors are parallel, then \\(\\theta = 0^\\circ, 180^\\circ\\), so \\(\\cos \\theta = \\pm 1\\), so \\(\\mathbf{v} \\cdot \\mathbf{w} = \\pm ||\\mathbf{v}|| \\cdot ||\\mathbf{w}||\\). More importantly, if the two vectors are perpendicular, then \\(\\theta = 90^\\circ, 270^\\circ\\), so \\(\\cos \\theta = 0\\), so \\(\\mathbf{v} \\cdot \\mathbf{w} = 0\\). That is, perpendicular vectors are orthogonal. They mean the same thing.\nIt’s more common to express this formula with \\(\\cos \\theta\\) on one side and the vector terms on the other so you can solve for the angle (or more commonly just the cosine of the angle). In this case, we have \\[\\cos \\theta = \\frac{\\mathbf{v} \\cdot \\mathbf{w}}{||\\mathbf{v}|| \\cdot ||\\mathbf{w}||}.\\]\nWhat matters more than anything is what this formula says and how to use it. Suppose, for example, you want to find the angle between the two vectors \\(\\mathbf{v} = (1,1)\\) and \\(\\mathbf{w} = (0, -1)\\). Then you’d have\n\\[\\begin{align*}\n\\mathbf{v} \\cdot \\mathbf{w} &= 1 \\cdot 0 + 1 \\cdot (-1) = -1, \\\\\n||\\mathbf{v}|| &= \\sqrt{1^2 + 1^2} = \\sqrt{2}, \\\\\n||\\mathbf{w}|| &= \\sqrt{0^2 + (-1)^2} = 1.\n\\end{align*}\\]\nPlugging this into the cosine formula gives,\n\\[\n\\cos \\theta = \\frac{-1}{\\sqrt{2}} \\quad \\Longrightarrow \\quad \\theta = \\cos^{-1}\\bigg(\\frac{-1}{\\sqrt{2}}\\bigg) = 135^\\circ.\n\\]\nYou can verify this is correct by plotting the two vectors and confirming that they’re about \\(135^\\circ\\) from each other, which corresponds to about 1.25 quarter turns around a circle. It’s interesting to note that the dot product will only be negative when the angle between the two vectors is obtuse, i.e. more than \\(90^\\circ\\), which is of course the case here.\n\n\nCode\nv = np.array([1, 1])\nw = np.array([0, -1])\nplot_vectors([v, w], title='$\\mathbf{v} \\cdot \\mathbf{w} = ||\\mathbf{v}||||\\mathbf{w}|| \\cos \\\\theta$', \n             text_offsets=[[0, 0], [0.1, 0]], ticks_every=0.5, xlim=(-1, 2), ylim=(-1.5, 1.5),\n             labels=['$\\mathbf{v}$', '$\\mathbf{w}$'], colors=['red', 'steelblue'], headwidth=7)\n\n\n\n\n\n\n\n\n\nIn machine learning, this formula for \\(\\cos \\theta\\) is called the cosine similarity. The reason for this is that the dot product itself is a measure of how similar two vectors are. To see why, consider two special cases:\n\nThe two vectors are parallel: This is as large as the dot product between two vectors can get in absolute value. The vectors are as similar as they can be in a sense. Up to a scalar multiple, they contain the same information.\nThe two vectors are perpendicular: This is as small as the dot product between two vectors can get in absolute value. The two vectors are as different as they can be in a sense. They share pretty much no information. Information about one vector tells you basically nothing about the other.\n\nThe cosine similarity is a function of two input vectors \\(\\mathbf{v}\\) and \\(\\mathbf{w}\\). Since we don’t actually care about the angle \\(\\theta\\) usually, we’ll more often denote the cosine similarity using a notation like \\(\\cos(\\mathbf{v},\\mathbf{w})\\) to make it clear it’s a function of its two input vectors,\n\\[\\cos(\\mathbf{v},\\mathbf{w}) = \\frac{\\mathbf{v} \\cdot \\mathbf{w}}{||\\mathbf{v}|| \\cdot ||\\mathbf{w}||}.\\]\nNote the cosine similarity is just a normalized dot product, since dividing by the norms forces \\(-1 \\leq \\cos(\\mathbf{v},\\mathbf{w}) \\leq 1\\). It thus captures the same idea of similarity that the dot product does, but it’s more useful when the lengths of vectors get out of control. This is particularly likely to happen in high dimensions, when \\(n >> 2\\). This is the so-called “curse of dimensionality”. We’ll come back to this idea in future lessons.\nHere’s a quick implementation of the cosine similarity function using numpy. There’s no built-in function to do it, but it’s easy enough to implement by making judicious use of the np.dot function. It should give the same answer found above for \\(\\cos \\theta\\), which is \\(-\\frac{1}{\\sqrt{2}} \\approx -0.707\\).\n\n\nCode\ndef cosine_similarity(v, w):\n    return np.dot(v, w) / np.sqrt(np.dot(v, v) * np.dot(w, w))\n\nprint(f'cos(v, w) = {cosine_similarity(v, w)}')\n\n\ncos(v, w) = -0.7071067811865475\n\n\nAlgorithm Analysis: Like the dot product, this function runs in \\(O(n)\\) time. There are three independent dot product operations happening here, each adding \\(O(n)\\) FLOPS. Since the outputs of dot products are scalars, the multiply and divide only add one FLOP each. The square root isn’t obvious, but you can assume it takes some constant number of FLOPS as well. The total must therefore be \\(O(n)\\)."
  },
  {
    "objectID": "notebooks/vector-spaces.html#other-norms",
    "href": "notebooks/vector-spaces.html#other-norms",
    "title": "5  Vector Spaces",
    "section": "5.8 Other Norms",
    "text": "5.8 Other Norms\nIt turns out that the norm I defined above is only one way to measure the length of a vector. It’s the most natural way to do so sense it corresponds to your intuitive notions of length, which itself relates to the Pythagorean Theorem. There are other ways to quantify vector length as well that aren’t as intuitive. Because they do sometimes show up in machine learning I’ll briefly mention a couple of these here.\nThe norm I’ve covered is called the 2-norm. It’s called this because it involves squares and square roots. We can write it in the form\n\\[||\\mathbf{v}|| = ||\\mathbf{v}||_2 = \\big(v_x^2 + v_y^2 \\big)^{1/2}.\\]\nIt turns out we can replace the twos with any other positive number \\(p>1\\) to get generalized norms, called p-norms,\n\\[||\\mathbf{v}||_p = \\big(v_x^p + v_y^p \\big)^{1/p}.\\]\nThe p-norms cover a large class of norms, since any \\(1 \\leq p \\leq \\infty\\) can define a valid norm. The 2-norm, as you’d guess, occurs when \\(p=2\\). A couple of other norms that show up in machine learning are the 1-norm when \\(p=1\\), and the infinity norm when \\(p=\\infty\\). For 2-dimensional vectors, these norms are\n\\[\\begin{align*}\n||\\mathbf{v}||_1 &= |v_x| + |v_y|, \\\\\n||\\mathbf{v}||_\\infty &= \\max\\big(|v_x|, |v_y|\\big).\n\\end{align*}\\]\nHere’s an example. I’ll calculate the \\(p=1, 2, \\infty\\) norms for the vector \\(\\mathbf{v}=(1,-2)\\). We have,\n\\[\\begin{align*}\n||\\mathbf{v}||_1 &= |1| + |-2| = 1 + 2 = 3, \\\\\n||\\mathbf{v}||_2 &= \\sqrt{1^2 + (-2)^2} = \\sqrt{1 + 4} = \\sqrt{5} \\approx 2.236, \\\\\n||\\mathbf{v}||_\\infty &= \\max\\big(|1|, |-2|\\big) = \\max(1, 2) = 2.\n\\end{align*}\\]\nNotice that \\(||\\mathbf{v}||_1 \\geq ||\\mathbf{v}||_2 \\geq ||\\mathbf{v}||_\\infty\\). This is a general fact.\nIt’s a little hard right now to describe why these norms are useful in machine learning since we don’t currently have the context. Just know that these norms do come up sometimes. I’ll go into more depth on the uses of these different norms as we apply them. In practice though, we’ll probably work with the regular 2-norm maybe 90% of the time.\nIn numpy, you can calculate any \\(p\\)-norm using the function np.linalg.norm(v, ord=p). Here’s an example.\n\n\nCode\nv = np.array([1, -2])\nprint(f'1-Norm of v: {np.linalg.norm(v, ord=1)}')\nprint(f'2-Norm of v: {np.linalg.norm(v, ord=2)}')\nprint(f'Infinity-Norm of v: {np.linalg.norm(v, ord=np.inf)}')\n\n\n1-Norm of v: 3.0\n2-Norm of v: 2.23606797749979\nInfinity-Norm of v: 2.0"
  },
  {
    "objectID": "notebooks/vector-spaces.html#linear-maps",
    "href": "notebooks/vector-spaces.html#linear-maps",
    "title": "5  Vector Spaces",
    "section": "5.9 Linear Maps",
    "text": "5.9 Linear Maps\nSo where do matrices fit into all this vector space stuff? It turns out that matrices correspond to functions that send vectors to new vectors. These kinds of functions are called linear maps. A linear map is any vector-valued function \\(\\mathbf{w} = \\mathbf{F}(\\mathbf{v})\\) that satisfies the principle of superposition: For any scalars \\(a,b\\) and any vectors \\(\\mathbf{u}, \\mathbf{v}\\),\n\\[\\mathbf{F}(a\\mathbf{u} + b\\mathbf{v}) = a\\mathbf{F}(\\mathbf{u}) + b\\mathbf{F}(\\mathbf{v}).\\]\nSaid differently, a linear map is a function in which we can always split the function up over sums and factor out scalar constants. For vectors in the plane, linear maps are exactly the functions that map a vector \\(\\mathbf{v}=(v_x,v_y)\\) in the plane to another vector \\(\\mathbf{w}=(w_x,w_y)\\) in the plane.\nTo see how linear maps relate to matrices, let’s look at a vector in terms of the standard basis, \\(\\mathbf{v}=x\\mathbf{e}_x + y\\mathbf{e}_y\\), and see how it behaves under the map \\(\\mathbf{w} = \\mathbf{F}(\\mathbf{v})\\). Using the principle of superposition, we have\n\\[\\begin{align*}\n\\mathbf{w} &= \\mathbf{F}(\\mathbf{v}) \\\\\n&= \\mathbf{F}(x\\mathbf{e}_x + y\\mathbf{e}_y) \\\\\n&= x \\mathbf{F}(\\mathbf{e}_x) + y \\mathbf{F}(\\mathbf{e}_y). \\\\\n&= \\begin{pmatrix} \\mathbf{F}(\\mathbf{e}_x) & \\mathbf{F}(\\mathbf{e}_y) \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix}. \\\\\n\\end{align*}\\]\nEvidently, if we define a \\(2 \\times 2\\) matrix \\(\\mathbf{A}\\) as the matrix whose column vectors are \\(\\mathbf{F}(\\mathbf{e}_x)\\) and \\(\\mathbf{F}(\\mathbf{e}_y)\\), then we just get \\(\\mathbf{w} = \\mathbf{A} \\mathbf{v}\\). That is, a linear map is completely equivalent to matrix-vector multiplication by some matrix, that matrix just being the matrix of column vectors that the basis vectors get mapped to.\nAs an example, suppose we had the following linear map and wanted to figure out its corresponding matrix,\n\\[(v_x + 2v_y, 3v_x + 4v_y) = \\mathbf{F}\\big((v_x, v_y)\\big).\\]\nWhat we can do is see how \\(\\mathbf{F}(\\mathbf{v})\\) acts on the standard basis vectors. Evidently,\n\\[\n\\mathbf{F}\\big((1, 0)\\big) = (1 + 0, 3 + 0) = (1, 3), \\quad\n\\mathbf{F}\\big((0, 1)\\big) = (0 + 2, 0 + 4) = (2, 4).\n\\]\nTreating these as column vectors, we can stack them to get the \\(2 \\times 2\\) matrix\n\\[\n\\mathbf{A} =\n\\begin{pmatrix}\n1 & 2 \\\\\n3 & 4 \\\\\n\\end{pmatrix}.\n\\]\nThus, for this example, the linear map is given by\n\\[\n\\mathbf{F}(\\mathbf{v}) = \\mathbf{A} \\mathbf{v} =\n\\begin{pmatrix}\n1 & 2 \\\\\n3 & 4 \\\\\n\\end{pmatrix}\n\\begin{pmatrix}\nv_x \\\\\nv_y \\\\\n\\end{pmatrix}.\n\\]\nSince both the input and output vectors are in the plane, we can visualize them on the same plot. Let’s look at what the above example does, for example, to \\(\\mathbf{v}=(1,1)\\). Algebraically, the output should be the vector \\(\\mathbf{w}=(3, 7)\\). Geometrically, this linear map seems to be doing two things, stretching the vector out by a factor of \\(\\frac{||\\mathbf{w}||}{||\\mathbf{v}||} \\approx 5.4\\), and rotating it counterclockwise by an angle of \\(\\cos(\\mathbf{v}, \\mathbf{w}) \\approx 53^\\circ\\). It’s a general fact that we can decompose a linear map into a combination of scalings and rotations. I’ll talk more about that in the next lesson.\n\n\nCode\nA = np.array([[1, 2], [3, 4]])\nv = np.array([1, 1]).reshape(-1, 1)\nplot_vectors([v.flatten(), (A @ v).flatten()], colors=['black', 'red'],\n             labels=['$\\mathbf{{v}}$', '$\\mathbf{{A}}\\mathbf{{v}}$'], text_offsets=[[0.01, -0.1], [-0.1, 0.05]],\n             title='$\\mathbf{{w}} = \\mathbf{{A}}\\mathbf{{v}}$',  xlim=(0, 5), ylim=(0, 8))\n\n\n\n\n\n\n\n\n\nThis plot doesn’t really tell us that much about the linear map though. It only says how the map acts on a single vector \\(\\mathbf{v}=(1,1)\\). What would be really illuminating is to see how the map acts on a bunch of different vectors. We can do that using what’s called a vector field plot. To show how these work, I’ll do a vector field plot of the previous example.\nHere’s how you should read a vector field plot. For each vector you see, imagine that vector as the output to the linear map whose input is the vector pointing from the origin to that point (which we never show explicitly in the plot). For example, at the point \\(\\mathbf{v}=(1,1)\\) you should expect to see the vector \\(\\mathbf{w}=(3,7)\\). Note that the vectors have been scaled down by a factor of around 25 so that you can see them all in the plot. Other than that, they should all point in the right directions, and be of the right proportion to each other.\nThe most obvious thing you’ll probably notice in the plot is the diagonal line running from the upper left to the lower right. Along this characteristic line, the vectors seem to all be getting scaled down by the linear map, but more or less they all point in the same direction, i.e. along the line. If you look more carefully, you’ll see another characteristic line as well. Notice how vectors not on this line all seem to be pointing along the opposite diagonal, from lower left to upper right. Those vectors in fact seem to be getting scaled up by the linear map. These characteristic lines and their scaling values are special. They’re called eigenvectors and eigenvalues respectively. They in a sense encode the properties of a matrix. I’ll talk more about those in the next lesson.\n\n\nCode\nA = np.array([[1, 2], [3, 4]])\nplot_vector_field(A, alpha=0.8, title=f'Vector Field of $F(v)=Av$,\\n$A=${A.tolist()}')\n\n\n\n\n\n\n\n\n\nLet’s briefly look at some special linear maps. The first one is the identity map,\n\\[\\mathbf{v} = \\mathbf{F}(\\mathbf{v}).\\]\nThe identity map always maps a vector to itself. It’s not hard to see what the corresponding matrix is. The name gives it away. Since \\(\\mathbf{e}_x = \\mathbf{F}(\\mathbf{e}_x)\\) and \\(\\mathbf{e}_y = \\mathbf{F}(\\mathbf{e}_y)\\), the corresponding matrix is the \\(2 \\times 2\\) identity matrix\n\\[\\mathbf{I} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ \\end{pmatrix}.\\]\nGraphically, the identity map just leaves a vector where it is. It neither rotates it nor scales it.\nHere’s what the vector field plot looks like. Notice that all the output vectors are mapping radially outward, and getting larger the further you get away from the origin. It’s harder to tell from the plot, but each output vector is also the same length as the input vectors, which of course aren’t shown.\n\n\nCode\nI = np.array([[1, 0], [0, 1]])\nplot_vector_field(I, alpha=0.8, title=f'Vector Field of $F(v)=Iv$,\\n$I=${I.tolist()}')\n\n\n\n\n\n\n\n\n\nAnother special linear map is the inverse map. If \\(\\mathbf{w} = \\mathbf{F}(\\mathbf{v})\\) is a linear map, its inverse map is\n\\[\\mathbf{v} = \\mathbf{F}^{-1}(\\mathbf{w}).\\]\nLike the name suggests, the inverse map just undoes the original map, in the sense that\n\\[\\mathbf{F}^{-1}(\\mathbf{F}(\\mathbf{v})) = \\mathbf{v}.\\]\nIf \\(\\mathbf{F}(\\mathbf{v}) = \\mathbf{A}\\mathbf{v}\\) is the matrix representation of the original map, then it should be obvious that \\(\\mathbf{F}^{-1}(\\mathbf{w}) = \\mathbf{A}^{-1}\\mathbf{w}\\) is the matrix representation of the inverse map. That is, the matrix corresponding to the inverse map is just the inverse of the matrix corresponding to the original map.\nHere’s what the vector field plot looks like for the example from before with\n\\[\n\\mathbf{F}(\\mathbf{v}) = \\mathbf{A} \\mathbf{v} =\n\\begin{pmatrix}\n1 & 2 \\\\\n3 & 4 \\\\\n\\end{pmatrix}\n\\begin{pmatrix}\nv_x \\\\\nv_y \\\\\n\\end{pmatrix}.\n\\]\nThe inverse map should be given by\n\\[\n\\mathbf{F}^{-1}(\\mathbf{w}) = \\mathbf{A}^{-1} \\mathbf{w} =\n\\begin{pmatrix}\n-2 & 1 \\\\\n\\frac{3}{2} & -\\frac{1}{2} \\\\\n\\end{pmatrix}\n\\begin{pmatrix}\nw_x \\\\\nw_y \\\\\n\\end{pmatrix}.\n\\]\nNotice how in this case the characteristic lines are the same, but their behavior has flipped. With \\(\\mathbf{A}\\), the upper left to lower right diagonal made vectors smaller while the other diagonal made them bigger. But \\(\\mathbf{A}^{-1}\\) seems to do the exact opposite. Also, if you look more carefully, you’ll see the direction of the arrows has reversed too, with them all flowing in instead of going out. Interesting.\n\n\nCode\nA = np.array([[1, 2], [3, 4]])\nA_inv = np.linalg.inv(A)\nplot_vector_field(A_inv, alpha=0.8, \n    title=f'Vector Field of $F^{{-1}}(v)=A^{{-1}}v$,\\n$A^{{-1}}=${A_inv.round(2).tolist()}')\n\n\n\n\n\n\n\n\n\nThe behavior of linear maps is perhaps the real reason why matrix multiplication is so important to linear algebra as opposed to, say, element-wise multiplication. To see why, suppose we have two linear maps in the plane, \\(\\mathbf{u} = \\mathbf{F}(\\mathbf{w})\\) and \\(\\mathbf{w} = \\mathbf{G}(\\mathbf{v})\\). What happens if we compose these two maps to get\n\\[\\mathbf{u} = \\mathbf{F}(\\mathbf{G}(\\mathbf{v}))?\\]\nSince both linear maps must have some matrix representation, we can write \\(\\mathbf{u} = \\mathbf{A}\\mathbf{w}\\) and \\(\\mathbf{w} = \\mathbf{B} \\mathbf{v}\\), where \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) are two matrices whose values are determined by how their respective maps act on the standard basis \\(\\mathbf{e}_x, \\mathbf{e}_y\\). If we thus compose the two maps, we evidently get\n\\[\\mathbf{u} = (\\mathbf{A} \\mathbf{B}) \\mathbf{v}.\\]\nThat is, the composition of two linear maps is equivalent to matrix multiplication. Notice the fact that matrix multiplication doesn’t commute also implies that composition of linear maps doesn’t commute either.\nWhile linear maps are interesting for linear algebra, in machine learning we’re more often interested in *affine maps. An affine map** is just a linear map shifted by some constant vector,\n\\[\\mathbf{F}(\\mathbf{v}) = \\mathbf{A}\\mathbf{v} + \\mathbf{b}.\\]\nIn machine learning, the constant vector \\(\\mathbf{b}\\) is usually called a bias vector. Graphically, the only real difference between an affine map and a linear map is that vectors will not just get rotated and scaled, but translated by \\(\\mathbf{b}\\).\nAs an example, consider the affine map\n\\[\n\\mathbf{F}(\\mathbf{v}) = \\mathbf{A} \\mathbf{v} + \\mathbf{b} =\n\\begin{pmatrix}\n1 & 2 \\\\\n3 & 4 \\\\\n\\end{pmatrix}\n\\begin{pmatrix}\nv_x \\\\\nv_y \\\\\n\\end{pmatrix} +\n\\begin{pmatrix}\n1 \\\\\n1 \\\\\n\\end{pmatrix}.\n\\]\nIn this case, \\(\\mathbf{b}=(1,1)\\). Graphically, you can imagine this map doing the same \\(\\mathbf{A} \\mathbf{v}\\) mapping from before, but translating the tail of each vector by \\(\\mathbf{b}=(1,1)\\). For example, if we take \\(\\mathbf{v}=(1,0)\\) the affine map would look as follows.\n\n\nCode\nA = np.array([[1, 2], [3, 4]])\nb = np.array([[1], [1]])\nv = np.array([[1], [0]])\nvectors = [x.flatten() for x in [v, A @ v, A @ v + b, b]]\nplot_vectors(\n    vectors, xlim=(-1, 3), ylim=(-1, 5), headwidth=5, colors=['black', 'blue', 'red', 'green'],\n    labels=['$\\mathbf{{v}}$', '$\\mathbf{{A}}\\mathbf{{v}}$', \n            '$\\mathbf{{A}}\\mathbf{{v}} + \\mathbf{{b}}$', '$\\mathbf{{b}}$'], \n    text_offsets=[[0, 0.1], [0.6, -0.8], [-1.7, -1.3], [0.1, -0.2]], \n    tails=[[0, 0], [b[0][0], b[1][0]], [0, 0], [0, 0]],\n    zorders=[0, 1, 2, 3],\n    title='Affine Map: $\\mathbf{F}(\\mathbf{v}) = \\mathbf{A}\\mathbf{v} + \\mathbf{b}$')\n\n\n\n\n\n\n\n\n\nIt’s a little hard to see what’s going on in the vector field plot of this affine map, but again all it’s doing is shifting the output vectors by \\(\\mathbf{b}\\). Notice the characteristic lines and values don’t really seem to change. They’re a property of \\(\\mathbf{A}\\), and I didn’t change \\(\\mathbf{A}\\) in this example.\n\n\nCode\nplot_vector_field(A, b=b, alpha=0.8, title=f'Vector Field of $F(v)=Av+b$,\\n$A=${A.tolist()}, $b=${b.tolist()}')"
  },
  {
    "objectID": "notebooks/vector-spaces.html#higher-dimensional-vector-spaces",
    "href": "notebooks/vector-spaces.html#higher-dimensional-vector-spaces",
    "title": "5  Vector Spaces",
    "section": "5.10 Higher-Dimensional Vector Spaces",
    "text": "5.10 Higher-Dimensional Vector Spaces\nIt may seem like everything I’ve said is special for the case of two dimensions, but it’s really not. Every single thing I’ve said extends exactly how you’d expect to vectors of arbitrary size \\(n\\). The only difference now is that you can’t visualize the stuff anymore. You just have to trust the math. I’ll restate all of the definitions from above here, but for \\(n\\)-dimensional vector spaces instead.\nA vector of size \\(n\\) can be defined as a 1-dimensional array of real numbers \\(x_0,x_1,x_2,\\cdots,x_{n-1}\\),\n\\[\\mathbf{x} = (x_0,x_1,x_2,\\cdots,x_{n-1}).\\]\nVectors can be added together, and multiplied by scalars. Vector addition is defined element-wise. If \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are two vectors, then\n\\[\\mathbf{x} + \\mathbf{y} = (x_0+y_0, x_1+y_1, \\cdots, x_{n-1}+y_{n-1}).\\]\nTo keep a running example through this section, I’ll use numpy to create two vectors \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) each of size \\(n=10\\). Here’s their vector sum.\n\n\nCode\nx = np.array([1, 2, 3, 4, 5, 5, 4, 3, 2, 1])\ny = np.array([1, 0, -1, 0, 1, 0, -1, 0, 1, 0])\n\nprint(f'x + y = {x + y}')\n\n\nx + y = [2 2 2 4 6 5 3 3 3 1]\n\n\nScalar multiplication is defined similarly. If \\(c \\in \\mathbb{R}\\) is some scalar and \\(\\mathbf{x}\\) is some vector, then\n\\[c\\mathbf{x} = (cx_0,cx_1,\\cdots,cx_{n-1}).\\]\n\n\nCode\nc = 5\nprint(f'c * x = {c * x}')\n\n\nc * x = [ 5 10 15 20 25 25 20 15 10  5]\n\n\nVectors of size \\(n\\) live in the \\(n\\)-dimensional vector space \\(\\mathbb{R}^n\\). By definition, any linear combination of two vectors must also live in the same vector space. That is, if \\(\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^n\\) are two vectors and \\(a,b \\in \\mathbb{R}\\) are two scalars, then \\(a \\mathbf{x} + b \\mathbf{y} \\in \\mathbb{R}^n\\).\nThe dot product or inner product between two vectors \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) of size \\(n\\) is defined as their sum product, i.e.\n\\[\\mathbf{x} \\cdot \\mathbf{y} = x_0y_0 + x_1y_1 + \\cdots + x_{n-1}y_{n-1}.\\]\n\n\nCode\nprint(f'x . y = {np.dot(x, y)}')\n\n\nx . y = 1\n\n\nThe norm (technically the 2-norm) of a vector is defined as the square root of its dot product with itself, i.e.\n\\[||\\mathbf{x}|| = ||\\mathbf{x}||_2 = \\sqrt{\\mathbf{x} \\cdot \\mathbf{x}} = \\sqrt{x_0^2 + x_1^2 + \\cdots + x_{n-1}^2}.\\]\nThis is just the \\(n\\)-dimensional generalization of the Pythagorean Theorem. We can also consider other \\(p\\) norms as well. In particular, the cases when \\(p=1\\) and \\(p=\\infty\\) sometimes show up in applications,\n\\[\\begin{align*}\n||\\mathbf{x}||_1 &= \\sum_{i=0}^{n-1} |x_i| = |x_0| + |x_1| + \\cdots + |x_{n-1}|, \\\\\n||\\mathbf{x}||_\\infty &= \\max_{i=0,\\cdots,n-1} |x_i| = \\max\\big(|x_0|, |x_1|, \\cdots, |x_{n-1}|\\big).\n\\end{align*}\\]\nIt will always be the case that \\(||\\mathbf{x}||_1 \\geq ||\\mathbf{x}||_2 \\geq ||\\mathbf{x}||_\\infty\\).\n\n\nCode\nprint(f'1-Norm of x: {np.linalg.norm(x, ord=1)}')\nprint(f'2-Norm of x: {np.linalg.norm(x, ord=2)}')\nprint(f'Infinity-Norm of x: {np.linalg.norm(x, ord=np.inf)}')\n\n\n1-Norm of x: 30.0\n2-Norm of x: 10.488088481701515\nInfinity-Norm of x: 5.0\n\n\nThe distance \\(d(\\mathbf{x}, \\mathbf{y})\\) between two vectors \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) is just the norm of their difference vector,\n\\[d(\\mathbf{x}, \\mathbf{y}) = ||\\mathbf{x}-\\mathbf{y}|| = \\sum_{i=0}^{n-1} \\sqrt{(x_i-y_i)^2} = \\sqrt{(x_0-y_0)^2 + (x_1-y_1)^2 + \\cdots + (x_{n-1}-y_{n-1})^2}.\\]\nWe can define the angle between any two vectors \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) of size \\(n\\) by making use of the same identity for the dot product, which still holds in \\(n\\) dimensions,\n\\[\\mathbf{x} \\cdot \\mathbf{y} = ||\\mathbf{x}|| \\cdot ||\\mathbf{y}|| \\cos \\theta.\\]\nUsing this identity, we can define the cosine similarity \\(\\cos(\\mathbf{x}, \\mathbf{y})\\) by solving for \\(\\cos \\theta\\),\n\\[\\cos(\\mathbf{x}, \\mathbf{y}) = \\frac{\\mathbf{x} \\cdot \\mathbf{y}}{||\\mathbf{x}|| \\cdot ||\\mathbf{y}||}.\\]\nThe dot product is a measure of how similar two vectors are, and the cosine similarity is a normalized measure of how similar two vectors are, since dividing by the norms forces \\(-1 \\leq \\cos \\theta \\leq 1\\).\n\n\nCode\nprint(f'cos(x, y) = {cosine_similarity(x, y)}')\n\n\ncos(x, y) = 0.04264014327112208\n\n\nA set of vectors \\(\\mathbf{x}_0, \\mathbf{x}_1, \\cdots, \\mathbf{x}_{k-1}\\) is linearly independent if no one vector is a linear combination of the rest,\n\\[\\mathbf{x}_j \\neq \\sum_{i \\neq j} c_i \\mathbf{x}_j.\\]\nIf one vector is a linear combination of the rest, they’re linearly dependent. If there are exactly \\(n\\) linear independent vectors in the set, it’s called a basis.\nWe can define the standard basis on \\(\\mathbb{R}^n\\) with the following complete set of size \\(n\\) unit vectors,\n\\[\\begin{align*}\n\\mathbf{e}_0 &= (1, 0, 0, \\cdots, 0), \\\\\n\\mathbf{e}_1 &= (0, 1, 0, \\cdots, 0), \\\\\n\\vdots \\ &= \\qquad \\vdots \\\\\n\\mathbf{e}_{n-1} &= (0, 0, 0, \\cdots, 1).\n\\end{align*}\\]\nThe standard basis is an orthonormal basis since each vector is a unit vector and they’re all mutually orthogonal, i.e.\n\\[\n\\mathbf{e}_i \\cdot \\mathbf{e}_j = \\delta_{ij} =\n\\begin{cases}\n1 & i = j, \\\\\n0 & i \\neq j.\n\\end{cases}\n\\]\nNotation: The symbol \\(\\delta_{ij}\\) is called the Kronecker delta. It’s just a shorthand way of writing something is \\(1\\) if \\(i=j\\) and \\(0\\) if \\(i \\neq j\\).\n\n\nCode\nn = 10\ne = [ei.flatten().astype(int) for ei in np.eye(n)]\nprint(f'e3 = {e[3]}')\nprint(f'e8 = {e[8]}')\nprint(f'e3 . e3 = {np.dot(e[3], e[3])}')\nprint(f'e3 . e8 = {np.dot(e[3], e[8])}')\n\n\ne3 = [0 0 0 1 0 0 0 0 0 0]\ne8 = [0 0 0 0 0 0 0 0 1 0]\ne3 . e3 = 1\ne3 . e8 = 0\n\n\nIf a basis is orthonormal, any vector \\(\\mathbf{x}\\) can be decomposed into a linear combination of the basis elements by taking the dot product \\(\\mathbf{x} \\cdot \\mathbf{e}_i\\). For the standard basis, these just give the vector components \\(x_i\\),\n\\[\\mathbf{x} = \\sum_{i=0}^{n-1} (\\mathbf{x} \\cdot \\mathbf{e}_i) \\mathbf{e}_i = \\sum_{i=0}^{n-1} x_i \\mathbf{e}_i = x_0 \\mathbf{e}_0 + x_1 \\mathbf{e}_1 + \\cdots x_{n-1} \\mathbf{e}_{n-1}.\\]\nEach term \\(x_i \\mathbf{e}_i\\) in the sum corresponds to the projection of \\(\\mathbf{x}\\) onto the \\(i\\)th axis. Each axis in \\(\\mathbb{R}^n\\) is still a single line, but now there are \\(n\\) of these axis lines, all perpendicular to each other.\nA linear map is a vector-valued function \\(\\mathbf{y}=\\mathbf{F}(\\mathbf{x})\\) between vector spaces that preserves the linear structure of the spaces. In general, \\(\\mathbf{x} \\in \\mathbb{R}^m\\) and \\(\\mathbf{y} \\in \\mathbb{R}^n\\) need not be in the same vector spaces. Either way, a linear map can always be expressed as a matrix-vector equation \\(\\mathbf{y}=\\mathbf{A}\\mathbf{x}\\), where \\(\\mathbf{A}\\) is some \\(m \\times n\\) matrix whose entries are determined by how the map acts on the standard basis vectors, i.e. for each column \\(i=0,1,\\cdots,n-1\\) we have\n\\[\\mathbf{A}_{:, i} = \\mathbf{F}(\\mathbf{e}_i).\\]\nMore generally, an affine map is a linear map shifted by some bias vector \\(\\mathbf{b} \\in \\mathbb{R}^m\\). Affine maps can always be expressed as a shifted matrix-vector equation, \\(\\mathbf{y}=\\mathbf{A}\\mathbf{x} + \\mathbf{b}\\).\nApplication: Roughly speaking a neural network is just a composite function of successive affine maps. The only real difference with a neural network is that in between each affine map we apply a non-linear activation function to make the output do more interesting things. Most of the time nowadays the activation function is just the ReLU function \\(\\text{ReLU}(\\mathbf{z})=\\max(\\mathbf{0}, \\mathbf{z})\\). All it does is sets any negative entries to zero. For example, here’s what a “single hidden layer neural network” might look like for a regression problem,\n\\[\\begin{align*}\n\\mathbf{z} &= \\mathbf{A}\\mathbf{x} + \\mathbf{b}, \\\\\n\\mathbf{a} &= \\text{ReLU}(\\mathbf{z}), \\\\\n\\mathbf{y} &= \\mathbf{B}\\mathbf{a} + \\mathbf{c}. \\\\\n\\end{align*}\\]\nThe main thing about a neural network though is that the values in the matrix and bias vectors are learned from the training data. That is, they’re tuned specifically to make sure the neural network approximates the true input-output relationship behavior in the data as well as possible.\nJust as with linear maps in the plane, linear maps in higher dimensions always preserve lines. Not just lines in fact, but planes and hyperplanes as well. These generalizations of lines are called linear subspaces. Linear subspaces will always be hyperplanes in \\(n\\)-dimensional space that pass through the origin. Think of them as planes passing through the origin, but in more dimensions. If the hyperplane spanned by \\(\\mathbf{x}_0, \\mathbf{x}_1, \\cdots, \\mathbf{x}_{k-1}\\) is some \\(k\\)-dimensional linear subspace of \\(\\mathbb{R}^n\\), then its image under the linear map will be a new \\(k\\)-dimensional linear subspace in \\(\\mathbb{R}^m\\) (if \\(k \\leq m\\), otherwise it’ll just be the full vector space \\(\\mathbb{R}^m\\) itself). Any linear combination of vectors in a given subspace will stay inside that subspace. It’s closed under vector space operations. For all practical purposes it’s a new vector space \\(\\mathbb{R}^k\\) unto itself."
  },
  {
    "objectID": "notebooks/matrix-algebra.html#properties-of-matrices",
    "href": "notebooks/matrix-algebra.html#properties-of-matrices",
    "title": "6  Matrix Algebra",
    "section": "6.1 Properties of Matrices",
    "text": "6.1 Properties of Matrices\n\n6.1.1 Matrix Spaces\nJust like vectors, matrices can be thought of as objects in their own matrix space. A matrix space is just a vector space, except it has two dimensions \\(m\\) and \\(n\\). We’ll denote the matrix space of \\(m \\times n\\) matrices with the symbol \\(\\mathbb{R}^{m \\times n}\\). Just like vector spaces, matrix spaces must be closed under linear combinations. If \\(\\mathbf{A}, \\mathbf{B} \\in \\mathbb{R}^{m \\times n}\\) are two matrices, then any matrix linear combination \\(\\mathbf{C} = a\\mathbf{A} + b\\mathbf{B}\\) must also be a valid \\(m \\times n\\) matrix in \\(\\mathbb{R}^{m \\times n}\\). This means matrices behave the same way under addition and scalar multiplication as vectors do.\nWhile this fact should be kind of obvious by now, here’s an example anyway. I’ll choose \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) to both be \\(2 \\times 2\\) here. Adding them together or scalar multiplying them should also obviously give a matrix that’s \\(2 \\times 2\\), since everything is element-wise.\n\n\nCode\na = 5\nA = np.array(\n    [[1, 1], \n     [1, 1]])\nB = np.array(\n    [[1, -1], \n     [-1, 1]])\nprint(f'{a}A = \\n{5 * A}')\nprint(f'A + B = \\n{A + B}')\n\n\n5A = \n[[5 5]\n [5 5]]\nA + B = \n[[2 0]\n [0 2]]\n\n\nSince every matrix corresponds to a linear map \\(\\mathbf{F}(\\mathbf{x}) = \\mathbf{A}\\mathbf{x}\\), the space of matrices also corresponds to the space of linear maps from vectors \\(\\mathbf{x} \\in \\mathbb{R}^n\\) to vectors \\(\\mathbf{y} \\in \\mathbb{R}^m\\). Recall that the composition of linear maps is equivalent to matrix multiplication. If \\(\\mathbf{F}(\\mathbf{y}) = \\mathbf{A}\\mathbf{y}\\) and \\(\\mathbf{G}(\\mathbf{x}) = \\mathbf{B}\\mathbf{x}\\) are two linear maps, then their composition is equivalent to the matrix product of the two maps,\n\\[\\mathbf{z}=\\mathbf{F}(\\mathbf{G}(\\mathbf{x})) = \\mathbf{A}\\mathbf{B}\\mathbf{x}.\\]\nThe composition, and hence the matrix multiplication operation, only makes sense when the two matrices are compatible, i.e. \\(\\mathbf{A} \\in \\mathbb{R}^{m \\times n}\\) and \\(\\mathbf{B} \\in \\mathbb{R}^{n \\times p}\\). It also follows from this relationship to linear maps (which are of course just functions) that matrix multiplication is associative, i.e. we can put parenthesis wherever we like,\n\\[\\mathbf{A}\\mathbf{B}\\mathbf{C} = (\\mathbf{A}\\mathbf{B})\\mathbf{C} = \\mathbf{A}(\\mathbf{B}\\mathbf{C}).\\]\nDo remember, however, that matrix multiplication (and function composition) doesn’t commute, i.e. \\(\\mathbf{A}\\mathbf{B} \\neq \\mathbf{B}\\mathbf{A}\\), even when the two matrices are compatible.\n\n\n6.1.2 Transposes\nRecall that every matrix \\(\\mathbf{A} \\in \\mathbb{R}^{m \\times n}\\) has a transpose matrix \\(\\mathbf{A}^\\top \\in \\mathbb{R}^{n \\times m}\\) that’s defined as the same matrix, but with the indices swapped,\n\\[(A^\\top)_{i,j} = A_{j,i}.\\]\nHere’s a quick example for a \\(2 \\times 3\\) matrix \\(\\mathbf{A}\\).\n\n\nCode\nA = np.array(\n    [[1, 2, 3], \n     [4, 5, 6]])\nprint(f'A^T = \\n{A.T}')\n\n\nA^T = \n[[1 4]\n [2 5]\n [3 6]]\n\n\nWhat happens if we multiply two transposed matrices? Suppose \\(\\mathbf{A}\\) is \\(m \\times n\\) and \\(\\mathbf{B}\\) is \\(n \\times p\\). Then \\(\\mathbf{A}\\mathbf{B}\\) is \\(m \\times p\\). That means its transpose \\((\\mathbf{A}\\mathbf{B})^\\top\\) should be \\(p \\times m\\). But \\(\\mathbf{A}^\\top\\) is \\(n \\times m\\) and \\(\\mathbf{B}^\\top\\) is \\(p \\times n\\). This implies that the transpose of the product can only make sense if it’s the product of the transposes, but in opposite order so the shapes match up right,\n\\[(\\mathbf{A}\\mathbf{B})^\\top = \\mathbf{B}^\\top \\mathbf{A}^\\top.\\]\nThis is not really a proof of this fact. If you want a proof, what you’ll want to do is look at the individual elements of each side, and show the equation must be true element-by-element. I won’t bore you with this. I’ll just give you an example with numpy so you can see they have to be equal. I’ll take \\(\\mathbf{A}\\) to be \\(3 \\times 2\\) and \\(\\mathbf{B}\\) to be \\(2 \\times 3\\), which means \\((\\mathbf{A}\\mathbf{B})^\\top\\) should be \\(2 \\times 2\\). Recall you can transpose a matrix in numpy using A.T or np.transpose(A).\n\n\nCode\nA = np.array(\n    [[1, 2, 3], \n     [4, 5, 6]])\nB = np.array(\n    [[-1, -2], \n     [-3, -4], \n     [-5, -6]])\nprint(f'(AB)^T = \\n{(A @ B).T}')\nprint(f'B^T A^T = \\n{B.T @ A.T}')\n\n\n(AB)^T = \n[[-22 -49]\n [-28 -64]]\nB^T A^T = \n[[-22 -49]\n [-28 -64]]\n\n\n\n\n6.1.3 Inverses\nWhen a matrix is square, i.e. \\(\\mathbf{A}\\) is \\(n \\times n\\), we can think of it as mapping vectors to other vectors in the same vector space \\(\\mathbb{R}^n\\). The identity map (the “do nothing” map) always maps a vector to itself. It corresponds to the \\(n \\times n\\) identity matrix\n\\[\n\\mathbf{I} =\n\\begin{pmatrix}\n1 & 0 & 0 & \\cdots & 0 \\\\\n0 & 1 & 0 & \\cdots & 0 \\\\\n0 & 0 & 1 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & 0 & \\cdots & 1 \\\\\n\\end{pmatrix}.\n\\]\nHere’s an example. I’ll use np.eye(n) to generate the identity matrix for \\(n=5\\).\n\n\nCode\nI = np.eye(5)\nprint(f'I = \\n{I}')\n\n\nI = \n[[1. 0. 0. 0. 0.]\n [0. 1. 0. 0. 0.]\n [0. 0. 1. 0. 0.]\n [0. 0. 0. 1. 0.]\n [0. 0. 0. 0. 1.]]\n\n\nRecall the inverse of a square matrix \\(\\mathbf{A}\\) is the matrix \\(\\mathbf{A}^{-1}\\) satisfying\n\\[\\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{I}.\\]\nThe inverse matrix \\(\\mathbf{A}^{-1}\\) will exist exactly when the determinant of \\(\\mathbf{A}\\) is nonzero, i.e. \\(\\text{det}(\\mathbf{A}) \\neq 0\\). If the determinant is zero, then the matrix is singular, and no inverse can be found no matter how hard you look for one.\nRecall that in numpy you can invert a square matrix using np.linalg.inv(A). It’s usually not a good idea to do so because inverting a matrix is numerically unstable, but you can in principle. The inverse calculation runs in \\(O(n^3)\\) time just like multiplication.\nHere’s an example where \\(\\mathbf{A}\\) is \\(2 \\times 2\\). You can already see from this example the numerical loss of precision creeping in, since neither \\(\\mathbf{A}^{-1}\\mathbf{A}\\) nor \\(\\mathbf{A}\\mathbf{A}^{-1}\\) exactly yield the identity matrix.\n\n\nCode\nA = np.array(\n    [[1, 2], \n     [3, 4]])\nA_inv = np.linalg.inv(A)\nprint(f'A^(-1) = \\n{A_inv}')\nprint(f'A^(-1) A = \\n{A_inv @ A}')\nprint(f'A A^(-1) = \\n{A @ A_inv}')\n\n\nA^(-1) = \n[[-2.   1. ]\n [ 1.5 -0.5]]\nA^(-1) A = \n[[1.00000000e+00 0.00000000e+00]\n [1.11022302e-16 1.00000000e+00]]\nA A^(-1) = \n[[1.0000000e+00 0.0000000e+00]\n [8.8817842e-16 1.0000000e+00]]\n\n\nJust like with the transpose, we can ask what happens if we try to invert the product of two matrices. You can convince yourself that the same kind of rule holds: the inverse of a product is the product of the inverses in reverse order,\n\\[(\\mathbf{A}\\mathbf{B})^{-1} = \\mathbf{B}^{-1} \\mathbf{A}^{-1}.\\]\nHere’s a \\(2 \\times 2\\) “proof” of this fact.\n\n\nCode\nA = np.array(\n    [[1, 2], \n     [3, 4]])\nB = np.array(\n    [[1, 0], \n     [1, 1]])\nA_inv = np.linalg.inv(A)\nB_inv = np.linalg.inv(B)\nAB_inv = np.linalg.inv(A @ B)\nprint(f'(AB)^(-1) = \\n{AB_inv}')\nprint(f'B^(-1) A^(-1) = \\n{B_inv @ A_inv}')\n\n\n(AB)^(-1) = \n[[-2.   1. ]\n [ 3.5 -1.5]]\nB^(-1) A^(-1) = \n[[-2.   1. ]\n [ 3.5 -1.5]]\n\n\nI encourage you to check this result using the fact I derived from the last lesson for \\(2 \\times 2\\) matrices,\n\\[\n\\mathbf{A} =\n\\begin{pmatrix}\na & b \\\\\nc & d \\\\\n\\end{pmatrix} \\quad \\Longrightarrow \\quad\n\\mathbf{A}^{-1} = \\frac{1}{ad-bc}\n\\begin{pmatrix}\nd & -b \\\\\n-c & a \\\\\n\\end{pmatrix}.\n\\]\nIt also turns out that the transpose and inverse operations commute with each other,\n\\[(\\mathbf{A}^\\top)^{-1} = (\\mathbf{A}^{-1})^\\top.\\]\nRecall the pseudoinverse of a matrix is the generalization of the inverse to rectangular matrices. If \\(\\mathbf{A}\\) is an \\(m \\times n\\) matrix, the pseudoinverse is defined by,\n\\[\n\\mathbf{A}^+ =\n\\begin{cases}\n(\\mathbf{A}^\\top \\mathbf{A})^{-1} \\mathbf{A}^\\top, & m > n \\\\\n\\mathbf{A}^\\top (\\mathbf{A} \\mathbf{A}^\\top)^{-1}, & m < n. \\\\\n\\end{cases}\n\\]\nFor the pseudoinverse to exist in either case, the smallest dimension needs to have all linearly independent vectors. Some properties of the matrix inverse also hold for the pseudoinverse,\n\\[(\\mathbf{A}\\mathbf{B})^+ = \\mathbf{B}^+ \\mathbf{A}^+,\\] \\[(\\mathbf{A}^\\top)^+ = (\\mathbf{A}^+)^\\top.\\]\nSome properties don’t though. For example, the pseudoinverse isn’t a two-sided inverse anymore. It’s a left inverse if \\(m > n\\), and a right inverse if \\(m < n\\).\n\n\n6.1.4 Determinant and Trace\nNotice something with the \\(2 \\times 2\\) matrix above. Since \\(\\text{det}(\\mathbf{A}) = ad - bc\\), we can evidently write\n\\[\\mathbf{A}^{-1} = \\frac{1}{\\text{det}(\\mathbf{A})} \\mathbf{\\tilde A},\\]\nwhere \\(\\mathbf{\\tilde A}\\) is some kind of matrix related to \\(\\mathbf{A}\\). The properties of \\(\\mathbf{\\tilde A}\\) aren’t important (it’s called the adjugate if you’re curious). But this general fact turns out to be true for any \\(n \\times n\\) matrix, except the formula for the determinant gets a lot more complicated. What’s important is that \\(\\mathbf{A}^{-1}\\) is inversely proportional to the determinant, i.e.\n\\[\\mathbf{A}^{-1} \\propto \\frac{1}{\\text{det}(\\mathbf{A})}.\\]\nThat’s why we can’t allow \\(\\text{det}(\\mathbf{A}) = 0\\), because then \\(\\mathbf{A}^{-1}\\) blows up due to the division by zero. Now, I’ve already said \\((\\mathbf{A}\\mathbf{B})^{-1} = \\mathbf{B}^{-1} \\mathbf{A}^{-1}\\). If then\n\\[\\mathbf{A}^{-1} \\propto \\frac{1}{\\text{det}(\\mathbf{A})}, \\quad \\mathbf{B}^{-1} \\propto \\frac{1}{\\text{det}(\\mathbf{B})},\\]\nwe evidently must have that\n\\[(\\mathbf{AB})^{-1} \\propto \\frac{1}{\\text{det}(\\mathbf{AB})} \\propto \\mathbf{B}^{-1} \\mathbf{A}^{-1} \\propto \\frac{1}{\\text{det}(\\mathbf{A}) \\cdot \\text{det}(\\mathbf{B})}.\\]\nIt thus seems to be the case that the determinant of a product is proportional to the product of the determinants. This proportionality turns out to be exact, i.e.\n\\[\\text{det}(\\mathbf{A}\\mathbf{B}) = \\text{det}(\\mathbf{A}) \\cdot \\text{det}(\\mathbf{B}) = \\text{det}(\\mathbf{B}) \\cdot \\text{det}(\\mathbf{A}).\\]\nIn general, the determinant of an \\(n \\times n\\) matrix \\(\\mathbf{A}\\) is a nasty \\(n\\) degree multivariate polynomial of the elements of \\(\\mathbf{A}\\). There’s no reliably easy way to calculate it except for small \\(n\\) matrices. In numpy, you can use np.linalg.det(A) to calculate the determinant, but just as with inverses, this is a numerically unstable operation, and so should be avoided where possible. Moreover, it runs in \\(O(n^3)\\) time, which is just as slow as matrix multiplication.\nHere’s an example. I’ll verify this “product rule” for determinants using two \\(3 \\times 3\\) matrices. The determinant of both matrices turns out to be \\(6\\), which means their product should have determinant \\(36\\).\n\n\nCode\nA = np.array(\n    [[3, 0, 0],\n     [1, 2, 0],\n     [1, 1, 1]])\nB = np.array(\n    [[1, 1, 1],\n     [0, 2, 1],\n     [0, 0, 3]])\ndet_A = np.linalg.det(A)\ndet_B = np.linalg.det(B)\ndet_AB = np.linalg.det(A @ B)\nprint(f'det(A) = {det_A}')\nprint(f'det(B) = {det_B}')\nprint(f'det(AB) = {det_AB}')\n\n\ndet(A) = 6.0\ndet(B) = 6.0\ndet(AB) = 36.0\n\n\nNotice in both cases the determinant happens to be the product of the diagonal elements\n\\[\\text{det}(\\mathbf{A}) = \\text{det}(\\mathbf{B}) = 1 \\cdot 2 \\cdot 3 = 6.\\]\nI rigged the result to come out this way. It’s not always true. It’s only true when a matrix is either lower triangular (the elements above the diagonal are all zero), upper triangular (the elements below the diagonal are all zero), or diagonal (the elements off the diagonal are all zero). In this example, \\(\\mathbf{A}\\) was lower triangular and \\(\\mathbf{B}\\) was upper triangular. I chose both to have the same diagonal elements (in different order) on purpose.\nMore generally, if \\(\\mathbf{A}\\) is diagonal or upper/lower triangular, then\n\\[\\text{det}(\\mathbf{A}) = \\prod_{i=0}^{n-1} A_{i,i} = A_{0,0} A_{1,1} \\cdots A_{n-1,n-1}.\\]\nIt’s not yet obvious, but we can always “change” a square matrix \\(\\mathbf{A}\\) into one of these three kinds of matrices, and then calculate the determinant of \\(\\mathbf{A}\\) this way. There are a few ways to do this. I’ll cover these when I get to matrix factorizations below.\nSome other properties of the determinant that you can verify are,\n\n\\(\\text{det}(\\mathbf{I}) = 1\\).\n\\(\\text{det}(\\mathbf{A}^\\top) = \\text{det}(\\mathbf{A})\\).\n\\(\\text{det}(\\mathbf{A}^{-1}) = \\frac{1}{\\text{det}(\\mathbf{A})}\\).\n\\(\\text{det}(c\\mathbf{A}) = c^n\\text{det}(\\mathbf{A})\\).\n\nThe determinant is one important way to get a scalar out of a matrix. Another useful scalar is the trace, which is far simpler to calculate. The trace of a matrix \\(\\mathbf{A}\\) is the sum of its diagonal elements, usually written\n\\[\\text{tr}(\\mathbf{A}) = \\sum_{i=0}^{n-1} A_{i,i} = A_{0,0} + A_{1,1} + \\cdots + A_{n-1,n-1}.\\]\nUnlike the determinant, the trace doesn’t split up over products. It instead splits over addition,\n\\[\\text{tr}(\\mathbf{A} + \\mathbf{B}) = \\text{tr}(\\mathbf{A}) + \\text{tr}(\\mathbf{B}).\\]\nThis is very easy to verify from the fact that the sum is element-wise, so \\(\\sum (A+B)_{i,i} = \\sum A_{i,i} + \\sum B_{i,i}\\).\nSome other fairly trivial properties the trace satisfies are,\n\n\\(\\text{tr}(\\mathbf{I}) = n\\).\n\\(\\text{tr}(\\mathbf{A}^\\top) = \\text{tr}(\\mathbf{A})\\).\n\\(\\text{tr}(c\\mathbf{A}) = c\\text{tr}(\\mathbf{A})\\).\n\\(\\text{tr}(\\mathbf{A}\\mathbf{B}) = \\text{tr}(\\mathbf{B}\\mathbf{A})\\).\n\nHere’s a “proof” of the last result on the same \\(3 \\times 3\\) matrices above. In numpy, you can calculate the trace using np.trace. It’s not unstable like the determinant is, and it’s fast to calculate since it’s only summing the \\(n\\) diagonal terms, which is \\(O(n)\\) time.\n\n\nCode\ntr_AB = np.trace(A @ B)\ntr_BA = np.trace(B @ A)\nprint(f'tr(AB) = {tr_AB}')\nprint(f'tr(BA) = {tr_BA}')\n\n\ntr(AB) = 13\ntr(BA) = 13\n\n\nIt’s kind of obvious what the determinant is good for. It tells you how invertible a matrix is. But what does the trace tell you? It turns out both the trace and the determinant also tell you something important about the scale of the matrix. We’ll see this in more depth below when we talk about eigenvalues.\n\n\n6.1.5 Linear Independence and Rank\nWe can always think of a matrix in terms of its column vectors. If \\(\\mathbf{A}\\) is \\(m \\times n\\), it has \\(n\\) column vectors \\(\\mathbf{A}_{:, 0}, \\mathbf{A}_{:, 1}, \\cdots, \\mathbf{A}_{:, n-1}\\) each of size \\(m\\). Concatenated together in order, the column vectors form the matrix itself,\n\\[\n\\mathbf{A} =\n\\begin{pmatrix}\n\\mathbf{A}_{:, 0} & \\mathbf{A}_{:, 1} & \\cdots & \\mathbf{A}_{:, n-1}\n\\end{pmatrix}.\n\\]\nIt turns out these column vectors also tell us how invertible a matrix is, but in a more general and useful way than the determinant does. Roughly speaking, a matrix is invertible if we can’t write any one column vector as a function of the other column vectors. This is just the definition of linear independence.\nRecall a set of vectors \\(\\mathbf{x}_0, \\mathbf{x}_1, \\cdots, \\mathbf{x}_{k-1}\\) is linearly independent if no one vector is a linear combination of the rest,\n\\[\\mathbf{x}_j \\neq \\sum_{i \\neq j} c_i \\mathbf{x}_j.\\]\nIf one vector is a linear combination of the rest, they’re linearly dependent.\nAn \\(n \\times n\\) matrix \\(\\mathbf{A}\\) is invertible if and only if its column vectors are all linearly independent. Equivalently, the column vectors span an \\(n\\)-dimensional vector space. To see why this is true, let’s look at a \\(2 \\times 2\\) matrix \\(\\mathbf{A}\\) with column vectors \\(\\mathbf{a}=\\binom{a}{b}\\) and \\(\\mathbf{b}=\\binom{c}{d}\\),\n\\[\n\\mathbf{A} = \\begin{pmatrix} \\mathbf{a} & \\mathbf{b} \\end{pmatrix} =\n\\begin{pmatrix}\na & b \\\\\nc & d \\\\\n\\end{pmatrix}.\n\\]\nNow, if \\(\\mathbf{a}\\) and \\(\\mathbf{b}\\) are linearly dependent, then \\(\\mathbf{b}\\) must be a scalar multiple of \\(\\mathbf{a}\\), say \\(\\mathbf{b} = \\beta \\mathbf{a}\\). Then \\(\\mathbf{A}\\) would look like\n\\[\n\\mathbf{A} = \\begin{pmatrix} \\mathbf{a} & \\beta \\mathbf{a} \\end{pmatrix} =\n\\begin{pmatrix}\na & \\beta a \\\\\nc & \\beta c \\\\\n\\end{pmatrix}.\n\\]\nThis means its determinant would be \\(\\text{det}(\\mathbf{A}) = \\beta ac - \\beta ac = 0\\), which of course means \\(\\mathbf{A}\\) can’t be invertible.\nGraphically, saying the column vectors are linearly dependent is saying they’ll map any vector onto the same subspace. For the \\(2 \\times 2\\) case, that means any vector \\(\\mathbf{v}\\) hit by \\(\\mathbf{A}\\) will get mapped onto the same line, no matter what \\(\\mathbf{v}\\) you pick. The matrix is collapsing, or projecting, the vector space down to a lower-dimensional subspace.\nHere’s a plot of this idea. I’ll make \\(\\mathbf{A}\\) have two linearly dependent columns, then plot its action on several different vectors, plotted in black. Acting on these by \\(\\mathbf{A}\\) will map them to the red vectors, which all lie on the same line in the plane. They’re all collapsing onto the same subspace, evidently the line \\(y=-x\\).\n\n\nCode\nbeta = 1.5\na0 = np.array([1, -1]).reshape(-1, 1)\na1 = beta * a0\nA = np.hstack([a0, a1])\nv = np.array([1, 1]).reshape(-1, 1)\nw = np.array([-1, 0]).reshape(-1, 1)\nu = np.array([1, -3]).reshape(-1, 1)\nvectors = [x.flatten() for x in [v, A @ v, w, A @ w, u, A @ u]]\n\nplot_vectors(vectors, colors=['black', 'red'] * 3, title='Linearly Dependence',\n             labels=['$\\mathbf{v}$', '$\\mathbf{A}\\mathbf{v}$'] + [''] * 4,\n             text_offsets=[[0, 0]] * 6, headwidth=5)\n\n\n\n\n\n\n\n\n\nThe number of linearly independent column vectors a matrix has is called its rank, written \\(\\text{rank}(\\mathbf{A})\\). Clearly it’ll always be the case that \\(\\text{rank}(\\mathbf{A}) \\leq n\\). When \\(\\text{rank}(\\mathbf{A}) = n\\) exactly the matrix is called full rank. Only full rank square matrices are invertible.\nHere’s an example. I’ll use np.linalg.matrix_rank(A) to calculate the rank of the above \\(2 \\times 2\\) example. Since \\(\\text{rank}(\\mathbf{A})=1<2\\), the matrix \\(\\mathbf{A}\\) must be singular, as I’ve of course already shown.\n\n\nCode\nrank = np.linalg.matrix_rank(A)\nprint(f'rank(A) = {rank}')\n\n\nrank(A) = 1\n\n\nThe idea of rank also extends to rectangular matrices. An \\(m \\times n\\) matrix \\(\\mathbf{A}\\) is called full rank if the vectors in its minimum dimension are linearly independent. That is, if \\(m < n\\), the row vectors must all be linearly independent. And if \\(m > n\\) the column vectors must all be linearly independent.\n\n\n6.1.6 Outer Products\nWe’ll frequently be interested in low rank matrices, which are matrices whose rank is much much less than the dimension, i.e. \\(\\text{rank}(\\mathbf{A}) << n\\). As we’ll see, low rank matrices are special because they can efficiently compress the information contained in a matrix, which often allows us to represent data more efficiently, or clean up data by denoising away “unnecessary” dimensions. In fact, approximating a matrix with a lower rank matrix is the whole idea behind dimension reduction, one of the core areas of unsupervised learning.\nThe most useful low-rank matrices are the outer products of two vectors. If \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are size \\(n\\) vectors, define their outer product by\n\\[\n\\mathbf{x} \\mathbf{y}^\\top =\n\\begin{pmatrix}\nx_0 y_0 & x_0 y_1 & \\cdots & x_0 y_{n-1} \\\\\nx_1 y_0 & x_1 y_1 & \\cdots & x_1 y_{n-1} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nx_{n-1} y_0 & x_{n-1} y_1 & \\cdots & x_{n-1} y_{n-1} \\\\\n\\end{pmatrix}.\n\\]\nNotice each column vector \\(\\mathbf{A}_{:, j}\\) of the outer product matrix is linearly proportional to the first column \\(\\mathbf{A}_{:, 0}\\), since\n\\[\\mathbf{A}_{:, j} = \\mathbf{x} y_j = \\mathbf{x} y_0 \\frac{y_j}{y_0} = \\frac{y_j}{y_0} \\mathbf{A}_{:, 0}.\\]\nThis means that only one column vector is linearly independent, which implies \\(\\text{rank}(\\mathbf{x} \\mathbf{y}^\\top)=1\\). The outer product is evidently rank-1, and hence highly singular. You’d never be able to invert it. But it is useful as we’ll see soon.\nHere’s an example of an outer product calculation. You can either calculate x @ y.T directly or use np.outer(x, y). Since both vectors are size \\(3\\), the outer product should be a \\(3 \\times 3\\) matrix with rank-1.\n\n\nCode\nx = np.array([1, 2, 3])\ny = np.array([3, 2, 1])\nouter = np.outer(x, y)\nprint(f'xy^T = \\n{outer}')\nprint(f'rank(xy^T) = {np.linalg.matrix_rank(outer)}')\n\n\nxy^T = \n[[3 2 1]\n [6 4 2]\n [9 6 3]]\nrank(xy^T) = 1\n\n\nYou can think of the outer product matrix as a kind of projection matrix. It always projects vectors onto the same one-dimensional line in \\(\\mathbb{R}^n\\). Why? Suppose \\(\\mathbf{v}\\) is some vector. If we hit it with the outer product matrix \\(\\mathbf{x} \\mathbf{y}^\\top\\), using the fact matrix multiplication is associative, we get\n\\[(\\mathbf{x} \\mathbf{y}^\\top) \\mathbf{v} = \\mathbf{x} (\\mathbf{y}^\\top \\mathbf{v}) = (\\mathbf{y} \\cdot \\mathbf{v}) \\mathbf{x}.\\]\nThat is, \\(\\mathbf{v}\\) just gets projected onto the space spanned by the vector \\(\\mathbf{x}\\). Evidently the other outer product vector \\(\\mathbf{y}\\) determines how long the projection vector will be. Here’s a visual representation of this idea for 2-dimensional vectors. Take\n\\[\\begin{align*}\n\\mathbf{x} &= (1, 1) \\\\\n\\mathbf{y} &= (1, -1) \\\\\n\\mathbf{v}_0 &= (-1, 2) \\quad &\\Longrightarrow \\quad (\\mathbf{y} \\cdot \\mathbf{v}_0) \\mathbf{x} &= (-3, -3) \\\\\n\\mathbf{v}_1 &= (2, 0) \\quad &\\Longrightarrow \\quad (\\mathbf{y} \\cdot \\mathbf{v}_1) \\mathbf{x} &= (2, 2) \\\\\n\\mathbf{v}_2 &= (2, -1) \\quad &\\Longrightarrow \\quad (\\mathbf{y} \\cdot \\mathbf{v}_2) \\mathbf{x} &= (3, 3). \\\\\n\\end{align*}\\]\nApplying the outer product \\(\\mathbf{x} \\mathbf{y}^\\top\\) to each \\(\\mathbf{v}_i\\) should project each vector onto the space spanned by \\(\\mathbf{x}=(1, 1)\\), which is just the line \\(y=x\\). Notice the projections are all proportional to \\((1, 1)\\), as they should be. In the plot below, each vector and its projection have the same color. The outer product vectors \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are shown in black.\n\n\nCode\nx = np.array([1, 1]).reshape(-1, 1)\ny = np.array([1, -1]).reshape(-1, 1)\nvs = [np.array([-1, 2]).reshape(-1, 1), \n      np.array([2, 0]).reshape(-1, 1), \n      np.array([2, -1]).reshape(-1, 1)]\nws = [(x @ y.T) @ v for v in vs]\nvectors = [vector.flatten() for vector in vs + ws + [x, y]]\nplot_vectors(\n    vectors, colors=['salmon', 'limegreen', 'steelblue'] * 2 + ['black', 'black'], headwidth=5, width=0.01,\n    labels=['$\\mathbf{v}_0$', '$\\mathbf{v}_1$', '$\\mathbf{v}_2$'] + [''] * 3 + ['$\\mathbf{x}$', '$\\mathbf{y}$'],\n    text_offsets = [[0, 0.2], [0, 0.2], [0.1, -0.3]] + [[0,0]] * 3 + [[-0.4, 0.15], [0, -0.3]], ticks_every=1,\n    title='Outer Product Projections', zorders=[0, 5, 1, 2, 4, 3, 4, 6, 7], xlim=(-3.5, 3.5), ylim=(-3.5, 3.5))"
  },
  {
    "objectID": "notebooks/matrix-algebra.html#special-matrices",
    "href": "notebooks/matrix-algebra.html#special-matrices",
    "title": "6  Matrix Algebra",
    "section": "6.2 Special Matrices",
    "text": "6.2 Special Matrices\nThere are many classes of matrices that have various special properties. I’ll quickly introduce a few that’ll be of interest to us in machine learning.\n\n6.2.1 Diagonal Matrices\nProbably the most basic class of matrices are the diagonal matrices. A diagonal matrix is an \\(m \\times n\\) matrix \\(\\mathbf{D}\\) whose elements are only non-zero on the diagonals, i.e. \\(D_{i,j} = 0\\) if \\(i \\neq j\\). For example, the following \\(3 \\times 3\\) matrix is diagonal since its only non-zero values lie on the diagonal,\n\\[\n\\mathbf{D} =\n\\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 2 & 0 \\\\\n0 & 0 & 3 \\\\\n\\end{pmatrix}.\n\\]\nWe’ve already seen an important diagonal matrix a few times, the identity matrix \\(\\mathbf{I}\\). The identity matrix is the diagonal matrix whose diagonal entries are all ones. It’s common to short-hand a diagonal matrix by just specifying its diagonal entries as a vector. In this notation, we’d use the short-hand\n\\[\\mathbf{D} = \\text{diag}(1,2,3).\\]\nto refer to the matrix in the above example. It means exactly the same thing, we’re just only specifying the diagonal elements. This is also the easiest way to define a diagonal matrix in numpy, by using np.diag. Notice that a diagonal matrix contains \\(n^2\\) elements, but we only need to specify \\(n\\) of them to fully determine what the matrix is (i.e. the diagonal elements themselves).\nIn a sense, a diagonal matrix can only scale a vector it acts on, not rotate it or reflect it. This is because multiplying diagonal matrix with a vector is equivalent to element-wise multiplying the diagonal elements with the vector, which causes each vector component to get stretched by some amount. For example, if \\(\\mathbf{x}=(1,1,1)\\), when the above example \\(\\mathbf{D}\\) acts on it, we’d get\n\\[\n\\mathbf{D}\\mathbf{x} =\n\\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 2 & 0 \\\\\n0 & 0 & 3 \\\\\n\\end{pmatrix}\n\\begin{pmatrix}\n1 \\\\\n1 \\\\\n1 \\\\\n\\end{pmatrix} =\n\\begin{pmatrix}\n1 \\\\\n2 \\\\\n3 \\\\\n\\end{pmatrix} =\n\\begin{pmatrix}\n1 \\\\\n2 \\\\\n3 \\\\\n\\end{pmatrix} \\circ\n\\begin{pmatrix}\n1 \\\\\n1 \\\\\n1 \\\\\n\\end{pmatrix}.\n\\]\nHere’s an example of how to define a diagonal matrix in numpy using np.diag. I’ll define the same matrix as the above example, and then act on the same vector to show it just scales the entries.\n\n\nCode\nD = np.diag([1, 2, 3])\nx = np.array([1, 1, 1]).reshape(-1, 1)\nprint(f'D = diag(1,2,3) = \\n{D}')\nprint(f'Dx = {(D @ x).flatten()}')\n\n\nD = diag(1,2,3) = \n[[1 0 0]\n [0 2 0]\n [0 0 3]]\nDx = [1 2 3]\n\n\n\n\n6.2.2 Symmetric Matrices\nAnother special class of matrices important to machine learning is the symmetric matrix. A symmetric matrix is a square matrix \\(\\mathbf{S}\\) that equals its own transpose, i.e. \\(\\mathbf{S}^\\top = \\mathbf{S}\\). They’re called symmetric matrices because their lower diagonals and upper diagonals are mirror images. Symmetric matrices can be thought of as the matrix equivalent of a real number.\nFor example, consider the matrix \\[\n\\mathbf{S} =\n\\begin{pmatrix}\n1 & -1 & -2 \\\\\n-1 & 2 & 1 \\\\\n-2 & 1 & 3 \\\\\n\\end{pmatrix}.\n\\]\nThis matrix is symmetric since the upper diagonal and lower diagonal are the same, i.e. \\(S_{i,j} = S_{j,i}\\). Symmetric matrices are very important as we’ll see. They’re the matrix generalization of the idea of a real number.\nSince the lower diagonal and upper diagonal of a symmetric matrix always equal, we only need to specify what the diagonal and upper diagonal are to fully determine the matrix. If \\(\\mathbf{S}\\) contains \\(n^2\\) entries, only \\[n + \\frac{1}{2}(n^2 - n) = \\frac{1}{2}n(n+1)\\]\nof those elements are actually unique. This fact can be used to shave a lot of time off of algorithms involving symmetric matrices. In numpy, you can check a matrix \\(\\mathbf{S}\\) is symmetric by checking that it equals its transpose. Due to numerical roundoff, you may want to wrap the condition inside np.allclose.\n\n\nCode\nS = np.array([\n    [1, -1, -2],\n    [-1, 2, 1],\n    [-2, 1, 3]])\nis_symmetric = lambda A: np.allclose(A, A.T)\nis_symmetric(S)\n\n\nTrue\n\n\n\n\n6.2.3 Upper and Lower Triangular Matrices\nClosely related to diagonal matrices are lower and upper triangular matrices. An \\(m \\times n\\) matrix \\(\\mathbf{L}\\) is lower-triangular if the entries in its upper diagonal are zero, i.e. \\(L_{i,j} = 0\\) when \\(i < j\\). Similarly, an \\(m \\times n\\) matrix \\(\\mathbf{U}\\) is upper-triangular if the entries in its lower diagonal are zero, i.e. \\(U_{i,j} = 0\\) when \\(i > j\\). I’ve already showed an example of these when I covered determinants. Here they are again,\n\\[\n\\mathbf{L} =\n\\begin{pmatrix}\n3 & 0 & 0 \\\\\n1 & 2 & 0 \\\\\n1 & 1 & 1 \\\\\n\\end{pmatrix}, \\qquad \\mathbf{U} =\n\\begin{pmatrix}\n1 & 1 & 1 \\\\\n0 & 2 & 1 \\\\\n0 & 0 & 3 \\\\\n\\end{pmatrix}.\n\\]\nUpper and lower triangular (and diagonal) matrices are useful because it’s easy to invert them and calculate their determinants. Just like symmetric matrices, only \\(\\frac{1}{2}n(n+1)\\) unique elements are needed to fully specify these matrices since an entire off-diagonal is all zeros.\n\n\n6.2.4 Orthogonal Matrices\nThe next class of matrices I’ll introduce are more subtle, but very important geometrically. These are the orthogonal matrices. An orthogonal matrix is an \\(n \\times n\\) matrix \\(\\mathbf{Q}\\) whose transpose is its inverse, i.e.\n\\[\\mathbf{Q}^\\top = \\mathbf{Q}^{-1} \\quad \\text{or} \\quad \\mathbf{Q}^\\top \\mathbf{Q}=\\mathbf{I}.\\]\nAs an example, consider the following matrix,\n\\[\n\\mathbf{Q} = \\frac{1}{\\sqrt{2}}\n\\begin{pmatrix}\n1 & 1 \\\\\n1 & -1 \\\\\n\\end{pmatrix} =\n\\begin{pmatrix}\n\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\\\\n\\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}} \\\\\n\\end{pmatrix}.\n\\]\nWe can check \\(\\mathbf{Q}\\) is orthogonal by checking it satisfies the condition \\(\\mathbf{Q}^\\top \\mathbf{Q}=\\mathbf{I}\\),\n\\[\n\\mathbf{Q}^\\top \\mathbf{Q} =\n\\frac{1}{\\sqrt{2}}\n\\begin{pmatrix}\n1 & 1 \\\\\n1 & -1 \\\\\n\\end{pmatrix}\n\\frac{1}{\\sqrt{2}}\n\\begin{pmatrix}\n1 & 1 \\\\\n1 & -1 \\\\\n\\end{pmatrix} =\n\\frac{1}{2}\n\\begin{pmatrix}\n2 & 0 \\\\\n0 & 2 \\\\\n\\end{pmatrix} =\n\\begin{pmatrix}\n1 & 0 \\\\\n0 & 1 \\\\\n\\end{pmatrix} = \\mathbf{I}.\n\\]\nNotice from this example that the column vectors \\(\\mathbf{q}_0, \\mathbf{q}_1\\) form an orthonormal basis for \\(\\mathbb{R}^2\\), since\n\\[\\mathbf{q}_0 \\cdot \\mathbf{q}_1 = 0, \\quad \\mathbf{q}_0 \\cdot \\mathbf{q}_0 = \\mathbf{q}_1 \\cdot \\mathbf{q}_1 = 1.\\]\nThis is a general fact. The column vectors of an orthogonal matrix \\(\\mathbf{Q}\\) form a complete set of orthonormal basis vectors for \\(\\mathbb{R}^n\\). Conversely, we can always form an orthogonal matrix by first finding an orthonormal basis and then creating column vectors out of the basis vectors. This is usually the way orthogonal matrices are constructed in practice using algorithms like the Gram-Schmidt Algorithm.\nIt’s not at all obvious, but the fact that the column vectors of \\(\\mathbf{Q}\\) form an orthonormal basis constrains the number of unique elements \\(\\mathbf{Q}\\) is allowed to have. Requiring each \\(\\mathbf{q}_i\\) means \\(n\\) total elements are already determined. The further requirement that the column vectors be mutually orthogonal determines another \\(\\frac{1}{2}n(n-1)\\). This means \\(\\mathbf{Q}\\) only has \\(n^2 - n - \\frac{1}{2}n(n-1) = \\frac{1}{2}n(n-1)\\) unique elements. For example, when \\(\\mathbf{Q}\\) is \\(2 \\times 2\\) it only has \\(\\frac{1}{2}2(2-1)=1\\) unique element. The other \\(3\\) are all determined by that one element. This unique element can be thought of as a rotation angle. I’ll come back to this in a minute.\nAn important fact about orthogonal matrices is that they preserve the dot products between vectors. If \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are two vectors, then\n\\[(\\mathbf{Q} \\mathbf{x}) \\cdot (\\mathbf{Q}\\mathbf{y}) = \\mathbf{x} \\cdot \\mathbf{y}.\\]\nThis follows from the fact that \\((\\mathbf{Q} \\mathbf{x})^\\top (\\mathbf{Q} \\mathbf{y}) = \\mathbf{x}^\\top \\mathbf{Q}^\\top\\mathbf{Q}\\mathbf{y} = \\mathbf{x}^\\top \\mathbf{I} \\mathbf{y} = \\mathbf{x}^\\top \\mathbf{y}\\). Since the dot product encodes the notions of length and angle, this fact implies that orthogonal matrices can’t change the lengths of vectors, nor the angles between vectors. Orthogonal matrices preserve the geometry of the vector space.\nThis fact suggests some deep intuition about what orthogonal matrices do. If they can’t change the lengths of vectors or the angles between them, then all they can do is rotate vectors or reflect them across some line. In fact, it turns out any \\(2 \\times 2\\) orthogonal matrix can be written in the form\n\\[\n\\mathbf{Q} =\n\\begin{pmatrix}\n\\cos \\theta & \\mp \\sin \\theta \\\\\n\\sin \\theta & \\pm \\cos \\theta \\\\\n\\end{pmatrix},\n\\]\nwhere \\(\\theta\\) is some angle (expressed in radians). When the right column vector is \\(\\binom{-\\sin\\theta}{\\cos\\theta}\\), \\(\\mathbf{Q}\\) is a pure rotation matrix. It will rotate any vector in the plane by an angle \\(\\theta\\), counterclockwise if \\(\\theta > 0\\), and clockwise if \\(\\theta < 0\\). When the right column vector is \\(\\binom{\\sin\\theta}{-\\cos\\theta}\\), \\(\\mathbf{Q}\\) becomes a reflection matrix; it’ll reflect vectors about the line at an angle \\(\\frac{\\theta}{2}\\) with the x-axis. The combination of these two together can generate any 2D rotation or reflection.\nHere’s a visual of this idea. I’ll take the unit vector \\(\\mathbf{e}_x=(1,0)\\) and use \\(\\mathbf{Q}\\) to rotate it by some angle, in this case \\(\\theta = 45^\\circ\\). Note the need to convert the angle to radians by multiplying the angle in degrees by \\(\\frac{\\pi}{180}\\). You should be able to confirm that the red vector is indeed the black vector \\(\\mathbf{e}_x\\) rotated counterclockwise by \\(45^\\circ\\) to the new vector \\(\\mathbf{Q}\\mathbf{e}_x = 2^{-1/2}(1,1)\\). The factor of \\(2^{-1/2}\\) appears to keep the vector normalized to unit length.\n\n\nCode\ntheta_degrees = 45\ntheta = theta_degrees * np.pi / 180\nQ = np.array([\n    [np.cos(theta), -np.sin(theta)], \n    [np.sin(theta), np.cos(theta)]])\nex = np.array([1, 0]).reshape(-1, 1)\nQex = Q @ ex\nplot_vectors([ex.flatten(), Qex.flatten()], colors=['black', 'red'], title=f'${theta_degrees}^\\circ$ Rotation',\n             labels=['$\\mathbf{e}_x$', '$\\mathbf{Q}\\mathbf{e}_x$'], text_offsets=[[-0.1, 0.1], [0, 0]],\n             ticks_every=1, xlim=(-0.5, 1.5), ylim=(-0.5, 1.5))\n\n\n\n\n\n\n\n\n\nI’ll finish this section by noting that orthogonal matrices always have determinant \\(\\pm 1\\). You can see this by applying the determinant product formula to \\(\\mathbf{Q}^\\top \\mathbf{Q}=\\mathbf{I}\\),\n\\[1 = \\text{det}(\\mathbf{I}) = \\text{det}(\\mathbf{Q}^\\top \\mathbf{Q}) = \\text{det}(\\mathbf{Q}^\\top) \\cdot \\text{det}(\\mathbf{Q}) = \\big(\\text{det}(\\mathbf{Q})\\big)^2,\\]\nwhich implies \\(\\text{det}(\\mathbf{Q}) = \\pm 1\\). This evidently divides orthogonal matrices into two distinct classes:\n\n\\(\\text{det}(\\mathbf{Q}) = +1\\): These are the orthogonal matrices that correspond to pure rotations.\n\\(\\text{det}(\\mathbf{Q}) = -1\\): These are the orthogonal matrices that correspond to reflections.\n\nI’ll verify that the rotation matrix I just plotted indeed has determinant \\(+1\\).\n\n\nCode\nprint(f'det(Q) = {np.linalg.det(Q)}')\n\n\ndet(Q) = 1.0\n\n\n\n\n6.2.5 Block Matrices\nSometimes a matrix might look kind of diagonal or triangular, but not exactly. For example, consider the following matrix,\n\\[\n\\mathbf{A} =\n\\begin{pmatrix}\n1 & 2 & 0 & 0 \\\\\n3 & 4 & 0 & 0 \\\\\n0 & 0 & 5 & 6 \\\\\n0 & 0 & 7 & 8 \\\\\n\\end{pmatrix}.\n\\]\nThis matrix looks kind of diagonal, but not exactly. Notice, however, that we can think about this matrix as being composed of four sub-matrices, or blocks,\n\\[\n\\mathbf{A}_{0,0} =\n\\begin{pmatrix}\n1 & 2 \\\\\n3 & 4 \\\\\n\\end{pmatrix}, \\quad\n\\mathbf{A}_{0,1} =\n\\begin{pmatrix}\n0 & 0 \\\\\n0 & 0 \\\\\n\\end{pmatrix}, \\quad\n\\mathbf{A}_{1,0} =\n\\begin{pmatrix}\n0 & 0 \\\\\n0 & 0 \\\\\n\\end{pmatrix}, \\quad\n\\mathbf{A}_{1,1} =\n\\begin{pmatrix}\n5 & 6 \\\\\n7 & 8 \\\\\n\\end{pmatrix}.\n\\]\nIf we think of \\(\\mathbf{A}\\) in terms of these 4 blocks, we can write it simply as\n\\[\n\\mathbf{A} =\n\\begin{pmatrix}\n1 & 2 & 0 & 0 \\\\\n3 & 4 & 0 & 0 \\\\\n0 & 0 & 5 & 6 \\\\\n0 & 0 & 7 & 8 \\\\\n\\end{pmatrix} =\n\\begin{pmatrix}\n\\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\\\ \\end{pmatrix} & \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\\\ \\end{pmatrix} \\\\\n\\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\\\ \\end{pmatrix} & \\begin{pmatrix} 5 & 6 \\\\ 7 & 8 \\\\ \\end{pmatrix} \\\\\n\\end{pmatrix} =\n\\begin{pmatrix}\n\\mathbf{A}_{0,0} & \\mathbf{A}_{0,1} \\\\\n\\mathbf{A}_{1,0} & \\mathbf{A}_{1,1} \\\\\n\\end{pmatrix}.\n\\]\nWhere the braces are doesn’t really effect anything other than how the elements are indexed. What matters is we can express this \\(4 \\times 4\\) matrix as a \\(2 \\times 2\\) block matrix that semantically represents the exact same matrix. Notice that in block form \\(\\mathbf{A}\\) is now a diagonal matrix. We call a matrix that can be blocked into diagonal form like this block diagonal. Block diagonal matrices are the most useful of the block matrices. If a block matrix is upper or lower triangular, we’d call it a block upper triangular or block lower triangular matrix, respectively.\nWhen matrices are in block form, you can manipulate them exactly the way you would if they weren’t. The only difference is that you have to remember matrix multiplication doesn’t commute. For example, we could write \\(\\mathbf{A}^2 = \\mathbf{A}\\mathbf{A}\\) in block form as\n\\[\n\\mathbf{A}^2 = \\mathbf{A}\\mathbf{A} =\n\\begin{pmatrix}\n\\mathbf{A}_{0,0} & \\mathbf{A}_{0,1} \\\\\n\\mathbf{A}_{1,0} & \\mathbf{A}_{1,1} \\\\\n\\end{pmatrix}\n\\begin{pmatrix}\n\\mathbf{A}_{0,0} & \\mathbf{A}_{0,1} \\\\\n\\mathbf{A}_{1,0} & \\mathbf{A}_{1,1} \\\\\n\\end{pmatrix} =\n\\begin{pmatrix}\n\\mathbf{A}_{0,0}\\mathbf{A}_{0,0} + \\mathbf{A}_{0,1}\\mathbf{A}_{1,0}  & \\mathbf{A}_{0,0}\\mathbf{A}_{0,1} + \\mathbf{A}_{0,1}\\mathbf{A}_{1,1} \\\\\n\\mathbf{A}_{1,0}\\mathbf{A}_{0,0} + \\mathbf{A}_{1,1}\\mathbf{A}_{1,0} & \\mathbf{A}_{1,0}\\mathbf{A}_{0,1} + \\mathbf{A}_{1,1}\\mathbf{A}_{1,1} \\\\\n\\end{pmatrix}.\n\\]\nThis would give the exact same answer as multiplying \\(\\mathbf{A}^2 = \\mathbf{A}\\mathbf{A}\\) in regular form, except we’d have extra braces floating around that we can ignore.\nNow, you might ask why we even care about blocking matrices like this. Probably the most important reason we care is hardware. Computer memory is typically divided into a sequence of fixed-sized blocks. When we want to operate on an array, the system has to go into memory and fetch where those array values are stored, perform the array operation, and then place the answer back into memory. During the fetch step, it will take a long time if the array elements are located far away from each other in different blocks. For this reason, programs tend to place array elements nearby each other in memory. But when one block runs out, the program has to go find a new block of memory to place the other elements. This suggests that if we want to efficiently fetch array elements from memory, we should do so block by block. That is, we should find a way to partition the array so each block of the array fits in the same block of memory. If we do this, we can perform operations much faster than we would if the program had to search new blocks every time it needed to perform an operation.\nThe best example of this is matrix multiplication. While it might take \\(O(n^3)\\) time to multiply two arrays in theory, don’t forget that asymptotic notation has a hidden constant term out front that we ignore. In real life, that constant can make a big difference. If we try to multiply two matrices without doing any blocking, we’d have a much larger constant than if we first blocked the matrices into blocks that would fit in one block of memory. In fact, this is what the LAPACK routines behind functions like np.matmul do. They don’t just naively multiply two matrices by running over a triple loop. They first block both matrices into sub-matrix blocks that fit efficiently in memory, and then run the triple loop block-style before putting everything back together. It’s for this reason more than anything else that numpy array methods run much faster than anything you’d write in python. When it comes to array operations, numpy and the LAPACK routines will swim laps around anything you’d code up yourself.\n\n\n6.2.6 Sparse Matrices\nA very useful class of matrices in applications are the sparse matrices. Sparse matrices are defined by the property that most of their entries are zero. Only a sparse number of elements are non-zero. When a matrix is sparse, we can often more efficiently store its elements using a different data structure that only keeps track of the non-zero elements and where they occur in the matrix. We can then define matrix algorithms in a way that they only operate on the non-zero entries, which can considerably speed up computation.\nFor example, suppose an \\(n \\times n\\) matrix \\(\\mathbf{A}\\) is sparse, with \\(k \\ll n\\) non-zero entries. If we wanted to multiply \\(\\mathbf{A}\\) with itself, it would usually take \\(O(n^3)\\) time and require \\(O(n^2)\\) words of memory. But, since \\(\\mathbf{A}\\) is sparse, we could multiply it with itself in \\(O(k^3)\\) and use only \\(O(k)\\) words of memory. The speedup comes from the fact that we only need to keep track of the non-zero elements when adding and multiplying elements in the matrix, which means we only need to keep track of the \\(k\\) non-zero elements in the multiplication.\nFor ordinary sized matrices, treating them as sparse doesn’t really benefit you much. It’s when matrices get huge that sparse methods can be useful. One example of this that comes up in machine learning is when we want to represent a corpus of text as a matrix of data. In that case, each row would be a document of text, and each column would be a word in the vocabulary of all possible words. Vocabulary sizes can get huge, often millions of words. If you have, say, 10,000 documents, that means you’d have a 10,000 by 1,000,000 matrix of data, which is pretty huge. Fortunately, any one document only contain a handful of words in the total vocabulary. This means the data matrix is sparse, and we can efficiently manipulate it using sparse methods.\nNumpy doesn’t have any direct methods to work with sparse matrices, but scipy does. To define a matrix as sparse in scipy, use scipy.sparse.csr_matrix. This will encode a sparse matrix using a CSR matrix, which is one of several ways to efficiently represent a sparse matrix. Once we’ve encoded a matrix as sparse, we can more or less use any of the operations we’re used to. To convert a sparse matrix back to a normal, dense matrix, use A.todense(). Here’s an example. I’ll convert the following matrix into sparse form,\n\\[\n\\mathbf{A} =\n\\begin{pmatrix}\n1 & 0 & 0 & 0 \\\\\n0 & 2 & 0 & 0 \\\\\n3 & 0 & 0 & 4 \\\\\n0 & 5 & 0 & 6 \\\\\n\\end{pmatrix}.\n\\]\nNotice how it’s only keeping track of the non-zero values and where in the matrix they occur.\n\n\nCode\nfrom scipy.sparse import csr_matrix\n\nA = np.array([\n    [1, 0, 0, 0], \n    [0, 2, 0, 0], \n    [3, 0, 0, 4], \n    [0, 5, 0, 6]])\nA_sparse = csr_matrix(A)\nprint(f'A_sparse = \\n{A_sparse}')\n\n\nA_sparse = \n  (0, 0)    1\n  (1, 1)    2\n  (2, 0)    3\n  (2, 3)    4\n  (3, 1)    5\n  (3, 3)    6\n\n\n\n\nCode\nA_dense = A_sparse.todense()\nprint(f'A_dense = \\n{A_dense}')\n\n\nA_dense = \n[[1 0 0 0]\n [0 2 0 0]\n [3 0 0 4]\n [0 5 0 6]]"
  },
  {
    "objectID": "notebooks/matrix-algebra.html#matrix-factorizations",
    "href": "notebooks/matrix-algebra.html#matrix-factorizations",
    "title": "6  Matrix Algebra",
    "section": "6.3 Matrix Factorizations",
    "text": "6.3 Matrix Factorizations\nGiven any two compatible matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\), we can get a third matrix \\(\\mathbf{C}\\) by matrix multiplication, \\(\\mathbf{C} = \\mathbf{A}\\mathbf{B}\\). Now suppose we wanted to go the other way. Given a matrix \\(\\mathbf{C}\\), how can we factor it back out into a product \\(\\mathbf{A}\\mathbf{B}\\)? This is the idea behind matrix factorization. In practice, we’re interested in factoring a matrix into a product of special types of matrices that are easier to work with, like symmetric, diagonal, or orthogonal matrices.\n\n6.3.1 LU Factorization\nProbably the most basic matrix factorization is the LU Factorization. LU factorization factors an \\(m \\times n\\) matrix \\(\\mathbf{A}\\) into a product of a lower triangular matrix \\(\\mathbf{L}\\) and an upper triangular matrix \\(\\mathbf{U}\\), \\[\\mathbf{A} = \\mathbf{L}\\mathbf{U}.\\]\nThe LU factorization is most useful for solving a system of linear equations. If \\(\\mathbf{A}\\mathbf{x}=\\mathbf{b}\\), we can do an LU factorization of \\(\\mathbf{A}\\) and write the system as \\(\\mathbf{LUx} = \\mathbf{b}\\). This can then be solved by breaking it into two steps, known as forward substitution and back substitution,\n\nForward substitution: Solve \\(\\mathbf{Ly} = \\mathbf{b}\\) for \\(\\mathbf{y}\\).\nBack Substitution: Solve \\(\\mathbf{Ux} = \\mathbf{y}\\) for \\(\\mathbf{x}\\).\n\nThese two steps are easy to do since each system can be solved by substitution, working from the “tip” of the triangle down. The LU factorization is essentially what matrix solvers like np.linalg.solve do to solve linear systems.\nOf course, the question still remains how to actually factor \\(\\mathbf{A}\\) into \\(\\mathbf{L}\\mathbf{U}\\). I won’t describe the algorithm to do this, or any matrix factorization really, since their inner workings aren’t that relevant to machine learning. If you’re curious, LU factorization is done using some variant of an algorithm known as Gaussian Elimination. Note the LU factorization in general is a cubic time algorithm, i.e. \\(O(n^3)\\) if \\(\\mathbf{A}\\) is \\(n \\times n\\).\nThe LU factorization can also be used to compute the determinant of a square matrix. Since \\(\\mathbf{L}\\) and \\(\\mathbf{U}\\) are triangular, their determinant is just the product of their diagonals. Using the product rule for determinants then gives\n\\[\\text{det}(\\mathbf{A}) = \\text{det}(\\mathbf{LU}) = \\text{det}(\\mathbf{L}) \\cdot \\text{det}(\\mathbf{U}) = \\prod_{i=0}^{n-1} L_{i,i} \\cdot U_{i,i}.\\]\nThe LU factorization can also be used to compute the inverse of a square matrix. The idea is to solve the matrix system of equations\n\\[\\mathbf{A} \\mathbf{X} = \\mathbf{I},\\]\nassuming \\(\\mathbf{X}=\\mathbf{A}^{-1}\\) are the \\(n^2\\) unknown variables you’re solving for. This system can be solved by using the same technique of forward substitution plus back substitution. Note that solving for both the determinant and inverse this way each takes \\(O(n^3)\\) time due to the LU decomposition. This is one reason why you should probably avoid calculating these quantities explicitly unless you really need them.\nStrangely, numpy doesn’t have a built-in LU factorization solver, but scipy does using scipy.linalg.lu. It factors a matrix into not two, but three products, \\(\\mathbf{A}=\\mathbf{PLU}\\). The \\(\\mathbf{P}\\) is a permutation matrix. It just accounts for the fact that sometimes you need to swap the rows before doing the LU factorization. I won’t go into that. Here’s the LU factorization of the above example matrix. I’ll also verify that \\(\\mathbf{A}=\\mathbf{LU}\\).\n\n\nCode\nfrom scipy.linalg import lu\n\nA = np.array([[1, 1], \n              [1, -1]])\nP, L, U = lu(A)\nprint(f'L = \\n{L}')\nprint(f'U = \\n{U}')\nprint(f'LU = \\n{L @ U}')\n\n\nL = \n[[1. 0.]\n [1. 1.]]\nU = \n[[ 1.  1.]\n [ 0. -2.]]\nLU = \n[[ 1.  1.]\n [ 1. -1.]]\n\n\n\n\n6.3.2 QR Factorization\nAnother useful factorization is to factor an \\(m \\times n\\) matrix \\(\\mathbf{A}\\) into a product of an \\(m \\times m\\) orthogonal matrix \\(\\mathbf{Q}\\) and an \\(m \\times n\\) upper triangular matrix \\(\\mathbf{R}\\),\n\\[\\mathbf{A} = \\mathbf{QR}.\\]\nThe QR factorization is useful if we want to create an orthonormal basis out of the column vectors of \\(\\mathbf{A}\\), since \\(\\mathbf{Q}\\) will give a complete set of basis vectors built from orthogonalizing \\(\\mathbf{A}\\). It’s also useful for calculating other random things of interest. Like LU factorization, it can be used to calculate determinants, since\n\\[\\text{det}(\\mathbf{A}) = \\text{det}(\\mathbf{QR}) = \\text{det}(\\mathbf{Q}) \\cdot \\text{det}(\\mathbf{R}) = 1 \\cdot \\text{det}(\\mathbf{R}) = \\prod_{i=0}^{n-1} R_{i,i}.\\]\nIt can also be used to find the inverse matrix. Use the fact that \\(\\mathbf{A}^{-1} = (\\mathbf{QR})^{-1} = \\mathbf{R}^{-1} \\mathbf{Q}^\\top\\), since \\(\\mathbf{Q}\\) is orthogonal. The matrix \\(\\mathbf{R}^{-1}\\) can be calculated efficiently via back-substitution since \\(\\mathbf{R}\\) just a triangular matrix. Both the determinant and inverse calculation again take \\(O(n^3)\\) time because the QR factorization does.\nIn practice, the QR factorization is done using algorithms like the Gram-Schmidt method or Householder reflections. Just like LU factorization, QR factorization is in general an \\(O(n^3)\\) algorithm. In numpy, you can get the QR factorization using np.linalg.qr(A).\nThe exact QR factorization I described is technically called the full QR factorization, since it orthogonalizes all of the columns, even if \\(\\mathbf{A}\\) isn’t full rank. Usually by default the algorithms only orthogonalize the first \\(r=\\text{rank}(\\mathbf{A})\\) columns. If you want to return the full QR factorization in numpy, you need to pass in the keyword argument mode = 'complete'.\nHere’s the full QR factorization of the same matrix from before.\n\n\nCode\nA = np.array([[1, 1], \n              [1, -1]])\nQ, R = np.linalg.qr(A, mode='complete')\nprint(f'Q = \\n{Q.round(10)}')\nprint(f'R = \\n{R.round(10)}')\nprint(f'QR = \\n{Q @ R}')\n\n\nQ = \n[[-0.70710678 -0.70710678]\n [-0.70710678  0.70710678]]\nR = \n[[-1.41421356  0.        ]\n [ 0.         -1.41421356]]\nQR = \n[[ 1.  1.]\n [ 1. -1.]]\n\n\n\n\n6.3.3 Spectral Decomposition\nThe spectral decomposition is a way to factor a symmetric matrix \\(\\mathbf{S}\\) into a product of an orthonormal matrix \\(\\mathbf{X}\\) and a diagonal matrix \\(\\mathbf{\\Lambda}\\),\n\\[\\mathbf{S} = \\mathbf{X \\Lambda X}^\\top.\\]\nThe matrix \\(\\mathbf{\\Lambda}\\) is called the eigenvalue matrix, and \\(\\mathbf{X}\\) is the eigenvector matrix. The diagonal entries of \\(\\mathbf{\\Lambda}\\) are called the eigenvalues of \\(\\mathbf{S}\\), denoted \\(\\lambda_i\\),\n\\[\\mathbf{\\Lambda} = \\text{diag}(\\lambda_0, \\lambda_1, \\cdots, \\lambda_n).\\]\nThe column vectors of \\(\\mathbf{X}\\) are called the eigenvectors of \\(\\mathbf{S}\\), denoted \\(\\mathbf{x}_i\\),\n\\[\\mathbf{X} = \\begin{pmatrix} \\mathbf{x}_0 & \\mathbf{x}_1 & \\cdots & \\mathbf{x}_{n-1} \\end{pmatrix}.\\]\nEigenvalues and eigenvectors arise from trying to find special “characteristic” lines in the vector space \\(\\mathbb{R}^n\\) that stay fixed when acted on by \\(\\mathbf{S}\\). Let \\(\\mathbf{x}\\) be the unit vector along one of these lines. Saying \\(\\mathbf{S}\\) can’t rotate \\(\\mathbf{x}\\) is equivalent to saying it can only scale \\(\\mathbf{x}\\) by some value \\(\\lambda\\). Finding these special characteristic lines is thus equivalent to solving the equation\n\\[\\mathbf{S}\\mathbf{x} = \\lambda \\mathbf{x}\\]\nfor \\(\\lambda\\) and \\(\\mathbf{x}\\). The vector \\(\\mathbf{x}\\) is the eigenvector (German for “characteristic vector”). The scalar \\(\\lambda\\) is its corresponding eigenvalue (German for “characteristic value”). We can rewrite this equation as \\((\\mathbf{S} - \\lambda \\mathbf{I})\\mathbf{x} = \\mathbf{0}\\), where \\(\\mathbf{0}\\) is the zero vector. Taking the determinant of \\(\\mathbf{S} - \\lambda \\mathbf{I}\\) and insisting it must be singular gives a polynomial equation, called the characteristic equation, that can (in principle) be solved for the eigenvalue \\(\\lambda\\),\n\\[\\text{det}(\\mathbf{S} - \\lambda \\mathbf{I}) = 0.\\]\nFor example, if \\(\\mathbf{S}\\) is a symmetric \\(2 \\times 2\\) matrix, we have\n\\[\n\\mathbf{S} =\n\\begin{pmatrix}\na & b \\\\\nb & d \\\\\n\\end{pmatrix} \\quad \\Longrightarrow \\quad\n\\mathbf{S} - \\lambda \\mathbf{I} =\n\\begin{pmatrix}\na-\\lambda & b \\\\\nb & d-\\lambda \\\\\n\\end{pmatrix} \\quad \\Longrightarrow \\quad\n\\text{det}(\\mathbf{S} - \\lambda \\mathbf{I}) = (a - \\lambda)(d - \\lambda) - b^2 = \\lambda^2 - (a + d)\\lambda + (ad-b^2) = 0.\n\\]\nNotice that \\(\\text{tr}(\\mathbf{S}) = a + d\\) and \\(\\text{det}(\\mathbf{S}) = ad-b^2\\), so the characteristic equation in this special \\(2 \\times 2\\) cases reduces to\n\\[\\lambda^2 - \\text{tr}(\\mathbf{S})\\lambda + \\text{det}(\\mathbf{S}) = 0.\\]\nThis is a quadratic equation whose solution is the two eigenvalues \\(\\lambda_0, \\lambda_1\\). Once the eigenvalues are known, they can be plugged back into the linear equation \\((\\mathbf{S} - \\lambda \\mathbf{I})\\mathbf{x} = \\mathbf{0}\\) to solve for the eigenvectors \\(\\mathbf{x}_0, \\mathbf{x}_1\\), e.g. using LU factorization.\nJust to put some numbers in, take the following specific \\(2 \\times 2\\) matrix\n\\[\n\\mathbf{S} =\n\\begin{pmatrix}\n2 & 1 \\\\\n1 & 2 \\\\\n\\end{pmatrix}.\n\\]\nSince \\(\\text{tr}(\\mathbf{S})=2+2=4\\) and \\(\\text{det}(\\mathbf{S})=2 \\cdot 2 - 1 \\cdot 1 = 3\\), the characteristic equation is\n\\[\\lambda^2 - 4\\lambda + 3 = 0 \\quad \\Longrightarrow \\quad (\\lambda-1)(\\lambda - 3) = 0 \\quad \\Longrightarrow \\quad \\lambda=1, 3.\\]\nThe eigenvalues for this matrix are thus \\(\\lambda_0 = 3\\) and \\(\\lambda_1 = 1\\). Note it’s conventional to order the eigenvalues from largest to smallest, though it isn’t required. The eigenvectors are gotten by solving the two systems\n\\[\n(\\mathbf{S} - \\lambda_0 \\mathbf{I})\\mathbf{x}_0 = \\mathbf{0} \\quad \\Longrightarrow \\quad\n\\begin{pmatrix}\n2-3 & 1 \\\\\n1 & 2-3 \\\\\n\\end{pmatrix}\n\\begin{pmatrix}\nx_0 \\\\\ny_0 \\\\\n\\end{pmatrix} =\n\\begin{pmatrix}\n0 \\\\\n0 \\\\\n\\end{pmatrix} \\quad \\Longrightarrow \\quad\n\\mathbf{x}_0 =\n\\frac{1}{\\sqrt{2}}\n\\begin{pmatrix}\n1 \\\\\n1 \\\\\n\\end{pmatrix} \\approx\n\\begin{pmatrix}\n0.707 \\\\\n0.707 \\\\\n\\end{pmatrix},\n\\]\n\\[\n(\\mathbf{S} - \\lambda_1 \\mathbf{I})\\mathbf{x}_1 = \\mathbf{0} \\quad \\Longrightarrow \\quad\n\\begin{pmatrix}\n2-1 & 1 \\\\\n1 & 2-1 \\\\\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\\ny_1 \\\\\n\\end{pmatrix} =\n\\begin{pmatrix}\n0 \\\\\n0 \\\\\n\\end{pmatrix} \\quad \\Longrightarrow \\quad\n\\mathbf{x}_1 =\n\\frac{1}{\\sqrt{2}}\n\\begin{pmatrix}\n1 \\\\\n-1 \\\\\n\\end{pmatrix} \\approx\n\\begin{pmatrix}\n0.707 \\\\\n-0.707 \\\\\n\\end{pmatrix}.\n\\]\nYou can easily check that \\(\\mathbf{x}_0\\) and \\(\\mathbf{x}_1\\) are orthogonal. Note the eigenvectors here have been normalized so \\(||\\mathbf{x}_0||=||\\mathbf{x}_1||=1\\). This isn’t required, but it’s the most common convention to ensure the eigenvector matrix \\(\\mathbf{X}\\) is a properly orthogonal.\nHere’s a plot of what this looks like. I’ll show that \\(\\mathbf{v}_0=\\sqrt{2}\\mathbf{x}_0=(1,1)\\) gets scaled by a factor of \\(\\lambda_0=3\\) when acted on by \\(\\mathbf{S}\\). Similarly, I’ll show that \\(\\mathbf{v}_1=\\sqrt{2}\\mathbf{x}_1=(1,-1)\\) gets scaled by a factor of \\(\\lambda_1=1\\) (i.e. not at all) when acted on by \\(\\mathbf{S}\\). Importantly, notice that \\(\\mathbf{S}\\) doesn’t rotate either vector. They stay along their characteristic lines, or eigenspaces, which in this example are the lines \\(y=\\pm x\\).\n\n\nCode\nS = np.array([\n    [2, 1], \n    [1, 2]])\nv0 = np.array([1, 1]).reshape(-1, 1)\nSv0 = S @ v0\nv1 = np.array([1, -1]).reshape(-1, 1)\nSv1 = S @ v1\nvectors = [x.flatten() for x in [v0, Sv0, v1, Sv1]]\nplot_vectors(\n    vectors, colors=['black', 'red', 'black', 'blue'], xlim=(-1, 4), ylim=(-2, 4), zorders=[1, 0, 2, 3], \n    labels=['$\\mathbf{v}_0$', '$\\mathbf{S}\\mathbf{v}_0$', '$\\mathbf{v}_1$', '$\\mathbf{S}\\mathbf{v}_1$'],\n    text_offsets=[[-0.45, 0.25], [0.05, 0.15], [0.1, -0.5], [0.05, 0.3]], \n    title='Eigenspaces of $\\mathbf{S}$')\n\n\n\n\n\n\n\n\n\nA result I won’t prove, called the spectral theorem, guarantees that the eigenvalues of a symmetric matrix will be real-valued, and that the eigenvectors will form an orthonormal basis for \\(\\mathbb{R}^n\\). This is why \\(\\mathbf{X}\\) ends up being an orthogonal matrix. The fact that the eigenvalues have to be real is why we can think of symmetric matrices as the matrix generalization of a real number.\nThe spectral decomposition \\(\\mathbf{S} = \\mathbf{X \\Lambda X}^\\top\\) is just a matrix way of writing the individual equations \\(\\mathbf{S}\\mathbf{x} = \\lambda \\mathbf{x}\\). Grouping the eigenvectors and eigenvalues into matrices, we can write these equations in one go as \\(\\mathbf{S}\\mathbf{X} = \\mathbf{\\Lambda} \\mathbf{X}\\), which is just the spectral decomposition.\nBack to our working example, putting the eigenvalues and eigenvectors into their respective matrices gives\n\\[\n\\mathbf{\\Lambda} =\n\\begin{pmatrix}\n3 & 0 \\\\\n0 & 1 \\\\\n\\end{pmatrix}, \\qquad\n\\mathbf{X} =\n\\frac{1}{\\sqrt{2}}\n\\begin{pmatrix}\n1 & 1 \\\\\n1 & -1 \\\\\n\\end{pmatrix}.\n\\]\nThat is, the symmetric matrix \\(\\mathbf{S}\\) factorizes into the spectral decomposition\n\\[\n\\mathbf{S} = \\mathbf{X \\Lambda X}^\\top =\n\\frac{1}{\\sqrt{2}}\n\\begin{pmatrix}\n1 & 1 \\\\\n1 & -1 \\\\\n\\end{pmatrix}\n\\begin{pmatrix}\n3 & 0 \\\\\n0 & 1 \\\\\n\\end{pmatrix}\n\\frac{1}{\\sqrt{2}}\n\\begin{pmatrix}\n1 & 1 \\\\\n1 & -1 \\\\\n\\end{pmatrix}.\n\\]\nWe can find the spectral decomposition of a symmetric matrix in numpy using np.linalg.eigh(S). Note that np.linalg.eig(S) will also work, but eigh is more efficient for symmetric matrices than eig. In either case, they return a pair of arrays, the first being the diagonals of \\(\\mathbf{\\Lambda}\\), the second being \\(\\mathbf{X}\\). I’ll also verify that the spectral decomposition indeed gives \\(\\mathbf{S}\\).\n\n\nCode\nS = np.array([[2, 1], \n              [1, 2]])\nlambdas, X = np.linalg.eigh(S)\nLambda = np.diag(lambdas)\nprint(f'Lambda = \\n{Lambda}')\nprint(f'X = \\n{X}')\nprint(f'X Lambda X^T = \\n{X @ Lambda @ X.T}')\n\n\nLambda = \n[[1. 0.]\n [0. 3.]]\nX = \n[[-0.70710678  0.70710678]\n [ 0.70710678  0.70710678]]\nX Lambda X^T = \n[[2. 1.]\n [1. 2.]]\n\n\nNotice something from the example I just worked. It turns out that \\(\\text{tr}(\\mathbf{S}) = 4 = \\lambda_0 + \\lambda_1\\) and \\(\\text{det}(\\mathbf{S}) = 3 = \\lambda_0 \\lambda_1\\). This fact turns out to always be true for \\(n \\times n\\) symmetric matrices, namely if \\(\\mathbf{S}\\) has eigenvalues \\(\\lambda_0, \\lambda_1, \\cdots, \\lambda_{n-1}\\), then\n\\[\\begin{align*}\n\\text{tr}(\\mathbf{S}) &= \\sum_{i=0}^{n-1} \\lambda_i = \\lambda_0 + \\lambda_1 + \\cdots + \\lambda_{n-1}, \\\\\n\\text{det}(\\mathbf{S}) &= \\prod_{i=0}^{n-1} \\lambda_i = \\lambda_0 \\cdot \\lambda_1 \\cdots \\lambda_{n-1}.\n\\end{align*}\\]\nThis fact implies that \\(\\mathbf{S}\\) will be invertible if and only if all the eigenvalues are non-zero, since otherwise we’d have \\(\\text{det}(\\mathbf{S})=0\\).\nGiven how important the spectral decomposition is to many applications, there are a lot of different algorithms for finding it, each with its own trade-offs. One popular algorithm for doing so is the QR algorithm. Roughly speaking, the QR algorithm works as follows:\n\nStart with \\(\\mathbf{S}_0 = \\mathbf{S}\\).\nFor some number of iterations \\(t=0,1,\\cdots, T-1\\) do the following:\n\nCalculate the QR factorization of \\(\\mathbf{S}_t\\): \\(\\mathbf{Q}_{t+1}, \\mathbf{R}_{t+1} = \\text{qr}(\\mathbf{S}_t)\\).\nUpdate \\(\\mathbf{S}_t\\) by reversing the factorization order: \\(\\mathbf{S}_{t+1} = \\mathbf{R}_{t+1} \\mathbf{Q}_{t+1}\\).\n\nTake \\(\\mathbf{\\Lambda} \\approx \\mathbf{S}_{T-1}\\) and \\(\\mathbf{X} \\approx \\mathbf{Q}_{T-1}\\).\n\nDue to the QR factorizations and matrix multiplications, this algorithm will be \\(O(n^3)\\) at each step, which all together gives a time complexity of \\(O(Tn^3)\\). It’s not at all obvious from what I’ve said why the QR algorithm even works. In fact, to work well it requires a few small modifications I won’t go into.\n\n\n6.3.4 Positive Definiteness\nThe eigenvalues of a symmetric matrix \\(\\mathbf{S}\\) are important because they in some sense specify how much \\(\\mathbf{S}\\) tends to stretch vectors in different directions. Most important for machine learning purposes though is the sign of the eigenvalues. The sign of the eigenvalues of a symmetric matrix essentially determine how hard it is to optimize a given function. This is especially relevant in machine learning, since training a model is all about optimizing the loss function of a model’s predictions against the data.\nIf \\(\\mathbf{S}\\) is \\(n \\times n\\), it will have \\(n\\) eigenvalues \\(\\lambda_0, \\lambda_1, \\cdots, \\lambda_{n-1}\\). Ignoring the fact that each eigenvalue can be zero, each one will be either positive or negative. That means the sequence of eigenvalues can have \\(2^n\\) possible arrangements of signs. For example, when \\(n=3\\), we could have any of the \\(2^3=8\\) possible sign arrangements for the eigenvalues \\((\\lambda_0, \\lambda_1, \\lambda_2)\\),\n\\[(+, +, +), \\ (+, +, -), \\ (+, -, +), \\ (-, +, +), \\ (+, -, -), \\ (-, +, -), \\ (-, -, +), \\ (-, -, -).\\]\nMost of these arrangements will have mixed signs, but there will always be exactly two arrangements that don’t, namely when the eigenvalues are all positive, and when the eigenvalues are all negative. Most useful in applications like machine learning are when the eigenvalues are positive, or non-negative.\nA symmetric matrix whose eigenvalues are all positive is called positive definite. A positive definite matrix is essentially the matrix equivalent of a positive real number. When \\(\\mathbf{S}\\) is positive definite, we sometimes write \\(\\mathbf{S} \\succ 0\\) to make the analogy clear. Similarly, if the eigenvalues are all non-negative, the matrix is called positive semi-definite, sometimes written \\(\\mathbf{S} \\succcurlyeq 0\\). A positive semi-definite matrix is the matrix generalization of a non-negative number. Clearly any positive definite matrix is also positive semi-definite.\nBy writing \\(\\mathbf{S}=\\mathbf{X \\Lambda X}^\\top\\) and expanding everything out term-by-term, it’s not hard to show that,\n\nif \\(\\mathbf{S}\\) is positive definite, then \\(\\mathbf{x}^\\top \\mathbf{S} \\mathbf{x} > 0\\) for any non-zero vector \\(\\mathbf{x} \\in \\mathbb{R}^n\\),\nif \\(\\mathbf{S}\\) is positive semi-definite, then \\(\\mathbf{x}^\\top \\mathbf{S} \\mathbf{x} \\geq 0\\) for any non-zero vector \\(\\mathbf{x} \\in \\mathbb{R}^n\\).\n\nExpressions of the form \\(\\mathbf{x}^\\top \\mathbf{S} \\mathbf{x}\\) are called quadratic forms. They’ll always be scalars since all they are is a dot product \\(\\mathbf{x} \\cdot \\mathbf{S} \\mathbf{x}\\). This is why a positive definite matrix extends the idea of a positive number, since a positive number \\(a\\) would satisfy \\(xax=ax^2 > 0\\) for any non-zero scalar \\(x\\). Similarly for the positive semi-definite case, a non-negative scalar would satisfy \\(xax \\geq 0\\).\nNote that two of the most important matrices in machine learning, the Hessian and the covariance matrix, are both positive semi-definite, so these things do come up in applications. As you’d probably guess, the easiest way to determine if a symmetric matrix is positive definite or semi-definite is to just calculate the eigenvalues and check their signs. For example, I showed before that the matrix\n\\[\n\\mathbf{S} =\n\\begin{pmatrix}\n2 & 1 \\\\\n1 & 2 \\\\\n\\end{pmatrix}\n\\]\nhas eigenvalues \\(\\lambda = 3, 1\\). Since both of these are positive, \\(\\mathbf{S}\\) is positive definite. It’s also positive semi-definite since they’re both non-negative. To check if a matrix is positive definite, for example, in numpy, you can do something like the following. Modify the inequality accordingly for the other types.\n\n\nCode\ndef is_positive_definite(S):\n    eigvals = np.linalg.eigvals(S)\n    return np.all(eigvals > 0)\n\nS = np.array([[2, 1], \n              [1, 2]])\nis_positive_definite(S)\n\n\nTrue\n\n\nSuppose \\(a \\neq 0\\) is some non-negative number. We know we can take its square root to get another non-negative number \\(\\sqrt{a} \\neq 0\\). Positive semi-definite matrices have a similar property. If \\(\\mathbf{S} \\succcurlyeq 0\\), then we can find a “square root” matrix \\(\\mathbf{R}\\) such that\n\\[\\mathbf{S} = \\mathbf{R} \\mathbf{R}^\\top.\\]\nIt turns out though that matrices have many possible square roots, not just one. For this reason, we might as well choose an \\(\\mathbf{R}\\) that has some convenient form. One useful form is to assume that \\(\\mathbf{R}\\) is triangular. When we do this, we get what’s called the Cholesky Factorization. If \\(\\mathbf{S}\\) is positive semi-definite, we’ll factor \\(\\mathbf{S}\\) into as,\n\\[\\mathbf{S} = \\mathbf{L} \\mathbf{L}^\\top,\\]\nwhere \\(\\mathbf{L}\\) is some lower triangular matrix, which also means \\(\\mathbf{L}^\\top\\) is upper triangular.\nHere’s an example. Let’s try to find the Cholesky factorization of the same symmetric matrix from before,\n\\[\n\\mathbf{S} =\n\\begin{pmatrix}\n2 & 1 \\\\\n1 & 2 \\\\\n\\end{pmatrix}.\n\\]\nTake \\(\\mathbf{L}\\) to be\n\\[\n\\mathbf{L} =\n\\begin{pmatrix}\n\\sqrt{2} & 0 \\\\\n\\frac{1}{\\sqrt{2}} & \\sqrt{\\frac{3}{2}} \\\\\n\\end{pmatrix} \\approx\n\\begin{pmatrix}\n1.414 & 0 \\\\\n0.707 & 1.225 \\\\\n\\end{pmatrix}.\n\\]\nThen\n\\[\n\\mathbf{L}\\mathbf{L}^\\top =\n\\begin{pmatrix}\n\\sqrt{2} & 0 \\\\\n\\frac{1}{\\sqrt{2}} & \\sqrt{\\frac{3}{2}} \\\\\n\\end{pmatrix}\n\\begin{pmatrix}\n\\sqrt{2} & \\frac{1}{\\sqrt{2}} \\\\\n0 & \\sqrt{\\frac{3}{2}} \\\\\n\\end{pmatrix} =\n\\begin{pmatrix}\n2 & 1 \\\\\n1 & 2 \\\\\n\\end{pmatrix} =\n\\mathbf{S},\n\\]\nhence \\(\\mathbf{L}\\) is the Cholesky factor or square root of \\(\\mathbf{S}\\). How I came up with \\(\\mathbf{L}\\) here isn’t important. We can do the Cholesky factorization in numpy using np.linalg.cholesky. Let’s check my answer above is correct. Looks like it is.\n\n\nCode\nS = np.array([[2, 1], \n              [1, 2]])\nL = np.linalg.cholesky(S)\nprint(f'L = \\n{L}')\nprint(f'L L^T = \\n{L @ L.T}')\n\n\nL = \n[[1.41421356 0.        ]\n [0.70710678 1.22474487]]\nL L^T = \n[[2. 1.]\n [1. 2.]]\n\n\nIn practice, the Cholesky factorization is calculated using variants on the same algorithms used to calculate the LU factorization. Indeed, the Cholesky and LU factorizations have a lot in common. Both methods factor a matrix into a product of lower and upper triangular matrices. The major difference is that Cholesky only needs to find one lower triangular matrix \\(\\mathbf{L}\\), not two. Like LU factorization, Cholesky runs in \\(O(n^3)\\) time, but only uses half the FLOPS that LU does.\nThis means positive semi-definite matrices have more efficient algorithms than general matrices do. For example, suppose you wanted to solve a linear system \\(\\mathbf{A}\\mathbf{x}=\\mathbf{b}\\). If you knew \\(\\mathbf{A}\\) was positive semi-definite, you could solve the system at half the cost by calculating the Cholesky factorization \\(\\mathbf{A}=\\mathbf{L}\\mathbf{L}^\\top\\), and then using substitution to solve for \\(\\mathbf{x}\\). Similar approaches apply for other matrix quantities, like the inverse or determinant.\n\n\n6.3.5 Singular Value Decomposition\nThe spectral decomposition is mostly useful for square symmetric matrices. Yet, the properties of eigenvalues and eigenvectors seem to be incredibly useful for understanding how a matrix behaves. They say something useful about the characteristic scales and directions of a matrix and its underlying linear operator. It turns out we can generalize the spectral decomposition to arbitrary matrices, but with some slight modifications. This modified factorization is called the singular value decomposition, or SVD for short.\nSuppose \\(\\mathbf{A}\\) is some arbitrary \\(m \\times n\\) matrix. It turns out we can always factor \\(\\mathbf{A}\\) into a product of the form\n\\[\\mathbf{A} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^\\top,\\]\nwhere \\(\\mathbf{U}\\) is an \\(m \\times m\\) orthogonal matrix called the left singular matrix, \\(\\mathbf{V}\\) is a different \\(n \\times n\\) orthogonal matrix called the left singular matrix, and \\(\\mathbf{\\Sigma}\\) is an \\(m \\times n\\) diagonal matrix called the singular value matrix.\nThe singular value matrix \\(\\mathbf{\\Sigma}\\) is a rectangular diagonal matrix. This means the diagonal will only have \\(k=\\min(m, n)\\) entries. The diagonal entries are called the singular values of \\(\\mathbf{A}\\), usually denoted \\(\\sigma_0, \\sigma_1, \\cdots, \\sigma_{k-1}\\). Unlike eigenvalues, singular values are required to be non-negative.\nThe column vectors of \\(\\mathbf{U}\\) and \\(\\mathbf{V}\\) are called the left and right singular vectors respectively. Since both matrices are orthogonal, their singular vectors will form an orthonormal basis for \\(\\mathbb{R}^m\\) and \\(\\mathbb{R}^n\\) respectively.\nNotice that whereas with the spectral composition \\(\\mathbf{S} = \\mathbf{X} \\mathbf{\\Lambda} \\mathbf{X}^\\top\\) has only a single orthogonal matrix \\(\\mathbf{X}\\), the SVD has two different orthogonal matrices \\(\\mathbf{U}\\) and \\(\\mathbf{V}\\) to worry about, and each one is a different size. Also, while \\(\\mathbf{\\Lambda}\\) can contain eigenvalues of any sign, \\(\\mathbf{\\Sigma}\\) can only contain singular values that are nonnegative.\nNonetheless, the two factorizations are related by the following fact: The singular values of \\(\\mathbf{A}\\) are the eigenvalues of the symmetric matrix \\(\\mathbf{S} = \\mathbf{A}^\\top \\mathbf{A}\\). Not only that, they’re also the eigenvalues of the transposed symmetric matrix \\(\\mathbf{S}^\\top = \\mathbf{A} \\mathbf{A}^\\top\\). This fact gives one way you could actually calculate the SVD. The singular value matrix \\(\\mathbf{\\Sigma}\\) will just be the eigenvalue matrix of \\(\\mathbf{S}\\) (and \\(\\mathbf{S}^\\top\\)). The left singular matrix \\(\\mathbf{U}\\) will be the eigenvector matrix of \\(\\mathbf{S}\\). The right singular matrix \\(\\mathbf{V}\\) will be the eigenvector matrix of \\(\\mathbf{S}^\\top\\). Very roughly speaking, this is what many SVD algorithms use, e.g. by applying the QR algorithm on both \\(\\mathbf{S}\\) and \\(\\mathbf{S}^\\top\\).\nCalculating the SVD by hand is much more of a pain than the spectral decomposition is because you have to do it twice, once on \\(\\mathbf{S}\\) and once on \\(\\mathbf{S}^\\top\\). I’ll spare you the agony of this calculation, and just use numpy to calculate the SVD of the following matrix,\n\\[\n\\mathbf{A} =\n\\begin{pmatrix}\n1 & 1 \\\\\n1 & 0 \\\\\n1 & -1 \\\\\n\\end{pmatrix}.\n\\]\nWe can use np.linalg.svd(A) to calculate the SVD of \\(\\mathbf{A}\\). It’ll return a triplet of arrays, in order \\(\\mathbf{U}\\), the diagonal of \\(\\mathbf{\\Sigma}\\), and \\(\\mathbf{V}^T\\). Note to get the full \\(\\mathbf{\\Sigma}\\) you can’t just use np.diag since \\(\\mathbf{\\Sigma}\\) won’t be square here. You have to add a row of zeros after to make the calculation work out. I’ll do this just using a loop and filling in the diagonals manually.\nNotice that the two singular values are positive, \\(\\sigma_0 = \\sqrt{3} \\approx 1.732\\) and \\(\\sigma_1 = \\sqrt{2} \\approx 1.414\\). In this example, the right singular matrix \\(\\mathbf{V}\\) is just \\(\\text{diag}(-1, 1)\\), which is clearly orthogonal. The left singular matrix \\(\\mathbf{U}\\) is a little harder to see, but it’s also orthogonal. Finally, the product \\(\\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^\\top\\) indeed gives \\(\\mathbf{A}\\).\n\n\nCode\nA = np.array([\n    [1, 1],\n    [1, 0],\n    [1, -1]])\nm, n = A.shape\nk = min(m, n)\nU, sigma, Vt = np.linalg.svd(A)\nSigma = np.zeros((m, n))\nfor i in range(k):\n    Sigma[i, i] = sigma[i]\nUSVt = U @ Sigma @ Vt\nprint(f'U = \\n{U.round(10)}')\nprint(f'Sigma = \\n{Sigma.round(10)}')\nprint(f'V = \\n{Vt.T.round(10)}')\nprint(f'U Sigma V^T = \\n{USVt.round(10)}')\n\n\nU = \n[[-0.57735027  0.70710678  0.40824829]\n [-0.57735027  0.         -0.81649658]\n [-0.57735027 -0.70710678  0.40824829]]\nSigma = \n[[1.73205081 0.        ]\n [0.         1.41421356]\n [0.         0.        ]]\nV = \n[[-1.  0.]\n [-0.  1.]]\nU Sigma V^T = \n[[ 1.  1.]\n [ 1.  0.]\n [ 1. -1.]]\n\n\nTo give you an intuition is to what the SVD is doing, suppose \\(\\mathbf{x} \\in \\mathbb{R}^n\\) is some size-\\(n\\) vector. Suppose we want to operate on \\(\\mathbf{x}\\) with \\(\\mathbf{A}\\) to get a new vector \\(\\mathbf{v} = \\mathbf{A}\\mathbf{x}\\). Writing \\(\\mathbf{A} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^\\top\\), we can do this operation in a sequence of three successive steps:\n\nCalculate \\(\\mathbf{y} = \\mathbf{V}^\\top \\mathbf{x}\\): The output is also a size-\\(n\\) vector \\(\\mathbf{y} \\in \\mathbb{R}^n\\). Since \\(\\mathbf{V}\\) is orthogonal, this action can only rotate (or reflect) \\(\\mathbf{x}\\) by some angle in space.\nCalculate \\(\\mathbf{z} = \\mathbf{\\Sigma}\\mathbf{y}\\): The output is now a size-\\(k\\) vector \\(\\mathbf{z} \\in \\mathbb{R}^k\\). Since \\(\\mathbf{\\Sigma}\\) is diagonal, it can only stretch \\(\\mathbf{y}\\) along the singular directions of \\(\\mathbf{V}\\), not rotate it.\nCalculate \\(\\mathbf{v} = \\mathbf{U}\\mathbf{z}\\): The output is now a size-\\(m\\) vector \\(\\mathbf{v} \\in \\mathbb{R}^m\\). Since \\(\\mathbf{U}\\) is orthogonal, this action can only rotate (or reflect) \\(\\mathbf{z}\\) by some angle in space.\n\nThe final output is thus a vector \\(\\mathbf{v} = \\mathbf{A}\\mathbf{x}\\) that first got rotated in \\(\\mathbb{R}^n\\), then scaled in \\(\\mathbb{R}^k\\), then rotated again in \\(\\mathbb{R}^m\\). So you can visualize this better let’s take a specific example. To make everything show up on one plot I’ll choose a \\(2 \\times 2\\) matrix, so \\(m=n=k=2\\), for example\n\\[\n\\mathbf{A} =\n\\begin{pmatrix}\n1 & 2 \\\\\n1 & 1 \\\\\n\\end{pmatrix}.\n\\]\nThe singular values to this matrix turn out to be \\(\\sigma_0 \\approx 2.618\\) and \\(\\sigma_1 \\approx 0.382\\). What I’m going to do is randomly sample a bunch of unit vectors \\(\\mathbf{x}\\), then apply the successive operations above to each vector. The original vectors \\(\\mathbf{x}\\) are shown in red, the vectors \\(\\mathbf{y} = \\mathbf{V}^\\top \\mathbf{x}\\) in blue, the vectors \\(\\mathbf{z} = \\mathbf{\\Sigma}\\mathbf{y}\\) in green, and finally the vectors \\(\\mathbf{v} = \\mathbf{U}\\mathbf{z}\\) in black. Notice that the red vectors just kind of fill in the unit circle, since they’re all unit vectors of length one. The blue vectors also fill in the unit circle, since \\(\\mathbf{V}^\\top\\) can only rotate vectors, not stretch them. The green vectors then get stretched out into an elliptical shape due to \\(\\mathbf{\\Sigma}\\). The distortion of the ellipse depends on the “distortion ratio” \\(\\frac{\\sigma_0}{\\sigma_1} \\approx 6.85\\). This means one axis gets stretched about \\(6.85\\) times as much as the other. Finally, since \\(\\mathbf{U}\\) can only rotate vectors, the black vectors then rotate these stretched vectors into their final position.\n\n\nCode\nA = np.array([\n    [1, 2],\n    [1, 1]])\nm, n = A.shape\nk = min(m, n)\nU, sigma, Vt = np.linalg.svd(A)\nSigma = np.diag(sigma)\nprint(f'Sigma = \\n{Sigma.round(10)}')\n\n\nSigma = \n[[2.61803399 0.        ]\n [0.         0.38196601]]\n\n\n\n\nCode\nplot_svd(A)\n\n\n\n\n\n\n\n\n\nThe “distortion ratio” \\(\\frac{\\sigma_0}{\\sigma_1}\\) mentioned above can actually be used as a measure of how invertible a matrix is. It’s called the condition number, denoted \\(\\kappa\\). For a general \\(n \\times n\\) matrix, the condition number is defined as the ratio of the largest to the smallest singular value,\n\\[\\kappa = \\frac{\\sigma_0}{\\sigma_{k-1}}.\\]\nThe higher the condition number is, the harder it is to invert \\(\\mathbf{A}\\). A condition number of \\(\\kappa=1\\) is when the singular values are the same. These are easiest to invert. Matrices with low \\(\\kappa\\) are called called well-conditioned matrices. The identity matrix has \\(\\kappa=1\\), for example. If one of the singular values is \\(0\\) then \\(\\kappa\\) will be infinite, meaning the matrix isn’t invertible at all. Matrices with high \\(\\kappa\\) are called ill-conditioned matrices. For this reason, the condition number is very often used in calculations when it’s important to make sure that \\(\\mathbf{A}\\) isn’t singular or close to singular. In numpy, you can calculate the condition number of a matrix directly by using np.linalg.cond(A).\n\n\n6.3.6 Low-Rank Approximations\nThe SVD is useful for many reasons. In fact, it’s probably the single most useful factorization in all of applied linear algebra. One reason this is true is because every matrix has one. When in doubt, if you can’t figure out how to do something with a matrix, you can take its SVD and try to work with those three matrices one-by-one. While that’s nice, the more useful application of the SVD to machine learning is that it’s a good way to compress or denoise data. To see why we need to look at the SVD in a slightly different way.\nSuppose \\(\\mathbf{A}\\) is some \\(m \\times n\\) matrix. Suppose \\(\\mathbf{u}_0, \\mathbf{u}_1, \\cdots, \\mathbf{u}_{m-1}\\) are the column vectors of \\(\\mathbf{U}\\), and \\(\\mathbf{v}_0, \\mathbf{v}_1, \\cdots, \\mathbf{v}_{n-1}\\) are the column vectors of \\(\\mathbf{V}\\). Suppose \\(\\sigma_0, \\sigma_1, \\cdots, \\sigma_{k-1}\\) are the singular values of \\(\\mathbf{A}\\), by convention ordered from largest to smallest. Then writing out the SVD in terms of the column vectors, and multiplying everything out matrix multiplication style, we have\n\\[\n\\mathbf{A} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^\\top =\n\\begin{pmatrix}\n\\mathbf{u}_0 & \\mathbf{u}_1 & \\cdots & \\mathbf{u}_{m-1}\n\\end{pmatrix}\n\\text{diag}\\big(\\sigma_0, \\sigma_1, \\cdots, \\sigma_{k-1}\\big)\n\\begin{pmatrix}\n\\mathbf{v}_0^\\top \\\\ \\mathbf{v}_1^\\top \\\\ \\cdots \\\\ \\mathbf{v}_{n-1}^\\top\n\\end{pmatrix} =\n\\sum_{i=0}^{k-1} \\sigma_i \\mathbf{u}_i \\mathbf{v}_i^\\top =\n\\sigma_0 \\mathbf{u}_0 \\mathbf{v}_0^\\top + \\sigma_1 \\mathbf{u}_1 \\mathbf{v}_1^\\top + \\cdots + \\sigma_{k-1} \\mathbf{u}_{k-1} \\mathbf{v}_{k-1}^\\top.\n\\]\nThat is, we can write \\(\\mathbf{A}\\) as a sum of outer products over the singular vectors, each weighted by its singular value. That’s fine. But why is it useful? All I did was re-write the SVD in a different form, after all. The gist of it is that we can use this formula to approximate \\(\\mathbf{A}\\) by a lower-dimensional matrix. Supposing we only kept the first \\(d < k\\) terms of the right-hand side and dropped the rest, we’d have\n\\[\\mathbf{A} \\approx \\mathbf{U}_d \\mathbf{\\Sigma}_d \\mathbf{V}_d^\\top = \\sigma_0 \\mathbf{u}_0 \\mathbf{v}_0^\\top + \\sigma_1 \\mathbf{u}_1 \\mathbf{v}_1^\\top + \\cdots + \\sigma_{d-1} \\mathbf{u}_{d-1} \\mathbf{v}_{d-1}^\\top.\\]\nThis approximation will be a rank-\\(d\\) matrix again of size \\(m \\times n\\). It’s rank \\(d\\) because it’s a sum of \\(d\\) “independent” rank-1 matrices. When \\(d<<k\\), this is called the low-rank approximation. While this approximation is low rank it still has size \\(m \\times n\\). It’s the inner dimensions that got cut from \\(k\\) to \\(d\\), not the outer dimensions. To get a true low-dimensional approximation, we need to multiply both sides by \\(\\mathbf{V}_d\\),\n\\[\\mathbf{A}_d =  \\mathbf{A} \\mathbf{V}_d = \\mathbf{U}_d \\mathbf{\\Sigma}_d.\\]\nWe’re now approximating the \\(m \\times n\\) matrix \\(\\mathbf{A}\\) with an \\(m \\times d\\) matrix I’ll call \\(\\mathbf{A}_d\\). Said differently, we’re compressing the \\(n\\) columns of \\(\\mathbf{A}\\) down to just \\(d<<n\\) columns. Note that we’re not dropping the last \\(n-d\\) columns, we’re building new columns that best approximate all of the old columns.\nLet’s try to understand why low rank approximations are useful, and that they indeed do give good approximations to large matrices. To do so, consider the following example. I’m going to load some data from a well-known dataset in machine learning called MNIST. It’s a dataset of images of handwritten digits. When the low-rank approximation is applied to data, it’s called principle components analysis, or PCA. PCA is probably the most fundamental dimension reduction algorithm, a way of compressing high-dimensional data into lower-dimensional data.\nEach image is size \\(28 \\times 28\\), which flatten out into \\(n = 28 \\cdot 28 = 784\\) dimensions. I’ll load \\(m=1000\\) random samples from the MNIST dataset. This will create a matrix \\(\\mathbf{A}\\) of shape \\(1000 \\times 784\\). I’ll go ahead and calculate the SVD to get \\(\\mathbf{U}\\), \\(\\mathbf{\\Sigma}\\), and \\(\\mathbf{V}^\\top\\). In this case, \\(k=\\min(m,n)=784\\), so these matrices will have sizes \\(1000 \\times 1000\\), \\(1000 \\times 784\\), and \\(784 \\times 784\\) respectively. As I mentioned before, numpy only returns the non-zero diagonals of \\(\\mathbf{\\Sigma}\\), which is a size \\(k=784\\) vector of the singular values. Thankfully, that’s all we’ll need here.\n\n\nCode\nm = 1000\nA = sample_mnist(size=m)\nU, sigma, Vt = np.linalg.svd(A)\nA.shape, U.shape, sigma.shape, Vt.shape\nprint(f'A.shape = {A.shape}')\nprint(f'U.shape = {U.shape}')\nprint(f'sigma.shape = {sigma.shape}')\nprint(f'Vt.shape = {Vt.shape}')\n\n\nA.shape = (1000, 784)\nU.shape = (1000, 1000)\nsigma.shape = (784,)\nVt.shape = (784, 784)\n\n\nThink of each row of \\(\\mathbf{A}\\) as representing a single image in the dataset, and each column of \\(\\mathbf{A}\\) as representing a single pixel of the image.\nSince these are images, I might as well show you what they look like. To do that, just pick a random row from the matrix. Each row will be a flattened image. To turn it into an image, we can just reshape the row to have shape \\(28 \\times 28\\), then plot it using plt.imshow. Below, I’m picking off the first row, which turns out to be an image of a handwritten \\(0\\).\n\n\nCode\nimg = A[0, :].reshape(28, 28)\nplt.imshow(img, cmap='Greys')\nplt.axis('off')\nplt.show()\n\n\n\n\n\n\n\n\n\nLet’s start by taking \\(d=2\\). Why? Because when \\(d=2\\) we can plot each image as a point in the xy-plane! This suggests a powerful application of the low-rank approximation, to visualize high-dimensional data. To calculate \\(\\mathbf{A}_d\\), we’ll need to truncate \\(\\mathbf{U}\\), \\(\\mathbf{\\Sigma}\\), and \\(\\mathbf{V}^\\top\\). To make the shapes come out right, we’ll want to drop the first \\(d\\) columns of \\(\\mathbf{U}\\) and the first \\(d\\) rows of \\(\\mathbf{V}^\\top\\). Once we’ve got these, we can calculate \\(\\mathbf{A}_d\\), which in this case will be size \\(1000 \\times 2\\).\n\n\nCode\nd = 2\nU_d, sigma_d, Vt_d = U[:, :d], sigma[:d], Vt[:d, :]\nA_d = A @ Vt_d.T\nprint(f'U_d.shape = {U_d.shape}')\nprint(f'sigma_d = {sigma_d}')\nprint(f'Vt_d.shape = {Vt_d.shape}')\nprint(f'A_d.shape = {A_d.shape}')\n\n\nU_d.shape = (1000, 2)\nsigma_d = [197.89062659  66.60026657]\nVt_d.shape = (2, 784)\nA_d.shape = (1000, 2)\n\n\nNow we have \\(m=1000\\) “images”, each with \\(d=2\\) “variables”. This means we can plot them in the xy-plane, taking \\(x\\) to be the first column A_d[:, 0], and \\(y\\) to be the second column A_d[:, 1]. Here’s a scatter plot of all images projected down to 2 dimensions. I can’t make out any patterns in the plot, and you probably can’t either. But at least we’ve found an interesting and sometimes useful way to visualize high-dimensional data.\n\n\nCode\nplt.scatter(A_d[:, 0], A_d[:, 1], s=1, alpha=0.8)\nplt.xticks([])\nplt.yticks([])\nplt.title(f'{m} MNIST Images')\nplt.show()\n\n\n\n\n\n\n\n\n\nHow good is our approximation? We can use the singular values to figure this out. In the low rank approximation, we’re keeping \\(d\\) singular values and dropping the remaining \\(k-d\\). Throwing away those remaining singular values is throwing away information about our original matrix \\(\\mathbf{A}\\). To figure out how much information we’re keeping in our approximation, we can just look at the ratio of the sum of singular values kept to the total sum of all singular values,\n\\[R_d = \\frac{\\sigma_0 + \\sigma_1 + \\cdots + \\sigma_{d-1}}{\\sigma_0 + \\sigma_1 + \\cdots + \\sigma_{k-1}}.\\]\nThis ratio is sometimes called the explained variance for reasons I’ll get into in a future lesson.\nIn the rank-2 case I just worked out, this ratio turns out to be \\(R_2 = \\frac{\\sigma_0 + \\sigma_1}{\\sum \\sigma_i} \\approx 0.087\\). That is, this rank-2 approximation is preserving about 8.7% of the information in the original data.\n\n\nCode\nR_d = np.sum(sigma_d) / np.sum(sigma)\nprint(f'R_d = {R_d}')\n\n\nR_d = 0.08740669535517863\n\n\nThat’s pretty bad. We can do better. Let’s take \\(d=100\\) and see how well that does. Of course, we won’t be able to plot the data in the xy-plane anymore, but it’ll better represent the original data. We’re now at \\(R_d \\approx 0.643\\), which means we’re preserving about 64.3% of the information in the original data, and we’re doing it using only \\(\\frac{100}{784} \\approx 0.127\\), or 12.7% of the total columns of \\(\\mathbf{A}\\).\n\n\nCode\nd = 100\nU_d, sigma_d, Vt_d = U[:, :d], sigma[:d], Vt[:d, :]\nA_d = A @ Vt_d.T\nprint(f'A_d.shape = {A_d.shape}')\n\n\nA_d.shape = (1000, 100)\n\n\n\n\nCode\nR_d = np.sum(sigma_d) / np.sum(sigma)\nprint(f'R_d = {R_d}')\n\n\nR_d = 0.6433751746962163\n\n\nAnother way to see how good our compression is is to “unproject” the compressed images and plot them. To unproject \\(\\mathbf{A}_d\\), just multiply on the right again by \\(\\mathbf{V}^\\top\\) to get the original \\(m \\times n\\) matrix approximation again,\n\\[\\mathbf{A} \\approx \\mathbf{A}_d \\mathbf{V}^\\top.\\]\nOnce I’ve done that, I can just pluck a random row from the approximation, resize it, and plot it using plt.imshow, just like before. Notice this time we can still clearly see the handwritten \\(0\\), but it’s a bit grainer than it was before. The edges aren’t as sharp. Nevertheless, we can still make out the digit pretty solidly.\n\n\nCode\nimg = (A_d @ Vt_d)[0, :].reshape(28, 28)\nplt.imshow(img, cmap='Greys')\nplt.axis('off')\nplt.show()\n\n\n\n\n\n\n\n\n\nBut why is this approach good for compression anyway? After all, we still have to unproject the rows back into the original \\(m \\times n\\) space. Maybe think about it this way. If you just stored the full matrix \\(\\mathbf{A}\\), you’d have tot store \\(m \\cdot n\\) total numbers. In this example, that’s \\(1000 \\cdot 784 = 784000\\) numbers you’d have to store in memory.\nBut suppose now we do the low rank approximation. What we can then do is just store \\(\\mathbf{A}_d\\) and \\(\\mathbf{V}\\) instead. That means we’d instead store \\(m \\cdot d + d \\cdot n\\) total numbers. In our example, that comes out to \\(1000 \\cdot 100 + 100 \\cdot 784 = 100000 + 78400 = 178400\\), which is only \\(\\frac{178400}{784000} \\approx 0.227\\) or 22.7% of the numbers we’d have to store otherwise. We’ve thus compressed our data by a factor of about \\(\\frac{1}{0.227} \\approx 4.4\\). That’s a 4.4x compression of the original images.\nNow, this kind of PCA compression isn’t perfect, or lossless, since we can’t recover the original images exactly. But we can still recover the most fundamental features of the image, which in this case are the handwritten digits. This kind of compression is lossy, since it irreversibly throws away some information in the original data. Yet, it still maintains enough information to be useful in many settings."
  },
  {
    "objectID": "notebooks/multivariate-calculus.html#multivariate-differentiation",
    "href": "notebooks/multivariate-calculus.html#multivariate-differentiation",
    "title": "7  Multivariate Calculus",
    "section": "7.1 Multivariate Differentiation",
    "text": "7.1 Multivariate Differentiation\nJust as we can differentiate univariate functions like \\(y=f(x)\\), we can also differentiate multivariate functions like \\(z=f(x,y)\\). The main difference is that we can take derivatives of many inputs variables, not just one.\n\n7.1.1 The Gradient\nSuppose \\(z=f(x,y)\\) and we want to ask the question, how does \\(z\\) change if we change \\(x\\) by an infinitesimal amount \\(dx\\), holding \\(y\\) constant? Evidently it would be \\(z + dz = f(x+dx, y)\\). If we pretend \\(y\\) is constant, this would mean\n\\[dz = f(x+dx, y) - f(x, y).\\]\nDividing both sides by \\(dx\\) we’d get something like a derivative. But it’s not the derivative since we’re only changing \\(x\\) and fixing \\(y\\). For this reason it’s called the partial derivative of \\(z\\) with respect to \\(x\\), and typically written with funny \\(\\partial\\) symbols instead of \\(d\\) symbols,\n\\[\\frac{\\partial z}{\\partial x} = \\frac{f(x+dx, y) - f(x, y)}{dx}.\\]\nSimilarly, we can ask the dual question, how does \\(z\\) change if we change \\(y\\) by an infinitesimal amount \\(dy\\), holding \\(x\\) constant? By the same logic, we’d get\n\\[dz = f(x, y + dy) - f(x, y),\\]\nand dividing by \\(dy\\) would give the partial derivative of \\(z\\) with respect to \\(y\\),\n\\[\\frac{\\partial z}{\\partial y} = \\frac{f(x, y + dy) - f(x, y)}{dy}.\\]\nBut these don’t tell us everything. We want to know how \\(z\\) changes if we change \\(x\\) and \\(y\\) arbitrarily, not if we hold one of them constant. That is, we want the full \\(dz\\). In the case when \\(y=f(x)\\), we saw that \\(dy=\\frac{dy}{dx}dx\\). If we only change \\(x\\), evidently \\(dz = \\frac{\\partial z}{\\partial x} dx\\). Similarly if we only change \\(y\\), then \\(dz = \\frac{\\partial z}{\\partial y} dy\\). It seems like if we want to change both, we should add these two effects together,\n\\[dz = \\frac{\\partial z}{\\partial x} dx + \\frac{\\partial z}{\\partial y} dy.\\]\nThis equation is called the bivariate chain rule. Since it depends on changes in both \\(x\\) and \\(y\\), \\(dz\\) is called the total differential. The chain rule tells us everything we need to know about how \\(z\\) changes when either \\(x\\) or \\(y\\) are perturbed by some small amount. The amount that \\(z\\) gets perturbed is \\(dz\\).\nIf we have a composite function like, say, \\(z=f(x,y)\\), \\(x=g(u, v)\\), \\(y=h(u, v)\\), we can do just like in the univariate chain rule and divide the total differential by \\(du\\) or \\(dv\\) to get the chain rule in partial derivative form,\n\\[\\frac{\\partial z}{\\partial u} = \\frac{\\partial z}{\\partial x}\\frac{\\partial x}{\\partial u} + \\frac{\\partial z}{\\partial y}\\frac{\\partial y}{\\partial u},\\]\n\\[\\frac{\\partial z}{\\partial v} = \\frac{\\partial z}{\\partial x}\\frac{\\partial x}{\\partial v} + \\frac{\\partial z}{\\partial y}\\frac{\\partial y}{\\partial v}.\\]\nThis is the form in which the bivariate chain rule usually appears in deep learning, but with many more variables.\nIt’s interesting to write this formula as a dot product of two vectors. If we define two vectors as follows,\n\\[\\frac{dz}{d\\mathbf{x}}=\\big(\\frac{\\partial z}{\\partial x}, \\frac{\\partial z}{\\partial y}\\big),\\]\n\\[d\\mathbf{x} = (dx, dy),\\]\nthen the chain rule would say\n\\[dz = \\frac{dz}{d\\mathbf{x}} \\cdot d\\mathbf{x}.\\]\nThis looks just like the equation for the ordinary derivative, \\(dy=\\frac{dy}{dx}dx\\),except there’s a dot product of vectors here.\nThe vector \\(\\frac{dz}{d\\mathbf{x}}\\) looks like the ordinary derivative, but it’s now a vector of partial derivatives. It’s called the gradient of \\(z=f(x,y)\\).\nIn many texts, the gradient is often written with the funny symbol \\(\\nabla f(x,y)\\). Other notations used are \\(\\frac{d}{d\\mathbf{x}}f(\\mathbf{x})\\), or \\(\\mathbf{f}'(\\mathbf{x})\\). I’ll often instead use the simpler notation of \\(\\mathbf{g}\\) or \\(\\mathbf{g}(x,y)\\) for the gradient when it’s clear what the function is we’re differentiating. All of these notations can represent the same gradient vector,\n\\[\\mathbf{g} = \\nabla f(\\mathbf{x}) = \\mathbf{f}'(\\mathbf{x}) = \\frac{d}{d\\mathbf{x}}f(\\mathbf{x}) = \\frac{dz}{d\\mathbf{x}}.\\]\nNote it’s very common in calculus and applications to abuse the difference between points and vectors. We might write \\(f(x, y)\\), or just \\(f(\\mathbf{x})\\), where it’s understood \\(\\mathbf{x}\\) is the vector \\(\\mathbf{x}=(x,y)\\). We’ll do this a lot. There’s no real difference between them.\nLet’s do an example. Consider the function \\(z=x^2+y^2\\). This function has a surface that looks like a bowl.\n\n\nCode\nx = np.linspace(-10, 10, 100)\ny = np.linspace(-10, 10, 100)\nf = lambda x, y: x**2 + y**2\n\nplot_function_3d(x, y, f, title='$z=x^2+y^2$', titlepad=10, labelpad=5, ticks_every=[4, 4, 50], dist=12)\n\n\n\n\n\n\n\n\n\nSuppose we treat \\(y\\) as constant, say \\(y=2\\). If we nudge \\(x\\) to \\(x+dx\\), then \\(z\\) would get nudged to\n\\[z+dz = f(x+dx,y) = (x+dx)^2 + y^2 = (x^2 + 2xdx + dx^2) + y^2 \\approx z + 2xdx.\\]\nThat is, \\[\\frac{\\partial z}{\\partial x} = 2x.\\]\nThis is exactly what we got before in the univariate case with \\(f(x)=x^2\\). This makes since. By treating \\(y\\) as constant we’re effectively pretending it’s not there in the calculation, which makes it act like we’re taking the 1D derivative \\(z=x^2\\).\nSince \\(z=x^2+y^2\\) is symmetric in \\(x\\) and \\(y\\), the exact same argument above would show\n\\[\\frac{\\partial z}{\\partial y} = 2y.\\]\nThe gradient vector would thus be \\[\\frac{dz}{d\\mathbf{x}} = (2x, 2y) = 2\\mathbf{x}, \\quad \\text{where }\\mathbf{x} = (x,y).\\]\nThe gradient looks exactly like the 1D version where \\(y=x^2\\) and \\(\\frac{dy}{dx}=2x\\), except there’s a vector \\(\\mathbf{x}\\) instead.\nJust as with the ordinary derivative, we can see that the gradient is a function of its inputs. The difference though is the gradient is a vector-valued function. Its output is a vector, not a scalar.\nNumerical differentiation extends naturally to the bivariate case as well. We can calculate partial derivatives numerically straight from their definitions, using reasonably small values like \\(dx=dy=10^{-5}\\). To get the gradient, just calculate the partials numerically and put them into an array.\nHere’s an example. I’ll calculate the partials \\(\\frac{\\partial z}{\\partial x}, \\frac{\\partial z}{\\partial y}\\) at the point \\(x_0=1, y_0=1\\). The partials are given by dzdx and dzdy respectively, and the gradient vector by grad. Notice the error is again on the order of \\(dx\\) and \\(dy\\), hence we get good agreement with the above equation when \\(x_0=1, y_0=1\\).\n\n\nCode\nx0 = y0 = 1\ndx = dy = 1e-5\n\ndzdx = (f(x0 + dx, y0) - f(x0, y0)) / dx\ndzdy = (f(x0, y0 + dy) - f(x0, y0)) / dy\n\ngrad = [dzdx, dzdy]\nprint(f'grad = {grad}')\n\n\ngrad = [2.00001000001393, 2.00001000001393]\n\n\n\n\n7.1.2 Visualizing Gradients\nIn the case of the ordinary univariate derivative \\(\\frac{dy}{dx}\\), we could think of it geometrically as the slope of the tangent line to \\(y=f(x)\\) at a point \\((x_0,y_0)\\). We can do something similar for the gradient \\(\\frac{dz}{d\\mathbf{x}}\\) by thinking of it as the vector of slopes defining a tangent plane to \\(z=f(\\mathbf{x})\\) at a point \\((\\mathbf{x}_0, z_0)\\).\nSuppose \\(z=f(x,y)\\). Let \\((x_0,y_0,z_0) \\in \\mathbb{R}^3\\) be a point in 3D space, with \\(z_0=f(x_0,y_0)\\). This is just a point on the 2D surface of \\(z=f(x,y)\\). Now, it doesn’t make much sense to talk about a single line that hugs this point, since there can now be infinitely many lines that hug that point. What we instead want to do is think about a plane that hugs the surface. This will be called the tangent plane. It’s given by\n\\[z = z_0 + \\frac{\\partial}{\\partial x}f(x_0,y_0) (x - x_0) + \\frac{\\partial}{\\partial y}f(x_0,y_0) (y - y_0),\\]\nor in vector notation just, \\[z = z_0 + \\frac{d}{d\\mathbf{x}} f(\\mathbf{x}_0) \\cdot (\\mathbf{x} - \\mathbf{x}_0), \\quad \\text{or} \\quad z = z_0 + \\mathbf{g}(\\mathbf{x}_0) \\cdot (\\mathbf{x} - \\mathbf{x}_0).\\]\nThis tangent plane will hug the surface of the function at the point \\((x_0,y_0,z_0)\\).\nHere’s an example, where I’ll calculate the tangent plane to \\(z=x^2+y^2\\) at the point \\((1,1)\\). Since I showed above that the gradient in this case is \\((2x, 2y)\\), the tangent line becomes \\(z=2 + 2(x-1) + 2(y-1)\\). Everything is done in an analogous way to the tangent line calculation from before.\n\n\nCode\nf = lambda x, y: x**2 + y**2\ndfdx = lambda x, y: (2 * x, 2 * y)\n\nx0 = y0 = 1\nz0 = f(x0, y0)\n\nx = np.linspace(-2 * x0, 2 * x0, 100)\ny = np.linspace(-2 * y0, 2 * y0, 100)\n\nf_tangent = lambda x, y: 2 * (x - x0) + 2 * (y - y0) + 2\n\nplot_tangent_plane(x, y, x0, y0, f, f_tangent, dfdx, plot_grad=True, grad_scale=2,\n                   title=f'Tangent Plane to $z=x^2+y^2$ at ${(x0, y0, z0)}$')\n\n\n\n\n\n\n\n\n\nIf you look at the plane, the partial of \\(z\\) with respect to \\(x\\) turns out to represent the slope of the line running along the plane parallel to the x-axis at the point \\((1,1)\\). Similarly, the partial of \\(z\\) with respect to \\(y\\) represents the slope of the line running along the plane parallel to the y-axis at the point \\((1,1)\\).\nThe gradient vector (shown in red) is both of these together, which gives a vector \\((2, 2)\\) that points in the steepest direction up the surface from the point \\((1,1)\\). Said differently, the gradient vector is the direction of steepest ascent.\nThis fact can be visualized easier by looking at the contour plot. In the contour plot, the tangent plane will appear as a line hugging tangent to the contour at the point \\((1,1)\\). The gradient vector will always point outward perpendicular to this line in the direction of steepest ascent of the function.\n\n\nCode\nplot_tangent_contour(x, y, x0, y0, f, f_tangent, dfdx, title=f'Tangent to $z=x^2+y^2$ at ${(x0, y0, z0)}$')\n\n\n\n\n\n\n\n\n\nHere’s an argument for why this is true. A contour is by definition a curve where \\(z\\) is constant. Imagine taking the surface of \\(z=f(x,y)\\) and at each \\(z\\) value slicing the surface parallel to the xy-plane. That’s all a contour is. This means that along any given contour we must have \\(dz=0\\), since \\(z\\) can’t change. But by the chain rule we already know\n\\[dz = \\frac{dz}{d\\mathbf{x}} \\cdot d\\mathbf{x}.\\]\nBut since \\(dz=0\\), this means \\[\\frac{dz}{d\\mathbf{x}} \\cdot d\\mathbf{x} = 0.\\]\nNow, recall two vectors \\(\\mathbf{x}\\) and \\(\\mathbf{x}\\) are orthogonal (i.e. perpendicular) if \\(\\mathbf{x} \\cdot \\mathbf{y} = 0\\). I’ve thus shown that the gradient vector \\(\\mathbf{g}\\) must be perpendicular to the differential vector \\(d\\mathbf{x}\\) along contours where \\(z\\) is constant.\nSince we’re confined to a contour of constant \\(z\\), any small changes \\(d\\mathbf{x}\\) as we move around the contour must be parallel to the contour, otherwise \\(dz\\) wouldn’t be zero. This means \\(\\mathbf{g}\\) must be perpendicular to the line tangent to the contour at \\((1,1)\\). That is, the gradient at \\((1,1)\\) is a vector pointing outward in the direction of steep ascent from the point \\((1,1)\\).\n\n\n7.1.3 The Hessian\nIn the univariate case, we had not just first derivatives \\(\\frac{dy}{dx}\\), but second derivatives \\(\\frac{d^2y}{dx^2}\\) too. In the multivariate case we can take second partial derivatives as well in the usual way, but there are now \\(2^2=4\\) different ways to calculate second derivatives,\n\\[\\frac{\\partial^2 z}{\\partial x^2}, \\frac{\\partial^2 z}{\\partial x \\partial y}, \\frac{\\partial^2 z}{\\partial y \\partial x}, \\frac{\\partial^2 z}{\\partial y^2}.\\]\nNote the partials are by convention applied from right to left. Thankfully this doesn’t matter, since for well-behaved functions the mixed partials commute with each other, i.e. \\[\\frac{\\partial^2 z}{\\partial x \\partial y} = \\frac{\\partial^2 z}{\\partial y \\partial x}.\\]\nJust as we could group first partial derivatives into a vector to get the gradient, we can group second partial derivatives into a matrix to get what’s called the Hessian matrix, \\[\n\\frac{d^2 z}{d\\mathbf{x}^2} =\n\\begin{pmatrix}\n\\frac{\\partial^2 z}{\\partial x^2} & \\frac{\\partial^2 z}{\\partial x \\partial y} \\\\\n\\frac{\\partial^2 z}{\\partial y \\partial x} & \\frac{\\partial^2 z}{\\partial y^2}\n\\end{pmatrix}.\n\\]\nThe Hessian is the multivariate generalization of the full second derivative, just as the gradient vector is the generalization of the full first derivative. I’ll often write the Hessian matrix with the symbol \\(\\mathbf{H}\\) or \\(\\mathbf{H}(\\mathbf{x})\\) for brevity.\nJust as the second derivative of a univariate function can be interpreted geometrically as representing the curvature of the curve \\(y=f(x)\\), the Hessian of a multivariate function represents the curvature of the surface \\(z=f(x,y)\\). This comes from looking at the multivariate tangent parabola\n\\[z = z_0 + \\mathbf{g}(\\mathbf{x}_0) \\cdot (\\mathbf{x} - \\mathbf{x}_0) + \\frac{1}{2}(\\mathbf{x} - \\mathbf{x}_0)^\\top \\mathbf{H}(\\mathbf{x}_0) (\\mathbf{x} - \\mathbf{x}_0).\\]\nThe curvature of the function can be obtained by looking at the eigenvalues of the Hessian at \\(\\mathbf{x}= \\mathbf{x}_0\\). Large eigenvalues represent steep curvature, while small eigenvalues represent shallow curvature. The sign of the eigenvalues indicate whether the function\n\nBowls upward: both eigenvalues are non-negative,\nBowls downward: both eigenvalues are non-positive,\nSaddles: one eigenvalue is positive, one is eigenvalue negative.\n\nCase (3) creates what’s called a saddlepoint, a point where the function slopes upwards in one direction, but downward in the other, creating the shape of something that resembles a horse’s saddle.\nFor the same working example \\(z=x^2+y^2\\), we’d have\n\\[\n\\mathbf{H} = \\frac{d^2 z}{d\\mathbf{x}^2} =\n\\begin{pmatrix}\n2 & 2 \\\\\n2 & 2\n\\end{pmatrix},\n\\]\nthat is, the Hessian of this function is constant, since no elements depend on \\(x\\) or \\(y\\).\nThe eigenvalues of this Hessian are \\(\\lambda=4,0\\), both of which are non-negative. Since the Hessian is constant, this means the function bowls upward at all points \\((x,y)\\). This also means this Hessian matrix is positive semi-definite.\n\n\nCode\nH = sp.Matrix([[2, 2], [2, 2]])\neigs = H.eigenvals()\nprint(f'eigenvalues = {list(eigs.keys())}')\n\n\neigenvalues = [4, 0]\n\n\nWhen a function’s Hessian is positive semi-definite, i.e. it bowls upward, it’s called a convex function. Convex functions are very important in optimization since convex functions always have a unique global minimum. Classical machine learning algorithms often take advantage of this fact.\nWhat about higher derivatives of multivariate functions? It turns out the \\(k\\)th derivative of a multivariate function is a rank-\\(k\\) tensor. This makes higher derivatives especially nasty, so we rarely see them.\n\n\n7.1.4 Differentiation in \\(n\\) Dimensions\nSimilarly, we can define all of these quantities for any n-dimensional multivariate function \\(y=f(\\mathbf{x})=f(x_0,x_1,\\cdots,x_{n-1}).\\) The partial derivative of \\(y\\) with respect to some \\(x_i\\) is the one whose only first order perturbation is \\(x_i+dx_i\\), with the rest staying fixed,\n\\[\\frac{\\partial y}{\\partial x_i} = \\frac{f(x_0,x_1,\\cdots,x_i+dx_i,\\cdots,x_{n-1}) - f(x_0,x_1,\\cdots,x_i,\\cdots,x_{n-1})}{dx_i}.\\]\nThat is, it’s the derivative of \\(y\\) with respect to \\(x_i\\) where all other inputs \\(x_j \\neq x_i\\) are held constant. The chain rule extends by adding a term for each \\(dx_i\\),\n\\[dy = \\sum_{i=0}^{n-1} \\frac{\\partial y}{\\partial x_i} dx_i = \\frac{\\partial y}{\\partial x_0} dx_0 + \\frac{\\partial y}{\\partial x_1} dx_1 + \\cdots + \\frac{\\partial y}{\\partial x_{n-1}} dx_{n-1},\\]\nOr, written as a dot product of \\(n\\) dimensional vectors,\n\\[\\begin{align*}\n\\frac{dy}{d\\mathbf{x}} &= \\bigg(\\frac{\\partial y}{\\partial x_0}, \\frac{\\partial y}{\\partial x_1},\\cdots,\\frac{\\partial y}{\\partial x_{n-1}} \\bigg), \\\\\nd\\mathbf{x} &= (dx_0, dx_1,\\cdots,dx_{n-1}), \\\\\ndy &= \\frac{dy}{d\\mathbf{x}} \\cdot d\\mathbf{x}.\n\\end{align*}\\]\nI’ll calculate a quick example with the \\(n\\) dimensional generalization of our running quadratic function,\n\\[y = x_0^2 + x_1^2 + \\cdots + x_{n-1}^2 = \\sum_{i=0}^{n-1} x_i^2.\\]\nSince each partial derivative gives \\(\\frac{\\partial y}{\\partial x_i} = 2x_i\\), the gradient for this function should be the n-dimensional vector\n\\[\\mathbf{g} = \\frac{dy}{d\\mathbf{x}} = (2x_0, 2x_1, \\cdots, 2x_{n-1}) = 2\\mathbf{x}.\\]\nUsing numpy we can efficiently calculate this function with the vectorized command np.sum(x ** 2). I’ll choose our point of interest to be the vector \\(\\mathbf{x}_0\\) of all ones. I’ll define a helper function dfdxi to calculate the ith partial derivative at \\(\\mathbf{x}_0\\). Note dx will be a vector of all zeros except at dx[i] = dxi. This will then be used in the function dfdx to calculate the gradient. It will loop over every index, calculate each partial, and put them in a vector grad. Observe that yet again we have a gradient vector of all twos to within an error of around 1e-5, except instead of 1 or 2 elements we have 100 of them.\n\n\nCode\ndef dfdxi(f, x0, i, dxi=1e-5):\n    dx = np.zeros(len(x0))\n    dx[i] = dxi\n    dydxi = (f(x0 + dx) - f(x0)) / dxi\n    return dydxi\n\ndef dfdx(f, x0, dxi=1e-5):\n    return np.array([dfdxi(f, x0, i, dxi=dxi) for i in range(len(x0))])\n\nf = lambda x: np.sum(x ** 2)\nx0 = np.ones(100)\ngrad = dfdx(f, x0)\nprint(f'grad.shape = {grad.shape}')\nprint(f'grad = \\n{grad}')\n\n\ngrad.shape = (100,)\ngrad = \n[2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001\n 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001\n 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001\n 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001\n 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001\n 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001\n 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001\n 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001\n 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001\n 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001\n 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001\n 2.00001]\n\n\nJust like with univariate functions, we can use the tangent plane to approximate the behavior of a multivariate function \\(f(\\mathbf{x})\\) near a point \\(\\mathbf{x}_0\\). This gives the first order approximation\n\\[f(\\mathbf{x}) \\approx f(\\mathbf{x}_0) + \\frac{d}{d\\mathbf{x}} f(\\mathbf{x}_0) \\cdot (\\mathbf{x} - \\mathbf{x}_0).\\]\nThe error in this approximation will again be quadratic in the distance between the two vectors, \\(||\\mathbf{x} - \\mathbf{x}_0||^2\\).\nThe Hessian matrix of second partial derivatives also extends to \\(n\\) dimensional scalar-valued functions \\(y = f(\\mathbf{x})\\). The difference is that instead of just \\(2^2=4\\) second partials, we now have \\(n^2\\) possible second partials. These can be organized into an \\(n \\times n\\) matrix\n\\[\n\\mathbf{H} = \\frac{d^2 y}{d\\mathbf{x}^2} =\n\\begin{pmatrix}\n\\frac{\\partial^2 y}{\\partial x_0^2} & \\frac{\\partial^2 y}{\\partial x_0 \\partial x_1} & \\cdots & \\frac{\\partial^2 y}{\\partial x_0 \\partial x_{n-1}} \\\\\n\\frac{\\partial^2 y}{\\partial x_1 \\partial x_0} & \\frac{\\partial^2 y}{\\partial x_1^2} & \\cdots & \\frac{\\partial^2 y}{\\partial x_1 \\partial x_{n-1}} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial^2 y}{\\partial x_{n-1} \\partial x_0} & \\frac{\\partial^2 y}{\\partial x_{n-1} \\partial x_1} & \\cdots & \\frac{\\partial^2 y}{\\partial x_{n-1}^2}\n\\end{pmatrix}.\n\\]\nThe mixed partials again typically all commute, which means \\(\\mathbf{H}\\) is a symmetric matrix, i.e. \\(\\mathbf{H}^\\top = \\mathbf{H}\\). The eigenvalues of \\(\\mathbf{H}\\) again determine the curvature of the function at any point \\(\\mathbf{x}=\\mathbf{x}_0\\). If the eigenvalues of the Hessian are all non-negative, \\(\\mathbf{H}\\) will be positive semi-definite, i.e. \\(\\mathbf{H} \\succcurlyeq 0\\). In the case, the function \\(f(\\mathbf{x})\\) will be a convex function and hence bowl upwards. If the Hessian isn’t positive semidefinite it’ll usually have saddlepoints, usually far more saddlepoints than minima or maxima in fact.\nWe can use the Hessian to define a second-order approximation to a multivariate function \\(f(\\mathbf{x})\\) near a point \\(\\mathbf{x}_0\\),\n\\[f(\\mathbf{x}) \\approx f(\\mathbf{x}_0) + \\mathbf{g}(\\mathbf{x}_0) \\cdot (\\mathbf{x} - \\mathbf{x}_0) + \\frac{1}{2}(\\mathbf{x} - \\mathbf{x}_0)^\\top \\mathbf{H}(\\mathbf{x}_0) (\\mathbf{x} - \\mathbf{x}_0).\\]\nThe error in this approximation will be cubic in the distance between the two vectors, \\(||\\mathbf{x} - \\mathbf{x}_0||^3\\).\n\n\n7.1.5 The Jacobian\nThus far we’ve seen the following two types of functions:\n\nscalar-valued functions of a scalar variable: \\(y=f(x)\\),\nscalar-valued functions of a vector variable: \\(y = f(\\mathbf{x})\\).\n\nAs you might expect, we can also have the equivalent vector-valued functions:\n\nvector-valued functions of a scalar variable: \\(\\mathbf{y} = f(x)\\),\nvector-valued functions of a vector variable: \\(\\mathbf{y} = f(\\mathbf{x})\\).\n\nThe most relevant of these two for machine learning purposes is the vector-valued function of a vector variable \\(\\mathbf{y} = f(\\mathbf{x})\\). These functions are just extensions of the scalar-valued vector variable functions \\(y = f(\\mathbf{x})\\) we’ve been working with so far, except now we can have \\(m\\) scalar-valued functions \\(y_i = f_i(\\mathbf{x})\\), which when put together make up a vector output\n\\[\\mathbf{y} = (y_0,y_1,\\cdots,y_{m-1}) = (f_0(\\mathbf{x}),f_1(\\mathbf{x}),\\cdots,f_{m-1}(\\mathbf{x})).\\]\nTo define the gradient of a vector-valued function, we just take the gradient of each output element \\(y_i=f_i(\\mathbf{x})\\). Doing this over all \\(m\\) output elements will give \\(m\\) gradients each of size \\(n\\),\n\\[\\frac{dy_0}{d\\mathbf{x}}, \\ \\frac{dy_1}{d\\mathbf{x}}, \\ \\cdots, \\ \\frac{dy_{m-1}}{d\\mathbf{x}}.\\]\nBy treating all these gradients as row vectors, we can assemble them into a single \\(m \\times n\\) matrix to get the derivative of the vector-valued function \\(\\mathbf{y} = f(\\mathbf{x})\\). This matrix is usually called the Jacobian matrix, sometimes denoted in short-hand by the symbol \\(\\mathbf{J}\\). It’s defined as the \\(m \\times n\\) of all possible first partial derivatives,\n\\[\n\\mathbf{J} = \\frac{d\\mathbf{y}}{d\\mathbf{x}} =\n\\begin{pmatrix}\n\\frac{\\partial y_0}{\\partial x_0} & \\frac{\\partial y_0}{\\partial x_1} & \\cdots & \\frac{\\partial y_0}{\\partial x_{n-1}} \\\\\n\\frac{\\partial y_1}{\\partial x_0} & \\frac{\\partial y_1}{\\partial x_1} & \\cdots & \\frac{\\partial y_1}{\\partial x_{n-1}} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial y_{m-1}}{\\partial x_0} & \\frac{\\partial y_{m-1}}{\\partial x_1} & \\cdots & \\frac{\\partial y_{m-1}}{\\partial x_{n-1}}\n\\end{pmatrix}.\n\\]\nTo see an example of a vector-valued function, consider the function \\(\\mathbf{y} = f(\\mathbf{x})\\) given by\n\\[\n\\mathbf{y} =\n\\begin{pmatrix}\ny_0 \\\\\ny_1\n\\end{pmatrix} =\n\\begin{pmatrix}\nx_0^3 + x_1^2 \\\\\n2 x_0 - x_1^4 \\\\\n\\end{pmatrix}.\n\\]\nThis is really two functions \\(y_0 = x_0^3 + x_1^2\\) and \\(y_1 = 2 x_0 - x_1^4\\). Here’s what its Jacobian would look like,\n\\[\n\\mathbf{J} =\n\\begin{pmatrix}\n\\frac{\\partial y_0}{\\partial x_0} & \\frac{\\partial y_0}{\\partial x_1} \\\\\n\\frac{\\partial y_1}{\\partial x_0} & \\frac{\\partial y_1}{\\partial x_1}\n\\end{pmatrix} =\n\\begin{pmatrix}\n3 x_0^2 & 2 x_1  \\\\\n2 & -4 x_1^3\n\\end{pmatrix}.\n\\]\nNotice each row of the Jacobian is the gradient of the elements of \\(\\mathbf{y}\\), as you’d expect,\n\\[\\frac{dy_0}{d\\mathbf{x}} = (3x_0^2, 2x_1), \\qquad \\frac{dy_1}{d\\mathbf{x}} = (2, -4x_1^3).\\]\n\n\n7.1.6 Application: The Softmax Function\nA more interesting example of vector-valued functions and Jacobians that’s very relevant to machine learning is the softmax function, defined by\n\\[\n\\mathbf{y} = \\text{softmax}(\\mathbf{x}) =\n\\begin{pmatrix}\ny_0 \\\\\ny_1 \\\\\n\\dots \\\\\ny_{n-1}\n\\end{pmatrix} =\n\\begin{pmatrix}\n\\frac{1}{Z} e^{x_0} \\\\\n\\frac{1}{Z} e^{x_1} \\\\\n\\dots \\\\\n\\frac{1}{Z} e^{x_{n-1}}\n\\end{pmatrix},\n\\]\nwhere \\(Z = \\sum_k e^{x_k}\\) is a normalizing constant, often called the partition function. This function shows up in machine learning as a way to create probabilities out of \\(n\\) categories. It takes inputs \\(x_i\\) of any real value and scales them so that \\(0 \\leq y_i \\leq 1\\) and \\(\\sum_i y_i = 1\\), so that the output is a valid probability vector.\nThe softmax is useful in defining models for multi-class classification problems, since it can be used to classify things into one of \\(n\\) classes. To classify an object as type \\(k\\), choose the index \\(k\\) such that \\(y_k\\) is the largest probability in the probability vector \\(\\mathbf{y}\\). More on this in future lessons.\nHere’s an example illustrating what the softmax function does. I’ll define a vector \\(x\\) of size \\(n=5\\) by randomly sampling from the interval \\([-1,1]\\). I’ll use a quick lambda function to implement the softmax. Observe what the softmax seems to do is take the elements of \\(x\\) and re-scale them so they’re all in the interval \\([0,1]\\). The outputs also indeed sum to one by construction.\n\n\nCode\nx = np.random.randn(5)\nprint(f'x = {x.round(3)}')\n\nsoftmax = lambda x: np.exp(-x) / np.sum(np.exp(-x))\ny = softmax(x)\nprint(f'y = {y.round(3)}')\nprint(f'sum(y) = {y.sum().round(10)}')\n\n\nx = [-0.542 -0.126 -0.854  1.209  0.322]\ny = [0.276 0.182 0.377 0.048 0.116]\nsum(y) = 1.0\n\n\nNote you would not want to implement the softmax this way at scale due to numerical instability. We’ll get back to this stuff in much more depth in a later lesson.\nSince we’ll need it later on anyway, let’s go ahead and calculate the Jacobian of the softmax function. Let’s work term by term, focusing on the \\(j\\)th partial derivative of \\(y_i=\\frac{1}{Z} e^{x_i}\\). First, notice that the derivative of the partition function is\n\\[\\frac{\\partial Z}{\\partial x_j} = \\frac{\\partial}{\\partial x_j} \\sum_k e^{x_k} = \\sum_k \\frac{\\partial}{\\partial x_j} e^{x_k} = e^{x_j}\\]\nsince the only term in the sum containing \\(x_j\\) is \\(e^{x_j}\\). Using this along with the quotient rule, we thus have\n\\[\nJ_{i,j} = \\frac{\\partial y_i}{\\partial x_j} = \\frac{\\partial}{\\partial x_j} \\frac{e^{x_i}}{Z} =\n\\frac{1}{Z^2}\\bigg(Z \\frac{\\partial e^{x_i}}{\\partial x_j} - e^{x_i} \\frac{\\partial Z}{\\partial x_j} \\bigg) =\n\\begin{cases}\n\\frac{e^{x_i}}{Z} \\big(1 - \\frac{e^{x_i}}{Z}\\big), & i = j \\\\\n-\\frac{e^{x_i}}{Z} \\frac{e^{x_j}}{Z}, & i \\neq j\n\\end{cases} \\ = \\\n\\begin{cases}\ny_i (1 - y_i), & i = k \\\\\n-y_i y_j & i, \\neq k.\n\\end{cases}\n\\]\nPutting all this into the Jacobian matrix, the \\(i=j\\) terms go in the diagonal, and the \\(i \\neq j\\) terms go in the off-diagonals, hence\n\\[\n\\mathbf{J} =\n\\begin{pmatrix}\ny_0 (1 - y_0) & -y_0 y_1 & \\cdots & -y_0 y_{n-1} \\\\\n-y_1 y_0 & y_1 (1 - y_1) & \\cdots & -y_1 y_{n-1} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n-y_{n-1} y_0 & -y_{n-1} y_1 & \\cdots & y_{n-1} (1 - y_{n-1})\n\\end{pmatrix}\n\\]\nIf you play with this expression a little bit, you’ll see we can write this softmax Jacobian efficiently as \\(\\mathbf{J} = \\text{diag}(\\mathbf{y}) - \\mathbf{y} \\mathbf{y}^\\top\\).\nHere’s the Jacobian for the above example where \\(n=5\\), which I’ll call grad. It’s common in machine learning to blur the distinction between Jacobians and gradients and just call everything a gradient. Notice grad is a \\(5 \\times 5\\) matrix.\n\n\nCode\ngrad = np.diag(y) - y @ y.T\nprint(f'grad = \\n{grad}')\n\n\ngrad = \n[[ 0.00841128 -0.26767389 -0.26767389 -0.26767389 -0.26767389]\n [-0.26767389 -0.08553925 -0.26767389 -0.26767389 -0.26767389]\n [-0.26767389 -0.26767389  0.10971353 -0.26767389 -0.26767389]\n [-0.26767389 -0.26767389 -0.26767389 -0.21971262 -0.26767389]\n [-0.26767389 -0.26767389 -0.26767389 -0.26767389 -0.15124236]]\n\n\nAside: We’ve talked about functions with scalar and vector inputs or outputs. What about functions with matrix or tensor inputs or outputs? We could just as well define scalar-valued functions of a matrix variable \\(y = f(\\mathbf{X})\\), matrix-valued function of a matrix variable \\(\\mathbf{Y} = f(\\mathbf{X})\\), etc. The derivative rules extend into these cases as well, but things get a lot more complicated. Taking any kind of derivative of these kinds of functions can cause the rank of the derivative to blow up. For example, the derivative of a (rank-2) matrix with respect to another (rank-2) matrix is now a rank-4 tensor. For at least partly this reason, derivatives of such functions are less commonly used. See this comprehensive paper if you’re interested in how to take derivatives of matrices.\n\n\n7.1.7 Gradient and Jacobian Rules\nHere are a few common gradient and Jacobian rules. I’ll state them assuming a vector-valued function with a vector input unless the scalar-valued form looks different, in which case I’ll state it explicitly. Don’t worry too much about how to derive these. Don’t even try to memorize them. This is just a reference.\n\n\n\n\n\n\n\n\nName\nGradient or Jacobian\nScalar Equivalent\n\n\nConstant Rule\n\\(\\frac{d}{d\\mathbf{x}} (c\\mathbf{y}) = c\\frac{d\\mathbf{y}}{d\\mathbf{x}}\\)\n\\(\\frac{d}{dx} (cy) = c\\frac{dy}{dx}\\)\n\n\nAddition Rule\n\\(\\frac{d}{d\\mathbf{x}}(\\mathbf{u} + \\mathbf{v}) = \\frac{d\\mathbf{u}}{d\\mathbf{x}} + \\frac{d\\mathbf{v}}{d\\mathbf{x}}\\)\n\\(\\frac{d}{dx}(u + v) = \\frac{du}{dx} + \\frac{dv}{dx}\\)\n\n\nProduct Rule (scalar-valued)\n\\(\\frac{d}{d\\mathbf{x}}(uv) = u\\frac{dv}{d\\mathbf{x}} + v\\frac{du}{d\\mathbf{x}}\\)\n\\(\\frac{d}{dx}(uv) = u\\frac{dv}{dx} + v\\frac{du}{dx}\\)\n\n\nProduct Rule (dot products)\n\\(\\frac{d}{d\\mathbf{x}}(\\mathbf{u}^\\top \\mathbf{v}) = \\mathbf{u}^\\top \\frac{d\\mathbf{v}}{d\\mathbf{x}} + \\mathbf{v}^\\top \\frac{d\\mathbf{u}}{d\\mathbf{x}}\\)\n\\(\\frac{d}{dx}(uv) = u\\frac{dv}{dx} + v\\frac{du}{dx}\\)\n\n\nChain Rule (scalar-valued, vector-valued)\n\\(\\frac{dz}{d\\mathbf{x}} = \\big(\\frac{dz}{d\\mathbf{y}}\\big)^\\top \\frac{d\\mathbf{y}}{d\\mathbf{x}}\\)\n\\(\\frac{dz}{dx} = \\frac{dz}{dy} \\frac{dy}{dx}\\)\n\n\nChain Rule (both vector-valued)\n\\(\\frac{d\\mathbf{z}}{d\\mathbf{x}} = \\frac{d\\mathbf{z}}{d\\mathbf{y}} \\frac{d\\mathbf{y}}{d\\mathbf{x}}\\)\n\\(\\frac{dz}{dx} = \\frac{dz}{dy} \\frac{dy}{dx}\\)\n\n\nConstant Function\n\\(\\frac{d}{d\\mathbf{x}} \\mathbf{c} = \\mathbf{0}\\)\n\\(\\frac{d}{dx} c = 0\\)\n\n\nSquared Two-Norm\n\\(\\frac{d}{d\\mathbf{x}} ||\\mathbf{x}||^2 = \\frac{d}{d\\mathbf{x}} \\mathbf{x}^\\top \\mathbf{x} = 2 \\mathbf{x}\\)\n\\(\\frac{d}{dx} x^2 = 2x\\)\n\n\nLinear Combination\n\\(\\frac{d}{d\\mathbf{x}} \\mathbf{c}^\\top \\mathbf{x} = \\mathbf{c}\\)\n\\(\\frac{d}{dx} cx = c\\)\n\n\nSymmetric Quadratic Form\n\\(\\frac{d}{d\\mathbf{x}} \\mathbf{x}^\\top \\mathbf{S} \\mathbf{x} = 2 \\mathbf{S} \\mathbf{x}\\)\n\\(\\frac{d}{dx} sx^2 = 2sx\\)\n\n\nAffine Function\n\\(\\frac{d}{d\\mathbf{x}} (\\mathbf{A}\\mathbf{x} + \\mathbf{b}) = \\mathbf{A}^\\top\\) or \\(\\frac{d}{d\\mathbf{x}} (\\mathbf{x}^\\top \\mathbf{A} + \\mathbf{b}) = \\mathbf{A}\\)\n\\(\\frac{d}{dx} (ax+b) = a\\)\n\n\nSquared Error Function\n\\(\\frac{d}{d\\mathbf{x}} ||\\mathbf{A}\\mathbf{x}-\\mathbf{b}||^2 = 2\\mathbf{A}^\\top (\\mathbf{A}\\mathbf{x}-\\mathbf{b})\\)\n\\(\\frac{d}{dx} (ax-b)^2 = 2a(ax-b)\\)\n\n\nCross Entropy Function\n\\(\\frac{d}{d\\mathbf{x}} (-\\mathbf{c}^\\top \\log \\mathbf{x}) = -\\frac{\\mathbf{c}}{\\mathbf{x}}\\) (element-wise division)\n\\(\\frac{d}{dx} (-c \\log x) = -\\frac{c}{x}\\)\n\n\nReLU Function\n\\(\\frac{d}{d\\mathbf{x}} \\max(\\mathbf{0}, \\mathbf{x}) = \\text{diag}(\\mathbf{x} \\geq \\mathbf{0})\\) (element-wise \\(\\geq\\))\n\\(\\frac{d}{dx} \\max(0, x) = \\text{$1$ if $x \\geq 0$ else $0$}\\)\n\n\nSoftmax Function\n\\(\\frac{d}{d\\mathbf{x}} \\text{softmax}(\\mathbf{x}) = \\text{diag}(\\mathbf{y}) - \\mathbf{y} \\mathbf{y}^\\top\\) where \\(\\mathbf{y} = \\text{softmax}(\\mathbf{x})\\)\n\n\n\n\nYou can calculate gradients and Jacobians in sympy, though in my opinion it can be kind of painful except in the simplest cases. Here’s an example, where I’ll calculate the Jacobian of the squared error function \\(||\\mathbf{A}\\mathbf{x}-\\mathbf{b}||^2\\).\nAside: There is also a nice online tool that lets you do this somewhat more easily.\n\n\nCode\nm = sp.Symbol('m')\nn = sp.Symbol('n')\nA = sp.MatrixSymbol('A', m, n)\nx = sp.MatrixSymbol('x', n, 1)\nb = sp.MatrixSymbol('b', m, 1)\n\ny = (A * x - b).T * (A * x - b)\ndydx = y.diff(x)\nprint(f'y = {y}')\nprint(f'dydx = {dydx}')\n\n\ny = (-b.T + x.T*A.T)*(A*x - b)\ndydx = 2*A.T*(A*x - b)"
  },
  {
    "objectID": "notebooks/multivariate-calculus.html#multivariate-integration",
    "href": "notebooks/multivariate-calculus.html#multivariate-integration",
    "title": "7  Multivariate Calculus",
    "section": "7.2 Multivariate Integration",
    "text": "7.2 Multivariate Integration\n\n7.2.1 Integration in 2 Dimensions\nWe can also integrate multivariate functions like \\(z=f(x,y)\\). Geometrically these integrals translate into calculating the volume under the surface of \\(z=f(x,y)\\). I’ll very briefly touch on this.\nThe idea here is to approximate the volume \\(V\\) under a surface not with \\(N\\) rectangles of width \\(dx\\) and height \\(f(x)\\), but instead with \\(N \\cdot M\\) rectangular prisms of base area \\(dA = dx \\cdot dy\\) and height \\(z=f(x,y)\\),\n\\[V = \\int_R f(x,y) dA = \\sum_{n=0}^{N-1} \\sum_{m=0}^{M-1} f(x_n,y_m) dxdy = f(x_0,y_0) dxdy + f(x_0,y_1) dxdy + \\cdots + f(x_1,y_0) dxdy + \\cdots + f(x_{N-1},y_{M-1}) dxdy.\\]\nRather than integrate from one endpoint \\(a\\) to another endpoint \\(b\\), we now have to integrate over a 2D region in the xy-plane that I’ll call \\(R\\).\nIf \\(R\\) is just a rectangle in the xy-plane, say \\(R = [a,b] \\times [c,d]\\) we can break the integral \\(\\int_R dA\\) into two integrals \\(\\int_a^b dx\\) and \\(\\int_c^d dy\\). If we can also factor \\(f(x,y) = g(x)h(y)\\), we can further break the integral into a product of two univariate integrals,\n\\[\\int_R f(x,y) dA = \\int_a^b \\int_c^d f(x,y) dxdy = \\bigg(\\int_a^b g(x) dx \\bigg) \\bigg( \\int_c^d h(y) dy \\bigg).\\]\nAs an example, suppose we wanted to integrate the function \\(f(x,y) = x^2 \\sqrt{y}\\) over the rectangle \\(R = [0,1] \\times [0,1]\\). This function factors into a product of two functions \\(g(x) = x^2\\) and \\(h(y) = \\sqrt{y}\\). We can thus integrate each individually to get\n\\[\\int_0^1 \\int_0^1 x^2 \\sqrt{y} dxdy = \\bigg(\\int_0^1 x^2 dx\\bigg) \\bigg(\\int_0^1 \\sqrt{y} dy\\bigg) = \\frac{1}{3} x^3 \\bigg |_{x=0}^1 \\cdot \\frac{2}{3} y^{3/2} \\bigg |_{y=0}^1 = \\frac{1}{3} \\cdot \\frac{2}{3} = \\frac{2}{9}.\\]\nIn general \\(R\\) won’t be a rectangle, but some arbitrary shape. And \\(f(x,y)\\) won’t usually factor. When this is the case we usually have to fall back to numerical integration methods.\n\n\n7.2.2 Application: Integrating the Gaussian\nOne of the most important functions in machine learning, if not all of science, is the Gaussian function\n\\[y = e^{-\\frac{1}{2} x^2}.\\]\nThe Gaussian is the function that gives the well-known bell-curve shape.\n\n\nCode\nx = np.arange(-10, 10, 0.1)\nf = lambda x:  np.exp(-1 / 2 * x ** 2)\nplot_function(x, f, xlim=(-3, 3), ylim=(-0, 1.5), title='Gaussian Function')\n\n\n\n\n\n\n\n\n\nIt’s very important in many applications of probability and statistics to be able to integrate the Gaussian function between two points \\(a\\) and \\(b\\),\n\\[\\int_a^b e^{-\\frac{1}{2} x^2} dx.\\]\nUnfortunately, this turns out to be impossible to do analytically, because the Gaussian function has no indefinite integral. No matter how hard you try, you’ll never find an elementary function \\(F(x)\\) whose derivative is \\(f(x)=e^{-\\frac{1}{2} x^2}\\).\nOne special case, however, where we can integrate the Gaussian analytically is when the region is the whole real line,\n\\[\\int_{-\\infty}^\\infty e^{-\\frac{1}{2} x^2} dx.\\]\nIt’s surprising we can even do this. We can do it using a trick. The trick is to square the Gaussian. Consider instead the function\n\\[f(x,y) = e^{-\\frac{1}{2} x^2} e^{-\\frac{1}{2} y^2} = e^{-\\frac{1}{2} (x^2+y^2)}.\\]\nConsider now the bivariate integral\n\\[\\int_{\\mathbb{R}^2} f(x,y) dxdy = \\int_{-\\infty}^\\infty \\int_{-\\infty}^\\infty e^{-\\frac{1}{2} (x^2+y^2)} dxdy.\\]\nThis integral doesn’t on the face of it look any easier, but we can do something with it that we can’t with the univariate integral: change variables. I won’t go into detail here, but if we define 2 new variables \\(r\\) and \\(\\theta\\) (which turn out to be polar coordinates)\n\\[r^2 = x^2+y^2, \\quad \\tan \\theta = -\\frac{y}{x},\\]\nthen \\(dxdy = rdrd\\theta\\), and we can re-write the bivariate integral as\n\\[\\int_{\\mathbb{R}^2} f(x,y) dxdy = \\int_0^\\infty \\int_0^{2\\pi} e^{-\\frac{1}{2}r^2} rdrd\\theta = \\bigg(\\int_0^{2\\pi} d\\theta \\bigg) \\bigg( \\int_0^\\infty re^{-\\frac{1}{2}r^2} dr \\bigg).\\]\nThis is just a product of two univariate integrals that we can evaluate. The first integral is easy,\n\\[\\int_0^{2\\pi} d\\theta = \\theta \\bigg |_{\\theta=0}^{2\\pi} = 2\\pi.\\]\nThe second integral is a little harder, but we can solve it by using another change of variables \\(u=r^2\\), so \\(du = 2rdr\\), to get\n\\[\\int_0^\\infty re^{-\\frac{1}{2}r^2} dr = \\int_0^\\infty e^{-\\frac{1}{2}u} \\bigg(\\frac{1}{2} du\\bigg) = \\frac{1}{2} \\int_0^\\infty e^{-\\frac{1}{2}u} du = -e^{-\\frac{1}{2}u} \\bigg |_{u=0}^\\infty = -(e^{-\\infty} - 1) = 1,\\]\nsince \\(e^{-\\infty} = \\frac{1}{e^{\\infty}} = \\frac{1}{\\infty} = 0\\). Putting these together, the bivariate integral is thus\n\\[\\int_{-\\infty}^\\infty \\int_{-\\infty}^\\infty e^{-\\frac{1}{2} (x^2+y^2)} dxdy = 2\\pi.\\]\nSince \\(e^{-\\frac{1}{2} (x^2+y^2)} = e^{-\\frac{1}{2} x^2} e^{-\\frac{1}{2} y^2}\\), we can factor this integral into a product to get\n\\[2\\pi = \\int_{-\\infty}^\\infty \\int_{-\\infty}^\\infty e^{-\\frac{1}{2} x^2} dxdy = \\bigg(\\int_{-\\infty}^\\infty e^{-\\frac{1}{2} x^2} dx \\bigg) \\bigg(\\int_{-\\infty}^\\infty e^{-\\frac{1}{2} y^2} dy\\bigg).\\]\nBoth of the integrals on the right are the same, so they must equal the same number, call it \\(A\\). We thus have an equation \\(A^2 = 2\\pi\\), which we can solve to get the area under each integral, which is \\(A=\\sqrt{2\\pi}\\). Thus, we’ve arrived at the final result for the univariate integral of the Gaussian,\n\\[\\int_{-\\infty}^\\infty e^{-\\frac{1}{2} x^2} dx = \\sqrt{2\\pi} \\approx 2.507.\\]\nAny time from now on you see the factors of \\(\\sqrt{2\\pi}\\) in a Gaussian function, this is where they come from.\nIt’s interesting that we can integrate a function all the way from \\(-\\infty\\) to \\(\\infty\\) and still get a finite number. This is because Gaussian functions rapidly decay, so most of their area ends up being around \\(x=0\\). In fact, the interval \\([-3,3]\\) alone contains 99.7% of the area of the under the bell curve!\nHere’s the same integral verified using sympy. Note the unusual notation sympy uses for \\(\\infty\\), which is sp.oo.\n\n\nCode\nx = sp.Symbol('x') \ny = sp.exp(-sp.Rational(1, 2) * x ** 2)\nintegral = y.integrate((x, -sp.oo, sp.oo))\nprint(f'y = {y}')\nprint(f'integral = {integral}')\n\n\ny = exp(-x**2/2)\nintegral = sqrt(2)*sqrt(pi)\n\n\n\n\n7.2.3 Integration in \\(n\\) Dimensions\nThe same idea extends to \\(n\\) dimensional functions \\(y=f(x_0,\\cdots,x_{n-1})\\). In this case we’re calculating the \\(n+1\\) dimensional hypervolume \\(V_{n+1}\\) under the \\(n\\) dimensional manifold \\(y=f(x_0,\\cdots,x_{n-1})\\). The hyperrectangles would now have base hyperarea \\(dA_n = dx_0dx_1\\cdots dx_{n-1}\\) and height \\(y\\), so\n\\[V_{n+1} = \\int_{R_n} f(x_0,\\cdots,x_{n-1}) dA_n = \\sum_{\\text{all hyperrectangles}} f(x_0,\\cdots,x_{n-1}) dx_0dx_1\\cdots dx_{n-1}.\\]\nIf we’re sufficiently lucky, we can factor a multivariate integral into a product of univariate integrals. We can do this as long as\n\nthe multivariate function \\(f(x_0,\\cdots,x_{n-1})\\) factors into a product of univariate functions \\[f(x_0,x_1,\\cdots,x_{n-1}) = f_0(x_0) f_1(x_1) \\cdots f_{n-1}(x_{n-1}),\\]\nthe integration region \\(R_n\\) is a product of rectangles, \\[R_n = [a_0,b_0] \\times [a_1,b_1] \\times \\cdots \\times [a_{n-1},b_{n-1}].\\]\n\nWhen this is the case, we can simplify the integral to\n\\[\\int_{R_n} f(x_0,x_1,\\cdots,x_{n-1}) dA_n = \\bigg(\\int_{a_0}^{b_0} f_0(x_0) dx_0\\bigg) \\bigg(\\int_{a_1}^{b_1} f_1(x_1) dx_1\\bigg) \\cdots \\bigg(\\int_{a_{n-1}}^{b_{n-1}} f_{n-1}(x_{n-1}) dx_{n-1}\\bigg).\\]\nWe can then evaluate each univariate integral one-by-one and put the results together to get the full multivariate integral.\nAs a quick example, suppose we wanted to integrate the following multivariate Gaussian function over all space,\n\\[f(x_0,x_1,\\cdots,x_{n-1}) = \\exp\\bigg(-\\frac{1}{2}||\\mathbf{x}||^2\\bigg) = \\exp\\bigg(-\\frac{1}{2}\\sum_{i=0}^{n-1}x_i^2\\bigg) = \\prod_{i=0}^{n-1} \\exp\\bigg(-\\frac{1}{2}x_i^2\\bigg).\\]\nSince each product on the right is independent, the integral splits up into a product itself, so we have\n\\[\\int_{\\mathbb{R}^n} f(x_0,x_1,\\cdots,x_{n-1}) dx_0dx_1\\cdots dx_{n-1} = \\prod_{i=0}^{n-1} \\int_{-\\infty}^\\infty \\exp\\bigg(-\\frac{1}{2}x_i^2\\bigg) dx_i = \\big(\\sqrt{2\\pi}\\big)^n = (2\\pi)^{n/2}.\\]\nIf you don’t understand what’s going on here, that’s fine. When you see multivariate integrals come up in future lessons, just think of them as a way to calculate the volumes under surfaces. That’s the most important thing to take away."
  },
  {
    "objectID": "notebooks/probability.html#randomness",
    "href": "notebooks/probability.html#randomness",
    "title": "8  Basic Probability",
    "section": "8.1 Randomness",
    "text": "8.1 Randomness\nProbability is a calculus for modeling random processes. There are things we just can’t predict with certainty given the information we have available. Stuff that we can’t predict with certainty we call random, or noise, or non-deterministic. Stuff we can predict with certainty we call deterministic or certain. Here are some examples of these two kinds of processes. The questions in the deterministic column have exact answers, while those in the random column do not.\n\n\n\n\n\n\n\nDeterministic Process\nRandom Process\n\n\n\n\nDoes \\(2+2=4\\)?\nWill it rain today?\n\n\nWhat is the capital of France?\nWhat is the result of rolling a pair of dice?\n\n\nHow many sides does a square have?\nWhat is the next card in a shuffled deck?\n\n\nWhat is the value of pi?\nWhat is the stock price of Apple tomorrow?\n\n\nWhat is the boiling point of water at sea level?\nWhat is the winning number for next week’s lottery?\n\n\n\nDeterministic processes aren’t terribly interesting. They either will occur with certainty, or they won’t. Random processes might occur. To quantify what we mean by might we’ll introduce the notion of probability. You can think of probability as a function mapping questions like “Will it rain today?” to a number between \\(0\\) and \\(1\\) that indicates our “degree of belief” in whether that question is true,\n\\[0 \\leq \\mathbb{Pr}(\\text{Will it rain today?}) \\leq 1.\\]\nThe question inside this probability function is called an event. An event is anything that might occur. Mathematically speaking, an event is a set that lives in some abstract sample space of all possible outcomes.\nWhen we’re certain an event will occur we say it has probability one, or a 100% chance of happening. When we’re certain an event will not occur we say it has probability zero, or a 0% chance of happening. These extremes are deterministic processes. Random processes are anything in between. For the question “Will it rain today?”, we might say there is a 20% chance of rain, in which case we believe \\(\\mathbb{Pr}(\\text{Will it rain today?}) = 0.2\\).\nA common theme we’ll see in machine learning is that we’re interested in mapping arbitrary data structures like strings to numerical data structures that we can do mathematical calculations with, like floats or arrays. In this particular example, it’s convenient to map the question “Will it rain today?” to a binary variable I’ll call \\(x\\), \\[\nx =\n\\begin{cases}\n1, & \\text{It will rain today} \\\\\n0, & \\text{It will not rain today}.\n\\end{cases}\n\\]\nThen asking for \\(\\mathbb{Pr}(\\text{Will it rain today?})\\) is the same thing as asking “what is the probability that \\(x=1\\)”, or equivalently, what is \\(\\mathbb{Pr}(x=1)\\)? Saying we believe there’s a 20% chance of rain today is equivalent to saying we believe there is a 20% chance that \\(x=1\\), i.e. \\(\\mathbb{Pr}(x=1)=0.2\\).\nVariables like \\(x\\) are called random variables. They’re a way of encoding random events numerically via some kind of encoding convention like I just used. It’s much more convenient to work with random variables than events or questions since we can now use all our usual mathematical tools like calculus and linear algebra to understand random processes.\nTo understand how random variables work, it’s often helpful to think of them as the outputs of random number generators. These are algorithms that generate, or sample, random numbers from some given distribution. Unlike regular functions, where a given input will always produce a definite output, a random number generator can (and usually will) produce different outputs every single time the same input is passed in.\nThe canonical example of a random number generator is called rand. It’s an algorithm for uniformly generating (pseudo) random real numbers \\(0 \\leq x \\leq 1\\). Every time we call rand we’ll get a different number with no clear pattern.\nHere’s an example. I’ll call rand via the numpy function np.random.rand a bunch of times and print the first 10 outputs. Notice how all over the place they seem to be. The only thing we know is they’re between zero and one.\n\n\nCode\nx = np.random.rand(100)\nx[:12]\n\n\narray([0.5488135 , 0.71518937, 0.60276338, 0.54488318, 0.4236548 ,\n       0.64589411, 0.43758721, 0.891773  , 0.96366276, 0.38344152,\n       0.79172504, 0.52889492])\n\n\nThink of a random variable informally as being some variable \\(x\\) whose values are determined by a function \\(x=f(n)\\), except the function can’t make up its mind or follow a pattern. On one sampling we might get \\(x=f(0)=0.548\\). Next, \\(x=f(1)=0.715\\). Next, \\(x=f(2)=0.603\\). Etc. We can’t force \\(x\\) to take on a definite value. It jumps around with no clear pattern.\n\n\nCode\nplt.scatter(range(len(x)), x)\nplt.xlabel('n')\nplt.ylabel('x')\nplt.title('$x = f(n)$')\nplt.show();\n\n\n\n\n\nSince random variable outputs jump around like this we need a different way to visualize them than just thinking of them as points on the number line. The most useful way to visualize random variables is using a histogram. To create a histogram, we sample a random variable a whole bunch of times, and plot a count of how many times the variable takes on each given value. We then show these counts in a bar chart with the heights indicating the counts for each value.\nIn matplotlib we can plot histograms of an array of samples x using the function plt.hist(x). Here’s an example. I’ll sample 100 values from rand and put them in an array x, then plot the histogram.\n\n\nCode\nx = np.random.rand(100)\nplt.hist(x)\nplt.show();\n\n\n\n\n\nNotice that we just sampled \\(100\\) different values, but we don’t see \\(100\\) different bars. That’s because histograms don’t plot bars for all values. First, the values get binned into some number of equally spaced subintervals, called bins, then the counts that get plotted are the counts of values inside each bin. In this case, the histogram divides the samples into \\(10\\) equally spaced bins. If you look carefully you should see \\(10\\) bars in the plot. We can change the number of bins by passing in a keyword bins specifying how many bints to take.\nSince I’ll be using histograms a lot in this lesson I’m going to write a helper function plot_histogram to bundle up the code to plot them nicely. Instead of using plt.hist, however, I’ll use the seaborn library’s sns.histplot, which creates much nicer looking histograms. Seaborn is an extension library of matplotlib made specifically for making nicer plots of data. Ignore the is_discrete argument for now. I’ll use it in the next section.\n\n\nCode\ndef plot_histogram(x, is_discrete=False, title='', **kwargs):\n    if is_discrete:\n        sns.histplot(x, discrete=True, shrink=0.8, **kwargs)\n        unique = np.unique(x)\n        if len(unique) < 15:\n            plt.xticks(unique)\n    else:\n        sns.histplot(x, **kwargs)\n    plt.title(title)\n    plt.show()\n\n\nIt’s still kind of hard to see if the \\(100\\) rand samples have any kind of pattern in the above histogram plot. Let’s now sample 10,000 numbers from rand and see if we can find one.\n\n\nCode\nx = np.random.rand(10000)\nplot_histogram(x, bins=10, title=f'rand({10000})')\n\n\n\n\n\nIt should be increasingly clear now that what’s going on is that rand is sampling numbers between 0 and 1 with equal probability. Each bin should contain roughly \\(\\frac{10000}{10}=1000\\) counts, since there are \\(10000\\) samples and \\(10\\) bins. Said differently, the values in each bin should have a \\(\\frac{1}{10}=0.1\\) probability of being sampled. For example, the values in the left-most bin, call it \\(I_0 = [0, 0.1]\\) should have\n\\[\\mathbb{Pr}(x \\in I_0) = \\mathbb{Pr}(0 \\leq x \\leq 0.1) = 0.1.\\]\nThis type of “flat”, equal probability sampling is called uniform random sampling.\nYou may be questioning that it’s indeed the case that each bin is truly getting sampled as much as the other bins. After all, the plot still clearly shows their heights vary a bit. Some bins have slightly more values than others do. We can look at how many counts are in the bin using np.histogram, which also defaults to \\(10\\) bins. You can see some bins have as many as \\(1037\\) values, some as few as \\(960\\) values.\n\n\nCode\nbin_counts, _ = np.histogram(x)\nbin_counts\n\n\narray([1025, 1036,  999,  981, 1037,  989,  956,  996,  976, 1005])\n\n\n\n8.1.0.1 Aside: Estimating the Fluctuation in Bin Counts\nThis variation in the bin counts is really due to the fact that we’re only sampling a finite number of values. To get true uniform sampling, where all bins have the same counts, we’d have to sample an infinitely large number of times.\nHere’s a rule of thumb for how much the bin counts should be expected to fluctuate as a function of the sample size. If \\(N\\) is the number of samples, and each bin \\(k\\) contains \\(N_k\\) counts (i.e. its bar height is \\(N_k\\)), then you can expect the counts to fluctuate above and below \\(N_k\\) by about\n\\[\\sigma_k = \\sqrt{N_k\\bigg(1 - \\frac{N_k}{N}\\bigg)}.\\]\nSaid differently, the counts should be expected to roughly lie in a range \\(N_k \\pm \\sigma_k\\). This notation means the same thing as saying the counts should roughly speaking lie in the range \\([N_k - \\sigma_k, N_k + \\sigma_k]\\). By “roughly”, I mean sometimes bins can have counts outside this range, but it’s uncommon.\nIn the above example, there are \\(N=10000\\) samples, and each bin has about \\(N_k=1000\\) counts, so you should expect the counts to fluctuate by about\n\\[\\sigma_k = \\sqrt{1000\\bigg(1 - \\frac{1000}{10000}\\bigg)} = 30,\\]\nwhich means the counts should rougly lie in the range \\(1000 \\pm 30\\). This seems to be in line with what we’re seeing experimentally. Notice as the sample size \\(N \\rightarrow \\infty\\), the fluctuations \\(\\sigma_k \\rightarrow 0\\). We’ll see where this rule comes from later (hint: the binomial distribution).\nBack to random variables. Broadly speaking we can divide random variables into two classes of distributions:\n\ndiscrete distributions: random variables that can only take on a discrete set of values.\ncontinuous distributions: random variables that can take on any continuum of real values.\n\nI’ll start by talking about the discrete case since it’s easier to understand."
  },
  {
    "objectID": "notebooks/probability.html#discrete-probability",
    "href": "notebooks/probability.html#discrete-probability",
    "title": "8  Basic Probability",
    "section": "8.2 Discrete Probability",
    "text": "8.2 Discrete Probability\nDiscrete random variables are variables that can only take on a discrete range of values. Usually this range is a finite set like \\(\\{0,1\\}\\) or \\(\\{1,2,3,4,5,6\\}\\) or something like that. But they could have an infinite range too, for example the set \\(\\mathbb{N}\\) of all non-negative integers. Rand is not an example of a discrete random variable, since there the range is all of the interval \\([0,1]\\).\nHere are some examples of real life things that can be modeled by a discrete random variable:\n\nModeling the rolls of a die with faces \\(1,2,3,4,5,6\\).\nModeling values from flipping a coin taking on a value of heads or tails.\nModeling a hand of poker, where there are 5 cards each drawn from the same deck of 52 cards.\nModeling the outputs of data used to train a machine learning classification model.\nModeling the number of heads gotten from flipping a coin a whole bunch of times.\nModeling the number of people entering a building per hour.\n\n\n8.2.1 Motivation: Rolling a Die\nConsider a very simple toy problem: rolling a die (singular of dice). If you’ve never seen dice before, they’re white cubes with black dots on each face of the cube. Each face gets some number of black dots on it between 1 and 6. People like to “roll” these dice in games by shaking and tossing them onto the ground. The person with the highest score, i.e. the most number of dots facing upward, wins that round.\n\n\n🎲\n\n\nLet’s think a little bit about a single die. Suppose I want to roll a single die. Having not rolled the die yet, what should I “expect” the value to be when I roll the die? Call this score \\(x\\). The possible values I can have are just the number of dots on each face of the die, i.e. \\(1,2,3,4,5,6\\). This alone doesn’t tell me what the chance is that any given \\(x\\) turns up in a roll. We need some other information.\nPerhaps your common sense kicks in and you think, “Well clearly each number has an equal chance of showing up if you roll the die”. This is called the principle of indifference. In practice you’d usually be right. You’re saying that, since we don’t have any other information to go on, each number should have an equal chance of showing up on each roll. That is, on any given roll, the random variable \\(x\\) should take on each value \\(k=1,2,\\cdots,6\\) with probability,\n\\[p_k = \\mathbb{Pr}(x=k) = \\frac{1}{6}.\\]\nThis just says that the probability of rolling \\(x=1\\) is \\(p_1 = \\frac{1}{6}\\), the probability of rolling \\(x=2\\) is also \\(p_2 = \\frac{1}{6}\\), etc. Notice that these probabilities satisfy two properties that all probabilities must satisfy: 1. Each probability is non-negative: \\(p_k = \\frac{1}{6} \\geq 0\\), 2. The sum of all the possible probabilities is one: \\(\\sum_{k=1}^6 p_k = p_1 + p_2 + p_3 + p_4 + p_5 + p_6 = 6 \\cdot \\frac{1}{6} = 1\\).\nThese two properties are the defining characteristics of a probability. The second condition is just a mathematical way of saying that rolling the die must return some value \\(x \\in \\{1,2,3,4,5,6\\}\\). It can’t just make up some new value, or refuse to answer.\nAnyway, suppose I rolled the die \\(N=36\\) times and got the following values:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRoll\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n\n\n\n\nValue\n3\n4\n5\n4\n3\n1\n3\n6\n5\n2\n1\n5\n4\n2\n1\n1\n1\n6\n5\n6\n3\n5\n5\n3\n3\n6\n6\n1\n5\n4\n2\n2\n4\n6\n2\n4\n\n\n\nWe can make a histogram out of these and check the principle of indifference by verifying the bins are all of about the same height (at least as close to the same as only 30 rolls will allow). Note that I’m now using is_discrete=True here, which tells the helper function to give each unique \\(k\\) its own bin.\n\n\nCode\nx = [3, 4, 5, 4, 3, 1, 3, 6, 5, 2, 1, 5, 4, 2, 1, 1, 1, 6, 5, 6, 3, 5, 5, 3, 3, 6, 6, 1, 5, 4, 2, 2, 4, 6, 2, 4]\nplot_histogram(x, is_discrete=True, title='36 Die Rolls')\n\n\n\n\n\nGiven the fact that I only rolled \\(36\\) times, this histogram looks very uniform, giving a pretty strong hint that each value has an equal probability of being rolled. Since most bars have height \\(6\\), they correspond to probabilities of \\(\\frac{6}{36}=\\frac{1}{6}\\), which is what our common sense expected. Note the counts can fluctuate in this case in a range of about \\(6 \\pm 2\\). This is an example of a fair die.\nWhat if our common sense was incorrect? What if I rolled the die a bunch of times and found out some numbers occurred a lot more often than others? This would happen if the die were weighted unevenly, or loaded. In this case we’re left to assign some weight \\(N\\) to each number \\(k\\).\nTo determine what the right weights should be empirically, probably the easiest way would again be to roll the die a bunch of times and count how many times each value \\(k\\) occurs. Those counts will be your weights \\(N_k\\). These are just the heights of each bin in the histogram. To turn them into probabilities \\(p_k\\), divide by the total number of rolls, call it \\(N\\). The probabilities would then be given approximately by\n\\[p_k = \\mathbb{Pr}(x=k) \\approx \\frac{N_k}{N}.\\]\nThat is, the probability \\(p_k\\) is just a ratio of counts, the fraction of times \\(x=k\\) occurred in \\(N\\) counts. As \\(N \\rightarrow \\infty\\) this equality goes from approximate to exact. In fact, we could define the probability \\(p_k = \\mathbb{Pr}(x=k)\\) as the limit\n\\[p_k = \\mathbb{Pr}(x=k) = \\lim_{N \\rightarrow \\infty} \\frac{N_k}{N}.\\]\nThis is an alternate way of defining a probability, different from the “degree of belief” approach I used above. This is usually called the frequentist or objective approach. In this approach, probability is the frequency of the number of times an outcome occurs in an experiment, i.e. \\(\\frac{N_k}{N}\\). In contrast, the “degree of belief” perspective is called the Bayesian or subjective approach. Both approaches have their uses, so we’ll go back and forth between the two as it suits us.\nTo test if your die is loaded, what you can do is roll the die \\(N\\) trials and calculate the probabilities. If they’re all roughly equal to \\(1/6\\) like the example above then the die is fair. Otherwise it’s loaded. Suppose when I’d rolled the die I’d instead gotten the following outcomes:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRoll\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n\n\n\n\nValue\n4\n4\n5\n4\n3\n5\n3\n6\n5\n6\n1\n5\n4\n5\n6\n5\n1\n6\n5\n6\n3\n5\n5\n4\n3\n6\n6\n4\n5\n4\n2\n5\n4\n6\n2\n4\n\n\n\nLet’s plot the histogram of these outcomes and compare to the fair die case.\n\n\nCode\nx = [4, 4, 5, 4, 3, 5, 3, 6, 5, 6, 1, 5, 4, 5, 6, 5, 1, 6, 5, 6, 3, 5, 5, 4, 3, 6, 6, 4, 5, 4, 2, 5, 4, 6, 2, 4]\nplot_histogram(x, is_discrete=True, title='36 Die Rolls (Round 2)')\n\n\n\n\n\nNotice how now the outcomes are skewed towards higher values. This clearly doesn’t look uniform anymore since most of the counts aren’t in the expected range of \\(6 \\pm 2\\). The die has been “loaded to roll high”.\nUsing the experimental approach we can estimate what the probability of rolling each value is. To do that, we can just take each value \\(k\\) and sum up the number of times \\(x=k\\) and divide it by the total counts \\(N\\). This will return an array of probabilities, where each index \\(k\\) contains the entry \\(p_{k+1} = \\frac{N_k}{N}\\).\n\n\nCode\nsupport = np.unique(x)\nN = len(x)\nNk = [sum([x == k]) for k in support]\np = Nk / N\n[f\"Pr(x={i+1}) = {round(p[i], 3)}\" for i in range(len(p))]\n\n\n['Pr(x=1) = 0.056',\n 'Pr(x=2) = 0.056',\n 'Pr(x=3) = 0.111',\n 'Pr(x=4) = 0.25',\n 'Pr(x=5) = 0.306',\n 'Pr(x=6) = 0.222']\n\n\n\n\n8.2.2 General Case\nOf course, there’s nothing special about a die. We can define probabilities in exactly the same way for any discrete random variable. A random variable \\(x\\) is called discrete if it can take on one of \\(n\\) countable values \\(x_0,x_1,\\cdots,x_{n-1}\\). Suppose we run an experiment \\(n\\) times and observe the outcomes of \\(x\\) at each trial. If \\(x=x_k\\) for some number of counts \\(n_j\\), then the probability \\(x=x_k\\) is given by the limit of running the experiment infinitely many times,\n\\[p_k = \\mathbb{Pr}(x=k) = \\lim_{N \\rightarrow \\infty} \\frac{N_k}{N}.\\]\nThe set of values that \\(x\\) can take on are called the support of the random variable. For values outside the support, it’s assumed the probability is zero. As will always be true with probabilities, it’s still the case that each probability must be non-negative, and they must all sum to one,\n\\[p_k \\geq 0, \\quad \\sum_{k=0}^{n-1} p_k = 1.\\]\nWhile we have an experimental way to calculate probabilities now, it would be useful to define probabilities as functions of random variables so we can study them mathematically. These functions are called probability distributions. Suppose the probabilities \\(p_k\\) are given by some function \\(p(x)\\) mapping outcomes to probabilities. When this is true, we say \\(x\\) is distributed as \\(p(x)\\), written in short-hand as \\(x \\sim p(x)\\). If \\(x\\) is discrete, we call the function \\(p(x)\\) a probability mass function, or PMF for short.\nIn the simple case of the fair die, since each \\(p_k = \\frac{1}{6}\\), its PMF is just the simple constant function \\(p(x) = \\frac{1}{6}\\). This distribution is an example of the discrete uniform distribution. If \\(x\\) is a discrete random variable taking on one of \\(k\\) outcomes, and \\(x\\) is distributed as discrete uniform, then its probabilities are given by \\(p_k = \\frac{1}{n}\\) for all \\(k\\). In histogram language, all bins have approximately the same number of counts.\nIn the less simple case of the loaded die we had to estimate each probability empirically. Supposing we could calculate those probabilities exactly, the PMF for that particular loaded die would look like\n\\[\np(x) =\n\\begin{cases}\n0.056, & x = 1, \\\\\n0.056, & x = 2, \\\\\n0.111, & x = 3, \\\\\n0.250, & x = 4, \\\\\n0.306, & x = 5, \\\\\n0.220, & x = 6.\n\\end{cases}\n\\]\nThis is an example of a categorical distribution. Their histograms can look completely arbitrary. Each bin can contain as many counts as it likes. All that matters is that \\(k\\) is finite and all the probabilities sum to one. Any time you take a discrete uniform random variable and weigh the outcomes (e.g. by loading a die) you’ll create a categorical distribution.\nTypically each distribution will have one or more parameters \\(\\theta\\) that can be adjusted to change the shape or support of the distribution. Instead of writing \\(p(x)\\) for the PMF, when we want to be explicit about the parameters we’ll sometimes write \\(p(x; \\theta)\\). The semi-colon is used to say that any arguments listed after it are understood to be parameters, not function inputs. In this notation, parameters of a distribution are assumed to be known, non-random values. We’ll relax this requirement below, but assume parameters are non-random for now.\nFor example, the discrete uniform distribution has two parameters indicating the lowest and highest values in the support, called \\(a\\) and \\(b\\). We could thus express its PMF as \\(p(x;a,b)\\), which means “the probability of \\(x\\) given known parameters \\(a\\) and \\(b\\)”.\nUsing these parameters, it’s also common to use special symbols as a short-hand for common distributions. For example, the discrete uniform distribution with parameters \\(a\\) and \\(b\\) is often shortened to something like \\(DU(a,b)\\). If we want to say \\(x\\) is a discrete uniform random variable, we’d write \\(x \\sim DU(a,b)\\). You’ll also sometimes see people use the symbol to write the PMF as well, for example \\(DU(x;a,b)\\).\n\n\n8.2.3 Discrete Distributions\nSome discrete probability distributions occur so frequently that they get a special name. Each one tends to occur when modeling certain kinds of phenomena. Here are a few of the most common discrete distributions. I’ll just state them and summarize their properties for future reference.\n\n8.2.3.1 Discrete Uniform Distribution\n\nSymbol: \\(DU(a,b)\\)\nParameters: Integers \\(a, b\\), where \\(a\\) is the minimum and \\(b-1\\) is the maximum value in the support\nSupport: \\(x=a,a+1,\\cdots,b-1\\)\nProbability mass function: \\[p(x; a,b) = \\frac{1}{b-a}, \\ \\text{ for } x = a, a+1, \\cdots, b-1.\\]\nCumulative distribution function: \\[\nP(x; a,b) =\n\\begin{cases}\n0 & x < a, \\\\\n\\frac{\\text{int}(x) - a}{b-a}, & a \\leq x \\leq b, \\\\\n1 & x \\geq 1.\n\\end{cases}\n\\]\nRandom number generator: np.random.randint(a, b)\nNotes:\n\nUsed to model discrete processes that occur with equal weight, or are suspected to (the principle of indifference)\nExample: The fair die, taking \\(a=1, b=7\\) gives \\(x \\sim D(1,7)\\) with \\(p(x) = \\frac{1}{7-1} = \\frac{1}{6}\\)\n\n\n\n\nCode\na = 1\nb = 7\nx = np.random.randint(a, b, size=100000)\nplot_histogram(x, is_discrete=True, stat='probability', title=f'$DU({a},{b})$ PMF')\n\n\n\n\n\n\n\n8.2.3.2 Bernoulli Distribution\n\nSymbol: \\(\\text{Ber}(\\text{p})\\)\nParameters: The probability of success \\(0 \\leq \\text{p} \\leq 1\\)\nSupport: \\(x=0,1\\)\nProbability mass function: \\[\np(x; \\text{p}) = \\text{p}^x (1-\\text{p})^{1-x} =\n\\begin{cases}\n1-\\text{p} & x = 0, \\\\\n\\text{p} & x = 1.\n\\end{cases}\n\\]\nCumulative distribution function: \\[\nP(x; \\text{p}) =\n\\begin{cases}\n0 & \\text{if } x < 0 \\\\\n1-p & \\text{if } 0 \\leq x < 1 \\\\\n1 & \\text{if } x \\geq 1.\n\\end{cases}.\n\\]\nRandom number generator: np.random.choice([0, 1], p=[1 - p, p])\nNotes:\n\nUsed to model binary processes where the probability of success can be estimated\nExample: Flipping a fair coin, where \\(\\text{tails} = 0\\), \\(\\text{heads} = 1\\), and \\(\\text{p}=\\frac{1}{2}\\)\nUsed for binary classification. Given an input \\(\\mathbf{x}\\) with some binary output \\(y=0,1\\). If \\(\\text{p}=\\hat y\\), then \\(y \\sim \\text{Ber}(\\hat y)\\).\nSpecial case of the binomial distribution where \\(n=1\\): \\(\\text{Ber}(\\text{p}) = \\text{Bin}(1, \\text{p})\\).\n\n\n\n\nCode\np = 0.7\nx = np.random.choice([0, 1], p=[1 - p, p], size=1000)\nplot_histogram(x, is_discrete=True, stat='probability', title=f'$Ber({p})$ PMF')\n\n\n\n\n\n\n\n8.2.3.3 Categorical Distribution\n\nSymbol: \\(\\text{Cat}(p_0,p_1,\\cdots,p_{k-1})\\) or \\(\\text{Cat}(\\mathbf{p})\\)\nParameters: \\(k\\) non-negative real numbers \\(p_j\\) that sum to one, each representing the probability of getting \\(x_j\\)\n\nCommonly written as a vector \\(\\mathbf{p} = (p_0,p_1,\\cdots,p_{k-1})\\)\n\nSupport: \\(x = 0, 1, \\cdots, k-1\\)\nProbability mass function: \\[\np(x; \\mathbf{p}) = \\begin{cases}\np_0 & x = 0, \\\\\np_1 & x = 1, \\\\\n\\vdots & \\vdots \\\\\np_{k-1} & x = k-1.\n\\end{cases}\n\\]\nCumulative distribution function: \\[\nP(x; \\mathbf{p}) =\n\\begin{cases}\n0 & \\text{if } x \\leq x_0 \\\\\np_0 & \\text{if } x_0 \\leq x \\leq x_1 \\\\\np_0 + p_1 & \\text{if } x_1 \\leq x \\leq x_2 \\\\\np_0 + p_1 + p_2 & \\text{if } x_2 \\leq x \\leq x_3 \\\\\n\\vdots & \\vdots \\\\\n1 & \\text{if } x \\geq x_{n-1}.\n\\end{cases}\n\\]\nRandom number generator: np.random.choice(np.arange(k), p=p)\nNotes:\n\nUsed to model categorical processes where a finite number of classes can occur with arbitrary probabilities\nUsed for multiclass classification. Given an input \\(\\mathbf{x}\\) with outputs in one of \\(k\\) classes \\(y=0,1,\\cdots,k-1\\). If \\(\\mathbf{p}=\\mathbf{\\hat y}\\), then \\(\\mathbf{y} \\sim \\text{Cat}(\\mathbf{\\hat y})\\).\nGeneralization of the Bernoulli distribution, allowing for \\(k\\) distinct outcomes instead of just \\(2\\).\nModels the values rolled from a die when \\(k=6\\).\n\n\n\n\nCode\np = [0.2, 0.5, 0.3]\nx = np.random.choice(np.arange(len(p)), p=p, size=1000)\nplot_histogram(x, is_discrete=True, stat='probability', title=f'$Cat{tuple(p)}$ PMF')\n\n\n\n\n\n\n\n8.2.3.4 Binomial Distribution\n\nSymbol: \\(\\text{Bin}(n, \\text{p})\\)\nParameters: The number of trials \\(n=1,2,3,\\cdots\\) and probability \\(0 \\leq \\text{p} \\leq 1\\) of success of each trial\nSupport: \\(x = 0, 1, \\cdots, n\\)\nProbability mass function: \\[p(x; n,\\text{p}) = \\binom{n}{x} \\text{p}^{x} (1-\\text{p})^{n-x}, \\ \\text{for} \\ x=0,1,\\cdots,n, \\ \\text{where} \\ \\binom{n}{x} = \\frac{n!}{x!(n-x)!}.\\]\nCumulative distribution function: \\[P(x; n,\\text{p}) = \\sum_{k=0}^{\\text{int}(x)} {n \\choose k} p^k(1-p)^{n-k}.\\]\nRandom number generator: np.random.binomial(n, p)\nNotes:\n\nUsed to model the number of successes from \\(n\\) independent binary processes (analogous to coin flips)\nExample: Flipping a fair coin \\(n\\) times and counting the number of heads\nGeneralization of the Bernoulli distribution. The sum of \\(n\\) independent Bernoulli variables is \\(\\text{Bin}(n, \\text{p})\\).\nThe number of counts in each bin of a histogram of independent samples can be modeled as a binomial random variable\n\n\n\n\nCode\nn = 10\np = 0.7\nx = np.random.binomial(n, p, size=1000)\nplot_histogram(x, is_discrete=True, stat='probability', title=f'$Bin{(n,p)}$ PMF')\n\n\n\n\n\n\n\n8.2.3.5 Poisson Distribution\n\nSymbol: \\(\\text{Poisson}(\\lambda)\\)\nParameters: A rate parameter \\(\\lambda \\geq 0\\)\nSupport: \\(x = 0, 1, 2, 3, \\cdots\\)\nProbability mass function: \\[p(x; \\lambda) = \\frac{\\lambda^x e^{-\\lambda}}{x!}, \\quad \\text{for} \\ x=0,1,2,3,\\cdots.\\]\nCumulative distribution function: \\[P(x; \\lambda) = e^{-\\lambda}\\sum_{k=0}^{\\text{int}(x)}\\frac{\\lambda^k}{k!}.\\]\nRandom number generator: np.random.poisson(lambda)\nNotes:\n\nUsed to model counting processes, like the number of calls coming into a call center, or the number of times a Geiger counter registers a click\nExample: The number of people walking through the door of a coffee shop per hour can be modeled as a Poisson distribution\n\n\n\n\nCode\nlambda_ = 4\nx = np.random.poisson(lambda_, size=1000)\nplot_histogram(x, is_discrete=True, stat='probability', title=f'$Poisson({lambda_})$ PMF')\n\n\n\n\n\n\n\n\n8.2.4 Probabilities of Multiple Outcomes\nWe’ve seen how to calculate the probabilities of any one outcome. The probability that \\(x=k\\) is given by \\(\\text{Pr}(x=k) = p(k)\\), where \\(p(k)\\) is the PMF. It’s natural to then ask how we can think about probabilities of multiple outcomes. For example, consider again the situation of rolling a fair die. Suppose we were interested in knowing what the probability was of rolling an even number, i.e. \\(x=2,4,6\\). How would we approach this? Your intuition suggests the right idea. We can just sum the probabilities of each outcome together,\n\\[\\mathbb{Pr}(x\\text{ is even}) = \\mathbb{Pr}(x=2,4,6) = p(2) + p(4) + p(6) = \\frac{1}{6} + \\frac{1}{6} + \\frac{1}{6} = \\frac{1}{2}.\\]\nThis same idea extends to any discrete set. Suppose we’re interested in the probability that some discrete random variable \\(x\\) takes on values in some set \\(E = \\{x_0, x_1, \\cdots, x_{m-1}\\}\\). Then all we need to do is some over the probabilities of all the outcomes in \\(E\\), i.e.\n\\[\\mathbb{Pr}(x \\in E) = \\sum_{k \\in E} p(k) = \\sum_{i=0}^{m-1} p(x_i) = p(x_0) + p(x_1) + \\cdots p(x_{m-1}).\\]\nWhen the set of interest is the entire support of \\(x\\), the right-hand side is just the sum the probability of all possible outcome, which is just one. Thus, we’ll always have \\(0 \\leq \\mathbb{Pr}(x \\in E) \\leq 1\\) for any set \\(E\\).\nThough we don’t really have to for discrete variables, it’s conventional to define another function \\(P(x)\\) called the cumulative distribution function, or CDF. It’s the probability \\(x \\in (-\\infty, x_0]\\) for some fixed value \\(x_0 \\in \\mathbb{R}\\),\n\\[P(x_0) = \\mathbb{Pr}(x \\leq x_0) = \\sum_{k \\leq x_0} p(k) = \\sum_{k=-\\infty}^{\\text{int}(x_0)} p(k),\\]\nwhere it’s understood that \\(p(k)=0\\) whenever \\(k\\) isn’t in the support of \\(x\\). Note the CDF is a real-valued function. We can ask about \\(P(x_0)\\) for any \\(x_0 \\in \\mathbb{R}\\), not just discrete values of \\(x_0\\).\nBut why should we care? It turns out if we know the CDF in some simple form, we can use it to calculate the probability \\(x\\) is in any other interval by differencing the CDF at the endpoints. Suppose we’re interested in the probability \\(a \\leq x \\leq b\\). If we know the CDF for a particular distribution in some simple form, we can just difference it to get the probability of being in the interval, i.e.\n\\[\\mathbb{Pr}(a \\leq x \\leq b) = \\mathbb{Pr}(x \\leq b) - \\mathbb{Pr}(x \\leq a) = P(b) - P(a).\\]\nThis fact is more useful for continuous distributions than discrete ones, since in the discrete case we can always just sum over the values, which is usually pretty quick to do.\n\n8.2.4.1 Application: Getting a Job\nHere’s a useful application where probabilities of multiple outcomes can sometimes come in handy. Suppose you’re applying to a bunch of jobs, and you want to know what is the probability that you’ll get at least one offer. Suppose you’ve applied to \\(n\\) jobs. For simplicity, assume each job has roughly the same probability \\(\\text{p}\\) of giving you an offer. Then each job application looks kind of like the situation of flipping a coin. If \\(x_i=1\\) you get an offer, if \\(x_i=0\\) you get rejected. We can thus think of each job application as a Bernoulli random variable \\(x_i \\sim \\text{Ber}(\\text{p})\\).\nNow, assume that the job applications are all independent of each other, so one company’s decision whether to give you an offer doesn’t affect another company’s decision to give you an offer. This isn’t perfectly true, but it’s reasonably true. In this scenario, the total number of offers \\(x\\) you get out of \\(n\\) job applications will then be binomially distributed, \\(x \\sim \\text{Bin}(n, \\text{p})\\).\nWe can use this fact to answer the question we started out with: What is the probability that you receive at least one offer? It’s equivalent to asking, if \\(x\\) is binomial, what is the probability that \\(x \\geq 1\\)? Now, since \\(x\\) is only supported on non-negative values, we have\n\\[\\begin{align*}\n\\mathbb{Pr}(x \\geq 1) &= \\mathbb{Pr}(x \\geq 0) - \\mathbb{Pr}(x=0) \\\\\n&= 1 - \\mathbb{Pr}(x=0) \\\\\n&= 1 - p(0;n,\\text{p}) \\\\\n&= 1 - \\binom{n}{0} \\text{p}^0 (1-\\text{p})^{n-0} \\\\\n&= 1 - \\frac{n!}{0!(n-0)!} (1-\\text{p})^n \\\\\n&= 1 - (1-\\text{p})^n.\n\\end{align*}\\]\nWe thus have a formula. The probability of receiving at least one job offer from applying to \\(n\\) jobs, assuming each gives an offer with probability \\(\\text{p}\\), and applications are independent of each other, is\n\\[\\mathbb{Pr}(\\text{at least one offer}) = 1 - (1-\\text{p})^n.\\]\nHere’s an example of how this formula can be useful. Suppose you believe you have a 10% chance of getting an offer from any one company you apply to, so \\(\\text{p}=0.1\\). If you apply to \\(n=10\\) jobs, you’ll have about a 34.86% chance of receiving at least one offer.\n\n\nCode\np = 0.1\nn = 10\nprob_offer = 1 - (1 - p) ** n\nprob_offer\n\n\n0.6513215599\n\n\nLet’s now ask how many jobs you’d have to apply to to give yourself at least a 90% chance of getting at least one job offer? Here’s what you can do. Let \\(O = \\mathbb{Pr}(\\text{at least one offer})\\), so \\(O = (1-p)^n\\). Set \\(O=0.9\\) and solve for \\(n\\). Then you’d have\n\\[\\begin{align*}\nO &= 1 - (1-p)^n \\\\\n(1-p)^n &= 1 - O \\\\\nn \\log(1-p) &= \\log(1 - O) \\\\\nn &= \\frac{\\log(1 - O)}{\\log(1 - p)}.\n\\end{align*}\\]\nPlugging in \\(p=0.1\\) and \\(O=0.9\\) gives \\(n \\approx 21.85\\). Thus, you’d need to apply to at least \\(n=22\\) jobs to have a decent chance of getting at least one offer. Here’s a plot of this idea. Each curve is a plot of \\(n=n(p)\\) for different choices of \\(O\\), in this case, 50%, 75%, 90%, and 99%.\n\n\nCode\np = np.linspace(0.01, 0.999, 100)\nO = [0.5, 0.75, 0.9, 0.99]\nfor o in O:\n    n = np.log(1 - o) / np.log(1 - p)\n    plt.plot(p, n, label=f'$O={round(o*100)}$%')\nplt.xticks(0.1 * np.arange(11))\nplt.ylim(0, 70)\nplt.title(\n    \"\"\"How many jobs would you have to \n    apply to to get at least one job offer\n    with confidence $O$?\"\"\".title(), fontsize=11)\nplt.xlabel('$p$')\nplt.ylabel('$n$')\nplt.grid(True, alpha=0.5)\nplt.legend()\nplt.show()\n\n\n\n\n\nThe moral of this story is that you have two ways to up your chances of getting a job offer: Up your chances of getting any one job (i.e. increase \\(p\\)), or apply to a lot more jobs (i.e. increase \\(n\\)). The more confident you want to be of getting an offer (i.e. \\(O\\)), the more jobs you’ll need to apply to. This same idea can be used to model the probability of at least one occurrence for any binary event similar to this."
  },
  {
    "objectID": "notebooks/probability.html#continuous-probability",
    "href": "notebooks/probability.html#continuous-probability",
    "title": "8  Basic Probability",
    "section": "8.3 Continuous Probability",
    "text": "8.3 Continuous Probability\nSo far we’ve covered discrete random variables, ones that take on a finite (or countably infinite) set of values. We can also consider random variables that take on a continuous range of values. For example, a continuous random variable \\(x\\) can take on values in the entire interval \\([0,1]\\), or the whole real line \\(\\mathbb{R} = (-\\infty, \\infty)\\). The key difference between continuous variables and discrete variables is that we have to think in terms of calculus now. Instead of points we’ll have infinitesimal areas. Instead of sums we’ll have integrals.\nIt may not be obvious to you that there are practical examples where continuous random variables would be useful. Here are some examples:\n\nModeling the behavior of random number generators like rand.\nModeling the total sales a business will do next quarter.\nModeling the time it takes for a customer to complete a purchase in an online store.\nModeling the amount of fuel consumed by a vehicle on a given day.\nModeling the height of waves in the ocean at a given time.\nModeling the length of a stay in a hospital by a typical patient.\nModeling the amount of rainfall in a specific region over a period of time.\nModeling the measured voltage of a car battery at any point in time.\n\nIn fact, any continuous variable you can think of could be treated as random depending on the situation. Even if a variable is completely deterministic, there may be situations where it’s helpful to think of it as random. The whole idea of Monte Carlo methods is based on this idea, in fact.\n\n8.3.1 Motivation: Rand Again\nI showed example of a continuous random variable already at the beginning of this lesson, when I introduced the idea of random number generators like rand. Rand is an example of a function that can (approximately) generate samples of a continuous random variable. In particular, it samples uniformly from the interval \\([0,1]\\). I already showed what its histogram looks like for a large number of samples. Here it is again.\n\n\nCode\nx = np.random.rand(10000)\nplot_histogram(x, bins=10, title=f'rand({10000})')\n\n\n\n\n\nLet’s now try to figure out how we should define the probability of values sampled from rand. In the discrete case, we were able to define probabilities by running an experiment (e.g. rolling a die a bunch of times). We could look at the ratio of the number of times \\(N_k\\) an outcome \\(k\\) occurred over the number of total trials \\(N\\). This made sense in the discrete case since we could reasonably well rely on each outcome \\(x=k\\) occurring enough times to get a meaningful count.\nThis approach doesn’t work well for continuous random variables. Suppose \\(x\\) is the random variable resulting from rand, uniform on the interval \\([0,1]\\). If I sample a single value from rand, there’s no reason to assume I’ll ever see that exact value again. There are uncountably infinitely many values to choose from in \\([0,1]\\), so I’m pretty much guaranteed to never see the same value twice. Instead of counting how many times each value occurs, what I can do is use the binning trick we saw with histograms. For example, I can divide \\([0,1]\\) up into ten subintervals (or bins)\n\\[I_0=[0,0.1], \\quad I_1=[0.1,0.2], \\quad I_3=[0.2,0.3], \\quad \\cdots, \\quad I_9=[0.9,1].\\]\nIf I sample one value from rand it’s guaranteed to be in one of these subintervals \\(I_k\\). If I sample a whole bunch of values from rand, say \\(N=1000\\), I should expect each \\(I_k\\) to contain about \\(N_k=100\\) counts (10% of the total since there are 10 bins). It thus seems to make perfect sense to define a probability on each \\(I_k\\),\n\\[\\mathbb{Pr}(x \\in I_k) = \\frac{N_k}{N} = \\frac{100}{1000} = \\frac{1}{10} = 0.1.\\]\n\n\nCode\nN = 10000\nM = 10\ndx = 1 / M\nx = np.random.rand(N)\nplot_histogram(x, bins=M, title=f'M=${M}$ subintervals of length $dx={dx}$')\n\n\n\n\n\nWe still want to approximate the discrete idea of having a probability \\(\\mathbb{Pr}(x=k)\\). How can we do it using this idea of subintervals? Enter calculus. What we can imagine doing is allowing each subinterval \\(I_k\\) to become infinitesimally small. Suppose we subdivide \\([0,1]\\) into \\(M\\) total subintervals each of infinitesimal length \\(dx\\), satisfying \\(M=\\frac{1}{dx}\\), i.e.\n\\[I_0=[0,dx], \\quad I_1=[dx, 2dx], \\quad I_2=[2dx, 3dx], \\quad \\cdots, \\quad I_{M-1}=[(M-1)dx, 1].\\]\nSuppose \\(x_0\\) is some point in one of these tiny intervals \\(I_k=[kdx, (k+1)dx]\\). Since each \\(I_k\\) is a very tiny interval, the probability that \\(x \\approx x_0\\) is pretty much exactly the same thing as the probability that \\(x \\in I_k\\). Let’s thus define the probability that \\(x \\approx x_0\\) as the probability that \\(x \\in I_k\\),\n\\[\\mathbb{Pr}(x \\approx x_0) = \\mathbb{Pr}(x \\in I_k) = \\lim_{N \\rightarrow \\infty} \\frac{N_k}{N}.\\]\nHere’s an approximate representation of this idea. I won’t be able to make \\(M=10^{300}\\) bins like I’d like, but I can at least make bins so you can see the point. I’ll need to generate a huge number of samples \\(N\\) so the histogram will populate. Notice each \\(N_k \\approx \\frac{N}{M} = 1000\\). That is,\n\\[\\mathbb{Pr}(x \\approx x_0) \\approx \\frac{N_k}{N} \\approx \\frac{N/M}{N} = \\frac{1}{M} = dx.\\]\nEvidently, the probability \\(x \\approx x_0\\) is infinitesimal, so very very tiny. This is why you’ll basically never sample the same value twice.\n\n\nCode\nN = 1000000\nM = N // 1000\ndx = 1 / M\nx = np.random.rand(N)\nplot_histogram(x, bins=M, title=f'M=${M}$ subintervals of length $dx={dx}$')\n\n\n\n\n\n\n\n8.3.2 General Case\nThe facts I’ve shown about rand extend to more general continuous random variables as well. Suppose \\(x\\) is supported on some interval \\([a,b]\\). It could even be infinite. Let’s divide this interval up into \\(M\\) tiny sub-intervals of length \\(dx\\), where \\(M\\) must satisfy \\(M = \\frac{b-a}{dx}\\),\n\\[I_0=[a,a+dx], \\quad I_1=[a+dx, a+2dx], \\quad I_2=[a+2dx, a+3dx], \\quad \\cdots, \\quad I_{M-1}=[a+(M-1)dx, b].\\]\nNow, run an experiment \\(N\\) times and count how many times outcomes occur, not for each \\(x\\), but for each subinterval \\(I_k=[a+kdx, a+(k+1)dx]\\). If \\(x_0 \\in I_k\\), that is, if \\(a+kdx \\leq x_0 \\leq a+(k+1)dx\\), then the probability that \\(x \\approx x_0\\) is defined by,\n\\[\\mathbb{Pr}(x \\approx x_0) = \\mathbb{Pr}(x \\in I_k) = \\lim_{N \\rightarrow \\infty} \\frac{N_k}{N}.\\]\nJust as with the uniform case before, it’s useful to think of the probability \\(\\mathbb{Pr}(x \\approx x_0)\\) as explicitly being proportional to the subinterval length \\(dx\\). In the uniform case it was just \\(\\mathbb{Pr}(x \\approx x_0)=dx\\) exactly. In the more general case, \\(\\mathbb{Pr}(x \\approx x_0)\\) may depend on the value of \\(x_0\\), so we need to weight the right-hand side by some non-negative weighting function \\(p(x) \\geq 0\\), so\n\\[\\mathbb{Pr}(x \\approx  x_0) = \\mathbb{Pr}(x \\in I_k) = p(x_0)dx.\\]\nThis weighting function \\(p(x)\\) is called the probability density function, or PDF for short. It’s the continuous analogue of the probability mass function from the discrete case (hence why I use the same notation). Unlike the discrete PMF, the PDF is not a probability all by itself. It’s a probability per infinitesimal unit \\(dx\\). That is, it’s a density. For this reason, the PDF need not sum to one. It only needs to be non-negative, i.e. all outputs \\(p(x_0)\\) should lie on or above the x-axis, never below it. But any one output \\(p(x_0)\\) can be arbitrarily large, even \\(\\infty\\)!\nWhat must be true is that all probabilities sum to one. Since each \\(\\mathbb{Pr}(x \\approx x_0)\\) is infinitesimal now, this means all probablities must integrate to one over the support of \\(x\\). If \\(x\\) is supported on \\([a,b]\\), then\n\\[\\mathbb{Pr}(a \\leq x \\leq b) = \\sum_{k=0}^{M-1} \\mathbb{Pr}(x \\in I_k) = \\int_a^b p(x)dx = 1.\\]\nThis means we can think of a PDF as being any non-negative function that integrates to one. In fact, any function that satisfies this property is a valid PDF for some continuous random variable.\nSpecifying the functional form of the PDF \\(p(x)\\) creates a continuous probability distribution. By specifying \\(p(x)\\), we’ve uniquely specified what the probabilities have to be for the variable \\(x\\). In the next section I’ll define some of the most common continuous distributions.\nJust as with discrete probabilities, we can get the probability that \\(x\\) is in any set by summing over all the values in that set. The only difference is we replace the sum with an integral over the set. For example, the probability that \\(c \\leq x \\leq d\\) is given by\n\\[\\mathbb{Pr}(c \\leq x \\leq d) = \\int_c^d p(x)dx.\\]\nWe can also define a cumulative distribution function \\(P(x)\\) for continuous probabilities in exactly the same way, except again replacing sums with integrals,\n\\[P(x_0) = \\mathbb{Pr}(x \\leq x_0) = \\int_{-\\infty}^{x_0} p(x')dx',\\]\nwhere it’s understood that \\(p(x')=0\\) whenever \\(x'\\) is outside the support of \\(x\\).\nIf we can obtain the CDF for a distribution, we can calculate the probability \\(x\\) is in any set without having to evaluate an integral. For example, if the set is again the interval \\([c,d]\\), then\n\\[\\mathbb{Pr}(c \\leq x \\leq d) = P(d) - P(a).\\]\nThis is just a restatement of the rule for definite integrals from the calculus lesson, if \\(f(x)=\\frac{d}{dx}F(x)\\), then\n\\[\\int_c^d f(x) dx = F(d) - F(c).\\]\nTo show a brief example, I’ll calculate the CDF of the rand distribution shown already, where \\(x\\) is uniform on \\([0,1]\\). I already showed that its PDF is just \\(p(x)=1\\) for all \\(0 \\leq x \\leq 1\\). Outside this interval \\(p(x)=0\\) everywhere. Using the PDF I can calculate the CDF by integrating. There are three cases to consider. If \\(x < 0\\), the CDF will just be \\(P(x)=0\\) since \\(p(x)=0\\). If \\(x > 1\\), \\(P(x) = 1\\) since we’re integrating over the whole support \\([0,1]\\). Otherwise, we’re integrating over some subinterval \\([0,x]\\), in which case \\(P(x)=x\\). That is,\n\\[\nP(x) = \\int_{-\\infty}^x p(x') dx' =\n\\begin{cases}\n0, & x < 0 \\\\\nx, & 0 \\leq x \\leq 1 \\\\\n1, & x > 1.\n\\end{cases}\\]\nHere’s a plot of both the PDF and CDF of rand. Notice the PDF is just the constant \\(p(x)=1\\) on \\([0,1]\\), whose area under the curve is just one, since the total probability must integrate to one. Also, notice how this same area is the exact same thing that the histogram tries to approximate. In fact, a histogram is just a discrete approximation to the area under a continuous PDF.\nFor the CDF, notice how the function starts at \\(P(x)=0\\) on the far left, and ramps up monotonically to \\(P(x)=1\\) as \\(x\\) increases. Every CDF will have this property. The only difference is what the ramp looks like. It’ll always be the case that \\(P(-\\infty)=0\\), \\(P(\\infty)=1\\), and some monotonic increasing curve connects these two extremes.\n\n\nCode\nx = np.linspace(0, 1, 100)\np = lambda x: np.ones(len(x))\nplot_function(x, p, xlim=(-0.5, 1.5), ylim=(-0.5, 1.5), set_ticks=True, title='Rand PDF')\n\n\n\n\n\n\n\nCode\nx = np.linspace(-1, 2, 100)\nP = lambda x: np.clip(x, 0, 1) ## quick way to define the piecewise CDF shown above\nplot_function(x, P, xlim=(-1, 2), ylim=(-0.5, 1.5), title='Rand CDF')\n\n\n\n\n\n\n\n8.3.3 Continuous Distributions\nAs with discrete distributions, some continuous distributions occur so frequently that they get a special name. Here are a few of the most common continuous distributions. I’ll just state them and summarize their properties for future reference.\n\n8.3.3.1 Uniform Distribution\n\nSymbol: \\(U(a,b)\\)\nParameters: The minimum \\(a\\) and maximum \\(b\\) values in the support\nSupport: \\(x \\in [a,b]\\)\nProbability density function: \\[p(x; a,b) = \\frac{1}{b-a}, \\ \\text{ for } a \\leq x \\leq b.\\]\nCumulative distribution function: \\[\nP(x; a, b) =\n\\begin{cases}\n0 & x < a, \\\\\n\\frac{x - a}{b-a}, & a \\leq x \\leq b, \\\\\n1 & x \\geq 1.\n\\end{cases}\n\\]\nRandom number generator: np.random.uniform(a, b)\nNotes:\n\nUsed to model continuous processes that occur with equal weight, or are suspected to (the principle of indifference)\nExample: The values sampled from rand, where \\(a=0\\) and \\(b=1\\), so \\(x \\sim U(0,1)\\).\nThe rand example \\(U(0,1)\\) is called the standard uniform distribution.\n\n\n\n\nCode\na, b = -2, 5\nx = np.linspace(a, b, 1000)\np = lambda x: 1 / (b - a) * np.ones(len(x))\nplot_function(x, p, xlim=(a - 0.5, b + 0.5), ylim=(-0.5 / (b - a), 1.5 / (b - a)), set_ticks=True,\n              title=f'$U({a},{b})$ PDF')\n\n\n\n\n\n\n\n8.3.3.2 Gaussian Distribution (Normal Distribution)\n\nSymbol: \\(\\mathcal{N}(\\mu, \\sigma^2)\\)\nParameters: The mean \\(\\mu \\in \\mathbb{R}\\) and variance \\(\\sigma^2 \\geq 0\\) of the distribution\nSupport: \\(x \\in \\mathbb{R}\\)\nProbability density function: \\[p(x; \\mu , \\sigma^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp{\\bigg(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\bigg)}.\\]\nCumulative distribution function: \\[P(x; \\mu , \\sigma^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\int_{-\\infty}^x \\exp{\\bigg(-\\frac{(x' - \\mu)^2}{2\\sigma^2}\\bigg)} dx'.\\]\nRandom number generator: np.random.normal(mu, sigma) (note it sigma is the square root of the variance \\(\\sigma^2\\))\nNotes:\n\nUsed to model the sum or mean of many continuous random variables, e.g. the distribution of unbiased measurements of some continuous quantity\nExample: The distribution of heights in a given population of people.\nUsed in machine learning to model the outputs of an L2 regression model. Given a input \\(\\mathbf{x}\\) with a continuous output \\(y\\), model \\(y = f(\\mathbf{x}) + \\varepsilon\\), where \\(\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)\\) is some random error term and \\(f(\\mathbf{x})\\) is some deterministic function to be learned. Then \\(y \\sim \\mathcal{N}(f(\\mathbf{x}),\\sigma^2)\\).\nThe special case when \\(\\mu=0, \\sigma^2=1\\) is called the standard Gaussian distribution, written \\(\\mathcal{N}(0,1)\\). Values sampled from a standard Gaussian are commonly denoted by \\(z\\). By convention, its PDF is denoted by \\(\\phi(z)\\) and its CDF by \\(\\Phi(z)\\).\nCan turn any Gaussian random variable \\(x\\) into a standard Gaussian or vice versa via the transformations \\[z = \\frac{x-\\mu}{\\sigma}, \\qquad x = \\sigma z + \\mu.\\]\nThe CDF of a Gaussian can’t be written in closed form since the Gaussian integral can’t be written in terms of elementary functions. Since the standard Gaussian CDF \\(\\Phi(z)\\) has a library implementation it’s most common to transform other Gaussian CDFs into standard form and then calculate that way. Use the function norm.cdf from scipy.stats to get the standard CDF function \\(\\Phi(z)\\).\n\n\n\n\nCode\nx = np.linspace(-10, 10, 1000)\np_gaussian = lambda x: 1 / np.sqrt(2 * np.pi) * np.exp(-1/2 * x**2)\nplot_function(x, p_gaussian, xlim=(-3, 3), ylim=(0, 0.5), set_ticks=False,\n              title=f'Standard Gaussian PDF')\n\n\n\n\n\n\n\nCode\nfrom scipy.stats import norm\n\nx = np.linspace(-3, 3, num=100)\nPhi = lambda x: norm.cdf(x)\nplot_function(x, Phi, xlim=(-3, 3), ylim=(0, 1), set_ticks=False, title='Standard Gaussian CDF')\n\n\n\n\n\n\n\n8.3.3.3 Laplace Distribution\n\nSymbol: \\(\\text{Laplace}(\\mu, s)\\)\nParameters: The mean \\(\\mu \\in \\mathbb{R}\\) and scale \\(s \\geq 0\\) of the distribution\nSupport: \\(x \\in \\mathbb{R}\\)\nProbability density function: \\[p(x; \\mu , s) = \\frac{1}{2s} \\exp\\bigg(-\\frac{|x-\\mu|}{s}\\bigg).\\]\nCumulative distribution function: \\[\nP(x; \\mu , s) =\n\\begin{cases}\n\\frac{1}{2} \\exp\\bigg(-\\frac{|x-\\mu|}{s}\\bigg), & x \\leq \\mu \\\\\n1 - \\frac{1}{2} \\exp\\bigg(-\\frac{|x-\\mu|}{s}\\bigg), & x > \\mu.\n\\end{cases}\n\\]\nRandom number generator: np.random.laplace(mu, s)\nNotes:\n\nUsed to model Gaussian-like situations where extreme values are somewhat more likely to occur than in a Gaussian. These are called outliers.\nExample: The distribution of finanical stock returns, where extreme returns are more likely than expected under a Gaussian distribution.\nUsed in machine learning to model the outputs of an L1 regression model. Given an input \\(\\mathbf{x}\\) with a continuous output \\(y\\), model \\(y = f(\\mathbf{x}) + \\varepsilon\\), where \\(\\varepsilon \\sim \\text{Laplace}(0, s)\\) is some random error term (that can be extreme-valued) and \\(f(\\mathbf{x})\\) is some deterministic function to be learned. Then the outputs are also Laplace distributed, with \\(y \\sim \\text{Laplace}(f(\\mathbf{x}), s)\\).\nThe special case when \\(\\mu=0, s=1\\) is called the standard Laplace distribution, written \\(\\text{Laplace}(0, 1)\\).\n\n\n\n\nCode\nx = np.linspace(-10, 10, 1000)\np_laplace = lambda x: 1 / (2 * np.pi) * np.exp(-np.abs(x))\nps = [p_gaussian, p_laplace]\nplot_function(x, ps, xlim=(-4, 4), ylim=(0, 0.5), set_ticks=False, labels=['Gaussian', 'Laplace'],\n             title='Gaussian vs Laplace PDFs')\n\n\n\n\n\n\n\n\n8.3.4 Cauchy Distribution\n\nSymbol: \\(\\text{Cauchy}(m, s)\\)\nParameters: The median \\(m \\in \\mathbb{R}\\) and scale \\(s > 0\\) of the distribution.\nSupport: \\(x \\in \\mathbb{R}\\)\nProbability density function: \\[p(x; m, s) = \\frac{1}{\\pi s} \\frac{1}{1 + \\big(\\frac{x-m}{s}\\big)^2}.\\]\nCumulative distribution function: \\[P(x; m, s) = \\frac{1}{\\pi} \\arctan \\bigg(\\frac{x-m}{s}\\bigg) + \\frac{1}{2}.\\]\nRandom number generator: s * np.random.standard_cauchy() + m\nNotes:\n\nUsed to model Gaussian-like situations where extreme values are highly likely to occur frequently.\nSuch a distribution is said to exhibit heavy-tailed behavior, since there’s a “heavy” amount of probability in the tails of the distribution, making extreme values likely to occur.\nExample: The distribution of computer program runtimes often exhibits heavy-tailed behavior.\nThe case when \\(m=0\\) and \\(s=1\\) is called the standard Cauchy distribution, denoted \\(\\text{Cauchy}(0, 1)\\).\nTechnically speaking, the mean of the Cauchy distribution doesn’t exist, so you have to use the median instead.\n\n\n\n\nCode\nx = np.linspace(-10, 10, 1000)\np_cauchy = lambda x: 1 / np.pi * 1 / (1 + x ** 2)\nps = [p_gaussian, p_laplace, p_cauchy]\nplot_function(x, ps, xlim=(-10, 10), ylim=(0, 0.5), set_ticks=False, labels=['Gaussian', 'Laplace', 'Cauchy'],\n             title='Gaussian vs Laplace vs Caucy PDFs')\n\n\n\n\n\n\n8.3.4.1 Exponential Distribution\n\nSymbol: \\(\\text{Exp}(\\lambda)\\)\nParameters: A rate parameter \\(\\lambda > 0\\)\nSupport: \\(x \\in [0,\\infty)\\)\nProbability density function: \\[p(x; \\lambda) = \\lambda e^{-\\lambda x}.\\]\nCumulative distribution function: \\[P(x; \\lambda) = 1 - e^{-\\lambda x}.\\]\nRandom number generator: np.random.exponential(lambda)\nNotes:\n\nUsed to model the time between two independent discrete events, assuming those events occur at a roughly constant rate.\nExample: The time between earthquakes in a given region, assuming earthquakes are rare and independent events.\nFrequently used to model the time between two Poisson distributed events. If the events are Poisson distributed and independent, then the time between any two events will be exponentially distributed.\n\n\n\n\nCode\nlambda_ = 1\nx = np.linspace(0, 20, 100)\np = lambda x: lambda_ * np.exp(-lambda_ * x)\nplot_function(x, p, xlim=(0, 20), ylim=(0, 1), set_ticks=False, title=f'$Exp({lambda_})$ PDF')"
  }
]