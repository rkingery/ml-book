[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Math and Programming for Machine Learning",
    "section": "",
    "text": "Preface\nThis is my book Math and Programming for Machine Learning. Still a work in progress."
  },
  {
    "objectID": "notebooks/programming.html#computer-organization",
    "href": "notebooks/programming.html#computer-organization",
    "title": "1  Computer Programming",
    "section": "1.1 Computer Organization",
    "text": "1.1 Computer Organization\nBefore diving into python it’s useful to briefly give a high-level overview of how a computer works. At a high level, a computer has a few fundamental components. I’ll just very briefly mention what these are.\nThe Processor\nThe processor is the component of the computer that actually performs computations. Traditionally computers had only a single processor known as a central processing unit or CPU. The CPU was where all computations were performed. Until 20 or so years ago, the CPU was single-core, meaning it could only perform a single operation at a time. In the past 20 years though CPUs have become multi-core, which means they’re capable of executing operations in parallel. That is, multiple operations can be executed at the same time, one on each core. Nowadays, each core executes operations at about 3.2 GigaHertz, which just means that in one second a given core can execute 3.2 billion cycles or operations.\nAnother important trend of the past 30 years or so is the use of specialty processors for performing specific tasks. The most important of these is the graphics processing unit or GPU. As the name suggests, the GPU was originally intended to be used to accelerate computer graphics. However, over the past 10-15 years machine learning researchers have realized that the GPU is also very good at performing machine learning computations as well. Since then, we frequently find ourselves training models on the GPU, and increasingly running inference as well. Variants of the GPU have recently been created specifically for doing machine learning operations. The most well-known is the tensor processing unit or TPU from Google, though there are many others as well.\nMemory\nMemory is the way that a computer stores data. Depending on what the data is being stored for, there are different components of memory that each tradeoff in terms of the amount of data that can be stored and the speed with which data can be fetched from memory (called latency or access time). The components of memory make up what’s called the memory hierarchy.\n\n\n\n\n\nAt the top of the hierarchy is what’s called register memory. This is where the processor stores the data it needs to access while performing computations. It’s fast, but not very big. The next level down is called cache memory. Cache is where data gets stored that needs to be quickly accessed, but is too big to fit in the registers. Strictly speaking there are different levels of caches, but I won’t dwell on this. The next level down memory hierarchy is called main memory or RAM (for random access memory). RAM is where all the data being actively used by the computer is stored, when it’s too big to fit in register or cache. The last level, the base of the hierarchy is called disk. This is where persistent data gets stored, i.e. data that’s meant to persist even if the computer is shut down. It used to be that disk memory was literally a disk with a sort of record-player dial that spun around to fetch a particular piece of data. Nowadays though disk memory is usually a solid state drive or SSD. For our purposes this doesn’t really mean that much, except that accessing data from disk is faster than it used to be. The last level of the hierarchy nowadays is the cloud. This is remote storage. Data is hosted on a completely different computer, called a server, that the user accesses via an internet connection. Cloud storage is where we’d store data too big to fit on a local computer. Typically large data on the cloud is stored and accessed via buckets.\nI/O\nThe last major component of a computer is I/O, a long-standing abbreviation for in-out. I/O is where all the interfacing between the user and the computer happens. The data printed to the screen is I/O. The keyboard is also I/O, since the user is feeding information into the computer. So are rendered images and video, audio, printers, FAX, scanners, copiers, etc. For our purposes, the only thing to note about I/O is that it’s slow. In programs where speed matters, you should try to minimizes the amount you print. That said, I/O is essential for debugging and experimental development, so we don’t want to completely avoid it.\nComputer Program\nStrictly speaking a computer program isn’t a component of a computer. It’s just some data that tells the computer to execute some sequence of instructions. At the lowest level a computer executes instructions in machine code. Executable code is just the raw bits that the computer directly executes on the hardware. One level up from executable code is what’s called assembly code. Assembly code is a somewhat more user friendly way to write machine code. It’s still very low level though and not very user friendly relative to say python. One level up from assembly is compiled code. Compiled code includes programming languages like C, C++, or Fortran whose code gets directly compiled down to machine code and executed all at once as an executable file. One level higher than compiled code is where python sits. This is called interpreted code. Interpretable code includes programming languages like python, Java, and Javascript. With interpreted code, instead of the program getting compiled and executed all at once, it’s allowed to execute line-by-line (like how we’re executing code in a Jupyter notebook).\nA computer program consists of instructions to be executed, written in some programming language that the computer can recognize via some compiler or interpreter. The program’s instructions are stored in a block of memory known as a heap. When a program is executed it also has more memory allocated for executing things like functions, called a stack. Each time a function, module, or class is called, the program builds a stack to execute that object and return the output to the main programming environment. The operating system only allocates a fixed amount of memory for a given program. If the program tries to use more memory than it’s been allocated it’ll result in what’s called a stack overflow. These usually result from things like infinite loops or recursions that the user forgot to check test for.\nOperating System\nThe fundamental set of computer programs that make a computer function is called its operating system. Nowadays the three main types of operating systems are Linux, Mac, and Windows. Each is built in its own particular way, but at a high level they all perform the same operations. Most importantly is the management and execution of processes, which are essentially computer programs plus their allocated blocks of memory. The operating system is what decides which processes get executed, and in which order. Usually those processes are executed concurrently, which allows the operating system to simultaneously keep many processes going at once. The user traditionally interacted with the operating system using a terminal, or a command line interface, though nowadays it’s more common to interact with the computer via a graphical user interface or GUI."
  },
  {
    "objectID": "notebooks/programming.html#python",
    "href": "notebooks/programming.html#python",
    "title": "1  Computer Programming",
    "section": "1.2 Python",
    "text": "1.2 Python\nPython is a programming language well-known for its simplicity and ease of use, which makes it highly popular among programmers. Python also has extensive library support for numerical computing, data science, and machine learning tasks. All of this together makes python by far the most popular programming language in machine learning and the data sciences in general. As of this writing there is no language that even comes close to matching python in the machine learning field in terms of popularity and general capabilities.\nPython is a high-level programming language. This essentially means it supports many layers of abstraction, allowing programmers to not have to worry about many of the low-level details of programming that other lower-level programming languages might require you to keep track of. Python is an interpreted language, which essentially means we can execute code line-by-line instead of having to wait until we’re finished and then compiling the entire file into an executable to run. This makes it easy experiment with code line-by-line. The trade-off, however, is that python can be slower than compiled languages, sometimes 10-100 times slower. We’ll talk about ways to address this trade-off in future lessons, particularly by using libraries that automatically run code in lower-level languages for us. For most of this book though we’ll treat the interpreted nature of python as a blessing.\nThe most basic operations python can perform is arithmetic. At its most basic level the language can be used as a calculator. For example, if you wanted to calculate 1.75 + 3.5 * 4, you’d just type it into a cell and run it directly, e.g. by pressing SHIFT-ENTER.\n\n1.75 + 3.5 * 4\n\n15.75\n\n\nAll of the usual arithmetic operations from a scientific calculator carry over to python as well. This even includes things like parenthesis, exponents, and order of operations. For example, we could calculate something like (4 - 2) / 7.3 * 5 directly. This will execute the statement (4 - 2) in parentheses first, then the multiplication by 5, and finally the division by 7.3.\n\n5 * (4 - 2) / 7.3\n\n1.36986301369863\n\n\nPython also supports several arithmetic operations that are less commonly found on a calculator. For example, we can integer divide two numbers to get the ratio of two numbers rounded down to the nearest whole number. Regular division is done using the / operator, while integer division is done using the // operator. For example, 5 / 2 is just \\(5 \\div 2 = 2.5\\), while 5 // 2 is division rounded down to the nearest integer, \\(5 \\ // \\ 2 = 2\\).\n\n5 / 2\n\n2.5\n\n\n\n5 // 2\n\n2\n\n\nWe can calculate exponents too, not just integer exponents, but fractional exponents as well. In python, exponentiation is done using the ** operator (not ^ as you might be used to). For example 2**3 is just \\(2^3 = 8\\), while 9**(1/2) is the square root of \\(9\\), i.e. \\(\\sqrt{9} = 9^{1/2} = 3\\).\n\n2**3\n\n8\n\n\n\n9**(1/2)\n\n3.0\n\n\nNote that python returned 3.0 instead of 3. Mathematically this doesn’t make a difference, but it does change the type of the number from an integer, or int, to a floating point number, or float. We can get the type of a number or any other variable by using the type function like so.\n\ntype(3)\n\nint\n\n\n\ntype(3.0)\n\nfloat\n\n\nEach line of code I’ve run is called a statement. Usually, running a statement won’t automatically print it so we can see it. We normally have to wrap a statement inside of the print function to print it out. The reason we haven’t had to do this so far is because Jupyter automatically prints the output of the last statement in a cell, unless you tell it not to. If we wanted to explicitly print out, say, 3.0 we’d just type print(3.0).\n\nprint(3.0)\n\n3.0\n\n\nNumbers like the integer 3 or the float 3.0 are called data structures or data types. Each data structure has a type, and each type has its own properties that govern how the data structure behaves. We’ve seen two types of data structures already, int and float. There are many others too. One important data structure is a string, or str. A string is a data structure for storing text. To make a string, type whatever text you want and place it inside of quotation marks. Python allows both single quotes ' and double quotes \" to specify a string. You just have to be consistent about it. You can’t start a string with ' and end it with \", for example. Here’s an example of a string that contains the text Hello!.\n\n\"hello\"\n\n'hello'\n\n\n\n'hello'\n\n'hello'\n\n\n\ntype('hello')\n\nstr\n\n\nAnother important data structure is a boolean, or bool. A boolean is any statement that can be True or False. Mathematical comparison operators like \\(<\\), \\(>\\), \\(\\leq\\), \\(\\geq\\), or \\(=\\) are all boolean. For example, \\(3 = 7\\) is either a true statement or a false statement. In python, these comparison operators are denoted by the symbols <, >, <=, >=, and == respectively. Notice how == is used for equality, not =, which means something completely different in python.\n\n3 == 7\n\nFalse\n\n\n\n3 <= 7\n\nTrue\n\n\n\ntype(3 == 7)\n\nbool\n\n\nJust as numbers can be composed together to get other numbers using arithmetic operators like \\(+, -, \\times, \\div\\), booleans can be composed to get other booleans using logical operators like AND, OR, and NOT. Each logic operator obeys the standard rules of logic. For example, if \\(A\\) is true and \\(B\\) is false, then \\(A\\) AND \\(B\\) is false, but \\(A\\) OR \\(B\\) is true. If \\(A\\) is true, then NOT \\(A\\) is false. In python, we’d use and, or, and not respectively for these operators.\nTo see a specific example, suppose \\(A\\) is the statement (3 == 7), and \\(B\\) is the statement (3 <= 7). We already saw that \\(A\\) in this case is true, but \\(B\\) is false. This means \\(A\\) AND \\(B\\) should be false, and \\(A\\) OR \\(B\\) should be true. Since \\(A\\) is true, NOT \\(A\\) should be false. And since \\(B\\) is false, NOT \\(B\\) should be true. Let’s make sure.\n\n(3 < 7) and (3 == 7)\n\nFalse\n\n\n\n(3 < 7) or (3 == 7)\n\nTrue\n\n\n\nnot (3 < 7)\n\nFalse\n\n\n\nnot (3 == 7)\n\nTrue\n\n\nWe don’t like to have to keep copying and pasting statements like this to use them later on. It’s convenient to assign them to a variable and carry around the variable instead. A variable is just a placeholder that stands for the object it’s referencing. To assign a value to a variable, we use the assignment operator =. This is why we can’t use = for equality. It represent variable assignment, which means something completely different under the hood.\nWe can use any alpha-numeric symbol to represent variable name as long as it doesn’t start with a number. For example, A, B are fine. So are foo or bar or foo_bar. Note that python is case-sensitive. This means that a variable named a will not be the same as a variable named A.\nAs an example, let’s assign (3 < 7) to the variable A and (3 == 7) to the variable B and verify that the same logical operations using A and B.\n\nA = (3 < 7)\nB = (3 == 7)\n\n\nA and B\n\nFalse\n\n\n\nA or B\n\nTrue\n\n\n\nnot B\n\nTrue\n\n\nAs long as we don’t change the value of a variable it’ll always reference whatever value it was originally assigned. If we re-assign the variable name to another value though, it’ll reference that new value instead. For example, if we assign x = 3, and then after assign x = 'foo', then x will there after be a string containing the words foo.\n\nx = 3\ntype(x)\n\nint\n\n\n\nx = 'foo'\ntype(x)\n\nstr\n\n\nAnother useful data structure in python is the list, or list. A list is an array, i.e. a collection of ordered objects or elements. Those objects can be any valid python data structure. To create a list in python we’d just wrap the objects inside of closing brackets. For example, to create a list containing the objects 1, 3.1415, and 'pi', use [1, 3.1415, 'pi']. The length of a list is the number of objects it contains. For example, the list [1, 3.1415, 'pi'] contains 3 elements. We can get the length of an array by using the len function on the list.\n\nx = [1, 3.1415, 'pi']\nprint(x)\n\n[1, 3.1415, 'pi']\n\n\n\ntype(x)\n\nlist\n\n\n\nlen(x)\n\n3\n\n\nNotice that the elements of a list need not even be of the same type. They can be essentially anything. We can even put lists inside of lists. For example, the list x below contains 3 elements, 1, 2, and the list [3, 4]. This means that the length of this list is 3, not 4 as you might expect. Be careful.\n\nx = [1, 2, [3, 4]]\nprint(x)\n\n[1, 2, [3, 4]]\n\n\n\nlen(x)\n\n3\n\n\nTo access the elements inside of a list we can use indexing. An index is a count from left to right of where in the list a given element is, starting from zero. Counting elements starting from zero is called zero-indexing. Not all programming languages start at zero, but most do. Be careful with this. Your mind naturally wants to count from one. It takes some getting used to. For example, the left-most element of a list has an index 0. The next one over has an index 1. And so on until we get to the end of a list. To get the value of a list at a given index, just pass the index in using brackets. For example, the element of x at index 0 is x[0], which is 1.\nIf a list has \\(n\\) total elements, its last index will be \\(n-1\\) since we’re counting from zero. In theory, this means we’d have to type something like x[len(x) - 1] to get the last element of a list. But python has a nice shortcut called negative indexing. If we index a list with a negative number, python counts starting from the end of the list. For example, the last element can be gotten using x[-1], the second-to-last with x[-2], and so on.\n\nx[0]\n\n1\n\n\n\nx[1]\n\n2\n\n\n\nx[-1]\n\n[3, 4]\n\n\nWhat if we wanted to pick out not just one element, but a subsequence of the list. For example, we might want the subsequence [0, 1] from x. We can do this using slicing. Slicing is a form of indexing where you specify both the start and end indices of the elements you want. Slicing is done using the : operator.\nFor example, to get the [0, 1] subsequence from x, we’d slice x to get x[0: 2]. This tells python to return the list whose elements run from x[0] to x[2], i.e. the list [0, 1]. We could also use x[0: -1], which tells python to return every element in x except for the last element x[-1].\n\nx[0: 2]\n\n[1, 2]\n\n\n\nx[0: -1]\n\n[1, 2]\n\n\nWe can add elements to a list we’ve already defined two ways. One way is to use the operator + to concatenate two lists together from left to right. Notice how + here is being used differently than simple addition. This is called operator overloading. Depending on what data structures we’re using, operators can mean completely different things. For example, to concatenate the lists [1, 2] and [3, 4] from left to right, we’d use [1, 2] + [3, 4], which will return a new list [1, 2, 3, 4].\nAnother way to add an element to a list is to use the append method. A method is an operation attached to a data structure. For example, to append 'foo' to the list x, we’d use x.append('foo'). This will add 'foo' to the end of x. Note append is an in-place operation, which means it doesn’t return any new data as output. It updates x automatically under the hood, without creating a new variable.\n\n[1, 2] + [3, 4]\n\n[1, 2, 3, 4]\n\n\n\nx = [1, 2, 3]\nx.append(4)\nprint(x)\n\n[1, 2, 3, 4]\n\n\nLists have the property of being mutable. This means we can update the elements of an already-defined list at any time. For example, we can find an element at a specific index in the list and change it to something else. This is usually a nice property to have, but sometimes it’s not. If we instead want a list that we can’t change once it’s defined, we’d use a related data structure called a tuple instead. A tuple looks exactly like a list, except it’s wrapped using parentheses instead of brackets. We can index into a tuple or find its length, but we can’t change elements or add new elements into it.\n\nx = (1, 2, 3, 4)\nprint(x)\n\n(1, 2, 3, 4)\n\n\n\ntype(x)\n\ntuple\n\n\n\nx[0]\n\n1\n\n\nIf we like, we can always re-cast a list as a tuple or vice versa by using the list or tuple functions. Each data structure has a function for re-casting like this, though they may not always do what you think they’ll do. For example, if we use int on 3.9, it’ll return the integer 3. It won’t round up. If you want to round a number up to the nearest integer, use round instead.\n\nx = [1, 2, 3]\ny = tuple(x)\nprint(y)\n\n(1, 2, 3)\n\n\n\nint(3.9)\n\n3\n\n\n\nround(3.9)\n\n4\n\n\nAnother important data structure is called a dictionary, or dict, sometimes also called a hash table in other languages. A dictionary behaves a lot like a list, except it allows us to use arbitrary indexes instead of having to count left to right from zero. These arbitrary indexes are called keys, and the elements they reference are called values. A key can be any non-mutable data structure, like an int, float, string, or even tuple. A value can be any object, just like with a list.\nTo define a dictionary in python, wrap the keys and values inside of braces. Instead of just specifying the elements, we have to specify both the keys and values using key: value notation for each one. For example, suppose we wanted to create a dictionary where key 'a' has value 1, key 'b' has value 2, and key 'c' has value 3. Then we’d write {'a': 1, 'b': 2, 'a': 3}. Note that dictionaries, like lists are mutable, which means we can always set a key to a new value, or add new key-value pairs.\nIndexing into a dictionary works the same as usual, except we’d index a value with its key, not its count. Note each key needs to be distinct. If you try to use the same key for multiple values, python will set that key to refer to the last value you set it to. Dictionaries also support the length operation. With dictionaries, len tells you how many key-value pairs the dictionary contains. For the previous example, that would be 3.\n\nx = {'a': 1, 'b': 2, 'c': 3}\nprint(x)\n\n{'a': 1, 'b': 2, 'c': 3}\n\n\n\ntype(x)\n\ndict\n\n\n\nlen(x)\n\n3\n\n\n\nx['a']\n\n1\n\n\nThe final python data structure I’ll mention is kind of trivial. It’s called None. None is often used as a null value or a placeholder for something else. One place None is useful is to define a list of specified size to be filled in with values later on. Sometimes you want to pre-define a list so that python can tell the hardware to allocate a certain amount of memory to the list, which can be useful for performance reasons. To pre-define a list of size n, where each element is initialized to None, we can use [None] * n. Notice again that we’re operator overloading, this time the * operator. When used with a list, * means to copy the elements in a list n times.\n\nx = [None] * 10\nprint(x)\n\n[None, None, None, None, None, None, None, None, None, None]"
  },
  {
    "objectID": "notebooks/programming.html#conditionals-statements",
    "href": "notebooks/programming.html#conditionals-statements",
    "title": "1  Computer Programming",
    "section": "1.3 Conditionals Statements",
    "text": "1.3 Conditionals Statements\nSuppose we only wanted to execute a statement in some cases but not others. These are called conditional statements. Conditional statements are frequently formulated something like “if x is true then do y”. For example, suppose we had some whole number \\(x\\) and wanted to know if it was even or odd. If \\(x\\) is a multiple of \\(2\\) it’s even. Otherwise it’s odd.\nWe can check \\(x\\) is a multiple of \\(2\\) by seeing if its remainder when divided by \\(2\\) is either \\(0\\) or \\(1\\). If the remainder is \\(0\\) then \\(x\\) is even. If the remainder is \\(1\\) then \\(x\\) is odd. In python, we can calculate the remainder of \\(x \\div y\\) using the modulo or % operator. For example, if x = 5, then x % 2 is 1, meaning x is odd. If x = 6, then x % 2 is 0, meaning x is even.\n\nx = 5\nx % 2\n\n1\n\n\n\nx = 6\nx % 2\n\n0\n\n\nAnyway, suppose we didn’t know what x was and wanted to figure out if it was even or odd. We could define a boolean variable called is_even that we’ll set to True if x is even, and False if x is odd. As a conditional statement, this might look like the following,\nif x % 2 == 0 then set is_even = True\nif x % 2 == 1 then set is_even = False\nTo create a conditional statement in python we’d first create an if statement in the format if condition:. Then below that line, we’d indent over exactly four spaces (or one tab) and type what it is we want to happen if condition is true.\nif condition:\n    do_something\nWe’d do this for each conditional statement we want to check. Note that only the conditions indented get executed when the statement is true. Once you indent back out the statement will execute whether the statement is true or not. This means python is sensitive to indentation, since indentation refers to being inside of a block.\nHere’s what an if statement might look like for checking whether a number x is even. In that case, condition would be a boolean, either x % 2 == 0 or x % 2 == 1. Let’s see how this works for x = 5, and then x = 6.\n\nx = 5\n\nif x % 2 == 0:\n    is_even = True\nif x % 2 == 1:\n    is_even = False\n    \nprint(is_even)\n\nFalse\n\n\n\nx = 6\n\nif x % 2 == 0:\n    is_even = True\nif x % 2 == 1:\n    is_even = False\n    \nprint(is_even)\n\nTrue\n\n\nSometimes we might want to execute a chain of conditional statements. We might want to execute the second conditional only if the first one is true, the third conditional only if both the first and second are true, and the fourth condition if none of the other conditions are true. We can do this by using an if else if else style conditional. In this kind of statement there are three different conditionals, an if statement, an else if statement, and an else statement. The else if statement only gets executed if the if statement is false, and the else statement only gets executed if both statements are false.\nFor example, suppose we wanted to increment some number x by one, but only if x is less than some other variable y. But if x is greater than y, we want to increment y by one instead. Finally, if x equals y, we want to reset both values to zero. As a conditional statement this might look as follows:\nif x < y then increment x to x + 1\nelse if x > y then increment y to y + 1\nelse set x = 0 and y = 0\nIn python, to execute a complex conditional like this we just need to chain them in order. An else if condition is specified using elif in python, and an else condition is specified using else. The chain would look something like this.\nif condition:\n    do_something\nelif other_condition:\n    do_something_else\nelse last_condition:\n    do_something_else_again\nHere’s what it looks like for the previous example. I’ll first take x = 5 and y = 10, which should trigger the if condition and increment x to 6. Then I’ll take x = 7 and y = 4, which should trigger the else if condition and increment y to 5. Finally, I’ll take x = 1 and y = 1, which should trigger the else condition and set both to 0. To do the incrementing I’m going to use the shorthand operator +=. Writing x += 1 means exactly the same thing as x = x + 1. Both increment x by one.\nI’m also going to use a format string to print the output. A format string is a way to print the output of a variable inside a string. They’re just strings with an f placed before the left quote. To pass a variable x into a format string we just use {x} at the place in the string we want x to render. Format strings are useful for doing nice printing. Instead of just typing print(x), we can type something like print(f'x = {x}') to explicitly make it clear in the output what we’re printing. It’s more helpful for readers this way since they don’t have to try to figure out what it is you’re printing.\n\nx = 5\ny = 10\n\nif x < y:\n    print('the if got executed')\n    x += 1\nelif y < x:\n    print('the elif got executed')\n    y += 1\nelse:\n    print('the else got executed')\n    x = 0\n    y = 0\n    \nprint(f'x = {x}')\nprint(f'y = {y}')\n\nthe if got executed\nx = 6\ny = 10\n\n\n\nx = 7\ny = 4\n\nif x < y:\n    print('the if got executed')\n    x += 1\nelif y < x:\n    print('the elif got executed')\n    y += 1\nelse:\n    print('the else got executed')\n    x = 0\n    y = 0\n    \nprint(f'x = {x}')\nprint(f'y = {y}')\n\nthe elif got executed\nx = 7\ny = 5\n\n\n\nx = 1\ny = 1\n\nif x < y:\n    print('the if got executed')\n    x += 1\nelif y < x:\n    print('the elif got executed')\n    y += 1\nelse:\n    print('the else got executed')\n    x = 0\n    y = 0\n    \nprint(f'x = {x}')\nprint(f'y = {y}')\n\nthe else got executed\nx = 0\ny = 0\n\n\nIt’s kind of annoying to write these complex statements out like this every time. When we’re working with simple conditionals, python has a one-line way to write them called ternary statements. These are useful when we want to set a variable if a condition is true, otherwise set it to something else if the condition is false. Instead of writing this:\nif condition:\n    x = foo\nelse:\n    x = bar\nWe can write the following one-liner to define x:\nx = foo if condition else false\nIt may seem confusing to write a statement this way if you’re not used to it, but it’s actually more readable if you try to read the statement literally from left to right. You’ll get used to it. As an example, we could use a ternary statement like this to write the is_even example using one line. Here’s what it’d look like for x = 6.\n\nx = 6\n\nis_even = True if (x % 2 == 0) else False\n\nprint(is_even)\n\nTrue"
  },
  {
    "objectID": "notebooks/programming.html#loops",
    "href": "notebooks/programming.html#loops",
    "title": "1  Computer Programming",
    "section": "1.4 Loops",
    "text": "1.4 Loops\nLet’s go back to lists. Suppose we had a large list of numbers of arbitrary length, and we needed to find the largest element in the list. How would we go about doing that? With what we’ve covered so far, we’d literally have to index into every single element one-by-one, print them all out, and then manually find the largest number. That might be fine if the list contains like ten numbers. But suppose it contained a million numbers. We clearly need a smarter way to proceed. This brings us to the topic of loops. A loop is a systematic way of iterating over some iterable data structure. To iterate over the elements of a list array in python, we can use the following syntax:\nfor x in array:\n    do_something\nThis is called a for loop. What it’ll do is pick out each element x in the list from left to right, and with that element it’ll execute do_something. For example, doing something could just mean printing out x. Here’s an example. I’ll iterate over all the whole numbers from \\(0\\) to \\(9\\) and print them out.\n\narray = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\nfor x in array:\n    print(x)\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\nIn python, a convenient way to create an array like this is to use the range function. Writing range(10) would create a list of all the whole numbers from \\(0\\) to \\(10-1=9\\). Strictly speaking, range(10) is not a list but an iterator. That just means it doesn’t store the elements in the list directly, but iterates over them. When executing a loop this doesn’t really matter since we’re iterating one-by-one anyway. If you need a range object as an explicit list, just re-cast it the usual way using list(range(10)).\nRange iterators can do more interesting things too. If we wanted to iterate starting from some other number, say 5, we could just use range(5, 10) instead, which will create an iterator of whole numbers from \\(5\\) to \\(9\\) instead. We can also skip over numbers in the sequence too. For example, if we only wanted the even numbers from \\(-2\\) to \\(10\\), we could use range(-2, 10, 2). The 2 on the right says to start from -2, counting to 10-1 in steps of 2.\n\nfor x in range(10):\n    print(x)\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\nprint(range(10))\nprint(list(range(10)))\n\nrange(0, 10)\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n\n\nprint(list(range(5, 10)))\nprint(list(range(-2, 10, 2)))\n\n[5, 6, 7, 8, 9]\n[-2, 0, 2, 4, 6, 8]\n\n\nBack to our original loop example. Suppose we had a large list of numbers and wanted to find the largest one. How could we do this using a for loop? Well, what we can do is loop through the list, using another variable largest to keep track of the largest element we’ve seen so far. To do that we’ll need to use an if statement inside the loop to check whether largest is larger than the current value or not. Once we get to the end of the list, largest will be the largest in the entire list. For this to work, we have to pre-initialize largest to some value smaller than any element in the list. A reasonable choice would be negative infinity, which in python is denoted by float('-inf').\nHere’s an example, using the list [5, 2, 9, 6, 0, 1, 8]. The largest element should be 9 in this case. Note that python actually has a builtin function to find the largest element in a list, called max. We could just use that instead and save having to write out the loop. Similarly, we can get the smallest element in a list using the min function.\n\narray = [5, 2, 9, 6, 0, 1, 8]\n\nlargest = float('-inf')\nfor x in array:\n    if x > largest:\n        largest = x\n        \nprint(largest)\n\n9\n\n\n\nprint(max(array))\nprint(min(array))\n\n9\n0\n\n\nWe can loop over other iterable data structures as well, like tuples and dictionaries. When looping over a dictionary, note that the loop iterates over the keys, not the values. Since keys aren’t really ordered, you can’t really predict what order the dictionary will iterate in either.\nWe can have loops inside of loops as well. One for loop inside another for loop is called a double loop. Double loops might be used, for example, when you have to iterate over a list of lists. For example, suppose we needed to iterate over the following list and find its largest element. Each element of array is a list itself. To access the actual values in the array we’d need to index twice. For example, to get element 3 from list 2, we’d index using array[2][3].\n\narray = [\n    [1, 0, 1, 0, 1],\n    [1, 2, 1, 2, 1],\n    [1, 3, 1, 3, 1],\n    [1, 4, 1, 4, 1]\n]\nprint(array)\n\n[[1, 0, 1, 0, 1], [1, 2, 1, 2, 1], [1, 3, 1, 3, 1], [1, 4, 1, 4, 1]]\n\n\n\narray[2][3]\n\n3\n\n\nSuppose we wanted to find the largest element in this array. How could we do it programmatically? What we could do is use a double loop, with the outer loop running over the inner lists, and the inner loop running over the elements of each list. We’d again need to keep a largest variable. Note that using max won’t work as is for a list of lists like this. It’ll just return one of the inner lists, not a number.\n\nlargest = float('-inf')\nfor row in array:\n    for x in row:\n        if x > largest:\n            largest = x\n\nprint(largest)\n\n4\n\n\nSuppose we had a list array, and wanted to create another list from array whose elements are squares of the elements in array. One way to do this would be to use a for loop like this. We’d initialize an empty array squares, loop over array, and append the square of each x to squares. This will work fine, creating the array [1, 4, 9, 16, 25].\n\narray = [1, 2, 3, 4, 5]\n\nsquares = []\nfor x in array:\n    x_sq = x ** 2\n    squares.append(x_sq)\n    \nprint(squares)\n\n[1, 4, 9, 16, 25]\n\n\nAn even simpler way to create squares is to use a list comprehension. A list comprehension is a python shorthand syntax for creating a list by iterating over another list. We can fill in squares with the same numbers using the following single line of code:\nsquares = [x**2 for x in array]\nNotice that all we’re essentially doing is doing the loop over array inside of the list definition for squares. This allows us to cut four lines of code down to one. It’s also more readable, once you get used to it at least. Note that list comprehensions only work when it’s simple to define squares from array. If the operation is too complex you’re stuck writing out the full loop. You’d be surprised how much we can do with list comprehensions though. We can do double loops inside of a list comprehension, and even ternary statements.\n\nsquares = [x**2 for x in array]\nprint(squares)\n\n[1, 4, 9, 16, 25]\n\n\nI’ll just briefly mention one other type of loop that occasionally shows up, called a while loop. A for loop just iterates over a list or some iterable. A while loop keeps iterating as long as some condition is true, whether there’s an iterable involved or not. The syntax of a while loop in python is:\nwhile condition:\n    do_something\nHere’s a simple example of this. I’ll define a while loop that continues as long as some variable x is less than 5. Each step of the loop will increment x by one. In order to use a while loop like this we first have to make sure to initialize x to some value, otherwise x will be undefined inside the loop when we try to increment it. I’ll initialize x to 0 in this example.\n\nx = 0\nwhile x < 5:\n    x += 1\n    \nprint(x)\n\n5"
  },
  {
    "objectID": "notebooks/programming.html#functions",
    "href": "notebooks/programming.html#functions",
    "title": "1  Computer Programming",
    "section": "1.5 Functions",
    "text": "1.5 Functions\nVery often we find ourselves needing to perform the same set of computations over and over. It’s convenient therefore to be able to package re-used computations up in a way that makes them easier to reuse later on. The most common way to do this is using a function. A computational function is an abstraction for mapping inputs to some desired outputs using a sequence of computational steps that get abstracted away from the user.\nIt’s hard to underestimate how important functions are to programming. They allow us to abstract away many repetitive, complex tasks so we can focus on other things. Any time you find yourself reusing the same code over and over, rather than copy and paste it you should consider defining a function to execute that code for you. Doing this well is more an art than a science. It’s in essence the bread and butter of software engineering.\nIn python, a function is defined by specifying a function signature that states the name of the function and the inputs it takes in. Once the computation inside a function gets performed, its output gets returned via a return statement. Here’s a simple example of a function called function that takes in two inputs foo and bar, and returns an output output\ndef function(foo, bar):\n    do_something\n    return output\nOnce the function has been defined, it can be called and executed later on using a command like\noutput = function(foo, bar)\nYou can place pretty much any computation you like inside a function. Only the inputs and outputs get exposed to the user when the function is used. All of the inner computation gets abstracted away. The variables that get used inside the function are all local variables unless specified otherwise. That means they won’t be available outside the function. They’re created when the function is called, and immediately destroyed when the function returns. This contrasts with global variables, which are any variables that persist after a function is called. The inputs and outputs are examples of global variables.\nLet’s do an example. Suppose we got tired of writing out loops to find the largest elements in an array and wanted to package it inside a function that we can execute in one line, e.g. like the max function does. Let’s packages this sequence of steps inside a function I’ll call maximum. The function takes in a list array and returns the largest element largest.\n\ndef maximum(array):\n    largest = array[0]\n    for x in array:\n        if x > largest:\n            largest = x\n    return largest\n\n\narray = [5, 2, 9, 6, 0, 1, 8]\nlargest = maximum(array)\nprint(largest)\n\n9\n\n\nAs written, this function as I’ve defined it only works on lists of numbers. It won’t work on lists of lists of numbers. What if we wanted to find the largest element of those? We could just write another function to flatten the list into a list of numbers, and then use the previous function on the flattened list. Here’s how we might do that. I’ll first define a function flatten that takes in a list array. Inside the function, I’ll define a new array called output to store the flattened array I’d like to return. Now, what we can do is loop over array and check if a given element x is a number or a list. If it’s a list, we’ll concatenate it to the end of output. If it’s not a list, we’ll just append it to output.\nNotice something very subtle here. I’m actually calling the function flatten inside the function itself! Does this even make sense? Strangely enough it does. It’s called recursion. We can recursively call a function inside itself as long as we have some kind of condition to terminate the recursion at some point. This condition is called a base case. In this function the base case is implicit. We can’t have a list with infinitely many lists nested in. Eventually it must hit a number, which will get appended to the list instead of having to call flatten again.\nIn general, if you fail to specify a base case, recursion will keep going indefinitely, which will cause a stack overflow. A stack overflow results when you’ve used up all the memory your program is allowed to use, which will crash the kernel and force you to start over.\nSo you can see how recursion works, I’m going to define a nested list and call flatten on it, while printing the input each time the function gets called. You can see the it starts by printing the original array, but then it prints [1, 2], the first list inside of array. Since the elements in [1, 2] are just numbers we’ve hit the base case and we can move onto the next element in array, which is the list [3, [4, [5, 6]]]. The second element in this list is another list, [4, [5, 6]], so it’ll recurse again on that. And then it’ll recurse again on [5, 6]. Once it’s hit this point there are no more lists left. All the numbers have been added to output, and we can return. The final output should be the flattened array [1, 2, 3, 4, 5, 6].\n\ndef flatten(array):\n    print(array)\n    output = []\n    for x in array:\n        if isinstance(x, list):\n            output += flatten(x)\n        else:\n            output.append(x)\n    return output\n\narray = [[1, 2], [3, [4, [5, 6]]]]\nflattened = flatten(array)\n\n[[1, 2], [3, [4, [5, 6]]]]\n[1, 2]\n[3, [4, [5, 6]]]\n[4, [5, 6]]\n[5, 6]\n\n\n\nprint(flattened)\n\n[1, 2, 3, 4, 5, 6]\n\n\nWe can now use flatten combined with maximum to find the largest element in any list we like, no matter how many lists are nested inside it. Let’s test it out on the above example, whose largest element should be 6. Looks like it works fine, at least on the example tested.\n\nmaximum(flattened)\n\n6\n\n\nThis doesn’t mean the function will work fine for all inputs though. You have to be very careful in making sure you explicitly lay out what the valid inputs should be as well as thinking about what edge case inputs are and whether the function gives the right value for those. One edge case for maximum might be an empty list, []. What’s the largest element in an empty list? It’s kind of an ill-defined question that we’d need to think about how to handle. An easy convention is to just return None if the input list is empty. If we do that then the function as already defined will work fine even on an empty list.\nFunctions can also take in keyword arguments, which are optional inputs that can be used to modify the state of the function. With a keyword argument, we’d specify a default value. When we call a function and don’t pass in that keyword argument it’ll default to that default value. Otherwise it’ll use the value of what we pass in. Here’s what the signature might look like for a function that takes a keyword argument keyword with a default value default.\ndef function(input, keyword=default):\n    do_something\n    return output\nAs an example, suppose we wanted to modify maximum by specifying an optional return value if_empty_return when an empty list is passed in. If no keyword is passed in, we’ll assume that if_empty_return has a default value of float('-inf'). To actually use the keyword argument as intended, we need to check whether the input array is empty, and if it is return whatever if_empty_return is set to.\nI’ll test the function on two cases. The first has no keyword argument is passed in. That one should return None when an empty list is passed in. The second has a keyword if_empty_return='empty list has no elements'. This one should return the string 'empty list has no elements' when an empty list is passed in.\n\ndef maximum(array, if_empty_return=None):\n    if len(array) == 0:\n        return if_empty_return\n    largest = array[0]\n    for x in array:\n        if x > largest:\n            largest = x\n    return largest\n\n\nmaximum([])\n\n-inf\n\n\n\nmaximum([], if_empty_return='empty list has no elements')\n\n'empty list has no elements'\n\n\nFrequently we’ll find ourselves implementing functions that only perform a single line of computation. For these simple kinds of functions python has a nice syntax that we can use to implement them in one line. These are called lambda functions. Lambda functions can be defined using the following syntax:\nfunction = lambda input: output\nCalling these functions works exactly the same way as ordinary functions do: output = function(input). Lambda functions only work when the output is really simple to calculate from the input. Thankfully, in this book most functions you’ll see are simple enough to state using lambda functions. That’s because most of those functions will be mathematical functions like \\(f(x) = x^2\\) or \\(f(x, y) = \\sqrt{x + y}\\). These functions are simple enough to do in one line.\nHere’s an example of a function I’ll call square that takes in a number x and returns its square.\n\nsquare = lambda x: x**2\nsquare(3)\n\n9\n\n\nAnd here’s another function I’ll call sqrt that takes in two numbers x and y and computes the square too of their sum.\n\nsqrt = lambda x, y: (x + y)**(1/2)\nsqrt(2, 2)\n\n2.0\n\n\nThere are more advanced topics related to functions as well. For example, we can imagine objects that not only execute code, but also hold data as well. These are called classes. They do indeed come up in machine learning, but I’ll defer that topic to when we need to use them. Another extension of functions is the idea of higher-level functions, which in python are called decorators. I’ll defer these for later as well.\nOne other extension of functions though that we will use is the idea of imports and libraries. Suppose we’ve written a bunch of functions that we’d like to use not just in one notebook, but across many notebooks. What we can do is put them all into a python script and import them into any notebook when we’d like to use them. For example, we might place the function maximum in a file utils.py. If we want to use maximum in a notebook, we’d first call import utils, and then use maximum by calling utils.maximum. Alternatively, we could import maximum directly from utils.py by calling from utils import maximum, in which case we can call maximum directly without referencing utils. We’ll see examples of this kind of thing in future lessons."
  },
  {
    "objectID": "notebooks/programming.html#algorithms",
    "href": "notebooks/programming.html#algorithms",
    "title": "1  Computer Programming",
    "section": "1.6 Algorithms",
    "text": "1.6 Algorithms\nAt their root, computer programs are about executing a given sequence of instructions. The way those instructions are written depend on the programming language, but the instructions themselves do not. A sequence of instructions for performing some given task is called an algorithm. An algorithm abstracts away the idea of what language we’re coding in, what hardware we’re using, what operating system we’re running on, and focuses only on the instructions we want to be executed. Very roughly speaking, you can think of a program or a function as an algorithm.\nWe’ve already seen an example of an algorithm in this lesson: Finding the maximum element in a list. In its original form, its python function looked like this:\n\ndef maximum(array):\n    largest = array[0]\n    for x in array:\n        if x > largest:\n            largest = x\n    return largest\n\nOne way to think about this function is as a few lines of python code that get interpreted into machine code and then executed in hardware. Another way to think about it is as a sequence of instructions in English:\n\nTake in an input list of numbers, which we’ll call array.\nDefine a variable largest to keep track of the largest element seen in the array so far. Initialize largest to the value in the first element of array.\nIterate through the list from left to right.\nAt each iteration, compare the current value x in the array to largest. If x is bigger than largest, update largest to have the value of x.\nOnce we’ve reached the end of the array, the value of largest will be the maximum element in the array. Return largest.\n\nThese instructions define an algorithm for finding the maximum element in a list of numbers. The instructions have nothing whatsoever to do with code. You can execute them by hand and get the maximum element too. When we talk about an algorithm, we generally care most about two things:\n\nCorrectness: Is the algorithm correct? That is, for a given input, does it always return the correct output?\nEfficiency: Is the algorithm sufficiently fast? That is, for an input of a given size, is the algorithm optimal in the sense that it’s computing the output as efficiently as it possibly can?\n\nTo evaluate the correctness of an algorithm we have two options, a theoretical way, and a practical way. The theoretical way is to mathematically prove the algorithm gives the correct result for a given input. The practical way is to do unit testing. If an algorithm gives an incorrect output for some input we say it has a bug. To evaluate the efficiency of an algorithm we again have two options, a theoretical way, and a practical way. The theoretical way is to look at its asymptotic behavior as a function of the input size. The practical way is to do profiling.\n\n1.6.1 Correctness\nLet’s talk about correctness first. Suppose we wanted to prove that maximum is a correct algorithm. That is, for any list of numbers array containing at least one number, the output largest will always be the maximum element in the list. Let’s proceed by supposing the algorithm is not correct and seeing if it leads to a contradiction. If it does, the algorithm must be correct.\nClaim: The maximum algorithm is correct for any input list of numbers containing at least one number.\nProof: Suppose maximum is not correct. That is, there is some input list array containing at least one number for which maximum does not return the largest value in array. Suppose x is the maximum element in the array, and the returned value largest is not equal to x. Since x is in the array, iterating over the array must eventually get us to x. Once we’ve reached x, we check if x > largest. If it is, we update largest to equal x. This means that largest must be at least as big as x. Can it be bigger? Well no. It’s initialized to the first element in array, and it only gets updated with other elements in the array. We’ve thus found a contradiction, since we assumed maximum did not in fact return the largest value in array. Thus, maximum must be correct for any array of numbers with at least one element. \\(Q.E.D.\\)\nThat’s the theoretical way to show an algorithm is correct. And it’s nice if we can do it. But most algorithms in the real world are complex enough that we can’t really prove it’s correct for any given input. Instead we have a practical way to test correctness, called unit testing. With unit testing, instead of trying to show an algorithm is correct for all inputs, we instead create a few test cases we’d like to test the algorithm on. That is, we create some example inputs for which we already know what the output should be, and then check that the algorithm gives the output we expect on those examples.\nSuppose we wanted to use unit testing on maximum. What we’d do is define a few simple example inputs where we already know what the largest element is. The more examples the better, and the more varied and complex the examples are the better. When creating a set of examples, it’s good to think about edge cases inputs that might occur, and to test as many different edge cases as we can.\nLet’s think about what these might look like for maximum. One thing we might want to do is test the algorithm on a few random input lists, where the maximum element could be anywhere in the list. Another thing we might want to think about are the edge cases. One case, for example, might be when the input list is empty. What do we return then? Another case might be when all the elements in the list are the same value, for example all zeros or ones. Another case might be when an input list is passed in that contains things other than numbers, for example strings or other lists.\nI’ll write a new function called test_maximum to do this. It takes in no inputs, and returns no outputs. All it does is tests that maximum gives the right outputs on a few example input lists. To test each example I’ll use the python assert statement. The syntax for an assert statement is:\nassert statement, optional_error_message\nIf statement is false, python will print optional_error_message and throw an assertion error. If no error message is passed it’ll just throw an assertion error with no message. If statement is true it’ll continue on to the next example. I’ll test the maximum algorithm on the following example lists: - input [5, 4, 3, 2, 1]: output 5 - input [4, 3, -5, 7, 0]: output 7 - input [1, 1, 1, 1, 1]: output 1 - input range(10000): output 9999 - input []: output None\n\ndef test_maximum():\n    assert maximum([5, 4, 3, 2, 1]) == 5\n    assert maximum([4, 3, -5, 7, 0]) == 7\n    assert maximum([1, 1, 1, 1, 1]) == 1\n    assert maximum(range(10000)) == 9999\n    assert maximum([]) == None\n\n\ntest_maximum()\n\nIndexError: list index out of range\n\n\nIt looks like this test fails on the last example, which tests that the maximum of an empty list is None. This shouldn’t be surprising. After all we assumed when defining maximum that the input was a list of numbers with at least one element. If we did want an empty list to return None we can just modify maximum to first check if the input list is empty, and if it is return None. Let’s do that and then run the test again. It should pass now.\n\ndef maximum(array):\n    if len(array) == 0:\n        return None\n    largest = array[0]\n    for x in array:\n        if x > largest:\n            largest = x\n    return largest\n\n\ntest_maximum()\n\n\n\n1.6.2 Efficiency\nLet’s now talk about the efficiency of an algorithm. The theoretical way to measure the efficiency of an algorithm is asymptotic complexity. The idea is to suppose the input has an arbitrary size \\(n\\), which we assume to be really really large. I’m being deliberately ambiguous about what I mean by size, since it depends on what the inputs are. If the input is a list, then the size is usually the length of the list. If the input is a string, then the size might be the length of the string. If the input is a number, the size might be the number of bits needed to describe that number. Etc.\nNotice that I said with asymptotic complexity that we assume the size \\(n\\) to be really really large. That is, \\(n \\gg 1\\). This is why we say asymptotic complexity. Exactly how large \\(n\\) needs to be is, to be honest, subjective. Sometimes \\(n=10\\) will do. Sometimes we’d need \\(n=1,000,000\\) or even bigger. Anyway, the idea of asymptotic complexity is to figure out how many steps an algorithm is performing, as a mathematical function of the input size \\(n\\). What exactly I mean by a step is subjective too. Usually we’d think of each executed line of code as being a step, unless that line of code is doing a lot of work like calling another function or doing a list comprehension. Anyway, once we have the number of steps as a function of \\(n\\), we’d then approximate it by its leading order term.\nI haven’t yet said what exactly I mean by efficiency either. What kind of efficiency are we talking about? Usually when we say efficiency we mean two things: 1) we want the algorithm to be fast, in that it runs in as little time as possible? 2) We want the algorithm to use as little memory as possible in the computation. This means we’re usually interested in two different types of asymptotic complexity, the time of an algorithm as a function of the input size, and the space of an algorithm as a function of the input size.\nIt’s probably a good idea at this point to do an example. Let’s try to estimate both the asymptotic time and memory usage of our maximum algorithm. We’ll assume the input list has an arbitrary size \\(n \\gg 1\\). Let’s start by estimating the asymptotic time. For simplicity, I’ll count each single line of a function as one step, unless it’s calling a function or running a loop. This may seem weird. Why should, say, defining a variable take the same amount of time as dividing two large numbers? You’re right, it’s arbitrary. But when \\(n\\) is large it won’t really matter that much, as we’ll see.\nFor simplicity, let’s take the maximum function that ignores the empty list check. It doesn’t really make a difference. It’s just easier to count the original one. Here’s a count of each line:\n\nThe first line is the function signature. This is just a kind of definition, so we’ll assume it’s one step.\nThe second line sets largest to the first value in the input array. We generally assume indexing a list takes a single step. So we’ll count this line as one step.\nThe third line specifies the for loop over the elements in array. The loop runs over all \\(n\\) elements. We’ll count the loop definition itself as one step. But we have to keep in mind that the inner body of the loop is being executed \\(n\\) times, once for each element in the array.\nThe fourth line is an if statement that checks if x > largest. We’re just seeing if one number is larger than another, so we’ll again assume this takes one step.\nThe fifth line sets largest = x provided the if statement is satisfied. We’ll again assume this takes one step.\nFinally we return largest. We’ll again assume this takes one step.\n\nNow that we’ve counted all the steps executed we need to add them up. Steps (1) through (3) each took a single step, for a total of \\(2\\) steps. Steps (4) and (5) each took a single step for each element in the loop. Since the loop ran \\(n\\) times, that means \\(2n\\) total steps ran inside the loop. Finally, step (6) took a single step, so \\(1\\) more. In total then, maximum took \\(2 + 2n + 1 = 2n + 3\\) total steps. But, what we’re really interested in is the total time, not the total number of steps. Let’s assume a single step takes some time \\(t\\) to run, on average. Then all we need to do to get the total time is to multiply the total number of steps by the time per step \\(t\\). Let’s call the total time \\(T(n)\\), since it’s a function of \\(n\\). Then we have\n\\[T(n) = (2n + 3) t = 2tn + 3t.\\]\nSo far we haven’t used the fact that \\(n \\gg 1\\) at all. Let’s now suppose that \\(n\\) is really really big. What will that mean then for the total time \\(T(n)\\)? Suppose for sake of argument that \\(n\\) is a million and \\(t=1\\). Then the term \\(2tn\\) will be two million, and the term \\(3t\\) will just be three. This means \\(2tn\\) is really really big compared to \\(3t\\), so for all practical purposes we have that \\(T(n) \\approx 2tn\\). The added constant \\(3t\\) is practically a rounding error.\nFor reasons I don’t necessarily agree with, it’s common to take one more step after this, which is to completely ignore the constant factors multiplying the leading power of \\(n\\). Instead of writing \\(T(n) = 2tn\\), we’d ignore the factor \\(2t\\) and just write \\(T(n) = O(n)\\). Read this as “the algorithm has an asymptotic time complexity on the order of \\(n\\)”. The notation \\(O(n)\\) is called big-O notation. The \\(O\\) just stands for on the order of.\nAs a good rule of thumb, the number of steps executed by the deepest nested loop will give you the asymptotic runtime of your algorithm. When counting this way, you have to be careful to count list comprehensions as loops, as well as any function that iterates over a list or any other iterable object. This rule of thumb will be true as long as there are no other function calls inside the algorithm, or recursions. For maximum, there was only a single loop that runs \\(n\\) times. Since no other function was being called, we can immediately conclude \\(T(n) = O(n)\\) in this case. If there was a double loop over the input we’d conclude \\(T(n) = O(n^2)\\). Etc. If there are other function calls you need to figure out what the runtime is of that function call. If there’s a recursion things can be more complicated.\nWhen \\(T(n) = O(n)\\) we’d say the algorithm is a linear time algorithm, since it’s linear in the input size \\(n\\). If \\(T(n) = O(n^2)\\) we’d say it’s a quadratic time algorithm. If \\(T(n) = O(n^3)\\) we’d say it’s a cubic time algorithm. Etc. One example of an algorithm that has quadratic time is counting the number of subsequences inside of a list. That is, the number of possible ways to slice the list and get a distinct sub-list. One example of a cubic time algorithm is matrix multiplication, something we’ll study in great detail in future lessons.\nPerhaps even more strange, it’s not even required that an algorithm’s asymptotic time be a power of \\(n\\). For example, recursive algorithms are quite often proportional to the logarithm of \\(n\\). Searching a sorted list for a value is an example of an \\(O(\\log n)\\) time algorithm. Sorting a list is an example of an \\(O(n \\log n)\\) time algorithm. Convolving an image with a filter is an example of an \\(O(n^2 \\log n)\\) algorithm. Etc. We can even have exponential time algorithms, i.e. algorithms that run in \\(O(2^n)\\) time. Exponential time algorithms are really slow, so slow you quite often wouldn’t use them. As a general rule,\n\\[O(1) \\ll O(\\log n) \\ll O(n) \\ll O(n \\log n) \\ll O(n^2) \\ll O(n^2 \\log n) \\ll O(n^3) \\ll \\cdots \\ll O(2^n).\\]\nThe fastest algorithms of all are the ones that don’t depend on the input size at all. For example, if all maximum did was return the string Hello World no matter what you passed in, it would be a constant time algorithm. Of course then it wouldn’t be correct. When an algorithm is constant time we’d write \\(O(1)\\).\nVery roughly speaking, we’d say an algorithm is fast if it’s less than cubic time, though this depends a lot on what the problem is and how big \\(n\\) can get. With this definition, we’d say our maximum algorithm is fast. Is it the fastest algorithm we could list to find the maximum of a list? It turns out it is, at least for an arbitrary unsorted list. If the list had some kind of structure we might be able to find the maximum faster, perhaps in \\(O(\\log n)\\) or even \\(O(1)\\) time.\nLet’s now try to count the total amount of memory or space the maximum algorithm uses. This works exactly the same way as counting the time, except we’re keeping track of the size of objects in the algorithm instead of the number of steps it takes to run. Suppose again the input is a list of size \\(n\\). Let’s first count the number of total elements in every object the algorithm sees. By convention, we ignore the size of the input when counting elements. We only count the elements in the intermediate objects and the output. Here’s a line-by-line count:\n\nThe first line is the function signature. We’ll assume no elements appear here.\nThe second line sets largest to the first value in the input array. We’re just setting a variable equal to a single element, i.e. the number array[0], which is one element. Since we’re copying array[0] into largest we have to count this element.\nThe third line specifies the for loop over the elements in array. The loop runs over all \\(n\\) elements. Here we have to be careful that the loop isn’t running over an object with multiple elements that we haven’t already counted. In this case it’s running over the input array, whose elements we’re ignoring. We’ll thus say this line has no new elements in it.\nThe fourth line is an if statement that checks if x > largest. There’s nothing new to count here, so we’ll again assume no new elements appear.\nThe fifth line sets largest = x provided the if statement is satisfied. This is just setting a variable equal to a number. But since we’ve already counted largest in line (2), we shouldn’t count it again. Thus, no new elements appear on this line.\nFinally we return largest. Since we’ve already counted largest, no new elements appear.\n\nIn total, we’ve only counted one element being used in maximum, namely the variable largest, which always stores a single number. Again, what we really care about though is how much memory is being used, not how many elements are being counted. Assume that each element, on average, uses \\(m\\) bits of memory. If we denote the total memory usage of the algorithm by \\(M(n)\\), then we’ve just shown that maximum has a total memory usage of\n\\[M(n) = m.\\]\nWe can play exactly the same tricks with memory that we did with time. Assume \\(n\\) is really big. Since \\(M(n) = m\\) doesn’t depend at all on \\(n\\) it’s clearly a constant function. That is, \\(M(n) = O(1)\\). The maximum algorithm thus uses constant asymptotic memory. We can thus finally summarize the asymptotic efficiency of the maximum algorithm as\n\\[T(n) = O(n), \\qquad M(n) = O(1).\\]\nJust as with asymptotic time, asymptotic memory can be proportional to powers of \\(n\\), as well as to \\(\\log n\\) or \\(2^n\\). Usually it’ll be proportional to a power of \\(n\\) unless some kind of recursion is going on. As a good rule of thumb, your asymptotic memory usage will be the size of the largest non-input object that appears in the function. This will be true as long as there aren’t recursive calls showing up. If they do show up things get more complicated.\nAll this asymptotics stuff is all well and good. It gives a half decent proxy for how slow an algorithm will be or how much memory it uses. But remember that it’s doing a lot of coarse graining. In calculating \\(T(n)\\) and \\(M(n)\\) we’re ignoring a lot of important details, like what hardware the algorithm is running on, what language it’s programmed in, whether things are being run in parallel, what else the operating system is running, etc. In practice, a much better way to estimate how slow an algorithm is, or how much memory it’s using, is to use a profiler. A profiler is a tool that runs a given routine and analyzes how much time or memory is being used, usually averaged over a bunch of runs.\nIn Jupyter Notebook, the easiest way to profile a function is to use the %timeit magic. A magic is any special command prefixed by % or %%. One of those special commands is timeit, which runs a given statement a bunch of times and estimates the average amount of time it takes to execute that statement. Let’s use timeit to profile our maximum function and see how long it’s taking to run. Just for fun, let’s compare it to python’s builtin max function. Theoretically both algorithms should be \\(O(n)\\). But what happens in practice? I’ll run both functions using the same fairly big input range(10000).\n\n%timeit maximum(range(10000))\n\n425 µs ± 5.8 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n\n\n\n%timeit max(range(10000))\n\n185 µs ± 4.46 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n\n\nEven though both algorithms are linear time and perform the same task, python’s builtin max function seems to run about twice as fast as our custom maximum function. What gives? Well, timeit is capturing not just the \\(O(n)\\) nature of the algorithm, but also the practical aspects as well. Things like, what processor are the operations running on, what priority does the operating system give the operations relative to other things the computer is doing, whether any operations are being run in parallel, what programming language is being used, how efficiently the memory hierarchy is being managed, etc. Algorithmic complexity captures none of these important things, while profilers capture essentially all of them.\nNot only can we profile the total runtime of a program, but we can even profile a program line by line to see which lines are running slow. In Jupyter you can do this using the %lprun magic. We can also profile the memory taken up by a program, but as a whole or line-by-line. In Jupyter you can do these with the %memit and %mprun magics. Usually we’ll be more worried about runtime than memory, but it is occasionally useful to profile the memory too. One major example is when running large models on the GPU. Since GPUs have a relatively small amount of memory, we have to be careful not to run out of GPU memory, which means it’s often useful to run a memory profiler on a training or inference loop."
  },
  {
    "objectID": "notebooks/basic-math.html#elementary-math",
    "href": "notebooks/basic-math.html#elementary-math",
    "title": "2  Basic Math",
    "section": "2.1 Elementary Math",
    "text": "2.1 Elementary Math\nIt’s useful in machine learning to be able to read and manipulate basic arithmetic and algebraic equations, particularly when reading research papers, blog posts, or documentation. I won’t go into depth on the basics of high school arithmetic and algebra. I do have to assume some mathematical maturity of the reader, and this seems like a good place to draw the line. I’ll just mention a few key points.\n\n2.1.0.1 Numbers\nRecall that numbers can come in several forms. We can have,\n\nNatural Numbers: These are positive whole numbers \\(0, 1, 2, 3, 4, \\cdots\\). Note the inclusion of \\(0\\) in this group. Following the computer science convention I’ll tend to do that. The set of all natural numbers is denoted by the symbol \\(\\mathbb{N}\\).\nIntegers: These are any whole numbers \\(\\cdots, -2, -1, 0, 1, 2, \\cdots\\), positive, negative, and zero. The set of all integers is denoted by the symbol \\(\\mathbb{Z}\\).\nRational Numbers: These are any ratios of integers, for example \\[\\frac{1}{2}, \\frac{5}{4}, -\\frac{3}{4}, \\frac{1000}{999}, \\cdots.\\] Any ratio will do, so long as the denominator (the bottom number) is not zero. The set of all rational numbers is denoted by the symbol \\(\\mathbb{Q}\\).\nReal Numbers: These are any arbitrary decimal numbers on the number line, for example \\[1.00, \\ 5.07956, \\ -0.99999\\dots, \\ \\pi=3.1415\\dots, \\ e=2.718\\dots, \\ \\cdots.\\] They include as a special case both the integers and the rational numbers, but also include numbers that can’t be represented as fractions, like \\(\\pi\\) and \\(e\\). The set of all real numbers is denoted by the symbol \\(\\mathbb{R}\\).\nComplex numbers: These are numbers with both real and imaginary parts, like \\(1 + 2i\\) where \\(i=\\sqrt{-1}\\). Complex numbers include the real numbers as a special case. Since they don’t really show up in machine learning we won’t deal with these after this. The set of all complex numbers is denoted by the symbol \\(\\mathbb{C}\\).\n\n\n\n2.1.0.2 Basic Algebra\nYou should be familiar with the usual arithmetic operations defined on these systems of numbers. Things like addition, subtraction, multiplication, and division. You should also at least vaguely recall the order of operations, which defines the order in which complex arithmetic operations with parenthesis are carried out. For example,\n\\[(5+1) \\cdot \\frac{(7-3)^2}{2} = 6 \\cdot \\frac{4^2}{2} = 6 \\cdot \\frac{16}{2} = 6 \\cdot 8 = 48.\\]\nYou should be able to manipulate and simplify simple fractions by hand. For example,\n\\[\\frac{3}{7} + \\frac{1}{5} = \\frac{3 \\cdot 5 + 1 \\cdot 7}{7 \\cdot 5} = \\frac{22}{35} \\approx 0.62857.\\]\nAs far as basic algebra goes, you should be familiar with algebraic expressions like \\(x+5=7\\) and be able to solve for the unknown variable \\(x\\),\n\\[x=7-5=2.\\]\nYou should be able to take an equation like \\(ax + b = c\\) and solve it for \\(x\\) in terms of coefficients \\(a, b, c\\),\n\\[\\begin{align*}\nax + b &= c \\\\\nax &= c - b \\\\\nx &= \\frac{c - b}{a}.\n\\end{align*}\\]\nYou should also be able to expand simple expressions like this,\n\\[\\begin{align*}\n(ax - b)^2 &= (ax - b)(ax - b) \\\\\n&= (ax)^2 - (ax)b - b(ax) + b^2 \\\\\n&= a^2x^2 - abx - abx + b^2 \\\\\n&= a^2x^2 - 2abx + b^2.\n\\end{align*}\\]\n\n\n2.1.0.3 Sets and Intervals\nIt’s also worth recalling what a set is. Briefly, a set is a collection of unique elements. Usually those elements are numbers. To say that an element \\(x\\) is an element of a set \\(S\\), we’d write \\(x \\in S\\), read “\\(x\\) is in \\(S\\)”. If \\(x\\) is not in the set, we’d write \\(x \\notin S\\). For example, the set of elements \\(1, 2, 3\\) can be denoted \\(S = \\{1, 2, 3\\}\\). Then \\(1 \\in S\\), but \\(5 \\notin S\\).\nI’ve already mentioned the most common sets we’ll care about, namely the natural numbers \\(\\mathbb{N}\\), integers \\(\\mathbb{Z}\\), rational numbers \\(\\mathbb{Q}\\), and real numbers \\(\\mathbb{R}\\). Also of interest will be the intervals,\n\nOpen interval: \\((a, b) = \\{x: a < x < b \\}\\).\nHalf-open left interval: \\((a, b] = \\{x: a < x \\leq b \\}\\).\nHalf-open right interval: \\([a, b) = \\{x: a \\leq x < b \\}\\).\nClosed interval: \\([a, b] = \\{x: a \\leq x \\leq b \\}\\).\n\nThink of intervals as representing line segments on the real line, connecting the endpoints \\(a\\) and \\(b\\). An open interval excludes the endpoints, a closed interval includes both endpoints, and the half open intervals include one or the other.\n\n\n2.1.1 Symbolic vs Numerical Computation\nThere are two fundamental ways to perform mathematical computations: numerical computation, and symbolic computation. You’re familiar with both even though you may not realize it. Numerical computation involves crunching numbers. You plug in numbers, and get out numbers. When you type something like 10.5 / 12.4 in python, it will return a number, like 0.8467741935483871. This is numerical computation.\n\n\nCode\n10.5 / 12.4\n\n\n0.8467741935483871\n\n\nThis contrasts with a way of doing computations that you learned in math class, where you manipulate symbols. This is called symbolic computation. Expanding an equation like \\((ax-b)^2\\) to get \\(a^2x^2 - 2abx + b^2\\) is an example of a symbolic computation. You see the presence of abstract variables like \\(x\\) that don’t have a set numeric value.\nUsually in practice we’re interested in numerical computations. We’ll mostly be doing that in this book. But sometimes, when working with equations, we’ll need to do symbolic computations as well. Fortunately, python has a library called SymPy, or sympy, that can do symbolic computation automatically. I won’t use it a whole lot in this book, but it will be convenient in a few places to show you that you don’t need to manipulate mathematical expressions by hand all the time.\nI’ve already imported sympy with the alias sp. Before defining a function to operate on, we first have to encode all the symbols in the problem as sympy Symbol objects. Once that’s done, we can create equations out of them and perform mathematical operations.\nHere’s an example of using sympy to expand the equation above, \\((ax-b)^2\\).\n\n\nCode\na = sp.Symbol('a')\nb = sp.Symbol('b')\nx = sp.Symbol('x')\na, b, x\n\n\n(a, b, x)\n\n\n\n\nCode\nequation = (a * x - b) ** 2\nexpanded = sp.expand(equation, x)\nprint(f'expanded equation: {expanded}')\n\n\nexpanded equation: a**2*x**2 - 2*a*b*x + b**2\n\n\nWe can also use sympy to solve equations. Here’s an example of solving the quadratic equation \\(x^2 = 6\\) for its two roots, \\(x = \\pm \\sqrt{6}\\).\n\n\nCode\nequation = x**2 - 6\nsolutions = sp.solve(equation, x)\nprint(f'solutions = {solutions}')\n\n\nsolutions = [-sqrt(6), sqrt(6)]\n\n\nSympy has a lot of functionality, and it can be a very difficult library to learn due to its often strange syntax for things. Since we won’t really need it all that often I’ll skip the in depth tutorial. See the documentation if you’re interested."
  },
  {
    "objectID": "notebooks/basic-math.html#univariate-functions",
    "href": "notebooks/basic-math.html#univariate-functions",
    "title": "2  Basic Math",
    "section": "2.2 Univariate Functions",
    "text": "2.2 Univariate Functions\nAs I’m sure you’ve seen before, a mathematical function is a rule that maps an input \\(x\\) to an output \\(y\\). More formally, a function \\(f(x)\\) is a rule that takes in a value \\(x\\) and maps it to a unique value \\(y=f(x)\\). These values can be either single numbers (called scalars), or multiple numbers (vectors or tensors). When \\(x\\) and \\(y\\) are both scalars, \\(f(x)\\) is called a univariate function.\nLet’s quickly cover some of the common functions you’d have seen before in a math class, focusing mainly on the ones that show up in machine learning. I’ll also cover a couple machine-learning specific functions you perhaps haven’t seen before.\n\n2.2.1 Affine Functions\nThe most basic functions to be aware of are the straight-line functions: constant functions, linear functions, and affine functions:\n\nConstant functions: \\(y=c\\) or \\(x=c\\)\n\nExamples: \\(y=2\\), \\(x=1\\)\n\nLinear functions: \\(y=ax\\)\n\nExamples: \\(y=-x\\), \\(y=5x\\)\n\nAffine functions: \\(y=ax+b\\)\n\nExamples: \\(y=-x+1\\), \\(y=5x-4\\)\n\n\nAll constant functions are linear functions, and all linear functions are affine functions. In the case of affine functions, the value \\(b\\) is called the intercept. It corresponds to the value where the function crosses the y-axis. The value \\(a\\) is called the slope. It corresponds to the steepness of the curve, i.e. its height over its width (or “rise” over “run”). Notice linear functions are the special case where the intercept is always the origin \\(x=0, y=0\\).\n\n2.2.1.1 Plotting\nWe can plot these and any other univariate function \\(y=f(x)\\) in the usual way you learned about in school. We sample a lot of \\((x,y)\\) pairs from the function, and plot them on a grid with a horizontal x-axis and vertical y-axis.\nBefore plotting some examples I need to mention that plotting in python is usually done with the matplotlib library. Typically what we’d do to get a very simple plot is:\n\nImport plt, which is the alias to the submodule matplotlib.pyplot\nGet a grid of x values we want to plot, e.g. using np.linspace or np.arange\nGet a grid of y values either directly, or by first defining a python function f(x)\nPlot x vs y by calling plt.(x, y), followed by plt.show().\n\nNote step (2) requires another library called numpy to create the grid of points. You don’t have to use numpy for this, but it’s typically easiest. Usually numpy is imported with the alias np. Numpy is python’s main library for working with numerical arrays. We’ll cover it in much more detail in future lessons.\nHere’s an example of how you might create a very crude plot with matplotlib. The following code would plot the function \\(y=x-1\\) from \\(x=-10\\) to \\(x=10\\). To do that, we’d first define an input grid of 100 x values using either np.linspace(-10, 10, 100) or np.arange(-10, 10, 0.2). Then, we’d define an output grid y by taking y = x - 1. This will calculate \\(y=x-1\\) for all \\(x\\) values in the grid at once. Finally, we’ll plot the inputs and outputs using plt.plot(x, y). The line plt.show() is usually called last. It tells matplotlib to render the plot.\nx = np.linspace(-10, 10, 100)\ny = x - 1\nplt.plot(x, y)\nplt.show()\nFor the rest of the plotting in this lesson I’m going to use a helper function plot_function, which takes in x and y, the range of x values we want to plot, and an optional title. I didn’t think the details of this helper function were worth going into now, so I abstracted it away into the file utils.py in this same directory. It uses matplotlib like I described, but with a good bit of styling to make the plot more readable. If you really want to see the details perhaps the easiest thing to do is create a cell below using ESC-B and type the command ??plot_function, which will print the code inside the function as the output.\nBack to talking about functions, let’s plot one example each of a constant function \\(y=2\\), a linear function \\(y=2x\\), and an affine function \\(2x-1\\).\n\n\nCode\nx = np.arange(-10, 10, 0.1)\nf = lambda x: 2 * np.ones(len(x))\nplot_function(x, f, xlim=(-5, 5), ylim=(-5, 5), ticks_every=[1, 1], \n              title='Constant Function: $y=2$')\n\n\n\n\n\n\n\n\n\n\n\nCode\nx = np.arange(-10, 10, 0.1)\nf = lambda x: 2 * x\nplot_function(x, f, xlim=(-5, 5), ylim=(-5, 5), ticks_every=[1, 1], \n              title='Linear Function: $y=2x$')\n\n\n\n\n\n\n\n\n\n\n\nCode\nx = np.arange(-10, 10, 0.1)\nf = lambda x: 2 * x - 1\nplot_function(x, f, xlim=(-5, 5), ylim=(-5, 5), ticks_every=[1, 1], \n              title='Affine Function: $y=2x-1$')\n\n\n\n\n\n\n\n\n\n\n\n\n2.2.2 Polynomial Functions\nPolynomial functions are just sums of positive integer powers of \\(x\\), e.g. something like \\(y=3x^2+5x+1\\) or \\(y=x^{10}-x^{3}+4\\). The highest power that shows up in the function is called the degree of the polynomial. For example, the above examples have degrees 2 and 10 respectively. Polynomial functions tend to look like lines, bowls, or roller coasters that turn up and down some number of times.\nA major example is the quadratic function \\(y=x^2\\), which is just an upward-shaped bowl. Its bowl-shaped curve is called a parabola. We can get a downward-shaped bowl by flipping the sign to \\(y=-x^2\\).\n\n\nCode\nx = np.arange(-10, 10, 0.1)\nf = lambda x: x ** 2\nplot_function(x, f, xlim=(-5, 5), ylim=(0, 10), ticks_every=[1, 1], \n              title='Quadratic Function: $y=x^2$')\n\n\n\n\n\n\n\n\n\nThe next one up is the cubic function \\(y=x^3\\). The cubic looks completely different from the bowl-shaped parabola.\n\n\nCode\nx = np.arange(-10, 10, 0.1)\nf = lambda x: x ** 3\nplot_function(x, f, xlim=(-5, 5), ylim=(-5, 5), ticks_every=[1, 1], \n              title='Cubic Function: $y=x^3$')\n\n\n\n\n\n\n\n\n\nPolynomials can take on much more interesting shapes than this. Here’s a more interesting polynomial degree 10,\n\\[y = (x^2 - 1)^5 - 5(x^2 - 1)^4 + 10(x^2 - 1)^3 - 10(x^2 - 1)^2 + 5(x^2 - 1) - 1.\\]\n\n\nCode\nx = np.arange(-10, 10, 0.1)\ndef f(x): \n    y = (x**2 - 1)**5 - 5 * (x**2 - 1)**4 + 10 * (x**2 - 1)**3 - \n    10 * (x**2 - 1)**2 + 5 * (x**2 - 1) - 1\nplot_function(x, f, xlim=(-3, 3), ylim=(-40, 40), ticks_every=[1, 10], \n              title='Arbitrary Polynomial')\n\n\n\n\n\n\n\n\n\n\n\n2.2.3 Rational Functions\nRational functions are functions that are ratios of polynomial functions. Examples might be \\(y=\\frac{1}{x}\\), or\n\\[y=\\frac{x^3+x+1}{x^2-1}.\\]\nThese functions typically look kind of like polynomial functions, but have points where the curve “blows up” to positive or negative infinity. The points where the function blows up are called poles or asymptotes.\nHere’s a plot of the function\n\\[y=\\frac{x^3+x+1}{x^2-1}.\\]\nNotice how weird it looks. There are asymptotes (the vertical lines) where the function blows up at \\(\\pm 1\\), which is where the denominator \\(x^2-1=0\\).\n\n\nCode\nx = np.arange(-10, 10, 0.01)\nf = lambda x: (x ** 3 + x + 1) / (x ** 2 - 1)\nplot_function(x, f, xlim=(-5, 5), ylim=(-5, 5), ticks_every=[1, 1], \n              title='Rational Function')\n\n\n\n\n\n\n\n\n\nHere’s a plot of \\(y=\\frac{1}{x}\\). There’s an asymptote at \\(x=0\\). When \\(x > 0\\) it starts at \\(+\\infty\\) and tapers down to \\(0\\) as \\(x\\) gets large. When \\(x < 0\\) it does the same thing, except flipped across the origin \\(x=y=0\\). This is an example of an odd function, a function that looks like \\(f(x)=-f(x)\\), which is clear in this case since \\(1/(-x)=-1/x\\). Functions like the linear function \\(y=x\\) and the cubic function \\(y=x^3\\) are also odd functions.\n\n\nCode\nx = np.arange(-10, 10, 0.1)\nf = lambda x: 1 / x\nplot_function(x, f, xlim=(-5, 5), ylim=(-5, 5), ticks_every=[1, 1], \n              title='Odd Function: $y=1/x$')\n\n\n\n\n\n\n\n\n\nA related function is \\(y=\\frac{1}{|x|}\\). The difference here is that \\(|x|\\) can never be negative. This means \\(f(x)=f(-x)\\). This is called an even function. Functions like this are symmetric across the y-axis. The quadratic function \\(y=x^2\\) is also an even function.\n\n\nCode\nx = np.arange(-10, 10, 0.1)\nf = lambda x: 1 / np.abs(x)\nplot_function(x, f, xlim=(-5, 5), ylim=(-1, 5), ticks_every=[1, 1], \n              title='Even Function: $y=1/|x|$')\n\n\n\n\n\n\n\n\n\n\n\n2.2.4 Power Functions\nFunctions that look like \\(y=\\frac{1}{x^n}\\) for some \\(n\\) are sometimes called inverse, hyperbolic. These can be represented more easily by using a negative power like \\(y=x^{-n}\\), which means the exact same thing as \\(y=\\frac{1}{x^n}\\).\nWe can extend \\(n\\) to deal with things like square roots or cube roots or any kind of root as well by allowing \\(n\\) to be non-integer. For example, we can represent the square root function \\(y=\\sqrt{x}\\) as \\(y=x^{1/2}\\), and the cube root \\(y=\\sqrt[3]{x}\\) as \\(y=x^{1/3}\\). Roots like these are only defined when \\(x \\geq 0\\).\nThe general class of functions of the form \\(y=x^p\\) for some arbitrary real number \\(p\\) are often called power functions.\nHere’s a plot of what the square root function looks like. Here \\(y\\) grows slower than a linear function, but still grows arbitrarily large with \\(x\\).\n\n\nCode\nx = np.arange(0, 10, 0.1)\nf = lambda x: np.sqrt(x)\nplot_function(x, f, xlim=(0, 5), ylim=(-2, 4), ticks_every=[1, 1], \n              title='Square Root: $y=\\sqrt{x}=x^{1/2}$')\n\n\n\n\n\n\n\n\n\nPower functions obey the following rules:\n\n\n\n\n\n\n\nRule\nExample\n\n\n\\(x^0 = 1\\)\n\\(2^0 = 1\\)\n\n\n\\(x^{m+n} = x^m x^n\\)\n\\(3^{2+5} = 3^2 3^5 = 3^8 = 6561\\)\n\n\n\\(x^{m-n} = \\frac{x^m}{x^n}\\)\n\\(3^{2-5} = \\frac{3^2}{3^5} = 3^{-3} \\approx 0.037\\)\n\n\n\\(x^{mn} = (x^m)^n\\)\n\\(2^{2 \\cdot 5} = (2^2)^5 = 2^{10} = 1024\\)\n\n\n\\((xy)^n = x^n y^n\\)\n\\((2 \\cdot 2)^3 = 2^3 2^3 = 4^3 = 2^6 = 64\\)\n\n\n\\(\\big(\\frac{x}{y}\\big)^n = \\frac{x^n}{y^n}\\)\n\\(\\big(\\frac{2}{4}\\big)^3 = \\frac{2^3}{4^3} = \\frac{1}{8}\\)\n\n\n\\(\\big(\\frac{x}{y}\\big)^{-n} = \\frac{y^n}{x^n}\\)\n\\(\\big(\\frac{2}{4}\\big)^{-3} = \\frac{4^3}{2^3} = 2^3 = 8\\)\n\n\n\\(x^{1/2} = \\sqrt{x} = \\sqrt[2]{x}\\)\n\\(4^{1/2} = \\sqrt{4} = 2\\)\n\n\n\\(x^{1/n} = \\sqrt[n]{x}\\)\n\\(3^{1/4} = \\sqrt[4]{3} \\approx 1.316\\)\n\n\n\\(x^{m/n} = \\sqrt[n]{x^m}\\)\n\\(3^{3/4} = \\sqrt[4]{3^3} = \\sqrt[4]{9} \\approx 1.732\\)\n\n\n\\(\\sqrt[n]{xy} = \\sqrt[n]{x} \\sqrt[n]{y}\\)\n\\(\\sqrt[4]{3 \\cdot 2} = \\sqrt[4]{3} \\sqrt[4]{2} \\approx 1.565\\)\n\n\n\\(\\sqrt[n]{\\frac{x}{y}} = \\frac{\\sqrt[n]{x}}{\\sqrt[n]{y}}\\)\n\\(\\sqrt[4]{\\frac{3}{2}} = \\frac{\\sqrt[4]{3}}{\\sqrt[4]{2}} \\approx 1.107\\)\n\n\n\nIt’s important to remember that power functions do not distribute over addition, i.e.\n\\[(x+y)^n \\neq x^n + y^n,\\]\nand by extension nor do roots,\n\\[\\sqrt[n]{x+y} \\neq \\sqrt[n]{x} + \\sqrt[n]{y}.\\]\n\n\n2.2.5 Exponentials and Logarithms\nTwo very important functions are the exponential function \\(y=\\exp(x)\\) and the logarithm function \\(y=\\log(x)\\). They show up surprisingly often in machine learning and the sciences, certainly more than most other special functions do.\nThe exponential function can be written as a power by defining a number \\(e\\) called Euler’s number, given by \\(e = 2.71828\\dots\\) . Like \\(\\pi\\), \\(e\\) is an example of an irrational number, i.e. a number that can’t be represented as a ratio of integers. Using \\(e\\), we can write the exponential function in the more usual form \\(y=e^x\\), where it’s roughly speaking understood that we mean “multiply \\(e\\) by itself \\(x\\) times”. For example, \\(\\exp(2) = e^2 = e \\cdot e\\).\nThe logarithm is defined as the inverse of the exponential function. It’s the unique function satisfying \\(\\log(\\exp(x)) = x\\). The opposite is also true since the exponential must then be the inverse of the logarithm function, \\(\\exp(\\log(x)) = x\\). This gives a way of mapping between the two functions,\n\\[\\log(a) = b \\quad \\Longleftrightarrow \\quad \\exp(b) = a.\\]\nHere are some plots of what the exponential and logarithm functions look like. The exponential function is a function that blows up very, very quickly. The log function grows very, very slowly (much more slowly than the square root does).\nNote the log function is only defined for positive-valued numbers \\(x \\geq 0\\), with \\(\\log(+0)=-\\infty\\). This is dual to the exponential function only taking on \\(y \\geq 0\\).\n\n\nCode\nx = np.arange(-5, 5, 0.1)\nf = lambda x: np.exp(x)\nplot_function(x, f, xlim=(-5, 5), ylim=(-1, 10), ticks_every=[1, 2], \n              title='Exponential Function: $y=\\exp(x)$')\n\n\n\n\n\n\n\n\n\n\n\nCode\nx = np.arange(0.01, 5, 0.1)\nf = lambda x: np.log(x)\nplot_function(x, f, xlim=(-1, 5), ylim=(-5, 2), ticks_every=[1, 1], \n              title='Logarithm Function: $y=\\log(x)$')\n\n\n\n\n\n\n\n\n\nThe exponential and logarithm functions I defined are the “natural” way to define these functions. We can also have exponential functions in other bases, \\(y=a^x\\) for any positive number \\(a\\). Each \\(a\\) has an equivalent logarithm, written \\(y = \\log_{a}(x)\\). The two functions \\(y=a^x\\) and \\(y=\\log_{a}(x)\\) are inverses of each other. When I leave off the \\(a\\), it’s assumed that all logs are the natural base \\(a=e\\), sometimes also written \\(\\ln(x)\\).\nTwo common examples of other bases that show up sometimes are the base-2 functions \\(2^x\\) and \\(\\log_{2}(x)\\), and the base-10 functions \\(10^x\\) and \\(\\log_{10}(x)\\). Base-2 functions in particular show up often in computer science because of the tendency to think in bits. Base-10 functions show up when we want to think about how many digits a number has.\nHere are some rules that exponentials and logs obey:\n\n\n\n\n\n\n\nRule\nExample\n\n\n\\(e^0 = 1\\)\n\n\n\n\\(\\log(1) = 0\\)\n\n\n\n\\(\\log(e) = 1\\)\n\n\n\n\\(e^{a+b} = e^a e^b\\)\n\\(e^{2+5} = e^2 e^5 = e^8 \\approx 2980.96\\)\n\n\n\\(e^{a-b} = \\frac{e^a}{e^b}\\)\n\\(e^{2-5} = \\frac{e^2}{e^5} = e^{-3} \\approx 0.0498\\)\n\n\n\\(e^{ab} = (e^a)^b\\)\n\\(e^{2 \\cdot 5} = (e^2)^5 = e^{10} \\approx 22026.47\\)\n\n\n\\(a^b = e^{b \\log(a)}\\)\n\\(2^3 = e^{3 \\log(2)} = 8\\)\n\n\n\\(\\log(ab) = \\log(a) + \\log(b)\\)\n\\(\\log(2 \\cdot 5) = \\log(2) + \\log(5) = \\log(10) \\approx 2.303\\)\n\n\n\\(\\log\\big(\\frac{a}{b}\\big) = \\log(a) - \\log(b)\\)\n\\(\\log\\big(\\frac{2}{5}\\big) = \\log(2) - \\log(5) \\approx -0.916\\)\n\n\n\\(\\log(a^b) = b\\log(a)\\)\n\\(\\log(5^2) = 2\\log(5) \\approx 3.219\\)\n\n\n\\(\\log_a(x) = \\frac{\\log(x)}{\\log(a)}\\)\n\\(\\log_2(5) = \\frac{\\log(5)}{\\log(2)} \\approx 2.322\\)\n\n\n\nHere’s an example of an equation involving exponentials and logs. Suppose you have \\(n\\) bits of numbers (perhaps it’s the precision in some float) and you want to know how many digits this number takes up in decimal form (what you’re used to). This would be equivalent to solving the following equation for \\(x\\),\n\\[\\begin{align*}\n2^n &= 10^{x} \\\\\n\\log(2^n) &= \\log(10^{x}) \\\\\nn\\log(2) &= x\\log(10) \\\\\nx &= \\frac{\\log(2)}{\\log(10)} \\cdot n \\\\\nx &\\approx 0.3 \\cdot n. \\\\\n\\end{align*}\\]\nFor example, you can use this formula to show that 52 bits of floating point precision translates to about 15 to 16 digits of precision. In numpy, the function np.log function calculates the (base-\\(e\\)) log of a number.\n\n\nCode\nn = 52\nx = np.log(2) / np.log(10) * n\nprint(f'x = {x}')\n\n\nx = 15.65355977452702\n\n\n\n\n2.2.6 Trigonometric Functions\nOther textbook functions typically covered in math courses are the trig functions: sine, cosine, tangent, cosine, cosecant, and cotangent. Of these functions, the most important to know are the sine function \\(y=\\sin x\\), the cosine function \\(y = \\cos x\\), and sometimes the tangent function \\(y = \\tan x\\).\nHere’s what their plots look like. They’re both waves that repeat themselves, in the sense \\(f(x + 2\\pi) = f(x)\\). The length for the function to repeat itself is called the period, in this case \\(2\\pi \\approx 6.28\\). Note that the cosine is just a sine function that’s shifted right by \\(\\frac{\\pi}{2} \\approx 1.57\\).\n\n\nCode\nx = np.arange(-10, 10, 0.1)\nf = lambda x: np.sin(x)\nplot_function(x, f, xlim=(-6, 6), ylim=(-2, 2),  ticks_every=[1, 0.5], \n              title='Sine Function: $y=\\sin(x)$')\n\n\n\n\n\n\n\n\n\n\n\nCode\nx = np.arange(-10, 10, 0.1)\nf = lambda x: np.cos(x)\nplot_function(x, f, xlim=(-6, 6), ylim=(-2, 2), ticks_every=[1, 0.5], \n              title='Cosine Function: $y=\\cos(x)$')\n\n\n\n\n\n\n\n\n\nTrig functions don’t really show up that much in machine learning, so I won’t remind you of all those obscure trig rules you’ve forgotten. I’ll just mention that we can define all the other trig functions using the sine and cosine as follows,\n\\[\\begin{align*}\n&\\tan x = \\frac{\\sin x}{\\cos x}, \\\\\n&\\csc x = \\frac{1}{\\sin x}, \\\\\n&\\sec x = \\frac{1}{\\cos x}, \\\\\n&\\cot x = \\frac{1}{\\tan x} = \\frac{\\cos x}{\\sin x}.\n\\end{align*}\\]\nWe can talk about the inverse of trig functions as well. These are just the functions that undo the trig operations and give you back the angle (in radians). Since none of the trig functions are monotonic, we can’t invert them on the whole real line, but only on a given range.\nBelow I’ll just list the inverse sine, cosine, and tangent functions and their defined input and output ranges. Note by historical convention, these inverse functions are usually called the arcsine, arccosine, and arctangent respectfully.\n\n\n\n\n\n\n\n\nInverse Function\nInput Range\nOutput Range\n\n\n\\(y = \\arcsin x = \\sin^{-1} x\\)\n\\(-1 \\leq x \\leq 1\\)\n\\(-90^\\circ \\leq y \\leq 90^\\circ\\)\n\n\n\\(y = \\arccos x = \\cos^{-1} x\\)\n\\(-1 \\leq x \\leq 1\\)\n\\(0^\\circ \\leq y \\leq 180^\\circ\\)\n\n\n\\(y = \\arctan x = \\tan^{-1} x\\)\n\\(-\\infty < x < \\infty\\)\n\\(-90^\\circ \\leq y \\leq 90^\\circ\\)\n\n\n\n\n\n2.2.7 Piecewise Functions\nThe functions covered so far are examples of continuous functions. Their graphs don’t have jumps or holes in them anywhere. Continuous functions we can often write using a single equation, like \\(y=x^2\\) or \\(y=1 + \\sin(x)\\). We can also have functions that require more than one equation to write. These are called piecewise functions. Piecewise functions usually aren’t continuous, but sometimes can be.\nAn example of a discontinuous piecewise function is the unit step function \\(y=u(x)\\) given by\n\\[\ny =\n\\begin{cases}\n0 & x < 0, \\\\\n1 & x \\geq 0.\n\\end{cases}\n\\]\nThis expression means \\(y=0\\) whenever \\(x < 0\\), but \\(y=1\\) whenever \\(x \\geq 0\\). It breaks up into two pieces, one horizontal line \\(y=0\\) when \\(x\\) is negative, and another horizontal line \\(y=1\\) when \\(x\\) is positive.\nUsing Boolean expressions, we can also write this function in a more economical way by agreeing to identify \\(x=1\\) with \\(\\text{TRUE}\\) and \\(x=0\\) with \\(\\text{FALSE}\\), which python does by default. In this notation, we can write\n\\[u(x) = [x \\geq 0],\\]\nwhich means exactly the same thing as the piecewise definition, since \\(x \\geq 0\\) is only true when (you guessed it), \\(x \\geq 0\\).\nHere’s a plot of this function. Note the discontinuous jump at \\(x=0\\).\n\n\nCode\nx = np.arange(-10, 10, 0.01)\nf = lambda x:  (x >= 0)\nplot_function(x, f, xlim=(-3, 3), ylim=(-1, 2), ticks_every=[1, 0.5], \n              title='Unit Step Function: $y=u(x)$')\n\n\n\n\n\n\n\n\n\nAn example of a piecewise function that’s continuous is the ramp function, defined by\n\\[\ny =\n\\begin{cases}\n0 & x < 0, \\\\\nx & x \\geq 0.\n\\end{cases}\n\\]\nThis function gives a horizontal line \\(y=0\\) when \\(x\\) is negative, and a \\(45^\\circ\\) line \\(y=x\\) when \\(x\\) is positive. Both lines connect at \\(x=0\\), but leave a kink in the graph.\nAnother way to write the same thing using Boolean expressions is \\(y = x \\cdot [x \\geq 0]\\), which is of course just \\(y = x \\cdot u(x)\\).\nIn machine learning it’s more common to write the ramp function using the \\(\\max\\) function as \\(y = \\max(0,x)\\). This means, for each \\(x\\), take that value and compare it with \\(0\\), and take the maximum of those two. That is, if \\(x\\) is negative take \\(y=0\\), otherwise take \\(y=x\\). It’s also more common to call this function a rectified linear unit, or ReLU for short. It’s an ugly, unintuitive name, but unfortunately it’s stuck in the field.\nHere’s a plot of the ramp or ReLU function. Notice how it stays at \\(y=0\\) for a while, then suddenly “ramps upward” at \\(x=0\\).\n\n\nCode\nx = np.arange(-10, 10, 0.1)\nf = lambda x:  x * (x >= 0)\nplot_function(x, f, xlim=(-3, 3), ylim=(-3, 3), ticks_every=[1, 1], \n              title='ReLU Function')\n\n\n\n\n\n\n\n\n\nLast, I’ll mention here the absolute value function \\(y = |x|\\), defined by the piecewise function\n\\[\ny = \\begin{cases}\nx & \\text{if } x \\ge 0 \\\\\n-x & \\text{if } x < 0.\n\\end{cases}\n\\]\nThe absolute value just ignores negative signs and makes everything positive. The function looks like the usual line \\(y=x\\) when positive, but like the negative-sloped line \\(y=-x\\) when negative. At \\(x=0\\) the two lines meet, creating a distinctive v-shape. To get the absolute value function in python, use abs or np.abs.\n\n\nCode\nx = np.arange(-5, 5, 0.1)\nf = lambda x: abs(x)\nplot_function(x, f, xlim=(-5, 5), ylim=(0, 5), ticks_every=[1, 1], \n              title='Absolute Value Function: $y=|x|$')\n\n\n\n\n\n\n\n\n\n\n\n2.2.8 Composite Functions\nWe can also have any arbitrary hybrid of the above functions. We can apply exponentials to affine functions, logs to sine functions, sines to exponential functions. In essence, this kind of layered composition of functions is what a neural network is as we’ll see later on.\nMath folks often write an abstract compositional function as a function applied to another function, like \\(y=f(g(x))\\) or \\(y=(f \\circ g)(x)\\). These can be chained arbitrarily many times, not just two. Neural networks do just that, often hundreds or thousands of times.\nConsider, for example, the function composition done by applying the following functions in sequence:\n\nan affine function \\(f(x) = wx+b\\)\nfollowed by a linear function \\(g(x) = -x\\)\nfollowed by an exponential function \\(h(x)=e^x\\)\nfollowed by a rational function \\(r(x)=\\frac{1}{x}\\)\n\nto get the full function \\[y = r(h(g(x))) = \\frac{1}{1 + e^{-(wx+b)}}.\\]\nHere’s a plot of what this function looks like for the “standard form” where \\(w=1, b=0\\). Notice that \\(0 \\leq y \\leq 1\\). The values of \\(x\\) get “squashed” to values between 0 and 1 after the function is applied.\n\n\nCode\nx = np.arange(-10, 10, 0.1)\nf = lambda x:  1 / (1 + np.exp(-x))\nplot_function(x, f, xlim=(-6, 6), ylim=(-0.2, 1.2), ticks_every=[2, 0.2], \n              title='Sigmoid Function')\n\n\n\n\n\n\n\n\n\nThis function is called the sigmoid function. The sigmoid is very important in machine learning since it in essence creates probabilities. We’ll see it a lot more. The standard form sigmoid function, usually written \\(\\sigma(x)\\), is given by\n\\[\\sigma(x) = \\frac{1}{1 + e^{-x}}.\\]\nArbitrary affine transformations of the standard form would then be written as \\(\\sigma(wx+b)\\).\nA similar looking function shows up sometimes as well called the hyperbolic tangent or tanh function, which has the (standard) form\n\\[\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}.\\]\nThe tanh function looks pretty much the same as the sigmoid except it’s rescaled vertically so that \\(-1 \\leq y \\leq 1\\).\nHere’s a plot of the tanh function. Notice how similar it looks to the sigmoid with the exception of the scale of the y-axis.\n\n\nCode\nx = np.arange(-10, 10, 0.1)\nf = lambda x: (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\nplot_function(x, f, xlim=(-5, 5), ylim=(-2, 2), ticks_every=[1, 0.5], \n              title='Tanh Function')\n\n\n\n\n\n\n\n\n\n\n\n2.2.9 Function Transformations\nSuppose we have some arbitrary function \\(f(x)\\) and we apply a series of compositions to get a new function \\[g(x)=a \\cdot f(b \\cdot (x + c)) + d.\\] We can regard each parameter \\(a,b,c,d\\) as doing some kind of geometric transformation to the graph of the original function \\(f(x)\\). Namely,\n\n\\(a\\) re-scales the function vertically (if \\(a\\) is negative it also flips \\(f(x)\\) upside down)\n\\(b\\) re-scales the function horizontally (if \\(b\\) is negative it also flips \\(f(x)\\) left to right)\n\\(c\\) shifts the function horizontally (left if \\(c\\) is positive, right if \\(c\\) is negative)\n\\(d\\) shifts the function vertically (up if \\(d\\) is positive, down if \\(d\\) is negative)\n\nHere’s an example of how these work. Consider the function \\(f(x)=x^2\\). We’re going to apply each of these transformations one by one to show what they do to the graph of \\(f(x)\\).\nFirst, let’s look at the transformation \\(g(x) = \\frac{1}{2} f(x) = \\frac{1}{2} x^2\\). Here \\(a=\\frac{1}{2}\\) and the rest are zero. I’ll plot it along side the original graph (the blue curve). Notice the graph gets flattened vertically by a factor of two (the orange curve).\n\n\nCode\nx = np.arange(-10, 10, 0.1)\nf = lambda x: x ** 2\n\n\n\n\nCode\na = 1 / 2\ng = lambda x: a * x ** 2\nplot_function(x, [f, g], xlim=(-3, 3), ylim=(-2, 10), ticks_every=[1, 2], \n              title=f'$a={a}$')\n\n\n\n\n\n\n\n\n\nNow consider at the transformation\n\\[g(x) = f\\big(\\frac{1}{2} x\\big) = \\bigg(\\frac{1}{2} x \\bigg)^2.\\]\nHere \\(b=\\frac{1}{2}\\) and the rest are zero. Notice the graph again gets flattened but in a slightly different way.\n\n\nCode\nb = 1 / 2\ng = lambda x: (b * x) ** 2\nplot_function(x, [f, g], xlim=(-3, 3), ylim=(-2, 10), ticks_every=[1, 2], \n              title=f'$b={b}$')\n\n\n\n\n\n\n\n\n\nNext, consider the transformation \\(g(x) = f(x-1) = (x-1)^2.\\) Here \\(c=1\\) and the rest are zero. Notice the graph’s shape doesn’t change. It just gets shifted right by \\(c=1\\) since \\(c\\) is negative.\n\n\nCode\nc = -1\ng = lambda x: (x + c) ** 2\nplot_function(x, [f, g], xlim=(-3, 3), ylim=(-7, 7), ticks_every=[1, 2], \n              title=f'$c={c}$')\n\n\n\n\n\n\n\n\n\nFinally, let’s look at the transformation \\(g(x) = f(x) + 2 = x^2 + 2\\). Here \\(d=2\\) and the rest are zero. Notice again the graph’s shape doesn’t change. It just gets shifted up by \\(d=2\\) units.\n\n\nCode\nd = 2\ng = lambda x: x ** 2 + d\nplot_function(x, [f, g], xlim=(-3, 3), ylim=(-1, 8), ticks_every=[1, 2], \n              title=f'$d={d}$')\n\n\n\n\n\n\n\n\n\nLet’s now put them all together to see what happens. We should see rescaling in both directions and shifts in both directions. It’s hard to see in the plot, but it’s all there if you zoom in. The vertex of the parabola is at the point \\(x=c=1, y=d=2\\). And the stretching factors due to \\(a=b=1/2\\) are both acting to flatten the parabola.\n\n\nCode\ng = lambda x: a * (b * (x + c)) ** 2 + d\nplot_function(x, [f, g], xlim=(-8, 8), ylim=(-2, 10), ticks_every=[2, 2], \n              title=f'$a={a}, b={b}, c={c}, d={d}$')"
  },
  {
    "objectID": "notebooks/basic-math.html#multivariate-functions",
    "href": "notebooks/basic-math.html#multivariate-functions",
    "title": "2  Basic Math",
    "section": "2.3 Multivariate Functions",
    "text": "2.3 Multivariate Functions\nWhat we’ve covered thus far only deals with univariate functions, functions where \\(y=f(x)\\), but \\(x\\) and \\(y\\) are just single numbers, i.e. scalars. In machine learning we’re almost always dealing with multivariate functions with lots of variables, sometimes billions of them. It turns out that most of what I’ve covered so far extends straight forwardly to multivariate functions with some small caveats, which I’ll cover below.\nSimply put, a multivariate function is a function of multiple variables. Instead of a single variable \\(x\\), we might have several variables, e.g. \\(x_0, x_1, x_2, x_3, x_4, x_5\\),\n\\[y = f(x_0, x_1, x_2, x_3, x_4, x_5).\\]\nIf you think about mathematical functions analogously to python functions it shouldn’t be surprising functions can have multiple arguments. They usually do, in fact.\nHere’s an example of a function that takes two arguments \\(x\\) and \\(y\\) and produces a single output \\(z\\), more often written as a bivariate function \\(z=f(x,y)\\). The example I’ll look at is \\(z = x^2 + y^2\\). I’ll evaluate the function at three points:\n\n\\(x=0\\), \\(y=0\\),\n\\(x=1\\), \\(y=-1\\),\n\\(x=0\\), \\(y=1\\).\n\nThe main thing to notice is the function does exactly what you think it does. If you plug in 2 values, you get out 1 value.\n\n\nCode\nf = lambda x, y: x ** 2 + y ** 2\nprint(f'z = f{(0, 0)} = {f(0, 0)}')\nprint(f'z = f{(1, -1)} = {f(1, -1)}')\nprint(f'z = f{(0, 1)} = {f(0, 1)}')\n\n\nz = f(0, 0) = 0\nz = f(1, -1) = 2\nz = f(0, 1) = 1\n\n\nWe can also have functions that map multiple inputs to multiple outputs. Suppose we have a function that takes in 2 values \\(x_0, x_1\\) and outputs 2 values \\(y_0, y_1\\). We’d write this as \\((y_0, y_1) = f(x_0, x_1)\\).\nConsider the following example,\n\\[(y_0, y_1) = f(x_0, x_1) = (x_0+x_1, x_0-x_1).\\]\nThis is really just two functions, both functions of \\(x_0\\) and \\(x_1\\). We can completely equivalently write this function as\n\\[y_0 = f_1(x_0, x_1) = x_0+x_1,\\] \\[y_1 = f_2(x_0, x_1) = x_0-x_1.\\]\nHere’s this function defined and evaluated at the point \\(x_0=1\\), \\(x_1=1\\).\n\n\nCode\nf = lambda x0, x1: (x0 + x1, x0 - x1)\nprint(f'(y0, y1) = {f(1, 1)}')\n\n\n(y0, y1) = (2, 0)\n\n\nFor now I’ll just focus on the case of multiple inputs, single output like the first example. These are usually called scalar-valued functions. We can also have vector-valued functions, which are functions whose outputs can have multiple values as well. I’ll focus on scalar-valued functions here.\nA scalar-valued function of \\(n\\) variables \\(x_0, x_1, \\cdots, x_{n-1}\\) has the form\n\\[y = f(x_0, x_1, \\cdots, x_{n-1}).\\]\nNote \\(n\\) can be as large as we want it to be. When working with deep neural networks (which are just multivariate functions of a certain form) \\(n\\) can be huge. For example, if the input is a \\(256 \\times 256\\) image, the input might be \\(256^2=65536\\) pixels. For a 10 second audio clip that’s sampled at 44 kHz, the input might be \\(10*44k=440k\\) amplitudes. Large numbers indeed.\nCalculating the output of multivariate functions is just as straight-forward as for univariate functions pretty much. Unfortunately, visualizing them is much harder. The human eye can’t see 65536 dimensions, only 3 dimensions. This in some sense means we need to give up on the ability to “graph” a function and instead find other ways to visualize it.\nOne thing that sometimes help to visualize high dimension functions is to pretend they’re functions of two variables, like \\(z=f(x,y)\\). In this special case we can visualize the inputs as an xy-plane, and the output as a third axis sticking out perpendicular to the xy-plane from the origin. Each \\(x,y\\) pair will map to one unique \\(z\\) value. Done this way, we won’t get a graph of a curve as before, but a surface.\nHere’s an example of what this might look like for the simple function \\(z=x^2+y^2\\). I’ll plot the function on the domain \\(-10 \\leq x \\leq 10\\) and \\(-10 \\leq y \\leq 10\\) using the helper function plot_3d. It takes in two lists of values x and y. I’ll use np.linspace to sample 100 points from -10 to 10 for each. Then I’ll define a lambda function that maps x and y to the output z. Passing these three arguments into the helper function gives us our 3D plot.\n\n\nCode\nx = np.linspace(-10, 10, 100)\ny = np.linspace(-10, 10, 100)\nf = lambda x, y: x**2 + y**2\n\n\n\n\nCode\nplot_function_3d(x, y, f, title='3D Plot: $z=x^2+y^2$', \n                 ticks_every=[5, 5, 50], labelpad=5, dist=12)\n\n\n\n\n\n\n\n\n\nNotice how the plot looks like an upward facing bowl. Imagine a bowl lying on a table. The table is the xy-plane. The bowl is the surface \\(z=x^2+y^2\\) we’re plotting. While the plot shows the general idea what’s going on, 3D plots can often be difficult to look at. They’re often slanted at funny angles and hide important details.\nHere’s another way we can visualize the same function: Rather than create a third axis for \\(z\\), we can plot it directly on the xy-plane as a 2D plot. Since we’re dealing with a surface, not a curve, we have to do this for lots of different \\(z\\) values, which will give a family of curves. For example, we might plot all of the following curves corresponding to different values of \\(z\\) in the xy-plane,\n\\[\\begin{align}\n25 &= x^2 + y^2, \\\\\n50 &= x^2 + y^2, \\\\\n75 &= x^2 + y^2, \\\\\n100 &= x^2 + y^2, \\\\\n125 &= x^2 + y^2, \\\\\n150 &= x^2 + y^2.\n\\end{align}\\]\nDoing this will give a family of curves on one 2D plot, with each curve representing some value of \\(z\\). In our example, these curves are all circles of radius \\(z^2\\). Each curve is called a level curve or level set.\nThese kinds of plots are called contour plots. A contour map can be thought of as looking at the surface from the top down, where each level set corresponds to slicing the function \\(z=f(x,y)\\) horizontally for different values of \\(z\\). This trick is often used in topographical maps to visualize 3D terrain on a 2D sheet of paper. Here is a contour plot for \\(z=x^2+y^2\\) using the above level curves.\n\n\nCode\nplot_countour(x, y, f, title='Countour Plot: $z=x^2+y^2$')\n\n\n\n\n\n\n\n\n\nNotice how we get a bunch of concentric rings in the contour plot, each labeled by some value (their \\(z\\) values). These rings correspond to the circles I was talking about. You can visually imagine this plot as looking down from the top of the bowl. In the middle you see the bottom. The rings get closer together the further out you go, which indicates that the bowl is sloping steeper the further out we get.\nWe’ll see more examples of multivariate functions in the coming lessons."
  },
  {
    "objectID": "notebooks/basic-math.html#systems-of-equations",
    "href": "notebooks/basic-math.html#systems-of-equations",
    "title": "2  Basic Math",
    "section": "2.4 Systems of Equations",
    "text": "2.4 Systems of Equations\nIn machine learning we’ll find ourselves frequently interested not just with single equations, but multiple equations each with many variables. One thing we might seek to do is solve these coupled systems, which means finding a solution that satisfies every equation simultaneously. Consider the following example,\n\\[\\begin{alignat*}{3}\n   x & {}+{} &  y & {}={} & 2  \\\\\n   2x & {}-{} &  3y & {}={} & 7.\n\\end{alignat*}\\]\nThis system consists of two equations, \\(x + y = 2\\), and \\(2x - 3y = 7\\). Each equation contains two unknown variables, \\(x\\) and \\(y\\). We need to find a solution for both \\(x\\) and \\(y\\) that satisfies both of these equations.\nUsually the easiest and most general way to solve simple coupled systems like this is the method of substitution. The idea is to solve one equation for one variable in terms of the other, then plug that solution into the second equation to solve for the other variable. Once the second variable is solved for, we can go back and solve for the first variable explicitly. Let’s start by solving the first equation for \\(x\\) in terms of \\(y\\). This is pretty easy,\n\\[x = 2 - y.\\]\nNow we can take this solution for \\(x\\) and plug it into the second equation to solve for \\(y\\),\n\\[\\begin{align*}\n2x - 3y &= 7 \\\\\n2(2 - y) - 3y &= 7 \\\\\n4 - 5y &= 7 \\\\\n5y &= -3 \\\\\ny &= -\\frac{3}{5}.\n\\end{align*}\\]\nWith \\(y\\) in hand, we can now solve for \\(x\\), \\(x = 2 - y = 2 + \\frac{3}{5} = \\frac{13}{5}\\). Thus, the pair \\(x=\\frac{13}{5}\\), \\(y=-\\frac{3}{5}\\) is the solution that solves both of these coupled equations simultaneously.\nHere’s sympy’s solution to the same system. It should of course agree with what I just got, which it does.\n\n\nCode\nx, y = sp.symbols('x y')\neq1 = sp.Eq(x + y, 2)\neq2 = sp.Eq(2 * x - 3 * y, 7)\nsol = sp.solve((eq1, eq2), (x, y))\nprint(f'x = {sol[x]}')\nprint(f'y = {sol[y]}')\n\n\nx = 13/5\ny = -3/5\n\n\nNotice that both of the equations in this example are linear, since each term only contains terms proportional to \\(x\\) and \\(y\\). There are no terms like \\(x^2\\) or \\(\\sin y\\) or whatever. Linear systems of equations are special because they can always be solved as long as there are enough variables. I’ll spend a lot more time on these when I get to linear algebra.\nWe can also imagine one or more equations being nonlinear. Provided we can solve each equation one-by-one, we can apply the method of substitution to solve these too. Here’s an example. Consider the nonlinear system\n\\[\\begin{align*}\ne^{x + y} &= 10  \\\\\nxy &= 1.\n\\end{align*}\\]\nLet’s solve the second equation first since it’s easier. Solving for \\(y\\) gives \\(y = \\frac{1}{x}\\). Now plug this into the first equation and solve for \\(x\\),\n\\[\\begin{align*}\ne^{x + y} &= 10  \\\\\ne^{x + 1/x} &= 10  \\\\\n\\log \\big(e^{x + 1/x}\\big) &= \\log 10 \\\\\nx + \\frac{1}{x} &= \\log 10 \\\\\nx^2 - \\log 10 \\cdot x + 1 &= 0 \\\\\nx &= \\frac{1}{2} \\bigg(\\log 10 \\pm \\sqrt{(\\log 10)^2 - 4}\\bigg) \\\\\nx &\\approx 0.581, \\ 1.722.\n\\end{align*}\\]\nNote here I had to use the quadratic formula, which I’ll assume you’ve forgotten. If you have a quadratic equation of the form \\(ax^2 + bx + c = 0\\), then it will (usually) have exactly two solutions given by the formula\n\\[x = \\frac{1}{2a} \\bigg(-b \\pm \\sqrt{b^2 - 4ac}\\bigg).\\]\nThis means we have two different possible solutions for \\(x\\), which thus means we’ll also have two possible solutions to \\(y\\) since \\(y=\\frac{1}{x}\\). Thus, this system has two possible solutions,\n\\[\\text{Solution 1: }x \\approx 0.581, \\ y \\approx 1.722,\\] \\[\\text{Solution 2: }x \\approx 1.722, \\ y \\approx 0.581.\\]\nIt’s interesting how symmetric these two solutions are. They’re basically the same with \\(x\\) and \\(y\\) swapped. This is because the system has symmetry. You can swap \\(x\\) and \\(y\\) in the system above and not change the equation, which means the solutions must be the same up to permutation of \\(x\\) and \\(y\\)!\nHere’s sympy’s attempt to solve this system.\n\n\nCode\nx, y = sp.symbols('x y')\neq1 = sp.Eq(sp.exp(x + y), 10)\neq2 = sp.Eq(x * y, 1)\nsol = sp.solve((eq1, eq2), (x, y))\nprint(f'x1 = {sol[0][0].round(5)} \\t y1 = {sol[0][1].round(5)}')\nprint(f'x2 = {sol[1][0].round(5)} \\t y2 = {sol[1][1].round(5)}')\n\n\nx1 = 0.58079     y1 = 1.72180\nx2 = 1.72180     y2 = 0.58079\n\n\nIn general, it’s not even possible to solve a system of nonlinear equations except using numerical methods. The example I gave was rigged so I could solve it by hand. General purpose root-finding algorithms exist that can solve arbitrary systems of equations like this numerically, no matter how nonlinear they are.\nTo solve a nonlinear system like this numerically, you can use the scipy function scipy.optimize.fsolve. Scipy is an extension of numpy that includes a lot of algorithms for working with non-linear functions. To use fsolve, you have to define the system as a function mapping a list of variables to a list of equations. You also have to specify a starting point x0 for the root finder. This tells it where to start looking for the root. Since nonlinear equations have multiple solutions, picking a different x0 can and will often give you a different root. I won’t dwell on all this since we don’t really need to deal with root finding much in machine learning.\n\n\nCode\nfrom scipy.optimize import fsolve\n\nsystem = lambda xy: [np.exp(xy[0] + xy[1]) - 10, xy[0] * xy[1] - 1]\nsolution = fsolve(system, x0=(1, 1))\nprint(f'solution = {solution}')\n\n\nsolution = [0.5807888 1.7217963]"
  },
  {
    "objectID": "notebooks/basic-math.html#sums-and-products",
    "href": "notebooks/basic-math.html#sums-and-products",
    "title": "2  Basic Math",
    "section": "2.5 Sums and Products",
    "text": "2.5 Sums and Products\n\n2.5.1 Sums\nWe typically find ourselves performing operations on large numbers of numbers at a time. By far the most common operation is adding up a bunch of numbers, or summation. Suppose we have some sequence of \\(n\\) numbers \\(x_0,x_1,x_2,\\cdots,x_{n-1}\\). They could be anything, related by a function, or not. If we wanted to sum them together to get a new number \\(x\\) we could write\n\\[x = x_0 + x_1 + x_2 + \\cdots + x_{n-1}.\\]\nBut it’s kind of cumbersome to always write like this. For this reason in math there’s a more compact notation to write sums called summation notation. We introduce the symbol \\(\\sum\\) for “sum”, and write \\[x = \\sum_{i=0}^{n-1} x_i.\\]\nRead this as “the sum of all \\(x_i\\) for \\(i=0,1,\\cdots,n-1\\) is \\(x\\)”. The index \\(i\\) being summed over is called a dummy index. It can be whatever we want since it never appears on the left-hand side. It gets summed over and then disappears. The lower and upper values \\(i=0\\) and \\(i=n-1\\) are the limits of the summation. The limits need not always be \\(i=0\\) and \\(i=n-1\\). We can choose them to be whatever we like as a matter of convenience.\nFrequently summation notation is paired with some kind of generating function \\(f(i) = x_i\\) that generates the sequence. For example, suppose our sequence is generated by the function \\(f(i) = i\\), and we want to sum from \\(i=1\\) to \\(i=n\\). We’d have\n\\[x = \\sum_{i=1}^n x_i = \\sum_{i=1}^n i = 1 + 2 + \\cdots + n = \\frac{1}{2} n(n+1).\\]\nThe right-hand term \\(\\frac{1}{2} n(n-1)\\) is not obvious, and only applies to this particular sum. I just wrote it down since it’s sometimes useful to remember. This is a special kind of sum called an arithmetic series. Here’s a “proof” of this relationship using sympy.\n\n\nCode\ni, n = sp.symbols('i n')\nsummation = sp.Sum(i, (i, 1, n)).doit()\nprint(f'sum i for i=1,...,n = {summation}')\n\n\nsum i for i=1,...,n = n**2/2 + n/2\n\n\nIn the general case when we don’t have nice rules like this we’d have to loop over the entire sum and do the sum incrementally.\nIn python, the equivalent of summation notation is the sum function, where we pass in the sequence we want to sum as a list. Here’s the arithmetic sum up to \\(n=10\\), which should be \\(\\frac{1}{2} 10 \\cdot (10+1) = 55\\).\n\n\nCode\nsum([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n\n\n55\n\n\nAnother useful sum to be aware of is the geometric series. A geometric series is a sum over a sequence whose generating function is \\(f(i) = r^i\\) for some real number \\(r \\neq 1\\). Its rule is\n\\[x = \\sum_{i=0}^{n-1} r^i = r^0 + r^1 + \\cdots + r^{n-1} = \\frac{1-r^n}{1-r}.\\]\nFor example, if \\(n=10\\) and \\(r=\\frac{1}{2}\\), we have\n\\[x = \\sum_{i=0}^{9} \\bigg(\\frac{1}{2}\\bigg)^i = \\frac{1-\\big(\\frac{1}{2}\\big)^{10}}{1-\\big(\\frac{1}{2}\\big)} = 2\\bigg(1-\\frac{1}{2^{10}}\\bigg) \\approx 1.998.\\]\n\n\nCode\nr = 1 / 2\nn = 10\nsum([r ** i for i in range(n)])\n\n\n1.998046875\n\n\nNotice how the term \\(\\big(\\frac{1}{2}\\big)^{10} \\approx 0.00098\\) is really small. We can practically ignore it. In fact, as \\(n \\rightarrow \\infty\\) we can completely ignore it, in which case\n\\[x = \\sum_{i=0}^{\\infty} \\bigg(\\frac{1}{2}\\bigg)^i = \\frac{1}{1-\\big(\\frac{1}{2}\\big)} = 2.\\]\nThis is an example of the infinite version of the geometric series. If \\(0 \\leq r \\leq 1\\), then\n\\[x = \\sum_{i=0}^{\\infty} r^i = r^0 + r^1 + r^2 + \\cdots = \\frac{1}{1-r}.\\]\nWhat happens when \\(r=1\\)? Clearly the rule breaks down at this point, since the denominator becomes infinite. But it’s easy enough to see what it is by writing out the sum,\n\\[x = \\sum_{i=0}^{n-1} 1^i = 1^0 + 1^1 + \\cdots + 1^{n-1} = \\underbrace{1 + 1 + \\cdots + 1}_{\\text{n times}} = n.\\]\nIn this case, if we send \\(n \\rightarrow \\infty\\), then \\(x\\) clearly blows up to \\(\\infty\\) too. You can see this by plotting the function \\(y = \\frac{1}{1-x}\\) and observing it asymptotes at \\(x=1\\).\n\n\nCode\nx = np.arange(0, 1, 0.01)\nf = lambda x: 1 / (1 - x)\nplot_function(x, f, xlim=(0, 1), ylim=(0, 100), ticks_every=[0.2, 20], \n              title='$y=1/(1-x)$')\n\n\n\n\n\nWe can always factor constants \\(c\\) out of sums. This follows naturally from just expanding the sum out,\n\\[\\sum_{i=0}^{n-1} c x_i = cx_0 + cx_1 + \\cdots + cx_{n-1} = c(x_0 + x_1 + \\cdots + x_{n-1}) = c\\sum_{i=0}^{n-1} x_i.\\]\nSimilarly, we can break sums up into pieces (or join sums together) as long as we’re careful to get the index limits right,\n\\[\\sum_{i=0}^{n-1} x_i = \\sum_{i=0}^{k} x_i + \\sum_{i=k+1}^{n-1} x_i.\\]\nWe can have double sums (sums of sums) as well. If \\(x_{i,j}\\) is some 2-index variable where \\(i=0,\\cdots,n-1\\) and \\(j=0,\\cdots,m-1\\), we can sum over both sets of indices to get \\(n \\cdot m\\) total terms,\n\\[\\sum_{i=0}^{n-1} \\sum_{j=0}^{m-1} x_{i,j} = \\sum_{j=0}^{m-1} \\sum_{i=0}^{n-1} x_{i,j} = x_{0,0} + x_{0,1} + \\cdots x_{0,m-1} + \\cdots + x_{n-1,0} + x_{n-1,1} + \\cdots x_{n-1,m-1}.\\]\nNotice the two sums can swap, or commute, with each other, \\(\\sum_i \\sum_j = \\sum_j \\sum_i\\). This follows by expanding the terms out like on the right-hand side and noting the must be equal in both cases.\n\n\n2.5.2 Products\nThe notation I’ve covered for sums has an analogue for products, called product notation. Suppose we want to multiply \\(n\\) numbers \\(x_0,x_1,\\cdots,x_{n-1}\\) together to get some number \\(x\\). We could write\n\\[x = x_0 \\cdot x_1 \\cdots x_{n-1},\\]\nbut we have a more compact notation for this as well. Using the symbol \\(\\prod\\) in analogy to \\(\\sum\\), we can write\n\\[x = \\prod_{i=0}^{n-1} x_i.\\]\nRead this as “the product of all \\(x_i\\) for \\(i=0,1,\\cdots,n-1\\) is \\(x\\)”.\nUnlike sums, python doesn’t have a native function to calculate products of elements in a sequence, but numpy has one called np.prod. Here’s an example. I’ll calculate the product of all integers between one and ten.\n\\[x = \\prod_{i=1}^{10} i = 1 \\cdot 2 \\cdot 3 \\cdot 4 \\cdot 5 \\cdot 6 \\cdot 7 \\cdot 8 \\cdot 9 \\cdot 10 = 3628800.\\]\n\n\nCode\nnp.prod([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n\n\n3628800\n\n\nLuckily, there aren’t any common products to remember. It’s just worth being familiar with the notation, since we’ll occasionally use it.\nProducts don’t obey quite the same properties sums do, so you have to be careful. When in doubt, just write out the product the long way and make sure what you’re doing makes sense. For example, pulling a factor \\(c\\) out of a product gives a factor of \\(c^n\\), not \\(c\\), since there are \\(c\\) total products multiplied together,\n\\[x = \\prod_{i=0}^{n-1} cx_i = cx_0 \\cdot cx_1 \\cdots cx_{n-1} = c^n(x_0 \\cdot x_1 \\cdots x_{n-1}) = c^n \\prod_{i=0}^{n-1} x_i.\\]\nIt’s worth noting (because we’ll use this fact), that we can turn products into sums by taking the log of the product,\n\\[\\log \\bigg(\\prod_{i=0}^{n-1} x_i \\bigg) = \\sum_{i=0}^{n-1} \\log x_i.\\]\nThis follows from the rule \\(\\log(x \\cdot y) = \\log x + \\log y\\), which extends to arbitrarily many products too."
  },
  {
    "objectID": "notebooks/basic-math.html#greek-alphabet",
    "href": "notebooks/basic-math.html#greek-alphabet",
    "title": "2  Basic Math",
    "section": "2.6 Greek Alphabet",
    "text": "2.6 Greek Alphabet\nLike many other technical fields, machine learning makes heavy use of the Greek alphabet to represent variable names in mathematical equations. While not all Greek characters are used, certain ones are worth being aware of. Below is a table of the Greek letters. You don’t need to memorize all of these letters, but it’s worth referencing this table whenever you encounter a symbol you don’t recognize."
  },
  {
    "objectID": "notebooks/numerical-computing.html#integers",
    "href": "notebooks/numerical-computing.html#integers",
    "title": "3  Numerical Computation",
    "section": "3.1 Integers",
    "text": "3.1 Integers\n\n3.1.1 Basics\nRecall the integers are whole numbers that can be positive, negative, or zero. Examples are 5, 100151, 0, -72, etc. The set of all integers is commonly denoted by the symbol \\(\\mathbb{Z}\\).\nIn python, integers (ints for short) are builtin objects of type int that more or less follow the rules that integers in math follow.\nAmong other things, the following operations can be performed with integers:\n\nAddition: \\(2 + 2 = 4\\).\nSubtraction: \\(2 - 5 = -3\\).\nMultiplication: \\(3 \\cdot 3 = 9\\)\n\nIn python this is the * operator, e.g. 3 * 3 = 9\n\nExponentiation: \\(2^3 = 2 \\times 2 \\times 2 = 8\\)\n\nIn python this is the ** operator, e.g. 2 ** 3 = 8.\n\nRemainder (or Modulo): the remainder of 10 when divided by 3 is 1, written \\(10 \\text{ mod } 3 = 1\\)\n\nIn python this is the % operator, e.g. 10 % 3 = 1.\n\n\nIf any of these operations are applied to two integers, the output will itself always be an integer.\nHere are a few examples.\n\n\nCode\nprint(f'2 + 2 = {2 + 2}')\nprint(f'2 - 5 = {2 - 5}')\nprint(f'3 * 3 = {3 * 3}')\nprint(f'10 % 3 = {10 % 3}')\nprint(f'2 ** 3 = {2 ** 3}')\n\n\n2 + 2 = 4\n2 - 5 = -3\n3 * 3 = 9\n10 % 3 = 1\n2 ** 3 = 8\n\n\nWhat about division? You can’t always divide two integers and get another integer. What you have to do instead is called integer division. Here you divide the two numbers and then round the answer down to the nearest whole number. Since \\(5 \\div 2 = 2.5\\), the nearest rounded down integer is 2.\nIn math, this “nearest rounded down integer” 2 is usually called the floor of 2.5, and represented with the funny symbol \\(\\lfloor 2.5 \\rfloor.\\) Using this notation we can write the above integer division as \\[\\big\\lfloor \\frac{5}{2} \\big\\rfloor = 2.\\]\nIn python, integer division is done using the // operator, e.g. 5 // 2 = 2. I’ll usually write \\(5 \\ // \\ 2\\) instead of \\(\\big\\lfloor \\frac{5}{2} \\big\\rfloor\\) when it makes sense, \\[5 \\ // \\ 2 = \\big\\lfloor \\frac{5}{2} \\big\\rfloor = 2.\\]\n\n\nCode\nprint(f'5 // 2 = {5 // 2}')\n\n\n5 // 2 = 2\n\n\nWe can also do regular division / with ints, but the output will not be an integer even if the answer should be, e.g. 4 / 2. Only integer division is guaranteed to return an integer. I’ll get to this shortly.\n\n\nCode\nprint(f'4 / 2 = {4 / 2}')\nprint(f'type(4 / 2) = {type(4 / 2)}')\n\n\n4 / 2 = 2.0\ntype(4 / 2) = <class 'float'>\n\n\n\n\nCode\nprint(f'4 // 2 = {4 // 2}')\nprint(f'type(4 // 2) = {type(4 // 2)}')\n\n\n4 // 2 = 2\ntype(4 // 2) = <class 'int'>\n\n\nDivision by zero is of course undefined for both division and integer division. In python it will always raise a ZeroDivisionError like so.\n\n\nCode\nprint(f'4 / 0 = {4 / 0}')\n\n\nZeroDivisionError: division by zero\n\n\n\n\nCode\nprint(f'4 // 0 = {4 // 0}')\n\n\nZeroDivisionError: integer division or modulo by zero\n\n\n\n\n3.1.2 Representing Integers\nJust like every other data type, on a computer integers are actually represented internally as a sequence of bits. A bit is a “binary digit”, 0 or 1. A sequence of bits is just a sequence of zeros and ones, e.g. 0011001010 or 1001001.\nThe number of bits used to represent a piece of data is called its word size. If we use a word size of \\(n\\) bits to represent an integer, then there are \\(2^n\\) possible integer values we can represent.\nIf integers could only be positive or zero, representing them with bits would be easy. We could just convert them to binary and that’s it. To convert a non-negative integer to binary, we just need to keep dividing it by 2 and recording its remainder (0 or 1) at each step. The binary form is then just the sequence of remainders, written right to left. More generally, the binary sequence of some arbitrary number \\(x\\) is the sequence of coefficients \\(b_k=0,1\\) in the sum\n\\[x = \\sum_{k=-\\infty}^\\infty b_k 2^k = \\cdots + b_2 2^2 + b_1 2^1 + b_0 2^0 + b_{-1} 2^{-1} + b_{-2} 2^{-2} + \\cdots.\\]\nHere’s an example. Suppose we wanted to represent the number \\(12\\) in binary.\n\n\\(12 \\ // \\ 2 = 6\\) with a remainder of \\(0 = 12 \\text{ mod } 2\\), so the first bit from the right is then \\(0\\).\n\\(6 \\ // \\ 2 = 3\\) with a remainder of \\(0 = 6 \\text{ mod } 2\\), so the next bit is \\(0\\).\n\\(3 \\ // \\ 2 = 1\\) with a remainder of \\(1 = 3 \\text{ mod } 2\\), so the next bit is \\(1\\).\n\\(1 \\ // \\ 2 = 0\\) with a remainder of \\(1 = 1 \\text{ mod } 2\\), so the next bit is \\(1\\).\n\nSo the binary representation of \\(12\\) is \\(1100\\), which is the sequence of coefficients in the sum\n\\[12 = 1 \\cdot 2^{3} + 1 \\cdot 2^{2} + 0 \\cdot 2^{1} + 0 \\cdot 2^{0}.\\]\nRather than keep doing these by hand, you can quickly convert a number to binary in python by using bin. It’ll return a string representing the binary sequence of that number, prepended with the special prefix 0b. To get back to the integer from, use int, passing in a base of 2.\n\n\nCode\nprint(f'bin(12) = {bin(12)}')\nprint(f\"int('0b110', 2) = {int('0b110', 2)}\")\n\n\nbin(12) = 0b1100\nint('0b110', 2) = 6\n\n\nThis representation works fine for non-negative integers, also called the unsigned integers in computer science. To represent an unsigned integer with \\(n\\) bits, just get its binary form and prepend it with enough zeros on the left until all \\(n\\) bits are used. For example, if we used 8-bit unsigned integers then \\(n=8\\), hence representing the number \\(12\\) would look like \\(00000110\\). Simple, right?\nUnsigned ints work fine if we never have to worry about negative numbers. But in general we do. These are called the signed integers in computer science. To represent signed ints, we need to use one of the bits to represent the sign. What we can do is reserve the left-most bit for the sign, \\(0\\) if the integers is positive or zero, \\(1\\) if the integer is negative.\nFor example, if we used 8-bit signed integers to represent \\(12\\), we’d again write \\(00000110\\), exactly as before. But this time it’s understood that left-most \\(0\\) is encoding the fact that \\(12\\) is positive. If instead we wanted to represent the number \\(-12\\) we’d need to flip that bit to a \\(1\\), so we’d get \\(10000110\\).\nLet’s now do an example of a simple integer system. Consider the system of 4-bit signed ints. In this simple system, \\(n=4\\) is the word size, and an integer \\(x\\) is represented with the sequence of bits\n\\[x \\equiv sb_1b_2b_3,\\]\nwhere \\(s\\) is the sign bit and \\(b_1b_2b_3\\) are the remaining 3 bits allowed to represent the numerical digits. This system can represent \\(2^4=16\\) possible values in the range \\([-2^3+1,2^3-1] = [-8,7]\\), given in the following table:\n\n\n\nInteger\nRepresentation\nInteger\nRepresentation\n\n\n\n\n-0\n1000\n+0\n0000\n\n\n-1\n1001\n1\n0001\n\n\n-2\n1010\n2\n0010\n\n\n-3\n1011\n3\n0011\n\n\n-4\n1100\n4\n0100\n\n\n-5\n1101\n5\n0101\n\n\n-6\n1110\n6\n0110\n\n\n-7\n1111\n7\n0111\n\n\n\nNote the presence of \\(-0 \\equiv 1110\\) in the upper left. This is because the system as I’ve defined it leaves open the possibility of two zeros, \\(+0\\) and \\(-0\\), since for zero the sign bit is redundant. A way to get around this is to encode the negative numbers slightly differently, by not just setting the sign bit to one, but also inverting the remaining bits and subtracting one from them. This is called the two’s complement representation. It’s how most languages, including python, actually represent integers. I won’t go into this representation in any depth, except to say that it gets rid of the need for \\(-0\\) and replaces it with \\(-2^{n-1}\\).\nHere’s what that table looks like for 4-bit integers. It’s almost the same, except there’s no \\(-0\\), instead a \\(-8\\). Notice the positive integers look exactly the same. It’s only the negative integers that look different. For them, the right three bits get inverted and added with a one.\n\n\n\nInteger\nTwo’s Complement\nInteger\nTwo’s Complement\n\n\n\n\n-1\n1111\n0\n0000\n\n\n-2\n1110\n1\n0001\n\n\n-3\n1101\n2\n0010\n\n\n-4\n1100\n3\n0011\n\n\n-5\n1011\n4\n0100\n\n\n-6\n1010\n5\n0101\n\n\n-7\n1001\n6\n0110\n\n\n-8\n1000\n7\n0111\n\n\n\nIt’s worth visualizing what integers look like on the number line, if for nothing else than to compare it with what floats look like later on. Below I’ll plot what a 6-bit signed integer system would look like. Such a system should go from -32 to 31. As you’d expect, we get a bunch of equally spaced points from -32 to 31.\n\n\nCode\nn = 6\nsix_bit_ints = range(-2**(n-1), 2**(n-1))\nplot_number_dist(six_bit_ints, title=f'Distribution of {n}-bit Signed Ints')\n\n\n\n\n\n\n\n\n\nIn python, integers are represented by default using a much bigger word size of \\(n=64\\) bits, called long integers, or int64 for short. This means (using two’s complement) we can represent \\(2^{64}=18446744073709551616\\) possible integer values in the range \\([-2^{63}, 2^{63}-1]\\).\nYou can see from this that 64-bit integers have a minimum integer allowed and a maximum integer allowed, which are\n\\[\\text{min\\_int}=-2^{63}=-9223372036854775808, \\qquad \\text{max\\_int}=2^{63}-1=9223372036854775807.\\]\nWhat I’ve said is technically only exactly true in older versions of pythons as well as other programming languages like C. It turns out newer versions of python have a few added tricks that allow you to represent essentially arbitrarily large integers. You can see this by comparing it to numpy’s internal int64 representation, which uses the C version. A numpy int64 outside the valid range will throw an overflow error.\n\n\nCode\nmin_int = -2 ** 63\nmax_int = 2 ** 63 - 1\n\n\n\n\nCode\nprint(f'min_int - 1 = {min_int - 1}')\nprint(f'min_int = {np.int64(min_int - 1)}')\n\n\nmin_int - 1 = -9223372036854775809\n\n\nOverflowError: Python int too large to convert to C long\n\n\n\n\nCode\nprint(f'max_int + 1 = {min_int + 1}')\nprint(f'max_int = {np.int64(min_int + 1)}')\n\n\nmax_int + 1 = -9223372036854775807\nmax_int = -9223372036854775807"
  },
  {
    "objectID": "notebooks/numerical-computing.html#floats",
    "href": "notebooks/numerical-computing.html#floats",
    "title": "3  Numerical Computation",
    "section": "3.2 Floats",
    "text": "3.2 Floats\n\n3.2.1 Basics\nWhat if we want to represent decimal numbers or fractions instead of whole numbers, like \\(1.2\\) or \\(0.99999\\), or even irrational numbers like \\(\\pi=3.1415926\\dots\\)? To do this we need a new system of numbers that I’ll call floating point numbers, or floats, for reasons I’ll explain soon. Floats will be a computer’s best attempt to represent the real numbers \\(\\mathbb{R}\\). They’ll represent real numbers only approximately with some specified precision.\nIn python, floats are builtin objects of type float. Floats obey pretty much the same operations that integers do with some minor exceptions:\n\nAddition: \\(1.2 + 4.3 = 5.5\\).\nSubtraction: \\(1.2 - 4.3 = -3.1\\).\nMultiplication: \\(1.2 \\times 4.3 = 5.16\\).\nExponentiation: \\(4.3^2 = 18.49\\).\nRemainder (or Modulo): \\(4.3 \\text{ mod } 1.2 = 0.7\\).\nInteger Division: \\(4.3 \\ // \\ 1.2 = 3.0\\).\nDivision: \\(4.3 \\div 1.2\\).\n\nLet’s verify the first few of these to see what’s going on.\n\n\nCode\nprint(f'1.2 + 4.3 = {1.2 + 4.3}')\nprint(f'1.2 - 4.3 = {1.2 - 4.3}')\nprint(f'1.2 * 4.3 = {1.2 * 4.3}')\n\n\n1.2 + 4.3 = 5.5\n1.2 - 4.3 = -3.0999999999999996\n1.2 * 4.3 = 5.159999999999999\n\n\nMost of them look right. But what the heck is going on with \\(1.2 - 4.3\\) and \\(1.2 \\times 4.3\\)? We’re getting some weird trailing nines that shouldn’t be there. This gets to how floats are actually represented on a computer.\n\n\n3.2.2 Representing Floats\nRepresenting real numbers on a computer is a lot more subtle than representing integers. Since a computer can only have a finite number of bits, they can’t represent infinitely many digits, e.g. in irrational numbers like \\(\\pi\\). Using finite word sizes will necessarily have to truncate real numbers to some number of decimal places. This truncation will create an error in the calculation called numerical roundoff.\nSo how should we represent a decimal number using \\(n\\) bits? As an example, let’s imagine we’re trying to represent the number \\(x=157.208\\). Perhaps the first thing you might think of is to use some number of those bits to represent the integer part, and some number to represent the fractional part. Suppose you have \\(n=16\\) bits available to represent \\(x\\). Then maybe you can use 8 bits for the integer part \\(157\\), and 8 bits for the fractional part \\(0.208\\). Converting both halves to binary, you’d get \\[157 \\equiv 10011101, \\quad 0.208 \\equiv 0011010100111111.\\]\nTruncating both sequences to 8 bits (from the left), you could thus adopt a convention that \\(157.208 \\equiv 10011101 \\ 00110101\\).\nThis system is an example of a fixed point representation. This has to do with the fact that we’re always using a fixed number of bits for the integer part, and a fixed number for the fractional part. The decimal point isn’t allowed to float, or move around to allocate more bits to the integer or fractional part depending which needs more precision. The decimal point is fixed.\nAs I’ve suggested, the fixed point representation seems to be limited and not terribly useful. If you need really high precision in the fractional part, your only option is to use a larger word size. If you’re dealing with really big numbers and don’t care much about the fractional part, you also need a larger word size so you don’t run out of numbers. A solution to this problem is to allow the decimal point to float. We won’t allocate a fixed number of bits to represent the integer or fractional parts. We’ll design it in such a way that larger numbers give the integer part more bits, and smaller numbers give the fractional part more bits.\nThe trick to allowing the decimal point to float is to represent not just the digits of a number but also its exponent. Think about scientific notation, where if you have a number like say \\(x=1015.23\\), you can write it as \\(1.01523 \\cdot 10^3\\), or 1.01523e3. That \\(3\\) is the exponent. It says something about how big the number is. What we can do is convert a number to scientific notation. Then use some number of bits to represent the exponent \\(3\\) and some to represent the remaining part \\(1.01523\\). This is essentially the whole idea behind floating point.\nIn floating point representation, instead of using scientific notation with powers of ten, it’s more typical to use powers of two. When using powers of two, the decimal part can always be scaled to be between 1 and 2, so they look like \\(1.567\\) or something like that. Since the \\(1.\\) part is always there, we can agree it’s always there, and only worry about representing the fractional part \\(0.567\\). We’ll call this term the mantissa. Denoting the sign bit as \\(s\\), the exponent as \\(e\\), and the mantissa as \\(m\\), we can thus right any decimal number \\(x\\) in a modified scientific notation of the form \\[x = (-1)^s \\cdot (1+m) \\cdot 2^{e}.\\] Once we’ve converted \\(x\\) to this form, all we need to do is to figure out how to represent \\(s\\), \\(m\\), and \\(e\\) using some number of bits of \\(n\\), called the floating point precision. Assume the \\(n\\) bits of precision allocate \\(1\\) bit for the sign, \\(n_e\\) bits for the exponent, and \\(n_m\\) bits for the mantissa, so \\(n=1+n_e+n_m\\).\nHere are the steps to convert a number \\(x\\) into its \\(n\\)-bit floating point representation.\n\nGiven some number \\(x\\), get its modified scientific notation form \\(x = (-1)^s \\cdot (1+m) \\cdot 2^e\\).\n\nDetermine the sign of \\(x\\). If negative, set the sign bit to \\(s=1\\), else default to \\(s=0\\). Set \\(x = |x|\\).\nKeep performing the operation \\(x = x \\ // \\ 2\\) until \\(1 \\leq x \\leq 2\\). Keep track of the number of times you’re dividing, which is the exponent \\(e\\).\nThe remaining part will be some \\(1 \\leq x \\leq 2\\). Write it in the form \\(x = 1 + m\\), where \\(m\\) is the mantissa.\n\nConvert the scientific notation form into a sequence of \\(n\\) bits, truncating where necessary.\n\nFor reasons I’ll describe in a second, it’s good to add a bias term \\(b\\) to the exponent \\(e\\) before converting the exponent to binary. Let \\(e'=e+b\\) be this modified exponent.\nConvert each of \\(e'\\) and \\(m\\) into binary sequences, truncated to sizes \\(n_e\\), and \\(n_m\\) respectively.\nConcatenate these binary sequences together to get a sequence of \\(n=1+n_e+n_m\\) total bits. By convention, assume the order of bit concatenation is the sign bit, then exponent bits, then the mantissa bits.\n\n\nThere are of course other ways you could do it, for example by storing the sequences in a different order. I’m just stating one common way it’s done.\nSince all of this must seem like Greek, here’s a quick example. Let’s consider the number \\(x=15.25\\). We’ll represent it using \\(n=8\\) bits of precision, where \\(n_e=4\\) is the number of exponent bits, \\(n_m=3\\) is the number of precision bits, and \\(b=10\\) is the bias.\n\nConvert \\(x=15.25\\) to its modified scientific notation.\n\nSince \\(x \\geq 0\\) the sign is positive, so \\(s=0\\).\nKeep integer dividing \\(x\\) by \\(2\\) until it’s less than \\(2\\). It takes \\(e=3\\) divisions before \\(x<2\\).\nWe now have \\(x = 1.90625 \\cdot 2^3\\). The mantissa is then \\(m = (1.90625-1) = 0.90625\\).\nIn modified scientific notation form we now have \\(x=(-1)^0 \\cdot (1 + 0.90625) \\cdot 2^3\\).\n\nConvert everything to binary.\n\nAdding the bias to the exponent gives \\(e'=3+10=13\\).\nConverting each piece to binary we get \\(e' = 13 \\equiv 1101\\), \\(m = 0.90625 \\equiv 11101\\).\nSince \\(m\\) requires more than \\(n_m=3\\) bits to represent, truncate off the two right bits to get \\(m \\equiv 111\\).\n\nThis truncation will cause numerical roundoff, since \\(0.90625\\) truncates to \\(0.875\\). That’s an error of \\(0.03125\\) that gets permanently lost.\n\nThe final representation is thus \\(x \\equiv 0 \\ 1101 \\ 111\\).\n\n\nBelow I show the example I just calculated. I print out both the scientific notation form and its binary representation.\n\n\nCode\nrepresent_as_float(15.25, n=8, n_exp=4, n_man=3, bias=10)\n\n\nscientific notation: (-1)^0 * (1 + 0.90625) * 2^3\n8-bit floating point representation: 0 1101 111\n\n\nSo what’s going on with the bias term \\(b\\)? Why do we need it? The easiest answer to give is that without it, we can’t have negative exponents without having to use another sign bit for them. Consider a number like \\(x=0.5\\). In modified scientific notation this would look like \\(x=(-1)^0 \\cdot (1+0) \\cdot 2^{-1} = 2^{-1}\\), meaning its exponent would be \\(e=-1\\). Rather than have to keep yet another sign bit for the exponent, it’s easier to just add a bias term \\(b\\) that ensures the exponent \\(e'=e+b\\) is always non-negative. The higher the bias, the more precision we can show in the range \\(-1 < x < 1\\). The trade-off is that we lose precision for large values of \\(x\\).\nOn top of floats defined the way I mentioned, we also have some special numbers that get defined in a floating point system. These are \\(\\pm 0\\), \\(\\pm \\infty\\), and \\(\\text{NaN}\\) or “not a number”. Each of these numbers is allocated its own special sequence of bits, depending on the precision.\n\n\\(+0\\) and \\(-0\\): These numbers are typically represented using a biased exponent \\(e'=0\\) (all zero bits) and a mantissa \\(m=0\\) (all zero bits). The sign bit is used to distinguish between \\(+0\\) and \\(-0\\). In our example, these would be \\(+0 \\equiv 0 \\ 0000 \\ 000\\) and \\(-0 \\equiv 1 \\ 0000 \\ 000\\).\n\\(+\\infty\\) and \\(-\\infty\\): These numbers are typically represented using the max allowed exponent (all one bits) and a mantissa \\(m=0\\) (all zero bits). The sign bit is used to distinguish between \\(+\\infty\\) and \\(-\\infty\\). In our example, these would be \\(+\\infty \\equiv 0 \\ 1111 \\ 000\\) and \\(-\\infty \\equiv 1 \\ 1111 \\ 000\\).\n\\(\\text{NaN}\\): This value is typically represented using the max allowed exponent (all one bits) and a non-zero \\(m \\neq 0\\). The sign bit is usually not used for \\(\\text{NaN}\\) values. Note this means we can have many different sequences that all represent \\(\\text{NaN}\\). In our example, any number of the form \\(\\text{NaN} \\equiv \\text{x} \\ 1111 \\ \\text{xxx}\\) would work.\n\nSo I can illustrate some points about how floating point numbers behave, I’m going to generate all possible \\(8\\)-bit floats (excluding the special numbers) and plot them on a number line, similar to what I did above with the \\(8\\)-bit signed integers. I’ll generate the floats using the using the helper function gen_all_floats, passing in the number of mantissa bits n_man=3, the number of exponent bits n_exp=4, and a bias of bias=10.\nFirst, I’ll use these numbers to print out some interesting statistics of this 8-bit floating point system.\n\n\nCode\neight_bit_floats = gen_all_floats(n=8, n_man=3, n_exp=4, bias=10)\nprint(f'Total number of 8-bit floats: {len(eight_bit_floats)}')\nprint(f'Most negative float: {min(eight_bit_floats)}')\nprint(f'Most positive float: {max(eight_bit_floats)}')\nprint(f'Smallest nonzero float: {min([x for x in eight_bit_floats if x > 0])}')\nprint(f'Machine Epsilon: {min([x for x in eight_bit_floats if x > 1]) - 1}')\n\n\nTotal number of 8-bit floats: 120\nMost negative float: -56.0\nMost positive float: 56.0\nSmallest nonzero float: 0.001953125\nMachine Epsilon: 0.25\n\n\nWe can see that this 8-bit system only contains 120 unique floats. We could practically list them all out. Just like with the integers, we see there’s a most negative float, \\(-56.0\\), and a most positive float, \\(56.0\\). The smallest float, i.e. the one closest to \\(0\\), is \\(0.001953125\\). Notice how much more precision the smallest float has than the largest ones do. The largest ones are basically whole numbers, while the smallest one has nine digits of precision. Evidently, floating point representations give much higher precision to numbers close to zero than to numbers far away from zero.\nWhat happens if you try to input a float larger than the max, in this case \\(56.0\\)? Typically it will overflow. This will result in either the system raising an error, or the number getting set to \\(+\\infty\\), in a sense getting “rounded up”. Similarly, for numbers more negative than the min, in this case \\(-56.0\\), either an overflow error will be raised, or the number will get “rounded down” to \\(-\\infty\\).\nYou have to be careful in overflow situations like this, especially when you don’t know for sure which of these your particular system will do. It’s amusing to note that python will raise an overflow error, but numpy will round to \\(\\pm \\infty\\). Two different conventions to worry about. Just as amusing, when dealing with signed integers, it’s numpy that will raise an error if you overflow, while python won’t care. One of those things…\nWhat happens when you try to input a float smaller than the smallest value, in this case \\(0.001953125\\)? In this case, the number is said to undeflow. Usually underflow won’t raise an error. The number will pretty much always just get set to \\(+0\\) (or \\(-0\\)). This is again something you have to worry about, especially if you’re dealing with small numbers in denominators, where they can lead to division by zero errors which do get raised.\nOverflow and underflow errors are some of the most common numerical bugs that occur in deep learning, and usually result from not handling floats correctly to begin with.\nI also printed out a special value called the machine epsilon. The machine epsilon, denoted \\(\\varepsilon_m\\), is defined as the smallest value in a floating point system that’s larger than \\(1\\). In some sense, \\(\\varepsilon_m\\) is a proxy for how finely you can represent numbers in a given \\(n\\)-bit floating point system. The smaller \\(\\varepsilon_m\\) the more precisely you can represent numbers, i.e. the more decimal places of precision you get access to. In our case, we get \\(\\varepsilon_m=0.25\\). This means numbers in 8-bit floating point tend to be \\(0.25\\) apart from each other on average, which means we can represent numbers in this system only with a measly 2-3 digits of precision.\nWith these numbers in hand let’s now plot their distribution on the number line. Compare with the plot of the signed integers I did above.\n\n\nCode\nplot_number_dist(eight_bit_floats, title='Distribution of 8-bit Floats')\n\n\n\n\n\n\n\n\n\nNotice how different this plot is from the ones for the signed integers. With the integers, the points were equally spaced. Now points close to \\(0\\) are getting represented much closer together than points far from \\(0\\). There are \\(74\\) of the \\(120\\) total points showing up just in the range \\([-1,1]\\). That’s over half!. Meanwhile, only \\(22\\) points total show up in the combined ranges of \\([-60,-10]\\) and \\([10,60]\\). Very strange.\nFeel free to play around with different floating point systems by using different choices for n, n_man, n_exp, and bias. Be careful, however, not to make n_exp too large or you may crash the kernel…\n\n\n3.2.3 Double Precision\nSo how does python represent floats? Python by default uses what’s called double precision to represent floats, also called float64. This means \\(n=64\\) total bits of precision are used, with \\(n_e=11\\), \\(n_m=52\\), and bias \\(b=1023=2^{10}-1\\). Double precision allows for a much larger range of numbers than 8-bit precision does:\n\nThe max value allowed is \\(2^{2^{n_e}-b} = 2^{1025} \\approx 10^{308}\\).\nThe min value allowed is \\(-2^{2^{n_e}-b} = -2^{1025} \\approx -10^{308}\\).\nNumbers outside the range of about \\([-10^{308}, 10^{308}]\\) will overflow.\nThe smallest values allowed are (plus or minus) \\(2^{-b+1} = 2^{-1022} \\approx 10^{-308}\\).\n\nUsing subordinal numbers, the smallest values are (plus or minus) \\(2^{-b-n_m+1} = 2^{-1074} \\approx 10^{-324}\\).\n\nNumbers inside the range of about \\([-10^{-308}, 10^{-308}]\\) will underflow.\n\nUsing subordinal numbers, this range is around \\([-10^{-324}, 10^{-324}]\\).\n\nThe machine epsilon is \\(\\varepsilon_m = 2^{-53} \\approx 10^{-16}\\).\nNumbers requiring more than about 15-16 digits of precision will get truncated, resulting in numerical roundoff.\nThe special numbers \\(\\pm \\infty\\), \\(\\pm 0\\), and \\(\\text{NaN}\\) are represented similarly as before, except using 64 bits.\n\nTo illustrate the point regarding numerical roundoff, here’s what happens if we try to use double precision floating point to define the constant \\(\\pi\\) to its first 100 digits? Notice it just gets truncated to its first 15 digits. Double precision is unable to keep track of the other 85 digits. They just get lost to numerical roundoff.\n\n\nCode\npi = 3.141592653589793238462643383279502884197169399375105820974944592307816406286208998628034825342117068\nprint(f'pi = {pi}')\n\n\npi = 3.141592653589793\n\n\nAnother thing to worry about is adding small numbers to medium to large sized numbers, e.g. \\(10 + 10^{-16}\\), which will just get rounded down to \\(10.0\\).\n\n\nCode\nprint(f'10.0 + 1e-16 = {10.0 + 1e-16}')\n\n\n10.0 + 1e-16 = 10.0\n\n\nNumerical roundoff is often an issue when subtracting two floats. Here’s what happens when we try to subtract two numbers that should be equal, \\(x=0.1+0.2\\) and \\(y=0.3\\). Instead of \\(y-x=0\\), we get \\(y-x \\approx -5.55 \\cdot 10^{-17}\\). The problem comes from the calculation \\(x=0.1+0.2\\), which caused a slight loss of precision in \\(x\\).\n\n\nCode\nx = 0.1 + 0.2\ny = 0.3\nprint(f'y-x = {y - x}')\n\n\ny-x = -5.551115123125783e-17\n\n\nA major implication of these calculations is that you should never test floating points for exact equality because numerical roundoff can mess it up. If you’d tried to test something like (y - x) == 0.0, you’d have gotten the wrong answer. Instead, you want to test that y - x is less than some small number tol, called a tolerance, i.e. abs(y - x) < tol.\n\n\nCode\ntol = 1e-5\nprint(f'(y - x == 0.0) = {y - x == 0.0}')\nprint(f'(abs(y - x) < tol) = {abs(y - x) < tol}')\n\n\n(y - x == 0.0) = False\n(abs(y - x) < tol) = True\n\n\nNumerical roundoff explains why we got the weird results above when subtracting \\(1.2 - 4.3\\). The imperfect precision in the two numbers resulted in a numerical roundoff error, leading in the trailing \\(9\\)s that should’ve rounded up to \\(-3.1\\) exactly. In general, subtracting floats is one of the most dangerous operations to do, as it tends to lead to the highest loss of precision in calculations. The closer two numbers are to being equal the worse this loss of precision tends to get.\nI mentioned that double precision has a smallest number of \\(2^{-1022} \\approx 10^{-308}\\), but caveated that by saying that, by using a trick called subordinal numbers, we can get the smallest number down to about \\(10^{-324}\\). What did I mean by this? It turns out that the bits where the biased exponent \\(e'=0\\) (i.e. all exponent bits are zero) go mostly unused in the standard version of double precision. By using this zero exponent and allowing the mantissa \\(m\\) to take on all its possible values, we can get about \\(2^{52}\\) more values (since the mantissa has 52 bits). This lets us get all the way down to \\(2^{-1022} \\cdot 2^{-52} = 2^{-1074} \\approx 10^{-324}\\).\nPython (and numpy) by default implements double precision with subordinal numbers, as we can see.\n\n\nCode\nprint(f'2 ** (-1074) = {2 ** (-1074)}')\nprint(f'2 ** (-1075) = {2 ** (-1075)}')\n\n\n2 ** (-1074) = 5e-324\n2 ** (-1075) = 0.0\n\n\nThe special numbers \\(\\pm \\infty\\), \\(\\pm 0\\), and \\(\\text{NaN}\\) are also defined in double precision. In python (and numpy) they’re given by the following commands,\n\n\\(\\infty\\): float('inf') or np.inf,\n\\(-\\infty\\): float('-inf') or -np.inf,\n\\(\\pm 0\\): 0,\n\\(\\text{NaN}\\): float('nan') or np.nan.\n\n\n\nCode\nprint(f\"float('inf') = {float('inf')}\")\nprint(f'np.inf = {np.inf}')\nprint(f\"float('-inf') = {float('-inf')}\")\nprint(f'-np.inf = {-np.inf}')\nprint(f'0 = {0}')\nprint(f'-0 = {-0}')\nprint(f\"float('nan') = {float('nan')}\")\nprint(f'np.nan = {np.nan}')\n\n\nfloat('inf') = inf\nnp.inf = inf\nfloat('-inf') = -inf\n-np.inf = -inf\n0 = 0\n-0 = 0\nfloat('nan') = nan\nnp.nan = nan\n\n\nYou may be curious what exactly \\(\\text{NaN}\\) (“not a number”) is and where it might show up. Basically, NaNs are used wherever values are undefined. Anytime an operation doesn’t return a sensible value it risks getting converted to NaN. One example is the operation \\(\\infty - \\infty = \\infty + (-\\infty)\\), which mathematically doesn’t make sense. No, it’s not zero…\n\n\nCode\nprint(f\"float('inf') + float('-inf') = {float('inf') + float('-inf')}\")\nprint(f'np.inf - np.inf = {np.inf - np.inf}')\n\n\nfloat('inf') + float('-inf') = nan\nnp.inf - np.inf = nan\n\n\nI’ll finish this section by mentioning that there are two other floating point representations worth being aware of: single precision (or float32), and half precision (or float16). Single precision uses 32 bits to represent a floating point number. Half precision uses 16 bits. It may seem strange to even bother having these less-precise precisions lying around, but they do have their uses. For example, half precision shows up in deep learning as a more efficient way to represent the weights of a neural network. Since half precision floats only take up 25% as many bits as default double precision floats do, using them can yield a 4x reduction in model memory sizes. We’ll see more on this later.\n\n\n3.2.4 Common Floating Point Pitfalls\nTo cap this long section on floats, here’s a list of common pitfalls people run into when working with floating point numbers, and some ways to avoid each one. This is probably the most important thing to take away from this section. You may find it helpful to reference later on. See this post for more information.\n\nNumerical overflow: Letting a number blow up to infinity (or negative infinity)\n\nClip numbers from above to keep them from being too large\nWork with the log of the number instead\nMake sure you’re not dividing by zero or a really small number\nNormalize numbers so they’re all on the same scale\n\nNumerical underflow: Letting a number spiral down to zero\n\nClip numbers from below to keep them from being too small\nWork with the exp of the number instead\nNormalize numbers so they’re all on the same scale\n\nSubtracting floats: Avoid subtracting two numbers that are approximately equal\n\nReorder operations so approximately equal numbers aren’t nearby to each other\nUse some algebraic manipulation to recast the problem into a different form\nAvoid differencing squares (e.g. when calculating the standard deviation)\n\nTesting for equality: Trying to test exact equality of two floats\n\nInstead of testing x == y, test for approximate equality with something like abs(x - y) <= tol\nUse functions like np.allclose(x, y), which will do this for you\n\nUnstable functions: Defining some functions in the naive way instead of in a stable way\n\nExamples: factorials, softmax, logsumexp\nUse a more stable library implementation of these functions\nLook for the same function but in log form, e.g. log_factorial or log_softmax\n\nBeware of NaNs: Once a number becomes NaN it’ll always be a NaN from then on\n\nPrevent underflow and overflow\nRemove missing values or replace them with finite values"
  },
  {
    "objectID": "notebooks/numerical-computing.html#array-computing",
    "href": "notebooks/numerical-computing.html#array-computing",
    "title": "3  Numerical Computation",
    "section": "3.3 Array Computing",
    "text": "3.3 Array Computing\nIn machine learning and most of scientific computing we’re not interested in operating on just single numbers at a time, but many numbers at a time. This is done using array operations. The most popular library in python for doing numerical computation on arrays is numpy.\nWhy not just do numerical computations in base python? After all, if we have large arrays of data we can just put them in a list. Consider the following example. Suppose we have two tables of data, \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\). Each table has \\(m=5\\) rows and \\(n=3\\) columns. The rows represent samples, e.g. measured in a lab, and the columns represent the variables, or features, being measured, call them \\(x\\), \\(y\\), and \\(z\\), if you like. I’ll define these two tables using python lists A and B below.\n\n\nCode\nA = [[3.5, 18.1, 0.3],\n     [-8.7, 3.2, 0.5],\n     [-1.3, 8.4, 0.2],\n     [5.6, 12.9, 0.9],\n     [-6.8, 19.7, 0.7]]\n\nB = [[-9.7, 12.5, 0.1],\n     [-5.1, 14.1, 0.6],\n     [-1.6, 3.7, 0.7],\n     [2.3, 19.3, 0.9],\n     [8.2, 9.7, 0.2]]\n\n\nSuppose we wanted to add the elements in these two tables together, index by index, like this,\n\\[\n\\begin{bmatrix}\nA[0][0] + B[0][0], & A[0][1] + B[0][1], & A[0][2] + B[0][2] \\\\\nA[1][0] + B[1][0], & A[1][1] + B[1][1], & A[1][2] + B[1][2] \\\\\nA[2][0] + B[2][0], & A[2][1] + B[2][1], & A[2][2] + B[2][2] \\\\\nA[3][0] + B[3][0], & A[3][1] + B[3][1], & A[3][2] + B[3][2] \\\\\nA[4][0] + B[4][0], & A[4][1] + B[4][1], & A[4][2] + B[4][2] \\\\\n\\end{bmatrix}.\n\\]\nIf we wanted to do this in python, we’d have to loop over all rows and columns and place the sums one-by-one inside an array \\(\\mathbf{C}\\), like this.\n\n\nCode\ndef add_arrays(A, B):\n    n_rows, n_cols = len(A), len(A[0])\n    C = []\n    for i in range(n_rows):\n        row = []\n        for j in range(n_cols):\n            x = A[i][j] + B[i][j]\n            row.append(x)\n        C.append(row)\n    return C\n\nC = add_arrays(A, B)\nprint(f'C = {np.array(C).round(2).tolist()}')\n\n\nC = [[-6.2, 30.6, 0.4], [-13.8, 17.3, 1.1], [-2.9, 12.1, 0.9], [7.9, 32.2, 1.8], [1.4, 29.4, 0.9]]\n\n\nNumpy makes this far easier to do. It implements element-wise array operatations, which allow us to operate on arrays with far fewer lines of code. In numpy, to perform the same adding operation we just did, we’d just add the two arrays together directly, \\(\\mathbf{A}+\\mathbf{B}\\).\nTo use numpy operations we have to convert data into the native numpy data type, the numpy array. Do this by wrapping lists inside the function np.array. Once we’ve done this, we can just add them together in one line. This will simultaneously element-wise add the elements in the array so we don’t have to loop over anything.\n\n\nCode\nA = np.array(A)\nB = np.array(B)\nprint(f'C = \\n{A+B}')\n\n\nC = \n[[ -6.2  30.6   0.4]\n [-13.8  17.3   1.1]\n [ -2.9  12.1   0.9]\n [  7.9  32.2   1.8]\n [  1.4  29.4   0.9]]\n\n\nThis is really nice. We’ve managed to reduce a double foor loop of 8 lines of code down to just 1 line with no loops at all. Of course, there are loops happening in the background inside the numpy code, we just don’t see them.\nNumpy lets us do this with pretty much any arithmetic operation we can think of. We can element-wise add, subtract, multiply, or divide the two arrays. We can raise them to powers, exponentiate them, take their logarithms, etc. Just like we would do so with single numbers. In numpy, arrays become first class citizens, treated on the same footing as the simpler numerical data types int and float. This is called vectorization.\nHere are a few examples of different vectorized functions we can call on A and B. All of these functions are done element-wise.\n\n\nCode\nprint(f'A - B = \\n{A-B}')\nprint(f'A / B = \\n{A / B}')\nprint(f'A ** B = \\n{A ** B}')\nprint(f'np.sin(A) = \\n{np.sin(A)}')\n\n\nA - B = \n[[ 13.2   5.6   0.2]\n [ -3.6 -10.9  -0.1]\n [  0.3   4.7  -0.5]\n [  3.3  -6.4   0. ]\n [-15.   10.    0.5]]\nA / B = \n[[-0.36082474  1.448       3.        ]\n [ 1.70588235  0.22695035  0.83333333]\n [ 0.8125      2.27027027  0.28571429]\n [ 2.43478261  0.66839378  1.        ]\n [-0.82926829  2.03092784  3.5       ]]\nA ** B = \n[[5.27885788e-06 5.25995690e+15 8.86568151e-01]\n [           nan 1.32621732e+07 6.59753955e-01]\n [           nan 2.62925893e+03 3.24131319e-01]\n [5.25814384e+01 2.71882596e+21 9.09532576e-01]\n [           nan 3.60016490e+12 9.31149915e-01]]\nnp.sin(A) = \n[[-0.35078323 -0.68131377  0.29552021]\n [-0.66296923 -0.05837414  0.47942554]\n [-0.96355819  0.85459891  0.19866933]\n [-0.63126664  0.32747444  0.78332691]\n [-0.49411335  0.75157342  0.64421769]]\n\n\nIf vectorization just made code easier to read it would be a nice to have. But it’s more than this. In fact, vectorization also makes your code run much faster in many cases. Let’s see an example of this. I’ll again run the same operations above to add two arrays, but this time I’m going to profile the code in each case. That is, I’m going to time each operation over several runs and average the times. The ones with the lowest average time is faster than the slower one, obviously. To profile in a notebook, the easiest way is to use the %timeit magic command, which will do all this for you.\n\n\nCode\nA = A.tolist()\nB = B.tolist()\n%timeit C = add_arrays(A, B)\n\n\n2.58 µs ± 7.26 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\n\n\nCode\nA = np.array(A)\nB = np.array(B)\n%timeit C = A + B\n\n\n412 ns ± 1.1 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n\n\nEven with these small arrays the numpy vectorized array addition is almost 10 times faster than the python loop array addition. This difference becomes much more pronounced when arrays are larger. The arrays just considered are only of shape \\((10,3)\\). We can easily confirm this in numpy using the methods A.shape and B.shape.\n\n\nCode\nprint(f'A.shape = {A.shape}')\nprint(f'B.shape = {B.shape}')\n\n\nA.shape = (5, 3)\nB.shape = (5, 3)\n\n\nLet’s try to run the add operations on much larger arrays of shape \\((10000,100)\\). To do this quickly I’ll use np.random.rand(shape), which will sample an array with shape shape whose values are uniformly between 0 and 1. More on sampling in a future lesson. Running the profiling, we’re now running about 100 times faster using numpy vectorization compared to python loops.\n\n\nCode\nD = np.random.rand(10000, 100)\nE = np.random.rand(10000, 100)\n\n\n\n\nCode\nD = D.tolist()\nE = E.tolist()\n%timeit F = add_arrays(D, E)\n\n\n118 ms ± 876 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\n\n\nCode\nD = np.array(D)\nE = np.array(E)\n%timeit F = D + E\n\n\n1.21 ms ± 32.7 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n\n\nSo why is numpy vectorization so much faster than using native python loops? Because it turns out that numpy by and large doesn’t actually perform array operations in python! When array operations are done, numpy compiles them down to low-level C code and runs the operations there, where things are much faster.\nNot only that, numpy takes advantage of very efficient linear algebra functions written over the course of decades by smart people. These functions come from low-level FORTRAN and C libraries like BLAS and LAPACK. They’re hand-designed to take maximum advantage of computational speed-ups where available. These include things like parallelization, caching, and hardware vectorization operations. Native python doesn’t take advantage of any of these nice things. The moral is, if you want to run array operations efficiently, you need to use a numerical library like numpy or modern variants like pytorch.\n\n3.3.1 Higher-Dimensional Arrays\nThe length of the shape of an array is called its dimension or rank. The arrays I showed above are examples of rank-2 or 2-dimensional arrays. We can define arrays with any number of dimensions we like. These arrays of different rank sometimes have special names:\n\nA 0-dimensional (rank-0) array is called a scalar. These are single numbers.\nA 1-dimensional (rank-1) array is called a vector. These are arrays with only one row.\nA 2-dimensional (rank-2) array is called a matrix. These are arrays with multiple rows.\nAn array of dimension or rank 3 or higher is called a tensor. These are arrays with multiple matrices.\n\nHere are some examples so you can see what they look like. Note I’m using dtype=np.float64 to explicitly cast the values as float64 when defining the arrays. Numpy’s vectorization operations work for all of these arrays regardless of their shape.\n\n\nCode\nscalar = np.float64(5)\nprint(f'scalar = {scalar}') # 0-dimensional\n\n\nscalar = 5.0\n\n\n\n\nCode\nvector = np.array([1, 2, 3], dtype=np.float64)\nprint(f'vector.shape = {vector.shape}')\nprint(f'vector = {vector}')\n\n\nvector.shape = (3,)\nvector = [1. 2. 3.]\n\n\n\n\nCode\nmatrix = np.array([[1, 2, 3], [4, 5, 6]], dtype=np.float64)\nprint(f'matrix.shape = {matrix.shape}')\nprint(f'matrix = \\n{matrix}')\n\n\nmatrix.shape = (2, 3)\nmatrix = \n[[1. 2. 3.]\n [4. 5. 6.]]\n\n\n\n\nCode\ntensor = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]], dtype=np.float64)\nprint(f'tensor.shape = {tensor.shape}')\nprint(f'tensor = \\n{tensor}')\n\n\ntensor.shape = (2, 2, 2)\ntensor = \n[[[1. 2.]\n  [3. 4.]]\n\n [[5. 6.]\n  [7. 8.]]]\n\n\nNumpy also supports array aggregation operations as well. Suppose you have a matrix A and want to get the sum of the values in each row of A. To do this, you could use np.sum(A, axis=1), where axis is the index of the dimension you want to sum over (the columns in this case). This will return a vector where the value at index \\(i\\) is the sum of elements in row \\(i\\). To sum over all elements in the array, don’t pass anything to axis.\n\n\nCode\nA = np.array([[1, 2, 3], [-1, -2, -3], [1, 0, -1]], dtype=np.float64)\nprint(f'A = \\n{A}')\nprint(f'sum over all A = {np.sum(A)}')\nprint(f'row sums of A = {np.sum(A, axis=1)}')\n\n\nA = \n[[ 1.  2.  3.]\n [-1. -2. -3.]\n [ 1.  0. -1.]]\nsum over all A = 0.0\nrow sums of A = [ 6. -6.  0.]\n\n\nIndexing into numpy arrays like A is more powerful than with python lists. Instead of having to awkwardly index like A[1][0], write A[1, 0]. To get all values in column index 1, write A[:, 1]. To get just the first and last row, we could just pass the index we want in as a list like this, A[[0, -1], :].\n\n\nCode\nprint(f'A[1, 0] = {A[1][0]} = {A[1, 0]}')\n\n\nA[1, 0] = -1.0 = -1.0\n\n\n\n\nCode\nprint(f'col 1 of A = {A[:, 1]}')\nprint(f'rows 0 and -1 of A = \\n{A[[0, -1], :]}')\n\n\ncol 1 of A = [ 2. -2.  0.]\nrows 0 and -1 of A = \n[[ 1.  2.  3.]\n [ 1.  0. -1.]]\n\n\nNumpy also supports Boolean masks as indexes. Suppose we want to get all the positive elements x >= 0 in A. We could create a mask A > 0, and pass that into A as an index to pick out the positive elements only.\n\n\nCode\nprint(f'mask of (A >= 0) = \\n{(A >= 0)}')\nprint(f'elements of (A >= 0) = \\n{A[A >= 0]}')\n\n\nmask of (A >= 0) = \n[[ True  True  True]\n [False False False]\n [ True  True False]]\nelements of (A >= 0) = \n[1. 2. 3. 1. 0.]"
  },
  {
    "objectID": "notebooks/numerical-computing.html#broadcasting",
    "href": "notebooks/numerical-computing.html#broadcasting",
    "title": "3  Numerical Computation",
    "section": "3.4 Broadcasting",
    "text": "3.4 Broadcasting\nBroadcasting is a set of conventions for doing array operations on arrays with incompatible shapes. This may seem like a strange thing to do, but it turns out knowing how and when to broadcast can make your code much shorter, more readable, and efficient. All modern-day numerical libraries in python support broadcasting, including numpy, pytorch, tensorflow, etc. So it’s a useful thing to learn.\n\n3.4.1 Motivation\nLet’s start with a simple example. Suppose we have an array of floats defined below. We’d like to add 1 to every number in the array. How can we do it? One “pythonic” way might be to use a list comprehension like so. This will work just fine, but it requires going back and forth between arrays and lists.\n\n\nCode\nx = np.array([1., 2., 3., 4., 5.])\nprint(f'x = {x}')\n\nx_plus_1 = np.array([val + 1 for val in x])\nprint(f'x + 1 = {x_plus_1}')\n\n\nx = [1. 2. 3. 4. 5.]\nx + 1 = [2. 3. 4. 5. 6.]\n\n\nWhat if we didn’t want to go back and forth like that? It is slow after all. Anytime numpy has to handoff back to python or vice versa it’s going to slow things down. Another thing we could try is to make a vector of ones of the same size as x, then add it to x. This is also fine, but it requires defining this extra array of ones just to add 1 to the original array.\n\n\nCode\nones = np.ones(len(x))\nx_plus_1 = x + ones\nprint(f'x + 1 = {x_plus_1}')\n\n\nx + 1 = [2. 3. 4. 5. 6.]\n\n\nWe’d like to be able to just add 1 to the array like we would with numbers. If x were a single number we’d just write x + 1 to add one to it, right? But technically we can’t do this if x is an array, since x has shape (5,) and 1 is just a number with no shape. This is where broadcasting comes in. Broadcasting says let’s define the operation x + 1 so that it means add 1 to every element of x.\n\n\nCode\nx_plus_1 = x + 1\nprint(f'x + 1 = {x_plus_1}')\n\n\nx + 1 = [2. 3. 4. 5. 6.]\n\n\nThis notation has the advantage of keeping array equations simple, while at the same time keeping all operations in numpy so that they run fast.\n\n\n3.4.2 Broadcasting Rules\nSuppose now that we have two arrays A and B of arbitrary shape and we want to operate on them, e.g. via the operations +, -, *, /, //, **. Here are the general broadcasting rules, quoted directly from the numpy documentation.\n\nNumpy DocumentationWhen operating on two arrays, numpy compares their shapes element-wise. It starts with the trailing (i.e. rightmost) dimensions and works its way left. Two dimensions are compatible when 1. they are equal, or2. one of them is 1 If these conditions are not met, a ValueError: operands could not be broadcast together exception is thrown, indicating that the arrays have incompatible shapes. The size of the resulting array is the size that is not 1 along each axis of the inputs.\n\nLet’s look at an example. First, suppose A has shape (2, 2, 3) and B has shape (3,). Suppose for simplicity that they’re both arrays of all ones. Here’s what this looks like, with B aligned to the right.\n\\[\\begin{align*}\nA &:& 2, & & 2, & & 3 \\\\\nB &:&   & &   & & 3 \\\\\n\\hline\nC &:& 2, & & 2, & & 3 \\\\\n\\end{align*}\\]\nHere are the broadcasting steps that will take place. Note that only B will change in this example. A will stay fixed.\n\nNumpy will start in the rightmost dimension, checking if they’re equal.\nBegin with A of shape (2, 2, 3) and B of shape (3,).\nIn this case, the rightmost dimension is 3 in both arrays, so we have a match.\nMoving left by one, B no longer has anymore dimensions, but A has two, each 2. These arrays are thus compatible.\nNumpy will now copy B to the left in these new dimensions until it has the same shape as A.\n\nCopy values of B twice to get B = [[1, 1, 1], [1, 1, 1]] with shape (2, 3).\nCopy values of B twice again to get B = [[[1, 1, 1], [1, 1, 1]], [[1, 1, 1], [1, 1, 1]]] with shape (2, 2, 3).\n\nThe shapes of A and B are now equal. The output array C will have shape (2, 2, 3).\n\nLet’s verify this is true on two simple arrays of ones. Let’s also print out what C looks like. Since only copying is taking place we should just be adding 2 arrays of ones, hence the output should sum 2 arrays of ones, giving one array C of twos.\n\n\nCode\nA = np.ones((2, 2, 3))\nB = np.ones(3,)\nprint(f'A.shape = {A.shape}')\nprint(f'B.shape = {B.shape}')\n\nC = A + B\nprint(f'C.shape = {C.shape}')\nprint(f'C = \\n{C}')\n\n\nA.shape = (2, 2, 3)\nB.shape = (3,)\nC.shape = (2, 2, 3)\nC = \n[[[2. 2. 2.]\n  [2. 2. 2.]]\n\n [[2. 2. 2.]\n  [2. 2. 2.]]]\n\n\nLet’s do one more example. Suppose now that A has shape (8, 1, 6, 1) and B has shape (7, 1, 5). Here’s a table of this case, again with B aligned to the right since it has the fewest dimensions.\n\\[\\begin{align*}\nA &:& 8, & & 1, & & 6, & & 1 \\\\\nB &:&    & & 7, & & 1, & & 5 \\\\\n\\hline\nC &:& 8, & & 7, & & 6, & & 5 \\\\\n\\end{align*}\\]\nHere are the broadcasting steps that will take place.\n\nStarting again from the right, dimensions 1 and 5 don’t match. But since A has a 1 rule (2) applies, so A will broadcast itself (i.e. copy its values) 5 times in this dimension to match B.\nMoving left by one we get 6 and 1. Now B will broadcast itself in this dimension 6 times to match A.\nMoving left again we get 1 and 7. Now A will broadcast itself in this dimension 7 times to match B.\nLast, we get 8 in A and B is out of dimensions, so B will broadcast itself 8 times to match A.\nThe shapes of A and B are now equal. The output C thus has shape (8, 7, 6, 5).\n\nHere again is an example on two arrays of ones. Verify that the shapes come out right.\n\n\nCode\nA = np.ones((8, 1, 6, 1))\nB = np.ones((7, 1, 5))\nprint(f'A.shape = {A.shape}')\nprint(f'B.shape = {B.shape}')\n\nC = A / B\nprint(f'C.shape = {C.shape}')\n\n\nA.shape = (8, 1, 6, 1)\nB.shape = (7, 1, 5)\nC.shape = (8, 7, 6, 5)\n\n\nThat’s pretty much all there is to broadcasting. It’s a systematic way of trying to copy the dimensions in each array until they both have the same shape. All this broadcasting is done under the hood for you when you try to operate on two arrays of different shapes. You don’t need to do anything but understand how the arrays get broadcast together so you can avoid errors in your calculations, sometimes very subtle errors. This can be a bit confusing to understand if you’re not used to it. We’ll practice broadcasting a good bit so you can get the hang of it."
  },
  {
    "objectID": "notebooks/numerical-computing.html#floating-point-operations",
    "href": "notebooks/numerical-computing.html#floating-point-operations",
    "title": "3  Numerical Computation",
    "section": "3.5 Floating Point Operations",
    "text": "3.5 Floating Point Operations\nTypically, when working with numerical algorithms it’s conventional to measure the time the algorithm takes to run using floating point operations, or FLOPS for short. The main difference between counting the FLOPs of a program and computing its algorithmic runtime is that we only count floating point operations when counting FLOPs. We don’t count lines that perform logical operations or lines that define a statement like a for loop or a function signature. Other than that, counting FLOPS works exactly the same way as computing algorithmic runtime, including the use of the big-O notation.\nLet’s do an example. Suppose we want to element-wise multiply two vectors x and y each of size \\(n\\). If we didn’t use numpy, we might implement this using the following simple function:\n def element_wise_multiply(x, y):\n    n = len(x)\n    z = [x[i] * y[i] for i in range(n)]\n    return z\nTo calculate the FLOPS of this function, we need to figure out how many operations of the type +, -, *, /, //, % are being executed as the function runs over the input. First, note that the function signature, the definition for n, and the return statement aren’t floating point operations. The only line doing any floating point is\nz = [x[i] * y[i] for i in range(n)]\nThis line is really a short-hand way of writing the following loop:\nz = [0] * n\nfor i in range(n):\n    z[i] = x[i] * y[i]\nNow, the definition of z isn’t actually a FLOP since the * is being used to define a list of length n. The only FLOPS evidently being performed are each of the z[i] = x[i] * y[i] operations. How many of these are being done? Since we’re doing one each time we run through the loop, we’re evidently doing n total FLOPS.\nThus, this simple element-wise multiplication algorithm us running in exactly \\(n\\) FLOPS. Just as we’d do for algorithmic runtime, we’d write this using big-O notation by saying the algorithm does \\(O(n)\\) total FLOPS.\nAs a very rough rule of thumb, algorithms that use less than \\(O(n^3)\\) FLOPS are considered “fast”, and algorithms that use at or above \\(O(n^3)\\) FLOPS are considered slow. Worst of all are algorithms that require exponentially many FLOPS or worse. Those are usually recursive in nature, often involving trees or graphs.\nJust as with algorithmic runtime and memory, take these asymptotic estimates with something of a grain of salt. They’re just abstractions. At the end of the day what we really care about is how fast our code runs and how much memory it’s using on our machines. Remember, asymptotic estimates ignore the constant factor out front of the leading term, which can often be a big deal in practice. When in doubt, always profile your code to figure out which lines are running slow and think about how you can speed them up."
  },
  {
    "objectID": "notebooks/calculus.html#infinitesimal-calculus",
    "href": "notebooks/calculus.html#infinitesimal-calculus",
    "title": "4  Calculus",
    "section": "4.1 Infinitesimal Calculus",
    "text": "4.1 Infinitesimal Calculus\n\n4.1.1 Infinitesimals\nPerhaps most fundamental to what calculus is is the idea of an infinitely small number, usually called an infinitesimal. A positive infinitesimal is a hyperreal number \\(\\varepsilon\\) that’s much smaller than any positive real number \\(x\\),\n\\[0 < \\varepsilon \\ll x.\\]\nA negative infinitesimal is defined similarly, except with the signs all flipped, \\(x \\ll \\varepsilon < 0\\). I say “hyperreal number” because infinitesimals aren’t technically real numbers. They extend the number line in a sense. Thankfully we won’t really have to worry about this technical distinction in this book.\nFor practical purposes, think of infinitesimals as a really, really small numbers. When we say infinitesimal in practice, we usually mean a really small number like \\(\\varepsilon = 10^{-10}\\). This means \\(\\varepsilon^2 = 10^{-20}\\). If we take say \\(x=1\\), then \\(\\varepsilon=10^{-10}\\) is really small compared to \\(x\\), but \\(\\varepsilon^2 = 10^{-20}\\) is really really small compared to \\(x\\), so small that it might as well be zero,\n\\[\\varepsilon^2 \\approx 0.\\]\nA computer wouldn’t even be able to tell the difference really between \\(x\\) and \\(x + \\varepsilon^2\\).\n\n\nCode\nx = 1\nepsilon = np.float64(1e-10)\nprint(f'x = {x}')\nprint(f'epsilon = {epsilon}')\nprint(f'x + epsilon = {1 - epsilon}')\nprint(f'epsilon^2 = {epsilon ** 2}')\nprint(f'x + epsilon^2 = {1 - epsilon ** 2}')\n\n\nx = 1\nepsilon = 1e-10\nx + epsilon = 0.9999999999\nepsilon^2 = 1.0000000000000001e-20\nx + epsilon^2 = 1.0\n\n\nTo see an example of how to work with infinitesimals, let’s try to simplify the expression \\((1 + \\varepsilon)^n\\), assuming \\(\\varepsilon\\) is some infinitesimal added to \\(1\\), and \\(n\\) is some positive integer. If we expand out the product in powers of \\(\\varepsilon\\), we’d have\n\\[(1 + \\varepsilon)^n = (1 + \\varepsilon) (1 + \\varepsilon) \\cdots (1 + \\varepsilon) = 1 + n\\varepsilon + \\frac{1}{2}n(n-1)\\varepsilon^2 + \\cdots + \\varepsilon^n.\\]\nNow, since \\(\\varepsilon^2 \\approx 0\\), all the powers of order \\(\\varepsilon^2\\) or higher vanish, so we’re just left with\n\\[(1 + \\varepsilon)^n \\approx 1 + n\\varepsilon.\\]\nThis neat little approximation is called the binomial approximation. In fact, it holds for any real number \\(n\\), not just positive integers.\n\n\n4.1.2 Infinitely Large Numbers\nThe inverse of an infinitesimal must evidently be a really big number \\(N = \\frac{1}{\\varepsilon}\\). These are called infinitely large numbers. Technically speaking, a positive infinitely large number is a hyperreal number that’s much larger than any positive real number \\(x\\),\n\\[0 \\leq x \\ll N.\\]\nSimilarly for infinitely large negative numbers, we’d have \\(N \\ll x < 0\\).\nIf in practice we’d think of an infinitesimal as a tiny number like \\(\\varepsilon = 10^{-10}\\), we’d think of an infinitely large number a really big number like \\(N=10^{10}\\). If \\(N\\) is really big, then \\(N^2\\) is really really big, so big that it might as well be infinity,\n\\[N^2 \\approx \\infty.\\]\nIf \\(x=1\\), then \\(x + N \\approx N\\). As far as a computer is concerned you can’t really even tell the difference.\n\n\nCode\nx = 1\nN = np.float64(1e10)\nprint(f'x = {x}')\nprint(f'N = {N}')\nprint(f'x + N = {1 + N}')\nprint(f'N^2 = {N ** 2}')\nprint(f'x + N^2 = {1 + N ** 2}')\n\n\nx = 1\nN = 10000000000.0\nx + N = 10000000001.0\nN^2 = 1e+20\nx + N^2 = 1e+20\n\n\n\n\n4.1.3 Continuity\nCalculus is mainly concerned with the study of functions that are continuous. Informally speaking, a function is continuous if its graph has no breaks in it. You could draw the graph on a piece of paper without lifting your pen. A better way of saying the same thing is that each point on the graph always has points infinitesimally close to it.\nFormally speaking, we’d say a function \\(y=f(x)\\) is continuous at a point \\(a\\) if \\(f(x)\\) is infinitesimally close to \\(f(a)\\) whenever \\(x\\) is infinitesimally close to \\(a\\). That is, any points nearby \\(a\\) must get mapped to points nearby \\(f(a)\\). The function itself is called continuous if it’s continuous for every choice of \\(a\\).\nJust about every function you’re familiar with is continuous. Constant functions, linear functions, affine functions, polynomials, exponentials, logarithms, roots, they’re all continuous. There are no breaks in their graphs.\nOne example of a function that is not continuous is the step function \\(y=u(x)\\) given by,\n\\[y =\n\\begin{cases}\n1, & x \\geq 0 \\\\\n0, & x < 0. \\\\\n\\end{cases}\n\\]\nThis function is continuous for every \\(x \\neq 0\\), but at \\(x=0\\) there’s a discontinuous jump from \\(y=0\\) to \\(y=1\\). If I pick any negative \\(x\\) infinitesimally close to \\(0\\) it’ll have \\(f(x)=0\\), which is not infinitesimally close to \\(f(0)=1\\).\n\n\nCode\nx = np.arange(-10, 10, 0.01)\nf = lambda x:  (x >= 0)\nplot_function(x, f, xlim=(-3, 3), ylim=(-1, 2), ticks_every=[1, 0.5], \n              title='$y=u(x)$')\n\n\n\n\n\n\n\n\n\nStep type functions aren’t the only examples of discontinuous functions though. Any function with a pole or asymptote will have a discontinuity at those points. For example, the rational function\n\\[y=\\frac{x^3+x+1}{x^2-1}.\\]\nevidently has poles at \\(x = \\pm 1\\), which means the function will be discontinuous at these two points. Everywhere else it’s continuous.\n\n\nCode\nx = np.arange(-10, 10, 0.01)\nf = lambda x: (x ** 3 + x + 1) / (x ** 2 - 1)\nplot_function(x, f, xlim=(-5, 5), ylim=(-5, 5), ticks_every=[1, 1], \n              title='$y = (x^3 + x + 1) / (x^2 - 1)$')\n\n\n\n\n\n\n\n\n\nIn practice these continuity distinctions usually don’t matter a whole lot. Step functions and rational functions are continuous almost everywhere. For all but a small handful of values everything’s fine and we can apply calculus as usual. However, there are pathological cases out there of functions that are nowhere continuous. An example might be any function generated by a random number generator. The points never connect. They’re all over the place. Functions like these we wouldn’t try to treat using calculus, at least directly.\n\n\nCode\nx = np.arange(-10, 10, 0.01)\nf = lambda x: np.random.rand(len(x))\nplt.figure(figsize=(4, 3))\nplt.scatter(x, f(x), s=1)\nplt.title('$y=rand(x)$')\nplt.xlabel('$x$')\nplt.ylabel('$y$')\nplt.show()\n\n\n\n\n\n\n\n4.1.4 Limits\nFor historical reasons, most modern calculus books don’t talk about infinitesimals or infinitely large numbers at all. It’s not because they’re not rigorous, at least anymore. It mostly has to do with the history of calculus. For a long time people had a hard time rigorously pinning down what exactly these infinitely small numbers were. To get around this, in the 19th century many mathematicians started to think instead in terms of limits.\nIn the language of infinitesimals, we’d say that the limit of a function \\(y=f(x)\\) as \\(x\\) approaches some value \\(a\\) is the value \\(L\\) such that\n\\[L \\approx f(a + \\varepsilon),\\]\nwhere \\(\\varepsilon\\) is any nonzero infinitesimal added to \\(a\\). Said differently, \\(f(x) \\approx L\\) whenever \\(x \\approx a\\) but \\(x \\neq a\\). Limits are usually written with the notation\n\\[\\lim_{x \\rightarrow a} f(x) = L.\\]\nIn the language of limits, we’d think of \\(x\\) being some value that gets closer and closer to \\(a\\), which then causes \\(f(x)\\) to get closer and closer to \\(L\\). This is a more dynamic view of calculus, where you imagine values moving towards a point instead of just nudging that point by some really small value.\nI personally find that the language of limits tends to complicate understanding of calculus. Anything phrased in terms of limits pretty much can be more easily phrased in terms of infinitesimals. Nevertheless, it’s good to know how to calculate limits. For continuous functions it’s easy. As long as \\(a\\) is a finite value and \\(f(x)\\) is continuous at \\(x=a\\), then the limit is just given by plugging in \\(f(a)\\) directly, i.e.\n\\[\\lim_{x \\rightarrow a} f(x) = f(a).\\]\nFor example, suppose we wanted to take the limit of \\(y = x^3 - 10x^2\\) as \\(x \\rightarrow 1\\). Since this function is continuous, the limit would be\n\\[\\lim_{x \\rightarrow 1} (x^3 - 10x^2) = 1^3 - 10 \\cdot 1^2 = 1 - 10 = -9.\\]\nWhen the function isn’t continuous you have to be more careful and actually add an infinitesimal to \\(a\\) to see what the function is doing nearby. It may be the case that what the function is doing to the left at \\(a-\\varepsilon\\) is different from what the function is doing to the right at \\(a+\\varepsilon\\). In that case the ordinary limit doesn’t exist and we have to talk about one-sided limits. I won’t spend time on these.\nAnother important case not covered by just plugging in \\(f(a)\\) directly is when we’re interested in infinite limits. That is, when \\(x \\rightarrow \\infty\\) or \\(x \\rightarrow -\\infty\\). In these cases we have to plug in an infinitely large number \\(N\\) and look at what’s happening as \\(N\\) gets really big. For example, suppose we wanted to take the limit of \\(y = x^3 - 10x^2\\) as \\(x \\rightarrow \\infty\\). If we plug an infinitely large number into this function we’d get \\(f(N) = N^3 - 10N^2\\). Since \\(N^3 \\gg N^2\\) when \\(N\\) is really big, we’d have \\(f(N) \\approx N^3\\). This is a cubic function, which of course goes off to infinity as \\(N\\) gets large. That is,\n\\[\\lim_{x \\rightarrow \\infty} (x^3 - 10x^2) = \\infty.\\]\nAs a more interesting example, suppose we wanted to take the limit of the following rational function as \\(x \\rightarrow \\infty\\),\n\\[y = \\frac{7x^5 + x^3 - x + 1}{2x^5 - 3x^3 + x^2 - 4}.\\]\nIf we plot this function, we can see that it’s evidently approaching some finite value as \\(x\\) gets large, something like \\(y = 3.5\\).\n\n\nCode\nx = np.linspace(-20, 20, 100)\nf = lambda x: (7 * x**5 + x**3 - x + 1) / (2 * x**5 - 3 * x**3 + x**2 - 4)\nplot_function(x, f, xlim=(0, 20), ylim=(0, 5), ticks_every=[2, 1], title_fontsize=13,\n              title='$y=\\\\frac{7x^5 + x^3 - x + 1}{2x^5 - 3x^3 + x^2 - 4}$ as $x \\\\rightarrow \\infty$')\n\n\n\n\n\nIn fact, it goes like the ratio of the leading powers in the numerator and denominator. If \\(N\\) is infinitely large, then\n\\[f(N) \\approx \\frac{7N^5}{2N^5} = \\frac{7}{2} = 3.5.\\]\nThis is a general fact for rational functions as \\(x \\rightarrow \\infty\\). The final limit we seek is thus\n\\[\\lim_{x \\rightarrow \\infty} \\frac{7x^5 + x^3 - x + 1}{2x^5 - 3x^3 + x^2 - 4} = \\frac{7}{2} = 3.5.\\]\nWith infinite limits, when in doubt, plot the function and see if it’s stabilizing to some value or going off to positive or negative infinity. Or just code up the function and plug in some large values. The vast majority of the time you’ll get the right answer.\n\n\n4.1.5 Asymptotic Notation\nAt their core, infinitesimals and infinitely large numbers are most useful for getting approximations to functions. Infinitesimals are useful when we want to know how a function behaves near zero. Infinitely large numbers are useful when we want to know how a function behaves near infinity. We can use these two notions to define a convenient simplified notation called asymptotic notation or big-O notation. Asymptotic notation is used to give us a general idea how a function behaves in extreme limits. It ignores the finer details of the function and just focuses on its growth behavior. This notation is extremely pervasive across computer science and applied mathematics, and well worth being familiar with.\nSuppose \\(f(x)\\) is some function. We say \\(f(x) = O(g(x))\\) in the infinite limit if for any infinitely large number \\(N\\), \\(f(N)\\) is bounded above by some real-valued constant multiple of \\(g(N)\\),\n\\[f(N) \\leq C \\cdot g(N).\\]\nInformally, this says the graph of \\(f(x)\\) lies under some constant multiple of the graph of \\(g(x)\\) when \\(x\\) is large. It doesn’t mean that \\(C \\cdot g(x)\\) is always bigger than \\(f(x)\\). It means that if \\(x\\) increases eventually there will come a point where \\(C \\cdot g(x)\\) is bigger than \\(f(x)\\).\nSimilarly, we say \\(f(x) = \\mathcal{O}(h(x))\\) in the zero limit if for any infinitesimal \\(\\varepsilon\\), \\(f(\\varepsilon)\\) is bounded above by some real-valued constant multiple of \\(g(\\varepsilon)\\),\n\\[f(\\varepsilon) \\leq C \\cdot h(\\varepsilon).\\]\nInformally, this says the graph of \\(f(x)\\) lies under some constant multiple of the graph of \\(h(x)\\) when \\(x\\) is small. Again, it doesn’t mean that \\(C \\cdot g(x)\\) is always bigger than \\(f(x)\\). It means that if \\(x\\) gets smaller and smaller, eventually there will come a point where \\(C \\cdot g(x)\\) is bigger than \\(f(x)\\).\nLet’s look at a simple example. Consider the cubic function\n\\[f(x) = x^3 + 2x^2 - x + 2.\\]\n\n\nCode\nx = np.linspace(-10, 10, 100)\nf = lambda x: x**3 + 2 * x**2 - x + 2\nplot_function(x, f, xlim=(-5, 5), ylim=(-3, 7), ticks_every=[1, 1])\n\n\n\n\n\nLet’s first ask what the function is doing as \\(x\\) gets infinitely large. It should be pretty clear from the graph that it’s shooting up to infinity. But how fast? Take \\(N\\) to be infinitely large. Then\n\\[f(N) = N^3 + 2N^2 - N - 1.\\]\nNow, if \\(N\\) is infinitely large, then \\(N\\) will be huge compared to \\(1\\), but \\(N^2\\) will be big compared to \\(N\\) too, and \\(N^3\\) will be big compared to \\(N^2\\). That is,\n\\[N^3 \\gg N^2 \\gg N \\gg 1.\\]\nIf we approximate the function with its highest order term, we just have\n\\[f(N) \\approx N^3.\\]\nThat is, near infinity, \\(f(x)\\) grows like the ordinary cubic function \\(g(x) = x^3\\). But is there some constant \\(C\\) we can scale \\(g(x)\\) by so that \\(f(N) \\leq C \\cdot g(x)\\)? Yes there is. Observe that near infinity we have\n\\[f(N) = N^3 + 2N^2 - N - 1 \\leq N^3 + 2N^3 + N^3 + N^3 = 5N^3.\\]\nThat is, if \\(C=5\\), then \\(f(N) \\leq C \\cdot g(x)\\). In the infinite limit we thus have\n\\[x^3 + 2x^2 - x + 2 = O(x^3).\\]\nThis is just a fancy way of saying that our function \\(f(x)\\) lies on or below the curve \\(y=5x^3\\) when \\(x\\) gets really big.\nLet’s now ask what the function is doing as \\(x\\) becomes infinitesimal. It looks like it takes on the value \\(y=-1\\) at \\(x=0\\). To see what the function is doing near zero, we can look at \\(y=f(\\varepsilon)\\) assuming \\(\\varepsilon\\) is infinitesimal,\n\\[f(\\varepsilon) \\approx \\varepsilon^3 + 2\\varepsilon^2 - \\varepsilon + 2.\\]\nNow, if \\(\\varepsilon\\) is tiny, then \\(\\varepsilon^2\\) is really tiny, and \\(\\varepsilon^3\\) is really really tiny. That is,\n\\[\\varepsilon^3 \\ll \\varepsilon^2 \\ll \\varepsilon \\ll 1.\\]\nIf we approximate the function with its lowest order term, we just have\n\\[f(\\varepsilon) \\approx 2.\\]\nThat is, very close to zero the function behaves like the constant function \\(h(x) = 2\\). Since we can just write \\(2 = 2 \\cdot 1\\), we might as well factor the \\(2\\) into the constant \\(C\\) and just take \\(h(x) = 1\\). Then in the zero limit we have\n\\[x^3 + 2x^2 - x + 2 = \\mathcal{O}(1).\\]\nThis just says that our function \\(f(x)\\) lies on or below the line \\(y=2\\) when \\(x\\) gets close to zero. Notice that you could’ve also gotten the same result by just evaluating \\(f(0)\\) directly to get \\(f(0) = 2\\). This is because this particular function is continuous.\nHere’s a plot so you can see what’s going on here. Notice that near zero, \\(f(x) \\leq 2.5 \\cdot h(x) = 2.5\\), and near infinity \\(f(x) \\leq 3 \\cdot g(x) = 3x^3\\).\n\n\nCode\nx = np.linspace(-10, 10, 100)\nf = lambda x: x**3 + 2 * x**2 - x + 2\ng_inf = lambda x: 5 * x**3\ng_0 = lambda x: 2.5 + 0 * x\nplot_function(x, [f, g_inf, g_0], xlim=(-5, 5), ylim=(0, 10), ticks_every=[1, 1], figsize=(5, 4),\n              labels=['$f(x)$', '$5g(x)$', '$2.5 h(x)$'])\n\n\n\n\n\nWhat I’ve just shown more or less holds true for any polynomial. If \\(f(x) = a_0 + a_1 x + \\cdots + a_n x^n\\) is some degree \\(n\\) polynomial, then in the infinite limit the highest degree term dominates, i.e. \\(f(x) = O(x^n)\\). In the zero limit the lowest degree term dominates, i.e. \\(f(x) = \\mathcal{O}(1)\\) provided \\(a_0 \\neq 0\\). If you remember nothing else about asymptotic notation, at least remember these two facts.\nThe same rule holds not just for polynomials, but for any function of powers, including rational and root functions. The most important difference here is that the lowest degree term won’t usually be a constant. It’ll be the term with the most negative power. For example, in the zero limit\n\\[\\sqrt{10 x} + x^{3/2} + \\frac{7}{x^3} = \\mathcal{O}\\bigg(\\frac{1}{x^3} \\bigg).\\]\nExponential and logarithmic functions lie on the extremes of the polynomials. In the infinite limit, the function \\(f(x) = e^x\\) will be larger than any power of \\(x\\). On the other hand, the log function \\(f(x) = \\log x\\) grows slower than any power of \\(x\\). The trig functions \\(f(x) = \\sin x\\) and \\(f(x) = \\cos x\\) are both of order one in both limits since they’re always bounded by \\(y=\\pm 1\\).\nHere are a few more general examples of working with asymptotic notation in both limits.\n\n\n\n\n\n\n\n\nFunction\nInfinite Limit\nZero Limit\n\n\n\\(f(x)=100\\)\n\\(O(1)\\)\n\\(O(1)\\)\n\n\n\\(f(x)=-x^4 + 2x^3 + 4x^2 + 8x + 16\\)\n\\(O(x^4)\\)\n\\(\\mathcal{O}(1)\\)\n\n\n\\(f(x)=4x^5 + 3x^4 + 2x^3 + x^2 + x\\)\n\\(O(x^5)\\)\n\\(\\mathcal{O}(x)\\)\n\n\n\\(f(x)=\\sqrt{10 x} + x^{3/2} + \\frac{7}{x^3}\\)\n\\(O(x^{3/2})\\)\n\\(\\mathcal{O}\\big(\\frac{1}{x^3} \\big)\\)\n\n\n\\(f(x)=10 e^x + 100 x^{1000} + x^3\\)\n\\(O(e^x)\\)\n\\(\\mathcal{O}(x^3)\\)\n\n\n\\(f(x)=\\log 50x - 5 x + 30 x^2\\)\n\\(O(x^2)\\)\n\\(\\mathcal{O}(\\log x)\\)\n\n\n\nWhen in doubt, when you want to figure out the asymptotic behavior of a function, plot it and see what it’s doing in the limit of interest. Here’s a plot of some common functions, zoomed out so we can get an idea of the infinite limit. Notice how exponential function quickly out-grows everything else. Next come the polynomials and roots ordered by the highest powers. Then the logarithms. Then the constant functions. And finally the negative powers.\n\n\nCode\nx = np.linspace(0, 20, 100)\nfs = [lambda x: 1 / x**2,\n      lambda x: 1 / x,\n      lambda x:  0 * x + 1,\n      lambda x:  np.log(x), \n      lambda x:  x,\n      lambda x: np.sqrt(x),\n      lambda x:  x ** 2, \n      lambda x:  np.exp(x)]\nplot_function(x, fs, xlim=(1, 10), ylim=(0, 10), ticks_every=[1, 1], figsize=(5, 4),\n              title='Infinite Limit', legend_loc='upper right', legend_fontsize=9,\n              labels=['$y = 1/x^2$',\n                      '$y = 1/x$',\n                      '$y=1$', \n                      '$y = \\log x$', \n                      '$y = x$',\n                      '$y = \\sqrt{x}$',\n                      '$y=x^2$',\n                      '$y = e^x$'])\n\n\n\n\n\n\n\n\n\nAnd here’s a plot of the same functions, zoomed in near zero to get an idea of the zero limit. Notice now how everything’s flipped around. It’s the negative powers that are the largest, followed by constant functions, then logs, then powers and roots. The exponential is the smallest one.\n\n\nCode\nx = np.linspace(0, 1, 100)\nfs = [lambda x: 1 / x**2,\n      lambda x: 1 / x,\n      lambda x:  0 * x + 1,\n      lambda x:  np.log(x), \n      lambda x:  x,\n      lambda x: np.sqrt(x),\n      lambda x:  x ** 2, \n      lambda x:  np.exp(x)]\nplot_function(x, fs, xlim=(0, 1), ylim=(0, 5), ticks_every=[0.2, 1], figsize=(5, 4),\n              title='Zero Limit', legend_loc='upper right', legend_fontsize=9,\n              labels=['$y = 1/x^2$',\n                      '$y = 1/x$',\n                      '$y=1$', \n                      '$y = \\log x$', \n                      '$y = x$',\n                      '$y = \\sqrt{x}$',\n                      '$y=x^2$',\n                      '$y = e^x$'])\n\n\n\n\n\n\n\n\n\nNote that frequently we omit which limit we’re talking about when we use asymptotic notation, assuming that it’s understood which limit we’re using in a given context. For example, when talking about the runtime, FLOPs, or memory of an algorithm, it’s always understood we’re in the infinite limit. When we’re talking about approximation errors or the convergence of numerical algorithms it’s understood we’re in the zero limit. In this book, I’ll use the italic big-O \\(O(\\cdot)\\) to refer to the infinite limit, and the script big-O \\(\\mathcal{O}(\\cdot)\\) to refer to the zero limit. Usually you’ll be able to tell which I’m talking about by the context."
  },
  {
    "objectID": "notebooks/calculus.html#differential-calculus",
    "href": "notebooks/calculus.html#differential-calculus",
    "title": "4  Calculus",
    "section": "4.2 Differential Calculus",
    "text": "4.2 Differential Calculus\n\n4.2.1 Differentials and Derivatives\nTo make it easier to tell what’s an infinitesimal with respect to what in a function, we often use the notation of differentials. If \\(x\\) is some variable, we’d denote any infinitesimal added to it by the symbol \\(dx\\), called the differential of \\(x\\). If \\(y=f(x)\\) is some function of \\(x\\), then the differential with respect to \\(y\\) is the amount that \\(y\\) changes if we change \\(x\\) to \\(x+dx\\),\n\\[dy = f(x+dx) - f(x).\\]\nIf it helps, you can think of \\(dx\\) as meaning “a little bit of \\(x\\)”, and \\(dy\\) as meaning “a little bit of \\(y\\)”.\nLet’s look at an example. Suppose \\(y=x^2\\). What is \\(dy\\)? Evidently, we’d have\n\\[dy = f(x + dx) - f(x) = (x+dx)^2 - x^2 = (x^2 + 2xdx + dx^2) - x^2 = 2xdx + dx^2.\\]\nThis is how much \\(y\\) changes if we change \\(x\\) by \\(dx\\). Now, if \\(dx\\) is infinitesimal, \\(dx^2 \\approx 0\\), which means\n\\[dy \\approx 2xdx.\\]\nNotice how if \\(dx\\) is infinitesimal, then \\(dy\\) will evidently be an infinitesimal too. If we change \\(x=1\\) by a really small amount \\(dx=10^{-10}\\), then we’d change \\(y=x^2=1\\) by \\(dy=2xdx=2 \\cdot 10^{-10}\\), which is also a really small change.\nThe ratio of these differentials says something about the rate that \\(y\\) changes if we change \\(x\\). It’s called the derivative of \\(y=x^2\\),\n\\[\\frac{dy}{dx} = 2x.\\]\nNotice that the derivative is not itself infinitesimal since all the differentials are on the left-hand side. It’s a real number, on the same scale as \\(x\\) and \\(y\\).\nOf course, we can calculate these things numerically too. We just have to be careful about floating point roundoff. Since we’re subtracting two numbers that are almost equal we’ll inevitably get a lot of roundoff if we make \\(dx\\) too small. When coding these things you have to be careful about how small you choose \\(dx\\). I’ll take \\(dx=10^{-5}\\) here. If the calculation were exact, we’d expect to get\n\\[dy=2xdx = 2 \\cdot 10^{-5} = 0.00002.\\]\nBut evidently we don’t get this exactly. In fact, since \\(dx^2 = 10^{-10}\\), we should expect to start seeing errors around 10 decimal places due to \\(dx\\) not being exactly infinitesimal, which is what we’re seeing here.\n\n\nCode\ndx = 1e-5\nx = 1\ny = 1 ** 2\ndy = (1 + dx) ** 2 - y\nprint(f'dy = {dy}')\nprint(f'dy/dx = {dy / dx}')\n\n\ndy = 2.0000100000139298e-05\ndy/dx = 2.00001000001393\n\n\nLet’s do another example. Suppose we have the function \\(y = x^3\\). Then we’d have\n\\[dy = f(x+dx) - f(x) = (x+dx)^3 - x^3 = (x^3 + 3x^2 dx + 3 x dx^2 + dx^3) - x^3 = 3x^2 dx + 3 x dx^2 + dx^3.\\]\nIf \\(dx\\) is infinitesimal, then \\(dx^2 \\approx 0\\), which also means \\(dx^3 \\approx 0\\). Thus, if \\(x\\) changes by a small amount \\(dx\\), then \\(y=x^3\\) changes by an amount\n\\[dy = 3x^2 dx.\\]\nThe derivative of \\(y=x^3\\) is again just the ratio of differentials,\n\\[\\frac{dy}{dx} = 3x^2.\\]\nHere’s a numerical calculation to verify this fact. I’ll again choose \\(x=1\\) and \\(dx=10^{-5}\\). We should expect to get \\(dy =3 \\cdot 10^{-5}\\) and \\(\\frac{dy}{dx} = 3\\). To within an error of about \\(dx^2=10^{-10}\\) it seems we do.\n\n\nCode\ndx = 1e-5\nx = 1\ny = 1 ** 3\ndy = (1 + dx) ** 3 - y\nprint(f'dy = {dy}')\nprint(f'dy/dx = {dy / dx}')\n\n\ndy = 3.0000300001109537e-05\ndy/dx = 3.000030000110953\n\n\nIn both examples, the amount that \\(y\\) changes if \\(x\\) changes by \\(dx\\) is evidently just the derivative times \\(dx\\),\n\\[dy = \\frac{dy}{dx} dx.\\]\nThe notation makes this fact look pretty trivial, since we can imagine canceling the \\(dx\\) terms on the right to get \\(dy\\). In fact, that’s exactly what we’re doing.\nIn general, if \\(y=f(x)\\) is some function that’s reasonably well-behaved at a point \\(x\\), then the differential of \\(y\\) at that point is\n\\[dy = f(x+dx) - f(x),\\]\nand the derivative of \\(y=f(x)\\) at that point is given by\n\\[\\frac{dy}{dx} = \\frac{f(x+dx) - f(x)}{dx}.\\]\nThis equality is exact provided \\(dx\\) is infinitesimal. Notice that both the differential and the derivative are themselves functions of \\(x\\). For this reason, it’s common to think of \\(\\frac{d}{dx}\\) is some kind of derivative operator and write\n\\[\\frac{dy}{dx} = \\frac{d}{dx} f(x), \\quad \\text{or} \\quad \\frac{dy}{dx} = f'(x).\\]\nWhat exactly did I mean when I said the function \\(f(x)\\) needs to be “reasonably well behaved” at the point \\(x\\)? For one thing, the function needs to be continuous. Continuity is what ensures that \\(dy = f(x+dx) - f(x)\\) will be infinitesimal whenever \\(dx\\) is infinitesimal. However, continuity is just a necessary condition for \\(f(x)\\) to be differentiable. It’s also necessary that the ratio\n\\[\\frac{dy}{dx} = \\frac{f(x+dx) - f(x)}{dx}\\]\nshouldn’t depend on choice of \\(dx\\). Provided \\(dx\\) is infinitesimal, \\(\\frac{dy}{dx}\\) should always be exactly the same number. This won’t be the case, for example, at points where there are kinks in the function’s graph. A major example of this is the ReLU function, which has a kink at \\(x=0\\). In practice though these kinks aren’t a huge problem. For example, for convex functions like the ReLU, we can slightly extend the notion of a derivative to the notion of a subderivative.\nWhen calculating a derivative numerically we have to account for the fact that realistically \\(dx\\) will not be infinitesimal. That means there will be an error when dividing \\(dy\\) by \\(dx\\) to get the derivative. We got the differential \\(dy\\) by dropping terms of order \\(dx^2\\). If we keep those terms, then dividing \\(dy\\) by \\(dx\\) will leave an error of order \\(dx\\). That is,\n\\[\\frac{dy}{dx} = \\frac{f(x+dx) - f(x)}{dx} + \\mathcal{O}(dx).\\]\nHere’s a quick python function diff that can numerically calculate the derivative of some function \\(f(x)\\) at a point \\(x\\). Here I chose \\(dx = 10^{-5}\\). The exact answer should be \\(\\frac{dy}{dx} = 2\\). Notice how the error \\(0.00001\\) is indeed of order \\(dx\\).\n\n\nCode\ndef diff(f, x, dx=1e-5):\n    dy = f(x + dx) - f(x)\n    return dy / dx\n\nf = lambda x: x ** 2\ndydx = diff(f, 1)\nprint(f'dy/dx = {dydx}')\n\n\ndy/dx = 2.00001000001393\n\n\nCalculating the derivative this way is called numerical differentiation. If you’re curious, it turns out you can improve the error in numerical differentiation calculations to order \\(dx^2\\) by centering the difference estimate \\(dy\\) symmetrically around \\(x\\). That is,\n\\[\\frac{dy}{dx} = \\frac{\\frac{1}{2}\\bigg(f\\big(x+\\frac{dx}{2}\\big)-f\\big(x-\\frac{dx}{2}\\big)\\bigg)}{dx} + \\mathcal{O}\\big(dx^2\\big).\\]\nOf course, the error goes to zero either way when \\(dx\\) is infinitesimal. It’s only when calculating derivatives numerically that we’ve got to worry about errors.\n\n\n4.2.2 Interpreting Derivatives\nSince \\(dy\\) is the change in \\(y\\) in response to the change in \\(x\\) by \\(dx\\), the derivative \\(\\frac{dy}{dx}\\) evidently represents some kind of a rate. It’s the rate that \\(y\\) changes in response to small changes in \\(x\\).\nDepending on what exactly \\(y\\) and \\(x\\) are we can think of the derivative as representing many different things. Here are some examples:\n\nIf \\(x\\) is the position of some particle and \\(t\\) is time, \\(\\frac{dx}{dt}\\) represents the speed or velocity of that particle at time \\(t\\).\nIf \\(p\\) is the pressure in the atmosphere at a height \\(z\\) from the Earth’s surface, then \\(\\frac{dp}{dz}\\) represents the pressure gradient, a measure of how much pressure varies with altitude.\nIf \\(q\\) is the quantity of some economic good and \\(p\\) is the price of that good, then \\(\\frac{dq}{dp}\\) represents the elasticity of that good, i.e. how responsive the good’s quantity is to changes in its price.\nIf \\(C\\) is the cost of some good and \\(q\\) is the amount of that good we have, then \\(\\frac{dC}{dq}\\) represents the marginal cost of that good. It’s the amount we’d have to pay to get one more unit of that good.\nIf \\(L\\) is the loss function of some machine learning algorithm and \\(\\theta\\) is some parameter of the model, then \\(\\frac{dL}{d\\theta}\\) represents the loss gradient, a measure of how sensitive the loss is to changes in that particular parameter.\n\nThe derivative also has a useful interpretation when thinking about a function graphically. It represents the slope of a function \\(y=f(x)\\) at the point \\(x\\). This follows from the fact that\n\\[dy = \\frac{dy}{dx} dx.\\]\nIf we relax the requirement that \\(dx\\) be infinitesimal, then it’s just the difference between two finite points, say \\(dx = x - a\\). That means \\(dy\\) is also the difference between two points \\(f(x) - f(a)\\). If we plug these into the above formula and solve for \\(f(x)\\), we get\n\\[f(x) \\approx f(a) + \\frac{d}{dx} f(a) (x - a).\\]\nIf we think of \\(a\\) as some fixed point and \\(x\\) as a variable, this is just the equation for a line. It’s a line passing through the point \\((a, f(a))\\) with a slope \\(\\frac{d}{dx} f(a)\\). This line is called the tangent line of the function at the point \\(a\\). To see why let’s do a quick example.\nSuppose again that \\(y=x^2\\). If we take \\(a=1\\), then \\(f(a) = 1\\) and \\(\\frac{d}{dx} f(a) = 2a = 2\\), so we have an equation\n\\[y \\approx 1 + 2(x - 1) = 2x - 1.\\]\nLet’s plot this line along with the function itself to see what’s going on. Notice the tangent line is hugging the function at the red point \\(a=1, f(a)=1\\), and that the slope of the line seems to hug the curve at that point. This is what it means to say that the derivative is the slope of a function at a point. It’s the slope of the tangent line passing through that point.\n\n\nCode\nf = lambda x: x ** 2\ndfdx = lambda x: 2 * x\n\na = 1\nx = np.arange(-3, 3, 0.1)\nf_line = lambda x: f(a) + dfdx(a) * (x - a)\n\nplot_function(x, [f, f_line], points=[[a, f(a)]], xlim=(-3, 3), ylim=(-2, 4),\n              title=f'Tangent to $y=x^2$ at $a$={a}')\n\n\n\n\n\n\n\n\n\nNotice a curious fact from this slope interpretation. At the minimum value of the function, in this case at \\(x=0\\), the slope is flat, which evidently implies that the derivative at the minimum is zero. We’ll exploit this fact in a future lesson when we talk about optimization.\nAnother way to interpret the equation\n\\[f(x) \\approx f(a) + \\frac{d}{dx} f(a) (x - a)\\]\nis that the right-hand side is the best linear approximation of the function at the point \\(x=a\\). Provided \\(x \\approx a\\), we can well-approximate \\(f(x)\\) by its tangent line. Another way of saying the same thing is that if we zoom in enough on the plot we can’t tell that the function isn’t linear.\n\n\nCode\nplot_function(x, [f, f_line], points=[[a, f(a)]], xlim=(0.7, 1.3), ylim=(0.7, 1.3),\n              title=f'$y=x^2$ near $a=1$')\n\n\n\n\n\n\n\n\n\n\n\n4.2.3 Second Derivatives\nDifferentials and derivatives cover the situation when we’re interested in changes in a function that are first-order in \\(dx\\). But suppose we’re interested in changes that are second-order in \\(dx\\)? This leads us to the notion of second differentials. If \\(dx^2\\) is a second-order differential change in \\(x\\), then \\(d^2y = d(dy)\\) is a second-order differential change in \\(y=f(x)\\). Using the fact that \\(dy=f(x+dx)-f(x)\\), it’s not too hard to show that\n\\[d^2 y  = f(x+dx)-2f(x)+f(x-dx).\\]\nThe ratio of second differentials is called the second derivative of \\(y\\) with respect to \\(x\\),\n\\[\\frac{d^2 y}{dx^2} = \\frac{f(x+dx)-2f(x)+f(x-dx)}{dx^2}.\\]\nAs with the first derivative \\(\\frac{dy}{dx}\\), this equality will only be exact when \\(dx\\) is infinitesimal. Otherwise there will be an error of order \\(dx^3\\).\nLet’s do a quick example. Consider again the cubic function \\(y=x^3\\). I already showed its first derivative is the function \\(\\frac{dy}{dx} = 3x^2\\). Let’s see what its second derivative is. Evidently, for \\(d^2y\\) we have\n\\[\\begin{align*}\nd^2 y &= f(x+dx)-2f(x)+f(x-dx) \\\\\n&= (x+dx)^3 - 2x^3 + (x-dx)^3 \\\\\n&= (x^3 + 3x^2dx + 3xdx^2 + dx^3) - 2x^3 + (x^3 - 3x^2dx + 3xdx^2 - dx^3) \\\\\n&= 6xdx^2.\n\\end{align*}\\]\nDividing both sides by \\(dx^2\\), the second derivative of \\(y=x^3\\) is evidently just \\(\\frac{d^2 y}{dx^2} = 6x\\). We can actually see this in a simpler way. Notice that\n\\[\\frac{d^2 y}{dx^2} = \\frac{d}{dx} \\frac{dy}{dx}.\\]\nThat is, it’s just the derivative of the derivative. Since we already knew \\(dy = 3x^2dx\\) and \\(d(x^2)=2xdx\\), we could just have done\n\\[\\begin{align*}\nd^2 y = d(dy) &= d(3x^2dx) \\\\\n&= 3d(x^2)dx \\\\\n&= 3(2xdx)dx \\\\\n&= 6xdx^2. \\\\\n\\end{align*}\\]\nJust like the first derivative is a function of \\(x\\), so is the second derivative. For this reason we’d also often write it as\n\\[\\frac{d^2 y}{dx^2} = \\frac{d^2}{dx^2} f(x) = f''(x).\\]\nWe can calculate the second derivative numerically similar to the way we did the first derivative, by taking the ratio of \\(d^2 y\\) with \\(dx^2\\). Again, we have to account for the fact that \\(dx\\) won’t be infinitesimal, which means there will be an error term. In the case of the second derivative, the error term will be of order \\(dx^2\\),\n\\[\\frac{d^2 y}{dx^2} = \\frac{f(x+dx)-2f(x)+f(x-dx)}{dx^2} + \\mathcal{O}(dx^2).\\]\nBy using center differences like we did the first derivative we can get the error down to order \\(dx^3\\). Here’s a function to calculate the second derivative numerically. I’ll test it again on the quadratic \\(y=x^2\\) at \\(x=1\\). The exact answer should be \\(\\frac{d^2 y}{dx^2} = 2\\). Since I’m dividing this time by \\(dx^2\\) I’ll use \\(dx = 10^{-3}\\) to avoid worries about numerical roundoff. Evidently the error is about \\(0.000002\\), which is of order \\(dx^2\\) as expected.\n\n\nCode\ndef second_diff(f, x, dx=1e-5):\n    d2y = f(x + dx) - 2 * f(x) + f(x - dx)\n    return d2y / dx**2\n\nf = lambda x: x ** 2\nd2ydx2 = second_diff(f, 1)\nprint(f'd2y/dx2 = {d2ydx2}')\n\n\nd2y/dx2 = 2.000002385926791\n\n\nTo see how the second derivative arises we need to ask what happens if we want to approximate a function \\(y=f(x)\\) not just with a line, but with a parabola. It turns out that if we want to approximate the function with a parabola about some point \\(x=a\\) we’d use something like this,\n\\[f(x) \\approx f(a) + \\frac{d}{dx} f(a) (x - a) + \\color{red}{\\frac{1}{2} \\frac{d^2}{dx^2} f(a) (x - a)^2}\\color{black}.\\]\nThe new term is shown in red. It’s quadratic in \\(x-a\\) and proportional to the second derivative of \\(y=f(x)\\) at the point \\(a\\). We can imagine writing it in a form\n\\[y = A(x-a)^2 + B(x-a) + C.\\]\nThis gives the equation of a parabola with vertex at \\(\\big(a-\\frac{B}{2A}, \\big(a-\\frac{B}{2A}\\big)^2\\big)\\). Evidently, the curvature of the parabola is determined by the coefficient \\(A = \\frac{1}{2} \\frac{d^2}{dx^2} f(a)\\). This means that the second derivative is a measure of the function’s curvature around a point. The larger the second derivative is, the steeper the parabola at that point will be, and hence the steeper the function’s curvature will be around \\(x=a\\).\nHere’s an example. Suppose we have the function \\(y = x^3 + 5x^2\\), and we wanted to approximate the function with a parabola about the point \\(x=1\\). The approximating parabola turns out to be\n\\[y = 8(x - 1)^2 + 13(x - 1) + 6.\\]\nThe plot of the function and the approximating parabola is shown below.\n\n\nCode\nf = lambda x: x ** 3 + 5 * x ** 2\ndfdx = lambda x: 3 * x ** 2 + 10 * x\nd2fdx2 = lambda x: 6 * x + 10\n\na = 1\nx = np.arange(-50, 50, 0.1)\n\nf_parabola = lambda x: f(a) + dfdx(a) * (x - a) + 1/2 * d2fdx2(a) * (x - a) ** 2\n\nplot_function(x, [f, f_parabola], points=[[a, f(a)]], xlim=(-10, 10), ylim=(-100, 100),\n              title=f'')\n\n\n\n\n\n\n\n\n\nWhile this looks like a bad approximation when \\(x\\) is far away from \\(x=1\\), especially if \\(x\\) is negative, if we zoom in closer to \\(x=1\\) we see that it actually does a very good job of approximating the function locally. In the range \\([0.5, 1.5]\\) you can’t even really tell the function isn’t parabolic.\n\n\nCode\nplot_function(x, [f, f_parabola], points=[[a, f(a)]], xlim=(-1, 2), ylim=(-5, 30),\n              title=f'')\n\n\n\n\n\n\n\n\n\nThe sign of the second derivative at \\(x=a\\) says something about which way the function is curving. If the second derivative is positive, the function is curving upward around \\(x=a\\). If the second derivative is negative, the function is curving downward at \\(x=a\\). In the edge case where the second derivative is zero you can’t tell. You have to go to higher order terms and look at the sign of those.\nIf we interpret the first derivative as some kind of rate, then we’d have to interpret the second derivative as a rate of a rate, or an acceleration. For example, if \\(y\\) was the position of an object, then the first derivative \\(\\frac{dy}{dx}\\) would represent its speed or velocity, while the second derivative \\(\\frac{d^2y}{dx^2}\\) would represent the object’s acceleration at a given point in time.\nThere are higher-order differentials and derivatives as well. We can take third differentials, fourth differentials, and so on. Fortunately, these higher terms don’t really seem to show up in machine learning, so it’s not worth going into them.\nI’ll just briefly mention a nice fact. For many functions in practice, we can keep doing the kind of approximation I’ve been doing indefinitely for higher and higher derivatives. If we take infinitely many terms we get what’s called the Taylor Series Expansion of \\(y=f(x)\\) about \\(x=a\\),\n\\[f(x) = \\sum_{n=0}^\\infty \\frac{1}{n!} \\frac{d^n}{dx^n} f(a) (x-a)^n = f(a) + \\frac{d}{dx} f(a) (x - a) + \\frac{1}{2} \\frac{d^2}{dx^2} f(a) (x - a)^2 + \\frac{1}{6} \\frac{d^3}{dx^3} f(a) (x - a)^3 + \\cdots\\]\nBy taking more and more terms in the Taylor Series we can get a better and better approximation of a function about a point \\(x=a\\) by using higher and higher degree polynomials. Note that when infinitely many terms are used the equality becomes exact. It’s only approximate if we ignore higher-order terms.\n\n\n4.2.4 Differentiation Rules\nIt would be useful to have a systematic way to calculate differentials and derivatives without having to go back to the definition each time or always do it numerically. In this section I’ll show some rules that makes this kind of thing much easier.\nTo start with, I’ll list the differentials and derivatives of some common functions. I won’t torture you by deriving all of these one-by-one.\n\n\n\n\n\n\n\n\nFunction\nDifferential\nDerivative\n\n\n\\(y = 0\\)\n\\(dy = 0\\)\n\\(\\frac{dy}{dx} = 0\\)\n\n\n\\(y = 1\\)\n\\(dy = 0\\)\n\\(\\frac{dy}{dx} = 0\\)\n\n\n\\(y = x\\)\n\\(dy = dx\\)\n\\(\\frac{dy}{dx} = 1\\)\n\n\n\\(y = x^n\\)\n\\(dy = nx^{n-1}dx\\)\n\\(\\frac{dy}{dx} = nx^{n-1}\\)\n\n\n\\(y = \\sqrt{x}\\)\n\\(dy = \\frac{dx}{2\\sqrt{x}}\\)\n\\(\\frac{dy}{dx} = \\frac{1}{2\\sqrt{x}}\\)\n\n\n\\(y = \\frac{1}{x}\\)\n\\(dy = -\\frac{dx}{x^2}\\)\n\\(\\frac{dy}{dx} = -\\frac{1}{x^2}\\)\n\n\n\\(y = e^x\\)\n\\(dy = e^xdx\\)\n\\(\\frac{dy}{dx} = e^x\\)\n\n\n\\(y = \\log{x}\\)\n\\(dy = \\frac{dx}{x}\\)\n\\(\\frac{dy}{dx} = \\frac{1}{x}\\)\n\n\n\\(y = \\sin{x}\\)\n\\(dy = \\cos{x}dx\\)\n\\(\\frac{dy}{dx} = \\cos{x}\\)\n\n\n\\(y = \\cos{x}\\)\n\\(dy = -\\sin{x}dx\\)\n\\(\\frac{dy}{dx} = -\\sin{x}\\)\n\n\n\\(y = \\sigma(x)\\)\n\\(dy = \\sigma(x)\\big(1-\\sigma(x)\\big)dx\\)\n\\(\\frac{dy}{dx} = \\sigma(x)\\big(1-\\sigma(x)\\big)\\)\n\n\n\\(y = \\tanh(x)\\)\n\\(dy = \\big(1 - \\tanh^2(x)\\big)dx\\)\n\\(\\frac{dy}{dx} = 1 - \\tanh^2(x)\\)\n\n\n\\(y = \\text{ReLU}(x)\\)\n\\(dy = u(x)dx = [x \\geq 0]dx\\)\n\\(\\frac{dy}{dx} = u(x) = [x \\geq 0]\\)\n\n\n\nLet’s now look at some general rules for differentials and derivatives. The most fundamental rule is that these things are linear. That is, if \\(a, b\\) are constant real numbers and \\(u, v\\) are functions,\n\\[d(au + bv) = adu + bdv, \\quad \\text{and} \\quad \\frac{d(au+bv)}{dx} = a \\frac{du}{dx} + b \\frac{dv}{dx}.\\]\nIt’s pretty easy to see this. Suppose \\(f(x) = au(x) + bv(x)\\). Then we’d have\n\\[\\begin{align*}\nd(au + bv) &= f(x + dx) - f(x) \\\\\n&= \\big(au(x+dx) + bv(x+dx)\\big) - \\big(au(x) + bv(x)\\big) \\\\\n&= a \\big(u(x+dx) - u(x) \\big) + b \\big(v(x+dx) - v(x) \\big) \\\\\n&= adu + bdv.\n\\end{align*}\\]\nFor example, that if we have the function \\(y = 5\\sin x + 10x^2 - 4\\), then we can write\n\\[dy = d(5\\sin x + 10x^2 - 4) = 5d(\\sin x) + 10d(x^2) - 4d(1) = 5\\cos x dx + 20xdx + 0dx,\\]\nor dividing both sides by \\(dx\\),\n\\[\\frac{dy}{dx} = 5\\cos x + 20x.\\]\nIf we wanted, we could take the second derivative just as easily,\n\\[d^2y = d(5\\cos x \\cdot dx + 20xdx) = 5dx \\cdot d(\\cos x) + 20dx^2 = -5\\sin x \\cdot dx^2 + 20 dx^2,\\]\nwhich means\n\\[\\frac{d^2y}{dx^2} = -5\\sin x + 20.\\]\nSympy can do all this for you. You’ll first need to define a variable x, then define y as a function of x. Once you’ve done this, you can get the first derivative with y.diff(x), and the second derivative with y.diff(x).diff(x).\n\n\nCode\nx = sp.Symbol('x')\ny = 5 * sp.sin(x) + 10 * x ** 2  - 4\ndydx = y.diff(x)\nd2dx2 = y.diff(x).diff(x)\nprint(f'y = {y}')\nprint(f'dy/dx = {dydx}')\nprint(f'd2y/dx2 = {d2dx2}')\n\n\ny = 10*x**2 + 5*sin(x) - 4\ndy/dx = 20*x + 5*cos(x)\nd2y/dx2 = 20 - 5*sin(x)\n\n\nHere’s a more interesting example that shows we can take differentials of any equation. It need not even be a single-valued function \\(y=f(x)\\). Any function of the form \\(g(x,y)=0\\) will do. Suppose we have the equation \\(x^2+y^2=1\\). This is the equation for a circle of radius 1. Taking the differential of both sides and solving for \\(dy\\), we have\n\\[\\begin{align*}\nd(x^2 + y^2) &= d(1) \\\\\nd(x^2) + d(y^2) &= 0 \\\\\n2xdx + 2ydy &= 0 \\\\\ndy &= -\\frac{x}{y}dx.\n\\end{align*}\\]\nEvidently the slope of a circle changes like \\(\\frac{dy}{dx} = -\\frac{x}{y}\\). Since the differential of a constant is always zero, this will be true for a circle of any radius.\n\n\nCode\nf1 = lambda x: np.sqrt(1 - x ** 2)\nf2 = lambda x: -np.sqrt(1 - x ** 2)\ndfdx1 = lambda x: -x / f1(x)\ndfdx2 = lambda x: -x / f2(x)\n\na = 0.5\nya = f1(a)\nx = np.arange(-2, 2, 0.001)\nif ya >= 0:\n    f_line = lambda x: ya + dfdx1(a) * (x - a)\nelse:\n    f_line = lambda x: ya + dfdx2(a) * (x - a)\n\nplot_function(x, [f1, f2, f_line], points=[[a, ya]], xlim=(-2, 2), ylim=(-2, 2),\n              colors=['steelblue', 'steelblue', 'orange'], \n              title=f'Tangent to $x^2+y^2=1$ at ({round(a, 3)},{round(ya, 3)})')\n\n\n\n\n\n\n\n\n\nAnother useful rule that differentials and derivatives follow is the product rule. If \\(u\\) and \\(v\\) are two functions, then the differential of their product \\(y=uv\\) is\n\\[d(uv) = udv + vdu.\\]\nIt’s pretty easy to see that this is true as well. If we nudge \\(y\\) a little bit to \\(y+dy\\), then we necessarily must nudge \\(u\\) to some \\(u+du\\) and \\(v\\) to some \\(v+dv\\). This means we have\n\\[\\begin{align*}\ndy = d(uv) &=  (u+du)(v+dv) - uv \\\\\n&=  (uv + udv + vdu + dudv) - uv \\\\\n&\\approx  udv + vdu, \\\\\n\\end{align*}\\]\nwhere the last step follows provided \\(dudv \\approx 0\\), which will be true if both \\(du\\) and \\(dv\\) are infinitesimal. Dividing both sides by \\(dx\\) gives the derivative equivalent,\n\\[\\frac{d(uv)}{dx} = u\\frac{dv}{dx} + v\\frac{du}{dx}.\\]\nAs an example, suppose we had the function \\(y=x^2e^x\\). Setting \\(u=x^2\\) and \\(v=e^x\\), we’d have\n\\[dy = d(uv) = udv + vdu = x^2 d(e^x) + e^x d(x^2) = x^2 e^x dx + 2xe^x dx.\\]\nDividing both sides by \\(dx\\) then gives the derivative, \\(\\frac{dy}{dx} = x^2 e^x + 2xe^x\\).\n\n\nCode\nx = sp.Symbol('x')\ny = x ** 2 * sp.exp(x)\ndydx = y.diff(x)\nprint(f'y = {y}')\nprint(f'dy/dx = {dydx}')\n\n\ny = x**2*exp(x)\ndy/dx = x**2*exp(x) + 2*x*exp(x)\n\n\nSuppose we have a function \\(y=\\frac{u}{v}\\) that’s a quotient of two functions \\(u\\) and \\(v\\). To find \\(dy\\), can write this as \\(y=uv^{-1}\\) and use the product rule,\n\\[dy = d(uv^{-1}) = ud(v^{-1}) + v^{-1}du = -\\frac{udv}{v^2} + \\frac{du}{v} = \\frac{vdu - udv}{v^2}.\\]\nThis is called the quotient rule,\n\\[d\\bigg(\\frac{u}{v}\\bigg) = \\frac{vdu - udv}{v^2}.\\]\nAgain, dividing both sides by \\(dx\\) gives the quotient rule for derivatives,\n\\[\\frac{dy}{dx} = \\frac{v\\frac{du}{dx} - u\\frac{dv}{dx}}{v^2}.\\]\nAs an example, suppose we wanted to differentiate \\(y=\\frac{12e^x + 1}{x^2 + 1}\\). Taking \\(u=12e^x + 1\\) and \\(v=x^2 + 1\\), we’d have\n\\[dy = \\frac{vdu - udv}{v^2} = \\frac{(x^2 + 1)d(12e^x + 1) - (12e^x + 1)d(x^2 + 1)}{(x^2 + 1)^2} = \\frac{12e^x(x^2 + 1)dx - 2x(12e^x + 1)dx}{(x^2 + 1)^2}.\\]\nOr dividing both sides by \\(dx\\),\n\\[\\frac{dy}{dx} = \\frac{12e^x(x^2 + 1) - 2x(12e^x + 1)}{(x^2 + 1)^2}.\\]\n\n\nCode\nx = sp.Symbol('x')\ny = (12 * sp.exp(x) + 1) / (x ** 2 + 1)\ndydx = y.diff(x)\nprint(f'y = {y}')\nprint(f'dy/dx = {dydx}')\n\n\ny = (12*exp(x) + 1)/(x**2 + 1)\ndy/dx = -2*x*(12*exp(x) + 1)/(x**2 + 1)**2 + 12*exp(x)/(x**2 + 1)\n\n\nThe last rule I’ll mention is by far the most important one to know for machine learning purposes. It’s called the chain rule. Suppose we had a composite function \\(y = f(g(x))\\), or equivalently \\(y=g(z)\\) and \\(z=f(x)\\). Then taking differentials we’d get\n\\[dy = \\frac{dy}{dz} dz = \\frac{dy}{dz} \\frac{dz}{dx} dx.\\]\nDividing both sides by \\(dx\\) gives the derivative version,\n\\[\\frac{dy}{dx} = \\frac{dy}{dz} \\frac{dz}{dx}.\\]\nThe notation makes this look trivial since it seems like all we’re doing is multiplying and dividing by \\(dz\\). But the chain rule is a pretty deep fact. It says if we have a complicated function we can always break it up into easier composite functions, differentiate those, then multiply the results together.\nHere’s an example. Suppose we had a function \\(y = e^{-\\frac{1}{2} x^2}\\). This looks pretty complicated but it’s not. Notice we can think about this as a composition of the form \\(y = e^z\\) where \\(z = -\\frac{1}{2} x^2\\). Differentiating each of these on their is easy,\n\\[\\begin{align*}\ndy &= d(e^z) = e^z dz, \\\\\ndz &= d\\bigg(-\\frac{1}{2} x^2\\bigg) = -xdx. \\\\\n\\end{align*}\\]\nIf we want to differentiate \\(y\\) with respect to \\(x\\) we just need to multiply these two together and substitute in for \\(z\\),\n\\[dy = \\frac{dy}{dz} \\frac{dz}{dx} dx = e^z (-x) dx = -x e^{-\\frac{1}{2} x^2} dx,\\]\nor again dividing both sides by \\(dx\\),\n\\[\\frac{dy}{dx} = -x e^{-\\frac{1}{2} x^2}.\\]\n\n\nCode\nx = sp.Symbol('x')\ny = sp.exp(-sp.Rational(1, 2) * x ** 2)\ndydx = y.diff(x)\nprint(f'y = {y}')\nprint(f'dy/dx = {dydx}')\n\n\ny = exp(-x**2/2)\ndy/dx = -x*exp(-x**2/2)\n\n\nWhat really makes the chain rule powerful for machine learning is that we can use it on arbitrarily many composition chains. For example, if we have a composition of three functions \\(y=f(u)\\), \\(u=g(z)\\), \\(z=h(x)\\), we’d have\n\\[\\frac{dy}{dx} = \\frac{dy}{du} \\frac{du}{dz} \\frac{dz}{dx}.\\]\nMore generally, if we had a complicated function of \\(n\\) compositions\n\\[\\begin{align*}\ny_0 &= x, \\\\\ny_1 &= f_1(y_0), \\\\\ny_2 &= f_2(y_1), \\\\\n\\vdots & \\qquad \\vdots \\\\\ny_n &= f_n(y_{n-1}), \\\\\ny &= y_n, \\\\\n\\end{align*}\\]\nthe chain rule would say\n\\[\\frac{dy}{dx} = \\prod_{i=1}^n \\frac{dy_i}{dy_{i-1}} = \\frac{dy}{dy_{n-1}} \\frac{dy_{n-1}}{dy_{n-2}} \\cdots \\frac{dy_2}{dy_1} \\frac{dy_1}{dx}.\\]\nThe last rule I’ll mention is a method to calculate the derivative of an inverse function. Suppose \\(y=f(x)\\). If the function is invertible, we can solve for \\(x\\) and get an inverse function \\(x = f^{-1}(y)\\). The inverse rule says if we know the derivative of one of these we can easily get the derivative of the other by inverting it,\n\\[\\frac{dx}{dy} = \\frac{1}{\\frac{dy}{dx}}.\\]\nAgain, the notation makes this look pretty trivial. What’s not said is that in going from one to the other we have to be careful what’s a function of what. On the left-hand side, \\(x\\) is a function of \\(y\\), but on the right-hand side \\(y\\) is a function of \\(x\\).\nFor example, suppose we had the function \\(y=e^x\\) and for some reason wanted to know \\(\\frac{dx}{dy}\\). What we can do is calculate \\(\\frac{dy}{dx} = e^x\\), and invert it to get \\(\\frac{dx}{dy}\\),\n\\[\\frac{dx}{dy} = \\frac{1}{\\frac{dy}{dx}} = \\frac{1}{e^x} = \\frac{1}{y}.\\]\nOf course, this is just the derivative of the logarithm function \\(x = \\log y\\), which makes sense since \\(x=\\log y\\) is the inverse of \\(y=e^x\\).\nHere’s a summary table of all these rules for reference. I’ll state them in derivative form and provide an example of each one for later reference.\n\n\n\n\n\n\n\n\nName\nRule\nExample\n\n\nLinearity\n\\(\\frac{d}{dx}(au + bv) = a\\frac{du}{dx} + b\\frac{dv}{dx}\\)\n\\(\\frac{d}{dx}(2x^2 + 5\\log x) = 2\\frac{d}{dx}x^2 + 5\\frac{d}{dx}\\log x = 4x + \\frac{5}{x}\\)\n\n\nProduct Rule\n\\(\\frac{d}{dx}(uv)=u\\frac{dv}{dx} + v\\frac{du}{dx}\\)\n\\(\\frac{d}{dx}(x e^x) = x \\frac{d}{dx}e^x + e^x \\frac{d}{dx} x = xe^x + e^x\\)\n\n\nQuotient Rule\n\\(\\frac{d}{dx}\\big(\\frac{u}{v}\\big) = \\frac{v\\frac{du}{dx}-u\\frac{dv}{dx}}{v^2}\\)\n\\(\\frac{d}{dx} \\frac{\\cos x}{x^2} = \\frac{x^2\\frac{d}{dx}\\cos x-\\cos x\\frac{d}{dx}x^2}{(x^2)^2} = \\frac{-x^2 \\sin x - 2x \\cos x}{x^4}\\)\n\n\nChain Rule\n\\(\\frac{dy}{dx} = \\frac{dy}{dz}\\frac{dz}{dx}\\)\n\\(\\frac{d}{dx} e^{\\sin x} = \\frac{d}{dy} e^y \\frac{d}{dx}\\sin x = e^{\\sin x} \\cos x\\)\n\n\nInverse Rule\n\\(\\frac{dx}{dy} = \\big(\\frac{dy}{dx}\\big)^{-1}\\)\n\\(y = 5x + 1 \\quad \\Longrightarrow \\quad \\frac{dx}{dy} = \\big(\\frac{dy}{dx}\\big)^{-1} = \\frac{1}{5}\\)"
  },
  {
    "objectID": "notebooks/calculus.html#integral-calculus",
    "href": "notebooks/calculus.html#integral-calculus",
    "title": "4  Calculus",
    "section": "4.3 Integral Calculus",
    "text": "4.3 Integral Calculus\nBy far, differential calculus is the most important area of calculus to know for machine learning purposes. The main reason for this is that in machine learning we seek to train models by optimizing functions, and (as I’ll cover in a future lesson) optimization is all about taking derivatives.\nNevertheless, integral calculus is the giant other half of calculus, so I owe it to at least briefly cover the subject. Knowing a little about integration will also make it easier to understand the later lessons on probability theory and distributions.\nWe can think about integral calculus in two distinct ways that turn out to be linked together by an important theorem. One way of thinking about an integral is as an inverse operation that undoes differentiation. The other way of thinking about an integral is as sum of a bunch of infinitesimal segments.\n\n4.3.1 Indefinite Integration\nSuppose we knew that some function \\(F(x)\\) had a derivative of \\(f(x)\\), i.e.\n\\[f(x) = \\frac{d}{dx} F(x).\\]\nIf we only knew \\(f(x)\\), how would we go about “undoing” the differentiation to get back \\(F(x)\\)? Essentially what we’d need to do is work backward.\nFor example, suppose we knew \\(f(x)=x\\). How could we find \\(F(x)\\)? Well, we already know that \\(\\frac{d}{dx} x^2 = 2x\\). If we divide both sides of this equation by \\(2\\) we’ve evidently got \\(F(x)\\), i.e. \\(F(x) = \\frac{1}{2} x^2\\). Strictly speaking, this is only one possible solution. We could add any constant to \\(F(x)\\) and not change the answer since the derivative of a constant is zero.\nThe function \\(F(x)\\) is called the indefinite integral of \\(f(x)\\). In textbooks it’s sometimes also called an antiderivative since it undoes the derivative operation. For reasons I’ll explain soon, we usually write the indefinite integral using a funny notation,\n\\[F(x) = \\int f(x) dx.\\]\nFor example, if \\(f(x) = x\\), we’d write\n\\[\\int x dx = \\frac{1}{2} x^2.\\]\nThe function on the right is the \\(F(x)=\\frac{1}{2} x^2\\) that undoes the function inside the integral, namely \\(f(x)=x\\).\nAs far as interpreting what an indefinite integral is, if you think of \\(f(x)\\) as the rate of some thing, then \\(F(x)\\) tells you the aggregate total of that thing. For example, if \\(f(x)\\) was the speed of an object as a function of time, then its integral \\(F(x)\\) would tell you how far that object has moved as a function of time. If \\(f(x)\\) was the demand of some good as a function of price, then \\(F(x)\\) would be the consumer surplus of that good, or how much consumers benefit from being able to buy that good for cheaper than they were willing to pay.\nWe could go through and figure out what the indefinite integral is one-by-one for the common functions we’ve seen so far, but I’ll spare you the detail. They’re given in the table below.\n\n\n\n\n\n\n\nFunction\nIntegral\n\n\n\\(y = 0\\)\n\\(\\int y dx = 0\\)\n\n\n\\(y = 1\\)\n\\(\\int y dx = x\\)\n\n\n\\(y = x\\)\n\\(\\int y dx = \\frac{1}{2}x^2\\)\n\n\n\\(y = \\sqrt{x}\\)\n\\(\\int y dx = \\frac{2}{3} x^{3/2}\\)\n\n\n\\(y = \\frac{1}{x}\\)\n\\(\\int y dx = \\log{x}\\)\n\n\n\\(y = x^n\\) where \\(n \\neq -1\\)\n\\(\\int y dx = \\frac{1}{n+1}x^{n+1}\\)\n\n\n\\(y = e^x\\)\n\\(\\int y dx = e^x\\)\n\n\n\\(y = \\log{x}\\)\n\\(\\int y dx = x \\log{x} - x\\)\n\n\n\\(y = \\sin{x}\\)\n\\(\\int y dx = -\\cos{x}\\)\n\n\n\\(y = \\cos{x}\\)\n\\(\\int y dx = \\sin{x}\\)\n\n\n\nBy construction, the indefinite integral is the inverse operation of the derivative operation. This means they cancel each other,\n\\[d\\bigg(\\int f(x) dx\\bigg) = f(x) dx, \\quad \\text{and} \\quad \\int dF(x) = \\int \\frac{d}{dx} F(x) dx = F(x).\\]\nThis fact is sometimes called the Leibniz Rule. It means that the indefinite integral inherits many of the properties of the derivative. Most importantly, it’s linear. If \\(a\\) and \\(b\\) are constants, and \\(u\\) and \\(v\\) are two functions, then\n\\[\\int \\big(au + bv\\big) dx = a \\int u dx + b \\int v dx.\\]\nFor example, if \\(y = -x^3 + 7\\sin x\\), we’d have\n\\[\\int y dx = \\int (-x^3 + 7\\sin x) dx = - \\int x^3 dx + 6 \\int \\sin x dx = -\\frac{1}{4} x^4 - 6 \\cos x.\\]\nAgain, you could technically add a constant to this answer and not change the result, but it’s not typical to do this, at least outside of calculus textbooks.\nAs with differentiation, sympy can calculate these things for you. Just define x and y, then calculate the indefinite integral with y.integrate(x).\n\n\nCode\nx = sp.Symbol('x')\ny = -x ** 3 + 7 * sp.sin(x)\nint_y = y.integrate(x)\nprint(f'y = {y}')\nprint(f'int y dx = {int_y}')\n\n\ny = -x**3 + 7*sin(x)\nint y dx = -x**4/4 - 7*cos(x)\n\n\nThe integral version of the product rule is called integration by parts. If we start with the product rule,\n\\[d(uv) = udv + vdu,\\]\nand integrate both sides, we get\n\\[\\int d(uv) = \\int udv + \\int vdu.\\]\nSince \\(uv = \\int d(uv)\\), we can re-arrange terms to get the rule for integration by parts,\n\\[\\int u dv = uv - \\int v du.\\]\nActually using integration by parts to calculate things isn’t often as straight forward as the product rule. To make integration by parts work in practice, you want to try to choose \\(u\\) and \\(v\\) so the right-hand side is easier to integrate than the left-hand side.\nAs a quick example, suppose we wanted to integrate \\(y = x e^x\\). What we could do is take \\(u = x\\) and \\(v=e^x\\), so \\(du=dx\\) and \\(dv = e^x dx\\). Then we’d have\n\\[\\int x e^x dx = x e^x - \\int e^x dx = x e^x - e^x.\\]\n\n\nCode\nx = sp.Symbol('x')\ny = x * sp.exp(x)\nint_y = y.integrate(x)\nprint(f'y = {y}')\nprint(f'int y dx = {int_y}')\n\n\ny = x*exp(x)\nint y dx = (x - 1)*exp(x)\n\n\nThough less widely known and rarely covered in textbooks for some reason, there’s also an integration by parts for the quotient rule. Using the quotient rule\n\\[d\\bigg( \\frac{u}{v} \\bigg) = \\frac{vdu - udv}{v^2},\\]\nand integrating both sides, we get\n\\[\\int d\\bigg( \\frac{u}{v} \\bigg) = \\int \\frac{vdu - udv}{v^2} = \\int \\frac{1}{v}du + \\int \\frac{u}{v^2}dv.\\]\nSince \\(\\frac{u}{v} = \\int d\\big(\\frac{u}{v}\\big)\\) we can again rearrange terms to get\n\\[\\int \\frac{1}{v} du = \\frac{u}{v} + \\int \\frac{u}{v^2} du.\\]\nJust as with the product integration by parts, using this in practice often involves reducing a complicated integral on the left-hand side to a less complicated integral on the right-hand side.\nThe integral version of the chain rule is called change of variables or substitution. Suppose we had a function \\(y=f(x)\\) and wanted to integrate it. One thing we could do is make a change of variable by letting \\(x\\) be a function of some other variable \\(u\\), say \\(x = g(u)\\). If we multiply and divide \\(f(x)dx\\) by \\(du\\), then\n\\[f(x) dx = f(g(u)) \\frac{dx}{du} du.\\]\nIntegrating both sides gives the change of variables formula,\n\\[\\int f(x) dx = \\int f(g(u)) \\frac{dx}{du} du.\\]\nThe left-hand side is integrated with respect to \\(x\\), while the right-hand side is integrated with respect to \\(u\\). Just like with integration by parts, using the change of variables formula in practice is more an art than a science. The tricky part is figuring out what \\(x=g(u)\\) should be to make the integral easier to compute.\nFor example, suppose we wanted to integrate the function \\(y = x e^{x^2}\\). What we could do is let \\(u=x^2\\). Then \\(du=2xdx\\), and so we’d have\n\\[\\int x e^{x^2} dx = \\int x e^{u} \\frac{1}{2x} du = \\frac{1}{2} \\int e^u du = \\frac{1}{2} e^u = \\frac{1}{2} e^{x^2}.\\]\n\n\nCode\nx = sp.Symbol('x')\ny = x * sp.exp(x ** 2)\nint_y = y.integrate(x)\nprint(f'y = {y}')\nprint(f'int y dx = {int_y}')\n\n\ny = x*exp(x**2)\nint y dx = exp(x**2)/2\n\n\nHere’s a summary table of the integral rules I’ve covered so far.\n\n\n\n\n\n\n\n\nName\nRule\nExample\n\n\nLeibniz Rule\n\\(\\frac{d}{dx} \\int y dx = y\\)\n\\(\\frac{d}{dx} \\int \\sin t dt = \\sin x\\)\n\n\nLinear Rule\n\\(\\int (au + bv) dx = a\\int u dx + b\\int v dx\\)\n\\(\\int (-1 + 5e^x) dx = -\\int 1 dx + 5\\int e^x dx = -x + 5e^x\\)\n\n\nIntegration By Parts (product version)\n\\(\\int u dv = uv - \\int v du\\)\n\\(\\int x e^x dx = \\int x d(e^x) = x e^x - \\int e^x dx = x e^x - e^x\\)\n\n\nIntegration By Parts (quotient version)\n\\(\\int \\frac{1}{v} du = \\frac{u}{v} + \\int \\frac{u}{v^2} dv\\)\n\\(\\int \\frac{\\sin \\sqrt{x}}{x^2} dx = \\frac{2\\cos x^{-1/2}}{x^{1/2}} + \\int x^{3/2} \\cos x^{-1/2}dx = \\frac{2\\cos x^{-1/2}}{x^{1/2}} - 2 \\sin x^{-1/2}\\)\n\n\nChange of Variables\n\\(\\int f(x) dx = \\int f(g(u)) \\frac{dx}{du} dx\\)\n\\(\\int x e^{x^2} dx = \\int e^{x^2} d\\big(\\frac{1}{2}x^2\\big) = \\frac{1}{2} \\int e^u du = \\frac{1}{2} e^u = \\frac{1}{2} e^{x^2}\\)\n\n\n\nBefore moving on, I’ll mention an important fact. While we can always take the derivative of a function and get a symbolic answer in terms of functions we’re used to, this is not always possible with integrals. In fact, most integrals we can write down we can’t evaluate using the methods covered here. There’s no way to find an explicit \\(F(x)\\) whose derivative is \\(f(x)\\) in terms of simple functions. A classic example of this is the Gaussian function\n\\[y = e^{-x^2}.\\]\nThere’s no way you’d be able to integrate this symbolically and get a closed form solution in terms of simple functions. Unfortunately, this is the rule, not the exception. This fact often makes integration a lot harder than differentiation in practice. Fortunately, we can always evaluate an integral numerically, but I’ll need to discuss the definite integral first so we can understand what that even means.\n\n\n4.3.2 Definite Integration\nI said that integral calculus essentially consists of two things, finding functions that undo a derivative, and summing up infinitesimals. The first part I just covered, the indefinite integral. I’ll now discuss the second part, and how it can be linked with the definite integral via the Fundamental Theorem of Calculus.\nSuppose now we had a function \\(y=f(x)\\) and we wanted to compute the area under this curve between two endpoints \\(x=a\\) and \\(x=b\\). So we have a working example, suppose the function is \\(y=\\sqrt{x}\\) and we want to find the area under its curve from \\(x=0\\) to \\(x=10\\).\n\n\nCode\nf = lambda x: np.sqrt(x)\nx = np.linspace(0, 10, 100)\na, b = 0, 10\nplot_function_with_area(x, f, a=a, b=b, title='$y=\\sqrt{x}$ on $[0,10]$')\n\n\n\n\n\n\n\n\n\nIf you had to estimate the area of something like this with no prior knowledge, what you might do is try to approximate the area by using a shape you already know how to find the area of. Perhaps the easiest thing to try would be a rectangle. Suppose we took a rectangle whose base was length \\(10\\) and whose height was \\(\\sqrt{10}\\). Then very roughly speaking we could claim the area \\(A\\) under the curve is\n\\[A \\approx 10 \\cdot \\sqrt{10} \\approx 31.62.\\]\nBut this actually isn’t a great estimate. Here’s what we just calculated the area of. Notice how much area above the curve we’re taking with this estimate. We’re over-estimating the true area by a good bit it seems. How can we improve this estimate?\n\n\nCode\nplot_approximating_rectangles(x, f, dx=10, title='$A \\\\approx$ 1 Rectangle', alpha=0.7)\n\n\nApproximate Area: 31.622776601683796\n\n\n\n\n\n\n\n\n\nOne thing we could do is use not one rectangle, but two rectangles. Suppose we took each rectangle to have half the width of the curve, say \\(dx=5\\). The left rectangle can have height \\(\\sqrt{5}\\), and the right can have height \\(\\sqrt{10}\\). If we sum up these two areas, we’d get\n\\[A \\approx 5 \\cdot \\sqrt{5} + 5 \\cdot \\sqrt{10} \\approx 26.99\\]\nHere’s what this approximation looks like. It’s clearly a lot better, but we’re still over-estimating the area by a decent bit.\n\n\nCode\nplot_approximating_rectangles(x, f, dx=5, title='$A \\\\approx $ 2 Rectangles', alpha=0.7)\n\n\nApproximate Area: 26.991728188340847\n\n\n\n\n\n\n\n\n\nIt seems like we can keep going with this strategy though. Let’s try \\(N=10\\) rectangles each of width \\(dx=1\\). Call the areas of these \\(N\\) rectangles in order \\(A_0, A_1, \\cdots, A_{N-1}\\). Each area will be a width \\(dx\\) times a height \\(y_i=\\sqrt{i+1}\\). Thus, we have\n\\[\\begin{align*}\nA \\approx \\sum_{i=0}^{N-1} A_i &= \\sum_{i=0}^{N-1} y_i dx \\\\\n&= y_0dx + y_1dx + y_2dx + \\cdots + y_9dx \\\\\n&= \\big(\\sqrt{1} + \\sqrt{2} + \\sqrt{3} + \\cdots + \\sqrt{10}\\big)\\cdot 1 \\\\\n&\\approx 22.468\n\\end{align*}\\]\nHere’s what this approximation looks like. It’s clearly a whole lot better. Notice how little extra area is left above the curve.\n\n\nCode\nplot_approximating_rectangles(x, f, dx=1, title='$A \\\\approx $ 10 Rectangles', alpha=0.7)\n\n\nApproximate Area: 22.468278186204103\n\n\n\n\n\n\n\n\n\nNow, it’s fair to ask if this approximation will ever become exact. Can we eventually take enough rectangles such that \\(A = \\sum y_i dx\\) exactly? In fact we can. The trick is to allow the rectangle widths \\(dx\\) to become infinitesimally thin. If we do that, while taking \\(N = \\lfloor \\frac{b-a}{dx} \\rfloor\\) rectangles, then \\(N\\) will become infinitely large and we’ll have an exact equality\n\\[A = \\sum_{i=0}^{N-1} y_i dx.\\]\nHere’s what this might look like. Notice how for all practical purposes it looks like we’re calculating the area of the curve exactly now.\n\n\nCode\nplot_approximating_rectangles(x, f, dx=0.1, title='$A \\\\approx N$ Rectangles', alpha=0.7, print_area=False)\n\n\n\n\n\n\n\n\n\nWhat I’ve just shown is that the exact area under the curve of a function \\(y=f(x)\\) is the sum of an infinitely large number of rectangles of height \\(y_i=f(x_i)\\) and infinitesimal width \\(dx\\). For historical reasons we usually write a sum of infinitely many infinitesimals using a special notation,\n\\[\\int_a^b y dx = \\sum_{i=0}^{N-1} y_i dx.\\]\nThis is called the definite integral of the function \\(y=f(x)\\) from \\(x=a\\) to \\(x=b\\). Historically, the reason the \\(\\int\\) was chosen is because it looks kind of like the S in “summa”, the Latin word for “sum”. This makes it clear that an integral is just a sum, a sum of infinitely many infinitesimal quantities \\(ydx\\).\nOf course, this says nothing about how to actually calculate these things. We’ve got nothing to go on but the definition. For numerical purposes that’s good enough. If we wanted to calculate a definite integral numerically, we could just take a bunch of really small rectangles like this and sum them up.\nBelow I’ll define a function called integrate that will take in the function f and the endpoints a and b, and return the definite integral. Since it’s just an area, the definite integral will always be a numerical value. I’ll default dx = 1e-4 to give a reasonable tradeoff between accuracy and speed.\nI’ll use this function to estimate the area of our running example, namely\n\\[A = \\int_0^{10} \\sqrt{x} dx.\\]\nEvidently, the exact area in this case seems to be about \\(A \\approx 21.082\\).\n\n\nCode\ndef integrate(f, a, b, dx=1e-4):\n    N = int((b - a) / dx)\n    interval = np.cumsum(dx * np.ones(N))\n    rectangles = np.array([f(x - dx/2) * dx for x in interval])\n    integral = np.sum(rectangles)\n    return integral\n\nf = lambda x: np.sqrt(x)\na, b = 0, 10\narea = integrate(f, a, b)\nprint(f'A = {area}')\n\n\nA = 21.081851128609895\n\n\nThis method of computing an integral is called numerical integration. We can actually improve the integrate function a lot by using \\(f\\big(x - \\frac{dx}{2}\\big) dx\\) for the rectangle areas instead of \\(f(x)dx\\). This is sometimes called the midpoint rule. It’s similar to differentiation, in that we improve convergence a lot by taking the midpoint function value instead of the right-most function value. As with differentiation, this centering trick reduces the error estimate by a factor of \\(dx\\). Doing this will allow you to use a much larger \\(dx\\) value to get the same desired accuracy.\nWhile it may not be obvious, the formula I gave to calculate the area under the curve actually calculates what’s called a signed area. That is, it counts area as positive when the curve is above the x-axis, and as negative when the curve is below the x-axis. To see this, notice if we took \\(y=-\\sqrt{x}\\), we’d have\n\\[\\int_0^{10} y dx = - \\int_0^{10} \\sqrt{x} dx \\approx -21.082.\\]\nWhy? Because this curve lies below the x-axis on the interval \\(0 \\leq x \\leq 10\\). This means the definite integral doesn’t calculate the area per se. It subtracts the area above the x-axis with the area below the x-axis.\nWhile we can always calculate definite integrals numerically, and in practice this is what you’d usually do, we can also derive a formula to calculate them exactly in many cases. This formula is important because it’s what links together the indefinite integral with the definite integral, i.e. undoing the derivative with summing up infinitesimals areas. It’s called the Fundamental Theorem of Calculus.\nSuppose we had a function \\(F(x)\\) whose derivative is \\(f(x)\\). The fundamental theorem makes the following two claims:\n\nFor any left endpoint \\(a\\), the definite integral is given by \\[F(x) = \\int_a^x f(t) dt + C,\\] where \\(C\\) is some additive constant that depends only on \\(a\\).\nFor any interval \\(a \\leq x \\leq b\\), the area under the curve is given by \\[\\int_a^b f(x) dx = F(b) - F(a).\\]\n\nNote I had to use \\(t\\) to represent the input variable in part (1) because I’m using \\(x\\) as the right endpoint. The right endpoint and the integration variable are two different things, and you shouldn’t confuse them. To see why (1) is true, suppose \\(A(x)\\) is the area under the curve of \\(y=f(x)\\) from \\(t=a\\) to \\(t=x\\), i.e.\n\\[A(x) = \\int_a^x f(t) dt.\\]\nIf we change the right endpoint \\(t=x\\) by an infinitesimal amount \\(dx\\), the area \\(A(x)\\) evidently changes by an amount\n\\[dA = A(x + dx) - A(x) = \\int_x^{x+dx} f(t) dt \\approx f(x) dx.\\]\nThe last equality follows from the fact that \\(dA\\) is just the area of a single rectangle of width \\(dx\\) and height \\(f(x)\\). Since \\(dF = f(x)dx\\) by definition, we thus have \\(dF = dA\\), which means \\(F(x) = A(x) + C\\), for some \\(C\\) that doesn’t depend on \\(x\\). This shows part (1).\nTo show part (2), notice from (1) that I can write\n\\[F(b) - F(a) = \\bigg(\\int_a^b f(x) dx + C\\bigg) - \\bigg(\\int_a^a f(x) dx + C\\bigg) = \\int_a^b f(x) dx.\\]\nThe last equality follows from the fact that \\(\\int_a^a f(x) dx\\) is the area of a rectangle with no width, which must be zero. This shows part (2).\nThis theorem gives us an exact way to calculate the area under curves, provided we can evaluate the integral. For the previous example of finding the area under \\(f(x)=\\sqrt{x}\\) from \\(x=0\\) to \\(x=10\\), we’d have\n\\[\\begin{align*}\n\\int_0^{10} \\sqrt{x} dx &= \\int_0^{10} x^{1/2} dx \\\\\n&= \\frac{x^{3/2}}{3/2} \\ \\bigg|_{x=0}^{x=10} \\\\\n&= \\frac{2}{3}x^{3/2} \\ \\bigg|_{x=0}^{x=10} \\\\\n&= \\frac{2}{3} 10^{3/2} - \\frac{2}{3} 0^{3/2} \\approx 21.082. \\\\\n\\end{align*}\\]\nThe notation \\(F(x) \\big|_{x=a}^{x=b}\\) is a common shorthand for writing \\(F(b)-F(a)\\) in integral calculations like this. You can also use sympy to calculate definite integrals like this via the method y.integrate((x, a, b)).\n\n\nCode\nx = sp.Symbol('x')\ny = sp.sqrt(x)\nA = y.integrate((x, 0, 10))\nprint(f'A = {A} ≈ {A.round(3)}')\n\n\nA = 20*sqrt(10)/3 ≈ 21.082\n\n\nA couple more rules for the definite integral I’ll mention involve the endpoints \\(a\\) and \\(b\\), called the limits of integration. Since all we’re doing is summing up rectangles, we can always split up a sum over the interval \\([a,b]\\) into a sum over two subintervals \\([a,c]\\) and \\([c,b]\\). That is, if \\(y=f(x)\\), then\n\\[\\int_a^b y dx = \\int_a^c y dx + \\int_c^d y dx.\\]\nWe can also swap the limits of integration if we agree on the convention that doing so flips the sign of the integral,\n\\[\\int_a^b y dx = -\\int_b^a y dx.\\]\nWe can even have either limit be infinite, provided that it makes sense to integrate the function over an infinite interval. We can do so as long as the output isn’t a NaN value like \\(\\infty - \\infty\\). For technical reasons these types of integrals are sometimes called improper integrals.\nFor example, suppose we had the function \\(y=e^{-x}\\). We can integrate this function from \\(a=0\\) to \\(b=\\infty\\) just fine. First, notice that taking a change of variables \\(u=-x\\) we get \\(du=-dx\\), so\n\\[\\int e^{-x} dx = -\\int e^u du = -e^{u} = -e^{-x}.\\]\nThen the definite integral is just\n\\[\\int_0^\\infty e^{-x} dx = -e^{-x} \\ \\bigg |_{x=0}^{x=\\infty} = -e^{-\\infty} + e^{-0} = 1.\\]\nThe last equality follows from the fact that \\(e^{-N} \\approx 0\\) when \\(N\\) is infinitely large. Strangely, the area under the curve of this infinite graph is exactly one. Curves with area one are called probability density functions. They can be used to create probability distributions. I’ll talk more on that in a few lessons.\n\n\nCode\nf = lambda x: np.exp(-x)\nx = np.linspace(0, 10, 100)\nplot_function_with_area(x, f, a=0, b=10, title='$y=e^{-x}$', alpha=0.7)"
  },
  {
    "objectID": "notebooks/linear-systems.html#linear-functions",
    "href": "notebooks/linear-systems.html#linear-functions",
    "title": "5  Linear Systems",
    "section": "5.1 Linear Functions",
    "text": "5.1 Linear Functions\nWe’ve already seen scalar linear functions, which have the form \\(y = ax\\). Linear functions, like the name suggests, represent lines in the plane. Since \\(y=0\\) if \\(x=0\\), those lines must always pass through the origin.\nThe coefficient \\(a\\) is called the slope of the function. It determines the steepness of the line and whether the line slants to the left or to the right. The slope also represents the derivative of the function, since\n\\[\\frac{dy}{dx} = a.\\]\nThe fact that the derivative is the slope tells us something about what \\(a\\) means practically speaking. It’s the amount that \\(y\\) changes in response to changes in \\(x\\). If we increase \\(x\\) by one unit, then \\(y\\) changes by \\(a\\) units. In this sense, you can also think of \\(a\\) as a weight or a gain that tells how much \\(x\\) influences \\(y\\).\nFor example, suppose you’re on a road trip, say from San Francisco to Los Angeles. You look at your speedometer and reason that you’re averaging a speed of about \\(a=60\\) miles per hour. If you’ve already driven for \\(x=5\\) hours and covered a distance of \\(y=300\\) miles, how much more distance will you cover if you drive for \\(dx=1\\) more hour? Clearly it’s \\(a=60\\) miles, which will bring your distance traveled to \\(y+dy=360\\) miles. That’s all the slope is saying.\nThe above example corresponds to the linear equation \\(y=60x\\). Here’s a plot of what this looks like. Nothing special, just a line with slope \\(60\\).\n\n\nCode\na = 60\nx = np.linspace(-3, 3, 100)\nf = lambda x: a * x\nplot_function(x, f, xlim=(-3, 3), ylim=(-100, 100), title=f'$y={a}x$')\n\n\n\n\n\n\n\n\n\nIf there are two inputs \\(x_0\\) and \\(x_1\\), a linear function would look like\n\\[y = a_0 x_0 + a_1 x_1.\\]\nThis defines a plane in 3-dimensional space that passes through the origin. Each coefficient again tells you something about how the output changes if that input is changed. If \\(x_0\\) is changed by one unit, holding \\(x_1\\) fixed, then \\(y\\) changes by \\(a_0\\) units. Similarly, if \\(x_1\\) is changed by one unit, holding \\(x_0\\) fixed, then \\(y\\) changes by \\(a_1\\) units.\nHere’s an example. Take \\(y = 5x_0 - 2x_1\\). It will look like the plane shown below. Changing \\(x_0\\) by one unit while holding \\(x_1\\) fixed will cause \\(y\\) to increase by \\(5\\) units. Changing \\(x_1\\) by one unit while holding \\(x_0\\) fixed will cause \\(y\\) to decrease by \\(2\\) units.\n\n\nCode\na0, a1 = 5, -2\nx0 = np.linspace(-3, 3, 100)\nx1 = np.linspace(-3, 3, 100)\nf = lambda x0, x1: a0 * x0 + a1 * x1\nplot_function_3d(x0, x1, f, azim=80, elev=25, ticks_every=[1, 1, 10],\n                 titlepad=6, labelpad=3, title=f'$y=5x_0 - 2x_1$')\n\n\n\n\n\n\n\n\n\nThis idea readily extends to \\(n\\) variables too, though you can’t visualize it anymore. If there are \\(n\\) variables \\(x_0, x_1, \\cdots, x_{n-1}\\), a linear function has the form\n\\[y = a_0 x_0 + a_1 x_1 + \\cdots a_{n-1} x_{n-1}.\\]\nThis equation now defines a hyperplane in \\(n\\)-dimensional space that passes through the origin. Each coefficient \\(a_i\\) represents how much \\(y\\) changes if \\(x_i\\) is increased by one unit, while holding all the other \\(x_j\\) fixed.\nWe can also think about systems of linear equations. For example, we can have 2 outputs \\(y_0, y_1\\), each of which is its own linear function of 3 input variables \\(x_0, x_1, x_2\\). It might look like this\n\\[\\begin{alignat*}{5}\ny_0 & {}={}   a_{0,0} x_0 & {}+{} &  a_{0,1} x_1 & {}+{} & a_{0,2} x_2 \\\\\ny_1 & {}={}   a_{1,0} x_0 & {}+{} &  a_{1,1} x_1 & {}+{} & a_{1,2} x_2. \\\\\n\\end{alignat*}\\]\nThe most general situation we’ll consider is a system of \\(m\\) linear equations with \\(n\\) inputs,\n\\[\n\\begin{array}{c<{x_0} c c<{x_1} c c<{\\cdots} c c<{x_{n-1}} c l}\ny_0 & = & a_{0,0}x_0 & + & a_{0,1}x_1 & + & \\cdots & + & a_{0,n-1}x_{n-1} \\\\\ny_1 & = & a_{1,0}x_0 & + & a_{1,1}x_1 & + & \\cdots & + & a_{1,n-1}x_{n-1} \\\\\n\\vdots & & \\vdots    &   & \\vdots    &   &  \\ddots  &   & \\quad \\vdots\\\\\ny_{m-1} & = & a_{m-1,0}x_0 & + & a_{m-1,1}x_1 & + & \\cdots & + & a_{m-1,n-1}x_{n-1}. \\\\\n\\end{array}\n\\]\nThis kind of linear system is often called an \\(m \\times n\\) linear system, or a system of \\(m\\) linear equations with \\(n\\) unknowns. There are \\(m \\cdot n\\) coefficients in this system, namely \\(a_{0,0}, \\cdots, a_{m-1,n-1}\\). Each \\(a_{i,j}\\) would tell you how much the output \\(y_i\\) would change if the input \\(x_j\\) was increased by one unit. Visually, you can think of an \\(m \\times n\\) linear system as corresponding to a set of \\(m\\) \\(n\\)-dimensional hyperplanes."
  },
  {
    "objectID": "notebooks/linear-systems.html#matrix-vector-notation",
    "href": "notebooks/linear-systems.html#matrix-vector-notation",
    "title": "5  Linear Systems",
    "section": "5.2 Matrix-Vector Notation",
    "text": "5.2 Matrix-Vector Notation\nLinear systems of equations are incredibly cumbersome to work with in all but the simplest cases of like 2 or 3 equations. There’s a much cleaner notation for working with these linear systems. Here’s what we can do. Notice we seem to have three separate types of objects showing up in these equations:\n\nThe \\(m\\) output variables \\(y_0, y_1, \\cdots, y_{m-1}\\).\nThe \\(m \\cdot n\\) coefficients \\(a_{0,0}, \\ a_{0,1}, \\ \\cdots, \\ a_{m-1,n-1}\\).\nThe \\(n\\) input variables \\(x_0, x_1, \\cdots, x_{n-1}\\).\n\nLet’s put each of these sets into their own array, and define an \\(m \\times n\\) linear system of equations to mean the same thing as the following expression,\n\\[\n\\begin{pmatrix}\ny_0 \\\\ y_1 \\\\ \\vdots \\\\ y_{m-1}\n\\end{pmatrix} =\n\\begin{pmatrix}\na_{0,0} & a_{0,1} & \\cdots & a_{0,n-1} \\\\\na_{1,0} & a_{1,1} & \\cdots & a_{1,n-1} \\\\\n\\vdots  & \\vdots  & \\ddots & \\vdots    \\\\\na_{m-1,0} & a_{m-1,1} & \\cdots & a_{m-1,n-1}\n\\end{pmatrix}\n\\begin{pmatrix}\nx_0 \\\\ x_1 \\\\ \\vdots \\\\ x_{n-1}\n\\end{pmatrix}.\n\\]\nEach of these arrays is a 2-dimensional array. The left-most array is a shape \\((m, 1)\\) array of outputs, and the right-most array is a shape \\((n, 1)\\) array of inputs. These are both called column vectors, or when we’re being sufficiently lazy just vectors. Even though they’re not technically 1-dimensional arrays, and hence not technically vectors, they’re close enough that they might as well be. They’re isomorphic to vectors. The middle array is a shape \\((m, n)\\) array of coefficients. We’ll call this array an \\(m \\times n\\) matrix.\nHere’s a couple of examples of going back and forth between equation notation and matrix-vector notation so you get the idea. It’s good to be able to do this kind of thing without thinking. I’ll frequently go back and forth from now on depending on which notation is most convenient.\n\\[\\begin{gather*}\n\\begin{alignedat}{3}\n   y_0 & {}={} & 3x_0 & {}+{} &  x_1  \\\\\n   y_1 & {}={} & x_0 & {}-{} &  2x_1\n\\end{alignedat}\n\\quad \\Longleftrightarrow \\quad\n\\begin{pmatrix}\ny_0 \\\\\ny_1\n\\end{pmatrix} =\n\\begin{pmatrix}\n3 & 1 \\\\\n1 & -2\n\\end{pmatrix}\n\\begin{pmatrix}\nx_0 \\\\\nx_1\n\\end{pmatrix},\n\\end{gather*}\\]\n\\[\\begin{gather*}\n\\begin{alignedat}{5}\n   y_0 & {}={} & x_0 & {}+{} &  2x_1 & {}+{} & 3x_2  \\\\\n   y_1 & {}={} & 4x_0 & {}-{} &  5x_1 & {} {} &\n\\end{alignedat}\n\\quad \\Longleftrightarrow \\quad\n\\begin{pmatrix}\ny_0 \\\\\ny_1 \\\\\n\\end{pmatrix} =\n\\begin{pmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 0\n\\end{pmatrix}\n\\begin{pmatrix}\nx_0 \\\\\nx_1 \\\\\nx_2\n\\end{pmatrix}.\n\\end{gather*}\\]\nIt’s convenient to use an abstract notation to express vectors and matrices so we can more easily manipulate them. If we define the symbol \\(\\mathbf{x}\\) to represent the column vector of inputs, the symbol \\(\\mathbf{y}\\) to represent the column vector of outputs, and the symbol \\(\\mathbf{A}\\) to represent the matrix of coefficients, we can write the same \\(m \\times n\\) linear system in the much simpler form\n\\[\\mathbf{y} = \\mathbf{A} \\mathbf{x}.\\]\nThis looks almost just like the simple one-dimensional linear equation \\(y=ax\\), except it’s packing a lot more into it that we’ll have to analyze. By convention, I’ll use bold-face characters to represent vectors and matrices (and tensors) in this book. For this most part, I’ll try to use lower-case letters for vectors, and upper-case letters for matrices. This is an almost universally followed convention, but it’s not unanimous.\nTo index into these arrays, I’ll mostly use subscript notation. For example, the element of \\(\\mathbf{x}\\) at index \\(i\\) will be denoted \\(x_i\\). The element of \\(\\mathbf{A}\\) at index \\((i,j)\\) will be denoted \\(A_{i,j}\\). Sometimes I’ll also use the code equivalent of \\(x[i]\\) or \\(A[i,j]\\) when it’s more clear. Following the python convention, I’ll always index starting from \\(0\\), so that an array of \\(n\\) elements goes from \\(0, 1, \\cdots, n-1\\), not from \\(1, 2, \\cdots, n\\) as is more typical in math books. I do this mainly to make going between math and code easier, as index errors can be a pain to deal with.\nIt may not be at all obvious, but having written a linear system as a matrix-vector equation, I’ve implicitly defined a new kind of array multiplication. To see this, I’ll define a new column vector that I’ll call \\(\\mathbf{A} \\mathbf{x}\\) whose elements are just the right-hand side of the linear system when written out,\n\\[\n\\mathbf{A} \\mathbf{x} =\n\\begin{pmatrix}\na_{0,0}x_0 & + & a_{0,1}x_1 & + & \\cdots & + & a_{0,n-1}x_{n-1} \\\\\na_{1,0}x_0 & + & a_{1,1}x_1 & + & \\cdots & + & a_{1,n-1}x_{n-1} \\\\\n\\vdots    &   & \\vdots    &   &  \\ddots  &   & \\quad \\vdots     \\\\\na_{m-1,0}x_0 & + & a_{m-1,1}x_1 & + & \\cdots & + & a_{m-1,n-1}x_{n-1} \\\\\n\\end{pmatrix}.\n\\]\nSetting the \\(i\\)th row of \\(\\mathbf{A} \\mathbf{x}\\) equal to the \\(i\\)th row of \\(\\mathbf{y}\\) must imply that each element \\(y_i\\) can be written\n\\[y_i = a_{i,0}x_0 + a_{i,1}x_1 + \\cdots + a_{i,n-1}x_{n-1} = \\sum_{k=0}^{n-1} a_{i,k}x_k.\\]\nThat is, each constant term \\(y_i\\) is the sum of the products of the \\(i\\)th row of the matrix \\(\\mathbf{A}\\) with the vector \\(\\mathbf{x}\\). This is matrix-vector multiplication, a special case of matrix multiplication, which I’ll get to shortly. Note that this operation is only defined when the number columns of \\(\\mathbf{A}\\) matches the size of \\(\\mathbf{x}\\). We say in this case that \\(\\mathbf{A}\\) and \\(\\mathbf{x}\\) are compatible.\nHere’s a quick example, where a \\(2 \\times 3\\) matrix \\(\\mathbf{A}\\) is matrix multiplied with a size \\(3\\) vector \\(\\mathbf{x}\\). For each row we’re element-wise multiplying that row of \\(\\mathbf{A}\\) with the vector \\(\\mathbf{x}\\) and then summing up the terms. The output will be the vector \\(\\mathbf{y}\\) of size \\(2\\).\n\\[\n\\mathbf{A} \\mathbf{x} =\n\\begin{pmatrix}\n\\color{red}{1} & \\color{red}{2} & \\color{red}{3} \\\\\n\\color{blue}{4} & \\color{blue}{5} & \\color{blue}{6}\n\\end{pmatrix}\n\\begin{pmatrix}\n1 \\\\\n1 \\\\\n1\n\\end{pmatrix} =\n\\begin{pmatrix}\n\\color{red}{1} \\color{black}{\\cdot} \\color{black}{1} \\color{black}{+} \\color{red}{2} \\color{black}{\\cdot} \\color{black}{1} \\color{black}{+} \\color{red}{3} \\color{black}{\\cdot} \\color{black}{1} \\\\\n\\color{blue}{4} \\color{black}{\\cdot} \\color{black}{1} \\color{black}{+} \\color{blue}{5} \\color{black}{\\cdot} \\color{black}{1} \\color{black}{+} \\color{blue}{6} \\color{black}{\\cdot} \\color{black}{1} \\\\\n\\end{pmatrix} =\n\\begin{pmatrix}\n6 \\\\\n15\n\\end{pmatrix} = \\mathbf{y}.\n\\]\nI used the color coding to help illustrate a point. Notice that each element of \\(\\mathbf{y}\\) is just that row of \\(\\mathbf{A}\\) being element-wise multiplied by \\(\\mathbf{x}\\) and summed over. The fancy term for this operation is a dot product. I’ll get into that more in the next lesson."
  },
  {
    "objectID": "notebooks/linear-systems.html#matrix-multiplication",
    "href": "notebooks/linear-systems.html#matrix-multiplication",
    "title": "5  Linear Systems",
    "section": "5.3 Matrix Multiplication",
    "text": "5.3 Matrix Multiplication\nMatrix-vector multiplication is just a special case of the more general matrix multiplication. If \\(\\mathbf{A}\\) is an \\(m \\times n\\) matrix and \\(\\mathbf{B}\\) is an \\(n \\times p\\) matrix, we’ll define their matrix multiplication as a new \\(m \\times p\\) matrix \\(\\mathbf{C}\\) whose elements are given by\n\\[C_{i,j} = \\sum_{k=0}^{n-1} A_{i,k} B_{k,j} = A_{i,0} B_{0,j} + A_{i,1} B_{1,j} + \\cdots + A_{i,n-1} B_{n-1,j}.\\]\nMatrix multiplication is always expressed symbolically by directly concatenating the two matrix symbols next to each other like \\(\\mathbf{A}\\mathbf{B}\\). We’d never use a multiplication symbol between them since those are often used to represent other kinds of multiplication schemes like element-wise multiplication or convolutions. Further, matrix multiplication is only defined when the numbers of columns in \\(\\mathbf{A}\\) equals the number of rows of \\(\\mathbf{B}\\). We say matrices satisfying this condition are compatible. If they can’t be multiplied, they’re called incompatible.\nIn words, matrix multiplication is the process where you take a row \\(i\\) of the left matrix \\(\\mathbf{A}\\), element-wise multiply it with a column \\(j\\) of the right matrix \\(\\mathbf{B}\\), and then sum up the results to get the entry \\(C_{i,j}\\) of the output matrix \\(\\mathbf{C}\\). Doing this for all pairs of rows and columns will fill in \\(\\mathbf{C}\\).\nHere’s an example where \\(\\mathbf{A}\\) is \\(3 \\times 3\\) and \\(\\mathbf{B}\\) is \\(3 \\times 2\\). The output matrix \\(\\mathbf{C}\\) will be \\(3 \\times 2\\).\n\\[\n\\begin{pmatrix}\n    \\color{red}{1} & \\color{red}{2} & \\color{red}{3} \\\\\n    \\color{blue}{4} & \\color{blue}{5} & \\color{blue}{6} \\\\\n    \\color{green}{7} & \\color{green}{8} & \\color{green}{9} \\\\\n\\end{pmatrix}\n\\begin{pmatrix}\n    \\color{orange}{6} & \\color{purple}{5} \\\\\n    \\color{orange}{4} & \\color{purple}{3} \\\\\n    \\color{orange}{2} & \\color{purple}{1} \\\\\n\\end{pmatrix} =\n\\begin{pmatrix}\n   \\color{red}{1} \\color{black}{\\cdot} \\color{orange}{6} \\color{black}{+} \\color{red}{2} \\color{black}{\\cdot} \\color{orange}{4} \\color{black}{+} \\color{red}{3} \\color{black}{\\cdot} \\color{orange}{2} & \\color{red}{1} \\color{black}{\\cdot} \\color{purple}{5} \\color{black}{+} \\color{red}{2} \\color{black}{\\cdot} \\color{purple}{3} \\color{black}{+} \\color{red}{3} \\color{black}{\\cdot} \\color{purple}{1} \\\\\n   \\color{blue}{4} \\color{black}{\\cdot} \\color{orange}{6} \\color{black}{+} \\color{blue}{5} \\color{black}{\\cdot} \\color{orange}{4} \\color{black}{+} \\color{blue}{6} \\color{black}{\\cdot} \\color{orange}{2} & \\color{blue}{4} \\color{black}{\\cdot} \\color{purple}{5} \\color{black}{+} \\color{blue}{5} \\color{black}{\\cdot} \\color{purple}{3} \\color{black}{+} \\color{blue}{6} \\color{black}{\\cdot} \\color{purple}{1} \\\\\n   \\color{green}{7} \\color{black}{\\cdot} \\color{orange}{6} \\color{black}{+} \\color{green}{8} \\color{black}{\\cdot} \\color{orange}{4} \\color{black}{+} \\color{green}{9} \\color{black}{\\cdot} \\color{orange}{2} & \\color{green}{7} \\color{black}{\\cdot} \\color{purple}{5} \\color{black}{+} \\color{green}{8} \\color{black}{\\cdot} \\color{purple}{3} \\color{black}{+} \\color{green}{9} \\color{black}{\\cdot} \\color{purple}{1} \\\\\n\\end{pmatrix} =\n\\begin{pmatrix}\n   20 & 14 \\\\\n   56 & 41 \\\\\n   92 & 68 \\\\\n\\end{pmatrix}.\n\\]\nAside: If you’re still having a hard time picturing what matrix multiplication is doing, you may find this online visualization tool useful.\nNote that matrix multiplication does not commute. That is, we can’t swap the order of the two matrices being multiplied, \\(\\mathbf{A}\\mathbf{B} \\neq \\mathbf{B}\\mathbf{A}\\). Try to multiply the previous example in the opposite order and see what happens. The matrices won’t even be compatible anymore.\nHowever, matrix multiplication is associative, which means you can group parentheses just like you ordinarily would. For example, multiplying three matrices \\(\\mathbf{A}, \\mathbf{B}, \\mathbf{C}\\) could be done by multiplying either the first two, and then the last; or the last two, and then the first. That is,\n\\[\\mathbf{A}\\mathbf{B}\\mathbf{C} = \\mathbf{A}(\\mathbf{B}\\mathbf{C}) = (\\mathbf{A}\\mathbf{B})\\mathbf{C}.\\]\n\n5.3.1 Matrix Multiplication Algorithm\nMatrix multiplication is perhaps the single most important mathematical operation in machine learning. It’s so important I’m going to write a function to code it from scratch before showing how to do it in numpy. I’ll also analyze the speed of the algorithm in FLOPS and the memory in terms of word size. Algorithmically, all matrix multiplication is doing is looping over every single element of \\(\\mathbf{C}\\) and performing the sum-product calculation above for each \\(C_{i,j}\\). I’ll define a function called matmul that takes in two numpy arrays A and B and multiplies them, returning the product C if the dimensions are compatible.\n\n\nCode\ndef matmul(A, B):\n    assert A.shape[1] == B.shape[0]\n    m, n, p = A.shape[0], A.shape[1], B.shape[1]\n    C = np.zeros((m, p))\n    for i in range(m):\n        for j in range(p):\n            for k in range(n):\n                C[i, j] += A[i, k] * B[k, j]\n    return C\n\n\n\n\nCode\nA = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]); print(f'A = \\n{A}')\nB = np.array([[6, 5], [4, 3], [2, 1]]); print(f'B = \\n{B}')\nC = matmul(A, B); print(f'C = AB = \\n{C.astype(A.dtype)}')\n\n\nA = \n[[1 2 3]\n [4 5 6]\n [7 8 9]]\nB = \n[[6 5]\n [4 3]\n [2 1]]\nC = AB = \n[[20 14]\n [56 41]\n [92 68]]\n\n\nLet’s take a quick look at what this function is doing complexity wise. First off, we’re pre-computing the output matrix \\(\\mathbf{C}\\). That’ll contribute \\(O(mp)\\) memory since \\(\\mathbf{C}\\) is \\(m \\times p\\). All of the FLOPS are happening inside the double loop over \\(m\\) and \\(p\\). For each \\(i,j\\) pair, the function is doing \\(n\\) total multiplications and \\(n-1\\) total additions, which means there’s \\(2n-1\\) FLOPs per \\(i,j\\) pair. Since we’re doing this operation \\(m \\cdot p\\) times, we’re thus doing \\(m \\cdot p \\cdot (2n-1)\\) total FLOPS in the matrix multiply. This gives us an \\(O(nmp)\\) algorithm in general. Matrix multiplication is an example of a cubic time algorithm since if \\(n=m=p\\) we’d have a \\(O(n^3)\\) FLOPS operation.\nAside: People have found algorithms that can matrix multiply somewhat faster than cubic time. For example, Strassen’s algorithm can matrix multiply in about \\(O(n^{2.8})\\) time. In practice, though, these algorithms tend to have large constants out front, which means they’re not that useful unless \\(n\\) is huge. If the matrices have special structure, e.g. banded matrices or sparse matrices, they have special algorithms that can multiply them even faster, for example by using the Fast Fourier Transform, which can matrix multiply as fast as \\(O(n^2 \\log^2 n)\\). This is what, for example, convolutional neural networks use.\nCubic time may seem fast since it’s polynomial time, but it’s really not that great unless the matrices are relatively small (say \\(n \\leq 1000\\) or so). For this reason, a lot of effort has instead gone into pushing down the algorithmic constant out front, e.g. by doing special hardware optimizations like SIMD, optimizing matrix blocks to take advantage of memory efficiency, or heavily parallelizing the operation by doing the inner loops in parallel. It’s also a good idea to push operations down to low-level compiled code written in FORTRAN or C, which is what numpy essentially does.\nIn the age of deep learning we’re finding ourselves needing to multiply a lot of matrices and needing to do it quickly. This has been enabled largely through the use of GPUs to do array computations. GPUs are essentially specially built hardware just to do array operations like matrix multiplication efficiently. It’s no exaggeration in fact to say that the recent deep learning revolution happened precisely because of GPUs.\nAnyway, we’d never want to implement matrix multiplication natively in python like this. It’s far too most of the time. In practice we’d use something like np.matmul(A, B). Numpy also supports a cleaner syntax using the @ operator. This means we can also express the matrix multiply as A @ B, which means exactly the same thing as np.matmul(A, B), just with cleaner syntax. This syntax is what I’ll typically use in this book.\nHere’s an example. I’ll multiply the same two matrices from before, but this time using numpy. To show it’s faster than native python matmul, I’ll run a quick profiler as well. You can see that even with these small matrices we’re still getting a 10x speedup using numpy over base python. The speedup can get up to 100x and higher for much larger matrices.\n\n\nCode\nC = A @ B\nprint(f'C = \\n{C.astype(A.dtype)}')\n\n\nC = \n[[20 14]\n [56 41]\n [92 68]]\n\n\n\n\nCode\n%timeit matmul(A, B)\n\n\n11.2 µs ± 14 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\n\n\nCode\n%timeit A @ B\n\n\n892 ns ± 4.44 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n\n\n\n\n5.3.2 Chained Matrix Multiplication\nWhat if we’d like to multiply three or more matrices together. I already said matrix multiplication is associative, so in theory we should be able to matrix multiply in any order and get the same answer. However, there are often computational advantages to multiplying them together in some particular sequence. For example, suppose we wanted to multiply \\(\\mathbf{D} = \\mathbf{A}\\mathbf{B}\\mathbf{C}\\). Suppose, \\(\\mathbf{A}\\) is \\(m \\times n\\), \\(\\mathbf{B}\\) is \\(n \\times p\\), and \\(\\mathbf{C}\\) is \\(p \\times q\\). No matter which order we do it, the output \\(\\mathbf{D}\\) will have size \\(m \\times q\\). But there are two ways we could do this multiplication.\n\n\\(\\mathbf{D} = \\mathbf{A}(\\mathbf{B}\\mathbf{C})\\): In this case, the \\(\\mathbf{E}=\\mathbf{B}\\mathbf{C}\\) computation requires \\(nq(2p-1)\\) FLOPS, and then the \\(\\mathbf{A}\\mathbf{E}\\) computation requires \\(mq(2n-1)\\) FLOPS. The total is thus the sum of these two, i.e. \\[nq(2p-1) + mq(2n-1) = O(npq+mnq) \\ \\ \\text{FLOPS}.\\]\n\\(\\mathbf{D} = (\\mathbf{A}\\mathbf{B})\\mathbf{C}\\): In this case, the \\(\\mathbf{F}=\\mathbf{A}\\mathbf{B}\\) computation requires \\(mp(2n-1)\\) FLOPS, and then the \\(\\mathbf{F}\\mathbf{C}\\) computation requires \\(mq(2n-1)\\) FLOPS. The total is thus the sum of these two, i.e. \\[mq(2p-1) + mp(2n-1) = O(mpq+mnp) \\ \\ \\text{FLOPS}.\\]\n\nLet’s put some numbers in to make it clear what’s going on. Suppose \\(m=1000\\), \\(n=2\\), \\(p=100\\), and \\(q=100\\). Then the first case takes\n\\[nq(2p-1) + mq(2n-1) = 339800 \\ \\ \\text{FLOPS},\\]\nwhile the second case takes a staggering\n\\[mq(2p-1) + mp(2n-1) = 20200000 \\ \\ \\text{FLOPS}.\\]\nIt would thus behoove us in this case to multiply the matrices in the first order to save on computation, \\(\\mathbf{D} = \\mathbf{A}(\\mathbf{B}\\mathbf{C})\\). Here’s a programmatic way to see this.\n\n\nCode\nm = 1000\nn = 2\np = 100\nq = 100\n\nprint(f'A(BC): {m * q * (2 * n - 1) + n * q * (2 * p - 1)} FLOPS')\nprint(f'(AB)C: {m * p * (2 * n - 1) + m * q * (2 * p - 1)} FLOPS')\n\n\nA(BC): 339800 FLOPS\n(AB)C: 20200000 FLOPS\n\n\nThe same issues extend to multiplying together arbitrarily many matrices. You can save a lot of compute by first taking time to find the optimal order to multiply them together before doing the computation. Don’t just naively multiply them in order. Numpy has a function np.linalg.multi_dot that can do this for you. If you pass in a list of matrices, it’ll multiply them together in the most efficient order to help save on computation. Here’s an example. I’ll profile the different ways we can do the \\(\\mathbf{A}\\mathbf{B}\\mathbf{C}\\) example above. Notice that indeed \\(\\mathbf{A}(\\mathbf{B}\\mathbf{C})\\) is much faster than \\((\\mathbf{A}\\mathbf{B})\\mathbf{C}\\). The multi_dot solution is roughly as fast as the \\(\\mathbf{A}(\\mathbf{B}\\mathbf{C})\\) solution, but it does take slightly longer because it first calculates the optimal ordering, which adds a little bit of time.\n\n\nCode\nA = np.random.rand(m, n)\nB = np.random.rand(n, p)\nC = np.random.rand(p, q)\n\n\n\n\nCode\n%timeit A @ (B @ C)\n\n\n64.8 µs ± 37.2 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n\n\n\n\nCode\n%timeit (A @ B) @ C\n\n\n526 µs ± 14 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n\n\n\n\nCode\n%timeit np.linalg.multi_dot([A, B, C])\n\n\n78.2 µs ± 223 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n\n\n\n\n5.3.3 Matrix Multiplication vs Element-Wise Multiplication\nWe’ve already seen a different way we can multiply two matrices (or any array), namely element-wise multiplication. For matrices, element-wise multiplication is sometimes called the Hadamard product. I’ll denote element-wise multiplication as \\(\\mathbf{A} \\circ \\mathbf{B}\\). Note that element-wise multiplication is only defined when the shapes of \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) are exactly equal (or can be broadcasted to be equal).\nIt’s important to mind the difference between matrix multiplication and element-wise multiplication of matrices. In general \\(\\mathbf{A} \\circ \\mathbf{B} \\neq \\mathbf{A} \\mathbf{B}\\). They’re defined completely differently,\n\\[\\begin{align*}\n(A \\circ B)_{i,j} &= A_{i,j} \\cdot B_{i,j} \\\\\n(AB)_{i,j} &= \\sum_k A_{i,k}B_{k,j}.\n\\end{align*}\\]\nIn numpy use A * B for element-wise multiplication and A @ B for matrix multiplication. To make it clear the two kinds of multiplication aren’t the same thing here’s an example.\n\n\nCode\nA = np.array([[1, 2], [3, 4]])\nB = np.array([[1, 0], [0, 1]])\nprint(f'A*B = \\n{A * B}')\nprint(f'AB = \\n{A @ B}')\n\n\nA*B = \n[[1 0]\n [0 4]]\nAB = \n[[1 2]\n [3 4]]\n\n\n\n\n5.3.4 Interpreting Matrix Multiplication\nThis stuff might seem kind of abstract so far. Why should we care about multiplying matrices? I’ll say more about that in the next lesson, but for now I want to mention a useful way to interpret matrix-vector multiplication and matrix multiplication, a way that you almost certainly never learned in school when you were taught about matrices.\nWe can think about a matrix in a few different ways. One way is just as a 2-dimensional array of numbers. We can also think of it as a stack of vectors. If \\(\\mathbf{A}\\) is an \\(m \\times n\\), we can think of each column of \\(\\mathbf{A}\\) as being a vector of size \\(m \\times 1\\). These are called the column vectors of \\(\\mathbf{A}\\). We can also think about each row of \\(\\mathbf{A}\\) as being a vector of size \\(1 \\times n\\). These are called the row vectors of \\(\\mathbf{A}\\). In keeping with the python convention, I’ll denote the column vectors of \\(\\mathbf{A}\\) by \\(\\mathbf{A}_{:, i}\\), and the row vectors of \\(\\mathbf{A}\\) by \\(\\mathbf{A}_{i, :}\\). Notice the use of the slice operator \\(:\\) here, which means “take everything in that dimension”.\nHere’s an example. Take \\(\\mathbf{A}\\) to be the following \\(2 \\times 3\\) matrix,\n\\[\n\\mathbf{A} =\n\\begin{pmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6\n\\end{pmatrix}.\n\\]\nThe column vectors of \\(\\mathbf{A}\\) are just\n\\[\\mathbf{A}_{:, 0} = \\begin{pmatrix} 1 \\\\ 4 \\end{pmatrix}, \\quad \\mathbf{A}_{:, 1} = \\begin{pmatrix} 2 \\\\ 5 \\end{pmatrix}, \\quad \\mathbf{A}_{:, 2} = \\begin{pmatrix} 3 \\\\ 6 \\end{pmatrix},\\]\nAnd the row vectors of \\(\\mathbf{A}\\) are just\n\\[\\mathbf{A}_{0, :} = \\begin{pmatrix} 1 & 2 & 3 \\end{pmatrix}, \\quad \\mathbf{A}_{1, :} = \\begin{pmatrix} 4 & 5 & 6 \\end{pmatrix}.\\]\nUsing the column vectors of \\(\\mathbf{A}\\) we can think about the matrix multiplication \\(\\mathbf{A} \\mathbf{x}\\) in an interesting way. If there are \\(n\\) total column vectors, we can write\n\\[\n\\mathbf{A} \\mathbf{x} =\n\\begin{pmatrix}\n\\mathbf{A}_{:, 0} & \\mathbf{A}_{:, 1} & \\cdots & \\mathbf{A}_{:, n-1}\n\\end{pmatrix}\n\\begin{pmatrix}\nx_0 \\\\\nx_1 \\\\\n\\vdots \\\\\nx_{n-1}\n\\end{pmatrix} =\nx_0 \\mathbf{A}_{:, 0} + x_1 \\mathbf{A}_{:, 1} + \\cdots x_{n-1} \\mathbf{A}_{:, n-1}.\n\\]\nEvidently, we can think of \\(\\mathbf{A} \\mathbf{x}\\) as some kind of mixture of the columns of \\(\\mathbf{A}\\), weighted by the elements of \\(\\mathbf{x}\\). Such a mixture is called a linear combination. In this terminology, we’d say that the matrix-vector multiplication \\(\\mathbf{A} \\mathbf{x}\\) is a linear combination of the columns of \\(\\mathbf{A}\\).\nFor example, if \\(\\mathbf{A}\\) is the \\(2 \\times 3\\) matrix from the previous example and \\(\\mathbf{x} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}\\), we’d have\n\\[\n\\mathbf{A} \\mathbf{x} =\n\\begin{pmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6\n\\end{pmatrix}\n\\begin{pmatrix}\n1 \\\\\n1 \\\\\n1\n\\end{pmatrix} = 1 \\cdot \\binom{1}{4} + 1 \\cdot \\binom{2}{5} + 1 \\cdot \\binom{3}{6} = \\binom{1+2+3}{4+5+6} = \\binom{6}{15}.\n\\]\nWe can think of matrix multiplication in a similar way. Suppose we want to multiply two matrices \\(\\mathbf{A} \\mathbf{X}\\). You can think of \\(\\mathbf{X}\\) as itself being a bunch of different column vectors \\(\\mathbf{X}_{:, i}\\), where for each of those column vectors we’re doing a matrix-vector multiplication \\(\\mathbf{A}\\mathbf{X}_{:, i}\\). That is, matrix multiplication is just a batch of weighted linear combinations of the columns of \\(\\mathbf{A}\\),\n\\[\n\\begin{array}{c<{x_0} c c<{x_1} c c<{\\cdots} c c<{x_{n-1}} c l}\n\\mathbf{A} \\mathbf{X}_{:, 0} & = & X_{0,0} \\mathbf{A}_{:, 0} & + & X_{1,0} \\mathbf{A}_{:, 1} & + & \\cdots & + & X_{m-1,0} \\mathbf{A}_{:, n-1} \\\\\n\\mathbf{A} \\mathbf{X}_{:, 1} & = & X_{0,1} \\mathbf{A}_{:, 0} & + & X_{1,1} \\mathbf{A}_{:, 1} & + & \\cdots & + & X_{m-1,1} \\mathbf{A}_{:, n-1} \\\\\n\\vdots & & \\vdots    &   & \\vdots    &   &  \\ddots  &   & \\quad \\vdots\\\\\n\\mathbf{A} \\mathbf{X}_{:, n-1} & = & X_{0,n-1} \\mathbf{A}_{:, 0} & + & X_{1,n-1} \\mathbf{A}_{:, 1} & + & \\cdots & + & X_{m-1,n-1} \\mathbf{A}_{:, n-1}. \\\\\n\\end{array}\n\\]\nThese aren’t the only ways to interpret what matrix multiplication is doing. I’ll cover a more geometric interpretation later, where it’ll turn out that matrix multiplication is the same thing as the composition of linear maps."
  },
  {
    "objectID": "notebooks/linear-systems.html#solving-linear-systems",
    "href": "notebooks/linear-systems.html#solving-linear-systems",
    "title": "5  Linear Systems",
    "section": "5.4 Solving Linear Systems",
    "text": "5.4 Solving Linear Systems\nOne of the most important things we’d like to do with linear systems is solve for their inputs. Suppose we have a linear equation of the form \\(ax=b\\) and we wanted to solve for \\(x\\). It’s clear in this case what we’d do. Provided \\(a \\neq 0\\), we’d divide both sides by \\(a\\) to get\n\\[x = \\frac{b}{a} = a^{-1} b.\\]\nWe’d like to be able to do something like this for an \\(m \\times n\\) linear system \\(\\mathbf{Ax} = \\mathbf{b}\\). But dividing by a matrix doesn’t really make sense. We need to figure out another way to proceed.\n\n5.4.1 Square Linear Systems\nPerhaps it would help to recall how we’d solve a system of equations. Suppose for example we have the following system of 2 linear equations with 2 unknowns \\(x_0\\) and \\(x_1\\),\n\\[\\begin{alignat*}{3}\n   x_0 & {}+{} &  x_1 & {}={} & 2  \\\\\n   x_0 & {}-{} &  x_1 & {}={} & 0. \\\\\n\\end{alignat*}\\]\nUsing the method that pretty much always works, substitution, we can solve these one at a time. The second equation says \\(x_0 = x_1\\). Plugging this into the first equation then says \\(x_0=x_1=1\\), which is our solution. This is an example of the more general \\(2 \\times 2\\) linear system\n\\[\\begin{alignat*}{3}\n   ax_0 & {}+{} &  bx_1 & {}={} & e \\\\\n   cx_0 & {}+{} &  dx_1 & {}={} & f.\n\\end{alignat*}\\]\nThis can be solved by substitution too, but I’ll spare you the details and use sympy to get the answer. It’s given by\n\\[\\begin{align*}\nx_0 &= \\frac{de-bf}{ad-bc} \\\\\nx_1 &= \\frac{af-ce}{ad-bc}.\n\\end{align*}\\]\n\n\nCode\nx0, x1 = sp.symbols('x_0 x_1')\na, b, c, d, e, f = sp.symbols('a b c d e f')\neq1 = sp.Eq(a * x0 + b * x1, e)\neq2 = sp.Eq(c * x0 + d * x1, f)\nsol = sp.solve((eq1, eq2), (x0, x1))\nprint(f'x0 = {sol[x0]}')\nprint(f'x1 = {sol[x1]}')\n\n\nx0 = (-b*f + d*e)/(a*d - b*c)\nx1 = (a*f - c*e)/(a*d - b*c)\n\n\nIt’s worth plotting what these equations look like to try to visualize what’s going on. Let’s look again at the specific set of equations\n\\[\\begin{alignat*}{3}\n   x_0 & {}+{} &  x_1 & {}={} & 2  \\\\\n   x_0 & {}-{} &  x_1 & {}={} & 0. \\\\\n\\end{alignat*}\\]\nEach of these equations corresponds to a line in the plane, namely\n\\[y = 2 - x, \\quad y = x.\\]\nIf we plot these two lines, the point where they intersect is \\((1,1)\\), which is the solution to the linear system.\n\n\nCode\nx = np.linspace(-3, 3, 100)\nf0 = lambda x: 2 - x\nf1 = lambda x: x\nplot_function(x, [f0, f1], xlim=(0, 3), ylim=(0, 3), title='2 Linear Equations, 2 Unknowns',\n              labels=[f'$y=2-x$', f'$y=x$'])\n\n\n\n\n\n\n\n\n\nMore generally, the coefficients \\(a,b,c,d,e,f\\) represent the slopes and intercepts of the two lines. Changing any of these will change the point of intersection, and hence also the solution to the \\(2 \\times 2\\) system. Notice there is one edge case, namely when \\(ad=bc\\). This is when the two lines are parallel. Since parallel lines don’t intersect, such a system would have no solution. You can also see this by noticing that the denominator for \\(x_0\\) and \\(x_1\\) blows up, since \\(D=ad-bc=0\\). These denominators are special. They essentially say whether or not a solution to a given linear system will even exist.\nLet’s look now at the general \\(3 \\times 3\\) linear system\n\\[\\begin{alignat*}{5}\n   ax_0 & {}+{} &  bx_1 & {}+{} & cx_2 {}={} & j \\\\\n   dx_0 & {}+{} &  ex_1 & {}+{} & fx_2 {}={} & k \\\\\n   gx_0 & {}+{} &  hx_1 & {}+{} & ix_2 {}={} & l.\n\\end{alignat*}\\]\nAccording to sympy, the solution to this system is evidently this monstrosity,\n\\[\\begin{align*}\nx_0 &= \\frac{bfl - bik - cel + chk + eij - fhj}{aei - afh - bdi + bfg + cdh - ceg} \\\\\nx_1 &= \\frac{-afl + aik + cdl - cgk - dij + fgj}{aei - afh - bdi + bfg + cdh - ceg} \\\\\nx_2 &= \\frac{ael - ahk - bdl + bgk + dhj - egj}{aei - afh - bdi + bfg + cdh - ceg}. \\\\\n\\end{align*}\\]\n\n\nCode\nx0, x1, x2 = sp.symbols('x_0 x_1 x_2')\na, b, c, d, e, f, g, h, i, j, k, l = sp.symbols('a b c d e f g h i j k l')\neq1 = sp.Eq(a * x0 + b * x1 + c * x2, j)\neq2 = sp.Eq(d * x0 + e * x1 + f * x2, k)\neq3 = sp.Eq(g * x0 + h * x1 + i * x2, l)\nsol = sp.solve((eq1, eq2, eq3), (x0, x1, x2))\nprint(f'x0 = {sol[x0]}')\nprint(f'x1 = {sol[x1]}')\nprint(f'x2 = {sol[x2]}')\n\n\nx0 = (b*f*l - b*i*k - c*e*l + c*h*k + e*i*j - f*h*j)/(a*e*i - a*f*h - b*d*i + b*f*g + c*d*h - c*e*g)\nx1 = (-a*f*l + a*i*k + c*d*l - c*g*k - d*i*j + f*g*j)/(a*e*i - a*f*h - b*d*i + b*f*g + c*d*h - c*e*g)\nx2 = (a*e*l - a*h*k - b*d*l + b*g*k + d*h*j - e*g*j)/(a*e*i - a*f*h - b*d*i + b*f*g + c*d*h - c*e*g)\n\n\nIgnore the details of this thing. Just notice the fact that all three unknowns seem to again have the same denominator, in this case\n\\[D = aei - afh - bdi + bfg + cdh - ceg.\\]\nIf \\(D=0\\), the \\(3 \\times 3\\) system will have no solution. I can keep going, next to \\(4 \\times 4\\) systems, then \\(5 \\times 5\\) systems, but hopefully you get the point. There will always be a common denominator \\(D\\) in the solutions that can’t be zero. These denominators have a name. They’re called determinants. If \\(\\mathbf{A}\\) is the \\(n \\times n\\) matrix of coefficients, we’ll denote its determinant by \\(\\det(\\mathbf{A})\\) or sometimes just by \\(|\\mathbf{A}|\\). We’ve thus stumbled on a general fact.\nFact: An \\(n \\times n\\) system of linear equations \\(\\mathbf{Ax}=\\mathbf{b}\\) where \\(\\mathbf{b} \\neq \\mathbf{0}\\) has a solution if and only if \\(\\det(\\mathbf{A}) \\neq 0\\). In fact, this solution is unique.\nNote there’s an edge case when \\(\\mathbf{b} = \\mathbf{0}\\) and \\(\\det(\\mathbf{A}) \\neq 0\\). In that one case, there will be infinitely many solutions. You can think of this as the situation where the solutions have a \\(\\frac{0}{0}\\), which is the one case where dividing by \\(0\\) could still give a finite number.\nBefore moving on, let’s visualize what the \\(3 \\times 3\\) linear system looks like. Consider the following specific example,\n\\[\\begin{alignat*}{5}\n   3x_0 & {}+{} &  2x_1 & {}+{} & x_2 {}={} & 0 \\\\\n   x_0 & {}+{} &  x_1 & {}-{} & x_2 {}={} & 1 \\\\\n   x_0 & {}-{} &  3x_1 & {}-{} & x_3 {}={} & -3. \\\\\n\\end{alignat*}\\]\nAgain using substitution, you can solve each equation one by one to check that this system has a solution at \\(x_0=-\\frac{1}{2}\\), \\(x_1=1\\), and \\(x_2=-\\frac{1}{2}\\). The three equations above form a set of three planes given by\n\\[\\begin{align*}\nz &= -3x - 2y + 0, \\\\\nz &= x + y - 1, \\\\\nz &= 4 . \\\\\n\\end{align*}\\]\nThe solution to this \\(3 \\times 3\\) system will be the point where all three of these planes intersect. It’s a perhaps a little hard to see in the plot, but hopefully you get the point. In general, the solution of an \\(n \\times n\\) linear system will occur at the point where the set of \\(n\\) hyperplanes all intersect. If any two of the hyperplanes are parallel, the determinant will be zero, and there won’t be a solution.\n\n\nCode\nx = np.linspace(-2.5, 1.5, 100)\ny = np.linspace(0, 2, 100)\nf1 = lambda x, y: -3 * x - 2 * y + 0\nf2 = lambda x, y: x + y - 1\nf3 = lambda x, y: x - 3 * y + 3\nplot_function_3d(x, y, [f1, f2, f3], azim=65, elev=25, ticks_every=[1, 1, 3], figsize=(5, 5), zorders=[0, 2, 1],\n                 colors=['steelblue', 'salmon', 'limegreen'], points=[[-0.5, 1, -0.5]], alpha=0.6, labelpad=3, \n                 dist=11, title='3 Equations, 3 Unknowns')\n\n\n\n\n\n\n\n\n\nLet’s now try to see if we can figure out a pattern, a way to systematically solve these linear systems. Let’s start by going back to the easy \\(2 \\times 2\\) case. Recall that the linear system\n\\[\\begin{alignat*}{3}\n   ax_0 & {}+{} &  bx_1 & {}={} & e \\\\\n   cx_0 & {}+{} &  dx_1 & {}={} & f. \\\\\n\\end{alignat*}\\]\nhas solutions given by\n\\[\\begin{align*}\nx_0 &= \\frac{de-bf}{ad-bc} \\\\\nx_1 &= \\frac{af-ce}{ad-bc}. \\\\\n\\end{align*}\\]\nNow, if we write this \\(2 \\times 2\\) linear system in matrix-vector notation, we’d have\n\\[\n\\mathbf{A}\\mathbf{x} =\n\\begin{pmatrix}\na & b \\\\\nc & d\n\\end{pmatrix}\n\\begin{pmatrix}\nx_0 \\\\\nx_1\n\\end{pmatrix} =\n\\begin{pmatrix}\ne \\\\\nf\n\\end{pmatrix}\n= \\mathbf{b},\n\\]\nand the solutions would look like\n\\[\n\\mathbf{x} =\n\\begin{pmatrix}\nx_0 \\\\\nx_1\n\\end{pmatrix} =\n\\begin{pmatrix}\n\\frac{de-bf}{ad-bc} \\\\\n\\frac{af-ce}{ad-bc}\n\\end{pmatrix}.\n\\]\nI’m going to manipulate the solutions so they have a suggestible form. Observe that we can write\n\\[\n\\mathbf{x} =\n\\begin{pmatrix}\nx_0 \\\\\nx_1\n\\end{pmatrix} =\n\\begin{pmatrix}\n\\frac{de-bf}{ad-bc} \\\\\n\\frac{af-ce}{ad-bc}\n\\end{pmatrix} =\n\\begin{pmatrix}\n\\frac{d}{ad-bc} & -\\frac{b}{ad-bc} \\\\\n-\\frac{c}{ad-bc} & \\frac{a}{ad-bc}\n\\end{pmatrix}\n\\begin{pmatrix}\ne \\\\\nf\n\\end{pmatrix} =\n\\frac{1}{ad-bc}\n\\begin{pmatrix}\nd & -b \\\\\n-c & a\n\\end{pmatrix}\n\\begin{pmatrix}\ne \\\\\nf\n\\end{pmatrix}.\n\\]\nOn the right-hand side, we seem to have some kind of matrix times the vector \\(\\mathbf{b}\\). Whatever that matrix is, it seems to “undo” \\(\\mathbf{A}\\). Let’s call that matrix \\(\\mathbf{A}^{-1}\\). Then the solution of the linear system in abstract notation would just be\n\\[\\mathbf{x} = \\mathbf{A}^{-1} \\mathbf{b}.\\]\nThis is the most general kind of solution we could write for a square linear system. Of course, the real hard part in solving a general \\(n \\times n\\) system is finding what exactly \\(\\mathbf{A}^{-1}\\) is.\nBut why did I use the notation \\(\\mathbf{A}^{-1}\\) for this matrix? Because it’s in some sense a way to “divide” by a matrix. Recall in the \\(1 \\times 1\\) case where \\(ax=b\\) the solution looked like \\(x=a^{-1}b\\). In that case, \\(a^{-1}\\) was literally the inverse of the number \\(a\\), since \\(aa^{-1} = a^{-1}a = 1\\). It turns out the matrix \\(\\mathbf{A}^{-1}\\) is the higher-dimensional generalization of \\(a^{-1}\\). It’s called the inverse of \\(\\mathbf{A}\\). To see why, notice in the \\(2 \\times 2\\) case if we multiply \\(\\mathbf{A}\\mathbf{A}^{-1}\\), we’d have\n\\[\n\\mathbf{A}\\mathbf{A}^{-1} =\n\\begin{pmatrix}\na & b \\\\\nc & d\n\\end{pmatrix}\n\\begin{pmatrix}\n\\frac{d}{ad-bc} & -\\frac{b}{ad-bc} \\\\\n-\\frac{c}{ad-bc} & \\frac{a}{ad-bc}\n\\end{pmatrix} =\n\\begin{pmatrix}\n\\frac{ad}{ad-bc}-\\frac{bc}{ad-bc} & -\\frac{ab}{ad-bc}+\\frac{ab}{ad-bc} \\\\\n\\frac{cd}{ad-bc}-\\frac{dc}{ad-bc} & -\\frac{cb}{ad-bc}+\\frac{da}{ad-bc}\n\\end{pmatrix} =\n\\begin{pmatrix}\n\\frac{ad-bc}{ad-bc} & \\frac{ab-ab}{ad-bc} \\\\\n\\frac{cd-cd}{ad-bc} & \\frac{ad-bc}{ad-bc}\n\\end{pmatrix} =\n\\begin{pmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{pmatrix}.\n\\]\nI’ll write the matrix on the right as \\(\\mathbf{I}\\). It’s called the identity matrix. It’s evidently the matrix generalization of the number \\(1\\). What I’ve just shown is that \\(\\mathbf{A}^{-1}\\) “undoes” \\(\\mathbf{A}\\) in the sense that \\(\\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{I}\\). Of course, since matrix multiplication doesn’t commute, this says nothing about what the reverse product \\(\\mathbf{A}^{-1}\\mathbf{A}\\) is. You can check in the \\(2 \\times 2\\) case that indeed we’d get \\(\\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{I}\\) as well. That is, \\(\\mathbf{A}^{-1}\\) is a two-sided inverse. A matrix and its inverse always commute.\nNotice that in the \\(2 \\times 2\\) case, the inverse matrix \\(\\mathbf{A}^{-1}\\) includes a division by the determinant \\(\\det(\\mathbf{A})\\),\n\\[\n\\mathbf{A}^{-1} =\n\\frac{1}{ad-bc}\n\\begin{pmatrix}\nd & -b \\\\\n-c & a\n\\end{pmatrix} =\n\\frac{1}{\\det(\\mathbf{A})}\n\\begin{pmatrix}\nd & -b \\\\\n-c & a\n\\end{pmatrix}.\n\\]\nEvidently, the inverse matrix only exists when \\(\\det(\\mathbf{A}) \\neq 0\\), since otherwise the matrix would blow up due to division by zero. This is a general statement. For any \\(n \\times n\\) matrix, its inverse exists if and only if its determinant is non-zero. For this reason, we say a square matrix with a non-zero determinant is invertible. If the determinant is zero, we call the matrix singular.\nOf course, it’s no longer obvious at all how to even find \\(\\mathbf{A}^{-1}\\) or \\(\\text{det}(\\mathbf{A})\\) when \\(n\\) is greater than \\(2\\) or \\(3\\). Thankfully, we don’t really need to know the gritty details of how to find these things. Just know that algorithms exist to calculate them. I’ll talk at a high level about how those algorithms work in a future lesson.\nIn numpy, you can solve a square linear system \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\) by using the command np.linalg.solve(A, b). While you could also solve a system by first calculating the inverse and then taking \\(\\mathbf{x} = \\mathbf{A}^{-1} \\mathbf{b}\\), this turns out to be a bad idea to do numerically. It’s actually better to avoid explicitly calculating \\(\\mathbf{A}^{-1}\\) unless you absolutely need it. The main reason is the inverse computation turns out to be highly prone to numerical loss of precision. Nevertheless, if you do need the inverse for some reason, you can get it with np.linalg.inv(A). Just like matrix multiplication, both of these functions are cubic time algorithms.\nHere’s an example. I’ll solve the \\(3 \\times 3\\) linear system below using np.solve. To do this, I’ll first need to convert everything to matrix-vector notation,\n\\[\\begin{gather*}\n\\begin{alignedat}{5}\n   x_0 & {}+{} &  2x_1 & {}+{} & 3x_2 {}={} & 1 \\\\\n   4x_0 & {}+{} &  5x_1 & {}-{} & 6x_2 {}={} & 1 \\\\\n   7x_0 & {}-{} &  8x_1 & {}-{} & 9x_3 {}={} & 1. \\\\\n\\end{alignedat}\n\\quad \\Longrightarrow \\quad\n\\begin{pmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6 \\\\\n7 & 8 & 9 \\\\\n\\end{pmatrix}\n\\begin{pmatrix}\nx_0 \\\\\nx_1 \\\\\nx_2 \\\\\n\\end{pmatrix} =\n\\begin{pmatrix}\n1 \\\\\n1 \\\\\n1 \\\\\n\\end{pmatrix}.\n\\end{gather*}\\]\n\n\nCode\nA = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]); print(f'A = \\n{A}')\nb = np.array([[1], [1], [1]]); print(f'b = \\n{b}')\nx = np.linalg.solve(A, b); print(f'x = \\n{x}')\n\n\nA = \n[[1 2 3]\n [4 5 6]\n [7 8 9]]\nb = \n[[1]\n [1]\n [1]]\nx = \n[[ 0.2]\n [-1.4]\n [ 1.2]]\n\n\nHere’s the determinant and inverse of this matrix. Notice how close it is to being non-singular, since \\(\\det(\\mathbf{A}) \\approx -10^{-15}\\) is tiny. This small determinant causes the inverse to be huge, with terms on the order of \\(10^{15}\\). This is where you can start to see the loss of precision creeping in. If we calculate \\(\\mathbf{A}\\mathbf{A}^{-1}\\) we won’t get anything looking like the identity matrix. Yet, using np.solve worked just fine. Multiply \\(\\mathbf{A}\\mathbf{x}\\) and you’ll get exactly \\(\\mathbf{b}\\) back.\n\n\nCode\nprint(f'det(A) = {np.linalg.det(A)}')\nprint(f'A^(-1) = \\n{np.linalg.inv(A)}')\nprint(f'A A^(-1) = \\n{A @ np.linalg.inv(A)}')\n\n\ndet(A) = -9.51619735392994e-16\nA^(-1) = \n[[ 3.15251974e+15 -6.30503948e+15  3.15251974e+15]\n [-6.30503948e+15  1.26100790e+16 -6.30503948e+15]\n [ 3.15251974e+15 -6.30503948e+15  3.15251974e+15]]\nA A^(-1) = \n[[ 0.  1.  0.]\n [ 0.  2.  0.]\n [-4.  3.  2.]]\n\n\n\n\n5.4.2 Rectangular Systems\nEverything I just covered applies only to square linear systems, where there are exactly as many equations as there are unknowns. In real life, the systems of equations we care about solving are rarely square. For example, in machine learning we’re usually dealing with matrices of data, where the rows represent the number of samples and the columns represent the number of features in the data. It’ll almost never be the case that we have exactly the same number of samples as we have features.\nA rectangular system is an \\(m \\times n\\) linear system \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\) where \\(m \\neq n\\). That is, the number of equations is different from the number of unknowns. Evidently there are two distinct cases to consider here:\n\nMore equations than unknowns (\\(m > n\\)): These are called over-determined systems. In an over-determined system, we have too many equations. It’ll usually be impossible to solve them all exactly.\nMore unknowns than equations (\\(m < n\\)): These are called under-determined systems. In an under-determined system, we don’t have enough equations. There will always be infinitely many ways to solve these kinds of systems.\n\n\n5.4.2.1 Over-Determined Systems\nIn either case, \\(\\mathbf{A}\\) won’t have a two-sided inverse anymore, nor will it have a determinant. What do we do? Let’s again start small. Let’s first look at a simple over-determined system, a \\(3 \\times 2\\) system. Consider the following example.\n\\[\\begin{gather*}\n\\begin{alignedat}{3}\n   2x_0 & {}+{} &  x_1 {}={} & -1 \\\\\n   -3x_0 & {}+{} &  x_1 {}={} & -2 \\\\\n   -x_0 & {}-{} &  x_1 {}={} & 1. \\\\\n\\end{alignedat}\n\\quad \\Longrightarrow \\quad\n\\begin{pmatrix}\n2 & 1 \\\\\n-3 & 1 \\\\\n-1 & 1 \\\\\n\\end{pmatrix}\n\\begin{pmatrix}\nx_0 \\\\\nx_1 \\\\\n\\end{pmatrix} =\n\\begin{pmatrix}\n-1 \\\\\n-2 \\\\\n1 \\\\\n\\end{pmatrix}.\n\\end{gather*}\\]\nGraphically, this system corresponds to 3 lines in the plane. Let’s plot them and see what’s going on. The equations for the lines are,\n\\[\\begin{align*}\ny &= -1 - 2x, \\\\\ny &= -2 + 3x, \\\\\ny &= 1 + x. \\\\\n\\end{align*}\\]\n\n\nCode\nx = np.linspace(-10, 10, 100)\nf0 = lambda x: -1 - 2 * x\nf1 = lambda x: -2 + 3 * x\nf2 = lambda x: 1 + x\nplot_function(x, [f0, f1, f2], xlim=(-6, 6), ylim=(-6, 6), \n              title='3 Linear Equations, 2 Unknowns',\n              labels=[f'$y=-1-2x$', f'$y=-2-3x$', f'$y=1-x$'], \n              legend_fontsize=9.5, legend_loc='upper left')\n\n\n\n\n\n\n\n\n\nFrom the plot, we can see that the three lines don’t all intersect at the same point, which means there’s no single solution that satisfies this particular system. In fact, this is general. It’s very unlikely that more than two lines will intersect at the same point, or more than three planes will intersect at the same point, etc.\nSo what do we do? If we can’t find an exact solution, can we at least find an approximately good solution? Yes we can. Let’s look at the situation abstractly for a minute. Suppose \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\) describes the over-determined example given above. Then \\(\\mathbf{A}\\) is a \\(3 \\times 2\\) matrix. We can’t invert it, nor can we take its determinant. But we can find a way “squarify it” somehow. To do that, I’ll need to introduce the transpose operation.\nIf \\(\\mathbf{A}\\) is some \\(m \\times n\\) matrix, either square or rectangular, we can swap its rows and columns to get an \\(n \\times m\\) matrix that’s somehow related to \\(\\mathbf{A}\\). This swapped matrix is called the transpose of \\(\\mathbf{A}\\). It’s usually denoted by the symbol \\(\\mathbf{A}^\\top\\), read “A transpose”. Formally, it’s defined by\n\\[A_{i,j}^\\top = A_{j,i}.\\]\nIn the above example, we’d have\n\\[\n\\mathbf{A} =\n\\begin{pmatrix}\n2 & 1 \\\\\n-3 & 1 \\\\\n-1 & 1 \\\\\n\\end{pmatrix} \\quad \\Longrightarrow \\quad\n\\mathbf{A}^\\top =\n\\begin{pmatrix}\n2 & -3 & 1 \\\\\n1 & 1 & 1 \\\\\n\\end{pmatrix}.\n\\]\nAll I did was swap the rows and columns. That’s all the transpose operation is doing. Since \\(\\mathbf{A}\\) in this example is \\(3 \\times 2\\), \\(\\mathbf{A}^\\top\\) must be \\(2 \\times 3\\).\nThe transpose gives us an interesting and sensible way to “squarify” a matrix. Consider what happens when we left multiply an \\(m \\times n\\) matrix \\(\\mathbf{A}\\) by its transpose. Evidently the product \\(\\mathbf{A}^\\top \\mathbf{A}\\) would have to be an \\(n \\times n\\) matrix. That is, it’s square. In the above example, we’d get the \\(2 \\times 2\\) matrix\n\\[\n\\mathbf{A}^\\top \\mathbf{A} =\n\\begin{pmatrix}\n14 & -2 \\\\\n-2 & 3 \\\\\n\\end{pmatrix}.\n\\]\nHere’s what this looks like in numpy. We can get the transpose of a matrix A by using either the method A.T or the function np.transpose(A).\n\n\nCode\nA = np.array([\n    [2, 1], \n    [-3, 1], \n    [-1, 1]])\nAt = A.T\nAtA = At @ A\nprint(f'A = \\n{A}')\nprint(f'A.T = \\n{At}')\nprint(f'A.T A = \\n{AtA}')\n\n\nA = \n[[ 2  1]\n [-3  1]\n [-1  1]]\nA.T = \n[[ 2 -3 -1]\n [ 1  1  1]]\nA.T A = \n[[14 -2]\n [-2  3]]\n\n\nNow, let’s go back to the over-determined system \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\). If we left-multiply both sides by \\(\\mathbf{A}^\\top\\), we’d get\n\\[\\mathbf{A}^\\top \\mathbf{A}\\mathbf{x} = \\mathbf{A}^\\top \\mathbf{b}.\\]\nMost of the time, the square matrix \\(\\mathbf{A}^\\top \\mathbf{A}\\) will be invertible. Provided that’s the case, we can write\n\\[\\mathbf{x} \\approx (\\mathbf{A}^\\top \\mathbf{A})^{-1} \\mathbf{A}^\\top \\mathbf{b}.\\]\nI use approximately equals here because this won’t usually give the exact solution to \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\). But it does give in some sense the best approximate solution you can get. For reasons I won’t go into much right this moment, this kind of approximate solution is called the least squares solution to the linear system. It’s the solution that minimizes the weird looking term \\((\\mathbf{A}\\mathbf{x} - \\mathbf{b})^\\top (\\mathbf{A}\\mathbf{x} - \\mathbf{b})\\), whatever that means.\nIn numpy, we can’t use np.linalg.solve(A, b) when a linear system isn’t square. If we want to find the least squares solution, we’ll need to use the function np.linalg.lstsq(A, b) instead. This function actually returns a lot more stuff than just the x we seek. For now I’ll ignore those other objects and show you what the least squares solution to the above \\(3 \\times 2\\) system looks like. Evidently, it’s\n\\[\n\\mathbf{x} \\approx\n\\begin{pmatrix}\n0.136 \\\\\n-0.578 \\\\\n\\end{pmatrix}.\n\\]\nIf you go back to the previous plot, you’ll see this point seems to lie close to the point where the blue and orange lines intersect. That’s interesting.\n\n\nCode\nb = np.array([[-1], [-2], [1]])\nx, _, _, _ = np.linalg.lstsq(A, b)\nprint(f'x ≈ \\n{x}')\n\n\nx ≈ \n[[ 0.13157895]\n [-0.57894737]]\n\n\nLet’s look again at the least squares solution \\(\\mathbf{x} \\approx (\\mathbf{A}^\\top \\mathbf{A})^{-1} \\mathbf{A}^\\top \\mathbf{b}\\). Notice that the matrix \\((\\mathbf{A}^\\top \\mathbf{A})^{-1} \\mathbf{A}^\\top\\) seems to function kind of like \\(\\mathbf{A}^{-1}\\), if it existed. For this reason, it’s called the pseudoinverse of \\(\\mathbf{A}\\), usually denoted by the special symbol \\(\\mathbf{A}^+\\). The pseudoinverse is in some sense the closest we can get to inverting the matrix of an over-determined system. Evidently, it satisfies the property that it’s a left inverse of \\(\\mathbf{A}\\),\n\\[\\mathbf{A}^+ \\mathbf{A} = \\mathbf{I}.\\]\n\n\n5.4.2.2 Under-Determined Systems\nI’ll come back to this more later. Let’s briefly take a look at the other type of rectangular system, the over-determined system where \\(m < n\\). In this case there are too many unknowns and not enough equations. As an example, consider the following \\(2 \\times 3\\) linear system,\n\\[\\begin{gather*}\n\\begin{alignedat}{3}\n   x_0 & {}+{} &  x_1 & {}+{} & x_2 {}={} & 2  \\\\\n   x_0 & {}-{} &  x_1 & {}+{} & x_2 {}={} & 0 \\\\\n\\end{alignedat}\n\\quad \\Longrightarrow \\quad\n\\begin{pmatrix}\n1 & 1 & 1 \\\\\n1 & -1 & 1 \\\\\n\\end{pmatrix}\n\\begin{pmatrix}\nx_0 \\\\\nx_1 \\\\\nx_2 \\\\\n\\end{pmatrix} =\n\\begin{pmatrix}\n2 \\\\\n0 \\\\\n\\end{pmatrix}.\n\\end{gather*}\\]\nGraphically, this system will look like two planes in 3D space,\n\\[\\begin{align*}\nz &= 2 - x - y, \\\\\nz &= -x + y. \\\\\n\\end{align*}\\]\nSince there are two planes, they’ll intersect not at a point, but at a line. Any point on this line will be a solution to the linear system.\n\n\nCode\nx = np.linspace(-3, 3, 100)\ny = np.linspace(-3, 3, 100)\nt = np.linspace(-1.9, 3.9, 100)\nf1 = lambda x, y: 2 - x - y\nf2 = lambda x, y: y - x\nplot_function_3d(x, y, [f1, f2], azim=30, elev=20, ticks_every=[1, 1, 2], figsize=(5, 5), zorders=[0, 1], dist=12,\n        colors=['steelblue', 'limegreen'], alpha=0.6, titlepad=-5, labelpad=2, title='2 Equations, 3 Unknowns',\n        lines=[[1 - t, np.full(len(t), 1), t]])\n\n\n\n\n\n\n\n\n\nTo see why this system has infinitely many solutions, let’s try to solve it. It’s easy enough using substitution. The second equation says \\(x_1 = x_0 + x_2\\). Plugging this into the first equation then says \\(x_2 = 1 - x_0\\). There’s no way to solve for \\(x_0\\) because we don’t have enough equations. We thus have to conclude that the solutions to this system look like\n\\[x_0 = x_0, \\quad x_1 = 1, \\quad x_2 = 1 - x_0.\\]\nAny choice of \\(x_0\\) will satisfy this linear system, which means it’ll have infinitely many solutions, which are of course just the points on the line above.\nIn general, we can almost exactly the same trick to “solve” these linear systems as we did with the over-determined systems. This time, instead of multiplying \\(\\mathbf{A}\\) on the left by \\(\\mathbf{A}^\\top\\), we’ll instead multiply on the right by \\(\\mathbf{A}^\\top\\),\n\\[\\mathbf{A}\\mathbf{A}^\\top \\mathbf{x} = \\mathbf{A}^\\top \\mathbf{b}.\\]\nProvided we can invert \\(\\mathbf{A}\\mathbf{A}^\\top\\), and usually we can, we’ll get a solution of the form\n\\[\\mathbf{x} = (\\mathbf{A}\\mathbf{A}^\\top)^{-1} \\mathbf{A}^\\top \\mathbf{b}.\\]\nNote that this gives only one of the infinitely many possible solutions to an under-determined linear system. For reasons I won’t go into now, it turns out the solution it gives is called the least norm solution. In a sense, this means it gives you the “smallest” vector \\(\\mathbf{x}\\) that satisfies \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\). By smallest, I mean it’s the vector such that the \\(1 \\times 1\\) matrix \\(\\mathbf{x}^\\top \\mathbf{x}\\) is minimized.\nIt turns out the matrix \\((\\mathbf{A}\\mathbf{A}^\\top)^{-1} \\mathbf{A}^\\top\\) on the right is also a pseudoinverse. It satisfies the property that it’s a right inverse of \\(\\mathbf{A}\\), in the sense that\n\\[\\mathbf{A} \\mathbf{A}^+ = \\mathbf{I}.\\]\nIn fact, there are many different kinds of pseudoinverses. The two I covered here are the most practical ones.\nIn numpy, you can solve an under-determined system by using the same np.linalg.lstsq(A, b) function from before. It’s able to tell which case you want by looking at the shape of the A you pass in. Here’s the least norm solution for the above example,\n\\[\n\\mathbf{x} =\n\\begin{pmatrix}\n\\frac{1}{2} \\\\\n1 \\\\\n\\frac{1}{2} \\\\\n\\end{pmatrix}.\n\\]\nYou can check it satisfies the linear system with the choice of \\(x_0 = \\frac{1}{2}\\).\n\n\nCode\nA = np.array([\n    [1, 1, 1], \n    [1, -1, 1]])\nb = np.array([[2], [0]])\nx, _, _, _ = np.linalg.lstsq(A, b)\nprint(f'x ≈ \\n{x}')\n\n\nx ≈ \n[[0.5]\n [1. ]\n [0.5]]\n\n\nI’ll come back to this stuff more in later lessons and fill in some of these missing pieces. I just want to close by mentioning that I’ve essentially just derived much of the ideas behind linear regression in this section. In fact, training a linear regression model is completely equivalent to finding either a least squares solution or a least norm solution to \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\). In that case, \\(\\mathbf{A}\\) represents the matrix of data, \\(\\mathbf{x}\\) represents the parameters the model needs to learn, and \\(\\mathbf{b}\\) represents the target values. Using what I’ve covered in this lesson, you could completely solve any linear regression problem you wanted from scratch just by using something like x = np.linalg.lstsq(A, b)."
  },
  {
    "objectID": "notebooks/vectors.html#visualizing-vectors",
    "href": "notebooks/vectors.html#visualizing-vectors",
    "title": "6  Vector Spaces",
    "section": "6.1 Visualizing Vectors",
    "text": "6.1 Visualizing Vectors\nLet’s go back to the simple 2-dimensional case. Imagine you have a point in the xy-plane, call it \\((x,y)\\). Now, we can think of this as a single point, but we can also imagine it differently. Suppose there was an arrow pointing from the origin \\((0,0)\\) to the point \\((x,y)\\). For example, if the point was \\((1,1)\\), this arrow might look like this.\n\n\nCode\npoint = np.array([1, 1])\nplot_vectors(point, title=f'Arrow From $(0,0)$ To $(1,1)$', ticks_every=0.5)\n\n\n\n\n\n\n\n\n\nUnlike the point \\((x,y)\\), the arrow \\((x,y)\\) has both a length and a direction. Its length is given by the Pythagorean Theorem. If the triangle has base \\(x\\) and height \\(y\\), then the length of the arrow is just its hypotenuse, i.e. \\(r = \\sqrt{x^2 + y^2}\\). The direction of the arrow is its angle \\(\\theta\\) with respect to the x-axis. This angle is just given by the inverse tangent of height over base, i.e. \\(\\theta = \\tan^{-1}\\big(\\frac{y}{x}\\big)\\).\nIn the example plotted, the length is \\(r=\\sqrt{1+1}=\\sqrt{2}\\), and the angle is \\(\\theta = \\tan^{-1}(1) = 45^\\circ\\). These two values uniquely specify the arrow, assuming it starts at the origin. If we know the length and direction, we know exactly which arrow we’re talking about.\nWhat I’ve just shown is another way to define a vector. A vector is an arrow in the plane. Said differently, a vector is just a point that’s also been endowed with a length (or magnitude) and a direction. The x and y values are called components of a vector. Usually we’ll write a vector in bold-face and its components in regular type but with subscripts indicating which component. For example, \\(\\mathbf{v}=(v_x,v_y)\\). Here’s the same arrow I plotted above, but explicitly labeled as a vector \\(\\mathbf{v}=(1,1)\\). Its components are \\(v_x=1\\) and \\(v_y=1\\).\n\n\nCode\nv = np.array([1, 1])\nplot_vectors(v, title='$\\mathbf{v}=(1,1)$', labels=['$\\mathbf{v}$'], ticks_every=0.5)\n\n\n\n\n\n\n\n\n\nNotation: It’s common to represent vectors in a few different ways depending on the situation. One way to represent a vector is as a column vector. This is what I did when doing matrix-vector multiplication. Another way, what I just introduced, is a flat vector, or a 1-dimensional array. This is more common when thinking about a vector geometrically. Yet another way is to think of a vector as a row vector, which is the transpose of a column vector. All of these representations conceptually represent the same object, but their shapes are different. Here’s an example: The size-2 vector \\(\\mathbf{v}=(1,1)\\) can be written in 3 different but all equivalent ways:\n\\[\\begin{align*}\n&\\text{Flat vector of shape } (2,): \\mathbf{v} = (1,1), \\\\\n&\\text{Column vector of shape } (2,1): \\mathbf{v} = \\begin{pmatrix}\n1 \\\\\n1 \\\\\n\\end{pmatrix}, \\\\\n&\\text{Row vector of shape } (1,2): \\mathbf{v}^\\top = \\begin{pmatrix}\n1 & 1 \\\\\n\\end{pmatrix}.\n\\end{align*}\\]\nBe careful when working with vectors in code to make sure you’re using the right shapes for the right situation or you’ll get shape mismatch errors (or worse a silent bug)."
  },
  {
    "objectID": "notebooks/vectors.html#vector-operations",
    "href": "notebooks/vectors.html#vector-operations",
    "title": "6  Vector Spaces",
    "section": "6.2 Vector Operations",
    "text": "6.2 Vector Operations\nThe magnitude, or length, of \\(\\mathbf{v}\\) is typically denoted by the symbol \\(||\\mathbf{v}||\\), called a norm,\n\\[||\\mathbf{v}|| = \\sqrt{v_x^2 + v_y^2}.\\]\nIn the above example with \\(\\mathbf{v}=(1,1)\\), its norm is \\(||\\mathbf{v}||=\\sqrt{1+1}=\\sqrt{2} \\approx 1.414\\).\nNotice that the norm must be non-negative since it’s the square root of a sum of squares, i.e. \\(||\\mathbf{v}|| \\geq 0\\). This should sound right, after all negative lengths don’t make any sense.\nWhat happens if we scale a vector \\(\\mathbf{v}\\) by some scalar \\(c\\)? By the rules of scalar-vector multiplication, the new vector should be \\(c\\mathbf{v}=(cx,cy)\\). Since the new vector has length \\(||c\\mathbf{v}||\\), a little math shows that\n\\[||c\\mathbf{v}|| = \\sqrt{(cv_x)^2 + (cv_y)^2} = \\sqrt{c^2(v_x^2 + v_y^2)} = |c| \\sqrt{v_x^2 + v_y^2} = |c| \\cdot ||\\mathbf{v}||.\\]\nThat is, the re-scaled vector \\(c\\mathbf{v}\\) just gets its length re-scaled by \\(c\\). That’s why \\(c\\) is called a scalar. It rescales vectors. Notice if \\(c\\) is negative, the length stays the same, but the direction gets reversed \\(180^\\circ\\) since in that case \\(c\\mathbf{v} = c(v_x, v_y) = -|c|(v_x,v_y)\\).\nHere’s what vector scaling looks like geometrically. I’ll plot the vector \\(\\mathbf{v}=(1,1)\\) again, but scaled by two numbers, one \\(c=2\\), the other \\(c=-1\\). When \\(c=2\\), the vector just doubles its length. That’s the light blue arrow. When \\(c=-1\\), the vector reverses its direction \\(180^\\circ\\), but maintains its length since \\(|c|=1\\). That’s the light orange arrow.\n\n\nCode\nv = np.array([1, 1])\nplot_vectors([v, -v, 2*v], xlim=(-2,3), ylim=(-2,3), title=f'Scaling Vectors', headwidth=7, ticks_every=1,\n             labels=['$\\mathbf{v}$', '$-\\mathbf{v}$', '$2\\mathbf{v}$'], \n             colors=['black', 'salmon', 'steelblue'],\n             text_offsets=[[-0.2, 0.2], [-0.2, 0.4], [-0.2, 0.2]])\n\n\n\n\n\n\n\n\n\nWhat does adding two vectors do? Let \\(\\mathbf{v}=(v_x,v_y)\\) and \\(\\mathbf{w}=(w_x,w_y)\\) be two vectors in the plane. Then their sum is \\(\\mathbf{v}+\\mathbf{w} = (v_x+w_x,v_y+w_y)\\). I’ll plot an example below with \\(\\mathbf{v}=(1,1)\\) and \\(\\mathbf{w}=(1,3)\\). Their sum should be\n\\[\\mathbf{v}+\\mathbf{w}=(1+1,1+3)=(2,4).\\]\n\n\nCode\nv = np.array([1, 1])\nw = np.array([1, 3])\nplot_vectors([v, w, v + w], xlim=(0, 3), ylim=(0, 5), title=f'Adding Two Vectors', ticks_every=1,\n             labels=['$\\mathbf{v}$', '$\\mathbf{w}$', '$\\mathbf{v}+\\mathbf{w}$'], \n             colors=['salmon', 'steelblue', 'black'])\n\n\n\n\n\n\n\n\n\nIt may not be obvious yet what vector addition is doing geometrically. Let me plot it slightly differently. What I’ll do is plot the vectors “head to tail” by taking the tail of \\(\\mathbf{w}\\) and placing it at the head of \\(\\mathbf{v}\\). Then the head of this translated \\(\\mathbf{w}\\) vector points at the head of the sum \\(\\mathbf{v}+\\mathbf{w}\\). We can do this “head to tail” stuff since the base of a vector is irrelevant. We can place the arrow wherever we want as long as we maintain its length and direction.\nInformally speaking, to add two vectors, just stack them on top of each other head to tail, and draw an arrow from the starting point to the ending point. You can geometrically add arbitrarily many vectors this way, not just two. Just keep stacking them.\n\n\nCode\nplot_vectors([v, w, v + w], xlim=(0, 3), ylim=(0, 5), title=f'Adding Two Vectors (Head to Tail)',\n             colors=['salmon', 'steelblue', 'black'],\n             tails=[[0, 0], [v[0], v[1]], [0, 0]], text_offsets=[[-0.5, -0.85], [0.5, -0.8], [-1.4, -1.6]],\n             labels=['$\\mathbf{v}$', '$\\mathbf{w}$', '$\\mathbf{v}+\\mathbf{w}$'],\n             zorders = [0, 1, 2], ticks_every=1)\n\n\n\n\n\n\n\n\n\nThe norm satisfies what’s known as the triangle inequality: If \\(\\mathbf{v}\\) and \\(\\mathbf{w}\\) are two vectors, then the length of their sum is less than the sum of their individual lengths, i.e.\n\\[||\\mathbf{v}+\\mathbf{w}|| \\leq ||\\mathbf{v}|| + ||\\mathbf{w}||.\\]\nYou can see this by staring at the plot above. The added lengths of \\(\\mathbf{v}\\) and \\(\\mathbf{w}\\) is larger than the length of their sum \\(\\mathbf{v}+\\mathbf{w}\\). In fact, the only time the lengths will be equal is if \\(\\mathbf{v}\\) and \\(\\mathbf{w}\\) are parallel to each other.\nWhat about subtracting two vectors? By combining the rules for scalar multiplication and vector addition, you can convince yourself that the difference of two vectors is also element-wise,\n\\[\\mathbf{v}-\\mathbf{w} = (v_x-w_x,v_y-w_y).\\]\nTo visualize what subtracting two vectors looks like, notice we can write subtraction as a sum like this, \\(\\mathbf{w} + (\\mathbf{v}-\\mathbf{w}) = \\mathbf{v}\\). Now use the same trick for adding vectors, only this time placing \\((\\mathbf{v}-\\mathbf{w})\\) at the head of \\(\\mathbf{w}\\), and noticing that it points to the sum of the two, which is \\(\\mathbf{v}\\).\nAn easy way to remember what subtracting two vectors looks like is to connect the two vectors you’re subtracting with a line segment, and place the head on the first vector. This trick will never fail you.\n\n\nCode\nv = np.array([1, 1])\nw = np.array([1, 3])\nplot_vectors([v, w, v - w], xlim=(-0.5, 1.5), ylim=(-0.5, 3.5), title=f'Subtracting Two Vectors', headwidth=4,\n             ticks_every=1, colors=['salmon', 'steelblue', 'black'],\n             tails=[[0, 0], [0, 0], [w[0], w[1]]], text_offsets=[[-0.5, -0.8], [-0.5, -1], [1.05, 3.8]],\n             labels=['$\\mathbf{v}$', '$\\mathbf{w}$', '$\\mathbf{v}-\\mathbf{w}$'])"
  },
  {
    "objectID": "notebooks/vectors.html#the-dot-product",
    "href": "notebooks/vectors.html#the-dot-product",
    "title": "6  Vector Spaces",
    "section": "6.3 The Dot Product",
    "text": "6.3 The Dot Product\nIt turns out we can understand both the lengths and angles of vectors in terms of a single operation called the dot product, also called the inner or scalar product. The dot product is a kind of multiplication between two vectors that returns a scalar. If \\(\\mathbf{v}=(v_x,v_y)\\) and \\(\\mathbf{w}=(w_x,w_y)\\) are two vectors in the plane, their dot product is defined as\n\\[\\mathbf{v} \\cdot \\mathbf{w} = v_x w_x + v_y w_y.\\]\nThat is, the dot product is just the sum of the element-wise products of the two vectors.\nIn terms of vectorized numpy code, the dot product is just the operation np.sum(v * w). Numpy also has a convenience function np.dot(v, w) that calculates it directly. Here’s the calculation of the dot product between the two vectors \\(\\mathbf{v}=(5,-1)\\) and \\(\\mathbf{w}=(2,4)\\). The answer should be\n\\[\\mathbf{v} \\cdot \\mathbf{w} = 5 \\cdot 2 + (-1) \\cdot 4 = 10 - 4 = 6.\\]\n\n\nCode\nv = np.array([5, -1])\nw = np.array([2, 4])\nprint(f'v . w = {np.dot(v, w)}')\nnp.sum(v * w) == np.dot(v, w)\n\n\nv . w = 6\n\n\nTrue\n\n\nAlgorithm Analysis: Evaluating the dot product uses \\(2n-1\\) or \\(O(n)\\) total FLOPS, since for a vector of size \\(n\\) there are \\(n\\) multiplications and \\(n-1\\) additions.\nHere are some fairly trivial properties the dot product satisfies. These follow straight from the definition.\n\nThe dot product of a vector with itself is nonnegative: \\(\\mathbf{v} \\cdot \\mathbf{v} \\geq 0\\).\nIt commutes: \\(\\mathbf{v} \\cdot \\mathbf{w} = \\mathbf{w} \\cdot \\mathbf{v}\\).\nIt distributes over scalar multiplication: \\(c\\mathbf{v} \\cdot \\mathbf{w} = \\mathbf{v} \\cdot c\\mathbf{w} = c(\\mathbf{v} \\cdot \\mathbf{w})\\).\nIt distributes over vector addition: \\((\\mathbf{u} + \\mathbf{v}) \\cdot \\mathbf{w} = \\mathbf{u} \\cdot \\mathbf{w} + \\mathbf{v} \\cdot \\mathbf{w}\\) and \\(\\mathbf{v} \\cdot (\\mathbf{u}+\\mathbf{w}) = \\mathbf{v} \\cdot \\mathbf{u} + \\mathbf{v} \\cdot \\mathbf{w}\\).\n\nNotation: The dot product is often written in several different ways in different fields. Another notation arises by thinking of the dot product as the matrix multiplication of a row vector \\(\\mathbf{v}^\\top = \\begin{pmatrix}v_x & v_y \\end{pmatrix}\\) with a column vector \\(\\mathbf{w} = \\begin{pmatrix} w_x \\\\ w_y \\end{pmatrix}\\). In that case,\n\\[\n\\mathbf{v}^\\top \\mathbf{w} =\n\\begin{pmatrix} v_x & v_y \\end{pmatrix}\n\\begin{pmatrix} w_x \\\\ w_y \\end{pmatrix}\n= v_x w_x + v_y w_y = \\mathbf{v} \\cdot \\mathbf{w}.\n\\]\nThis is the most commonly used notation for the dot product in machine learning. I’ll use it more frequently after this lesson.\nWe can write the norm or length of a vector in terms of the dot product. Observe that by dotting \\(\\mathbf{v}\\) with itself, I get\n\\[\\mathbf{v} \\cdot \\mathbf{v} = v_x^2 + v_y^2 = ||\\mathbf{v}||^2.\\]\nTaking the square root of both sides, you can see that the norm or length of a vector is just the square root of its dot product with itself,\n\\[||\\mathbf{v}|| = \\sqrt{\\mathbf{v} \\cdot \\mathbf{v}}.\\]\nWe can also talk about the distance between any two vectors \\(\\mathbf{v}\\) and \\(\\mathbf{w}\\). Denote the distance between these two vectors as \\(d(\\mathbf{v}, \\mathbf{w})\\). Since the difference vector is \\(\\mathbf{v} - \\mathbf{w}\\), the distance between the two vectors is evidently just the length of the difference vector,\n\\[d(\\mathbf{v}, \\mathbf{w}) = ||\\mathbf{v} - \\mathbf{w}|| = \\sqrt{(\\mathbf{v} - \\mathbf{w}) \\cdot (\\mathbf{v} - \\mathbf{w})} = \\sqrt{(v_x-w_x)^2 - (v_y-w_y)^2}.\\]\nFor example, the distance between the two vectors \\(\\mathbf{v}=(1,1)\\) and \\(\\mathbf{w}=(1, 0)\\) is\n\\[d(\\mathbf{v}, \\mathbf{w}) = ||\\mathbf{v} - \\mathbf{w}|| = \\sqrt{(1-1)^2 + (1-0)^2} = 1.\\]\nIf a vector \\(\\mathbf{e}\\) has norm \\(||\\mathbf{e}||=1\\) it’s called a unit vector. We can convert any non-zero vector \\(\\mathbf{v}\\) into a unit vector by dividing by its norm, which is called normalizing \\(\\mathbf{v}\\). The unit vector gotten from normalizing \\(\\mathbf{v}\\) I’ll call \\(\\mathbf{e_v}\\). It’s given by\n\\[\\mathbf{e_v} = \\frac{\\mathbf{v}}{||\\mathbf{v}||}.\\]\nFor example, if \\(\\mathbf{v}=(1,1)\\), its norm is \\(||\\mathbf{v}||=\\sqrt{2}\\), so if we wanted to normalize it into a new unit vector \\(\\mathbf{e_v}\\), we’d have\n\\[\\mathbf{e_v} = \\frac{\\mathbf{v}}{||\\mathbf{v}||} = \\frac{\\mathbf{v}}{\\sqrt{2}} = \\frac{1}{\\sqrt{2}}(1,1) \\approx (0.707, 0.707).\\]\nUnit vectors will always point in the same direction as the vector used to normalize them. The only difference is they’ll have length one. In the plane, unit vectors will always lie along the unit circle. Here’s a plot of this idea using the previous example.\n\n\nCode\nv = np.array([1, 2])\nev = v / np.sqrt(2)\nplot_vectors([v, ev], title='$\\mathbf{e}_v = ||\\mathbf{v}||^{-1} \\mathbf{v}$', ticks_every=0.5, zorders=[0, 1],\n             text_offsets=[[0.01, 0.05], [-0.2, 0.2]], colors=['steelblue', 'red'],\n             labels=['$\\mathbf{v}$', '$\\mathbf{e}_v$'], headwidth=6)"
  },
  {
    "objectID": "notebooks/vectors.html#projections",
    "href": "notebooks/vectors.html#projections",
    "title": "6  Vector Spaces",
    "section": "6.4 Projections",
    "text": "6.4 Projections\nLet \\(\\mathbf{e}_x=(1,0)\\). It’s the unit vector pointing along the positive x-axis. Notice the dot product between \\(\\mathbf{v}=(v_x, v_y)\\) and \\(\\mathbf{e}_x\\) is just\n\\[\\mathbf{v} \\cdot \\mathbf{e}_x = v_x \\cdot 1 + v_y \\cdot 0 = v_x.\\]\nEvidently the dot product \\(\\mathbf{v} \\cdot \\mathbf{e}_x\\) “picks” out the x-component of \\(\\mathbf{v}\\), namely \\(v_x\\). The vector \\(v_x \\mathbf{e}_x = (v_x,0)\\) gotten by rescaling \\(\\mathbf{e}_x\\) by \\(v_x\\) is called the projection of \\(\\mathbf{v}\\) onto the x-axis. It’s the vector you’d get by dropping \\(\\mathbf{v}\\) perpendicular to the x-axis.\nSimilarly, if \\(\\mathbf{e}_y = (0,1)\\) is the unit vector along the positive y-axis, we can “pick out” the y-component of \\(\\mathbf{v}\\) by taking the dot product of \\(\\mathbf{v}\\) with \\(\\mathbf{e}_y\\), i.e. \\(v_y = \\mathbf{v} \\cdot \\mathbf{e}_y\\). The vector \\(v_y\\mathbf{e}_y\\) is the projection of \\(\\mathbf{v}\\) onto the y-axis.\nEvidently, then, \\(\\mathbf{v}\\) is just the sum of projections of \\(\\mathbf{v}\\) onto all of the axes,\n\\[\\mathbf{v} = v_x \\mathbf{e_x} + v_y \\mathbf{e_y}.\\]\nThis is yet another way to express a vector in terms of its components. Just project down onto the axes and sum up the linear combination.\nHere’s what this looks like when \\(\\mathbf{v}=(0.5,1)\\). In this example, the projection onto the x-axis is just \\(v_x \\mathbf{e}_x=(0.5, 0)\\), and the projection onto the y-axis is just \\(v_y \\mathbf{e_y}=(0,1)\\). Using these projections, we can write \\(\\mathbf{v}=(0.5,1)\\) as \\(\\mathbf{v} = 0.5 \\mathbf{e}_x + \\mathbf{e}_y\\).\n\n\nCode\nv = np.array([1, 2])\nex = np.array([1, 0])\ney = np.array([0, 1])\nplot_vectors([v, v[0] * ex, v[1] * ey], title='Projections Of $\\mathbf{v}$', ticks_every=0.5,\n             text_offsets=[[0.02, 0.1], [-0.1, 0.2], [0.05, 0.05]], colors=['red', 'steelblue', 'steelblue'],\n             labels=['$\\mathbf{v}$', '$v_x \\mathbf{e}_x$', '$v_y \\mathbf{e}_y$'], headwidth=4,\n             xlim=(-0.5, 2.5), ylim=(-0.5, 2.5))"
  },
  {
    "objectID": "notebooks/vectors.html#linear-independence",
    "href": "notebooks/vectors.html#linear-independence",
    "title": "6  Vector Spaces",
    "section": "6.5 Linear Independence",
    "text": "6.5 Linear Independence\nI just showed we can decompose any vector \\(\\mathbf{v} \\in \\mathbb{R}^2\\) into its projections \\(\\mathbf{v} = v_x \\mathbf{e}_x + v_y \\mathbf{e}_y\\). The fact we can do this is because the unit vectors \\(\\mathbf{e}_x\\) and \\(\\mathbf{e}_y\\) are special, for a few reasons.\nThe first reason these vectors are special is that they don’t lie along the same line in the plane. Said differently, we can’t write one vector as a scalar multiple of the other, \\(\\mathbf{e}_x \\neq c \\mathbf{e}_y\\) for any scalar \\(c\\). Vectors with this property are called linearly independent.\nMore generally, a set of \\(k\\) vectors \\(\\mathbf{v}_0, \\mathbf{v}_1, \\cdots, \\mathbf{v}_{k-1}\\) is called linearly independent if no one vector \\(\\mathbf{v}_j\\) in the set can be written as a linear combination of the rest. That is, for any choice of scalars \\(c_0, c_1, \\cdots, c_{k-1}\\),\n\\[\\mathbf{v}_j \\neq \\sum_{i \\neq j} c_i \\mathbf{v}_i.\\]\nA set of vectors that isn’t linearly independent is called linearly dependent. In a linearly dependent set, you can always express at least one vector as a linear combination of the rest, for example by finding a choice of scalars \\(c_0, c_1, \\cdots, c_{k-1}\\), you could write \\(\\mathbf{v}_0\\) as\n\\[\\mathbf{v}_0 = \\sum_{i=1}^{k-1} c_i \\mathbf{v}_i = c_1 \\mathbf{v}_1 + c_2 \\mathbf{v}_2 + \\cdots + c_{k-1} \\mathbf{v}_{k-1}.\\]\nLinearly dependent sets of vectors are redundant in a sense. We have more than we need. We can always keep dropping vectors from the set until the ones remaining are linearly independent.\nThe vector space spanned by all linear combinations of a set of vectors is called the span of that set. The span of a single vector will always be a line, since a linear combination of any one vector is just the scalar multiples of that vector. The span of any two linearly independent vectors will always be a plane. The span of \\(k\\) linearly independent vectors will form a \\(k\\)-dimensional hyperplane.\nAs a simple example, consider the following set of vectors in the plane,\n\\[\\begin{align*}\n\\mathbf{v}_0 &= (1, 0), \\\\\n\\mathbf{v}_1 &= (0, 1), \\\\\n\\mathbf{v}_2 &= (1, 1).\n\\end{align*}\\]\nIf you stare at these for a second, you’ll see that \\(\\mathbf{v}_2 = \\mathbf{v}_0 + \\mathbf{v}_1\\), so this set can’t be linearly independent. The third vector is redundant. Any two vectors in this set span the exact same plane \\(\\mathbb{R}^2\\). In fact, you’ll never have more than 2 linearly independent vectors of size 2. Why?\n\n\nCode\nv0 = np.array([1, 0])\nv1 = np.array([0, 1])\nv2 = np.array([1, 1])\nplot_vectors(\n    [v2, v0, v1], colors=['salmon', 'steelblue', 'limegreen'], xlim=(-0.5, 1.5), ylim=(-0.5, 1.5),\n    ticks_every=0.5, zorders=[0, 1, 2, 3], headwidth=5, text_offsets=[[0.03, 0.05], [0.03,0.05], [0.03,0.05]],\n    title='$\\mathbf{v}_0$, $\\mathbf{v}_1$, $\\mathbf{v}_2=\\mathbf{v}_0+\\mathbf{v}_1$', \n    labels=['$\\mathbf{v}_2$', '$\\mathbf{v}_0$', '$\\mathbf{v}_1$'])\n\n\n\n\n\n\n\n\n\nFor vectors in \\(\\mathbb{R}^2\\), there are only two possibilities, they either lie on the same line, or they span the whole plane. This follows from the fact that any vector \\(\\mathbf{v}\\) can be decomposed as \\(\\mathbf{v} = v_x \\mathbf{e}_x + v_y \\mathbf{e}_y\\). An implication of this fact is that a set of vectors in \\(\\mathbb{R}^2\\) can only be linearly independent if it contains only one or two vectors. If it contains a third vector, that vector must be a linear combination of the other two. The maximum number of linearly independent vectors in a set is the dimension of the vector space. Since \\(\\mathbb{R}^2\\) is 2-dimensional, it can only sustain 2 linearly independent vectors at a time."
  },
  {
    "objectID": "notebooks/vectors.html#basis-vectors",
    "href": "notebooks/vectors.html#basis-vectors",
    "title": "6  Vector Spaces",
    "section": "6.6 Basis Vectors",
    "text": "6.6 Basis Vectors\nIn \\(\\mathbb{R}^2\\), if we can find any two vectors \\(\\mathbf{a}\\) and \\(\\mathbf{b}\\) that are linearly independent, then we can write any other vector \\(\\mathbf{v}\\) as a linear combination of those two vectors,\n\\[\\mathbf{v} = v_a \\mathbf{a} + v_b \\mathbf{b}.\\]\nThe set \\(\\{\\mathbf{a}, \\mathbf{b}\\}\\) is called a basis. We can use vectors in this set as a “basis” to write any other vector.\nMore generally, a set of \\(k\\) vectors \\(\\mathbf{v}_0, \\mathbf{v}_1, \\cdots, \\mathbf{v}_{k-1}\\) form a basis for a vector space if the following two conditions hold,\n\nThe vectors are all linearly independent,\nThe vectors span the full vector space.\n\nAnother way of saying the same thing is that a basis is a set of exactly \\(n\\) linearly independent vectors, where \\(n\\) is the dimension of the vector space. A basis contains the minimal number of vectors needed to span the vector space.\nThe special vectors \\(\\mathbf{e}_x\\) and \\(\\mathbf{e}_y\\) form a basis for \\(\\mathbb{R}^2\\), since we can write any other vector as a linear combination of those two. Not only are these two vectors a basis, however. They satisfy two other useful properties,\n\nThey’re both unit vectors, \\(||\\mathbf{e}_x|| = ||\\mathbf{e}_y|| = 1\\).\nThey’re orthogonal to each other, that is, \\(\\mathbf{e}_x \\cdot \\mathbf{e}_y = 0\\).\n\nA basis satisfying these two properties is called an orthonormal basis. An orthonormal basis is special in that it allows us to pick out the components of a vector directly by just taking dot products with the basis vectors. It’s only true in an orthonormal basis that we can write the components of a vector \\(\\mathbf{v}\\) as,\n\\[\\begin{align*}\nv_x &= \\mathbf{v} \\cdot \\mathbf{e}_x, \\\\\nv_y &= \\mathbf{v} \\cdot \\mathbf{e}_y.\n\\end{align*}\\]\nThe set \\(\\{\\mathbf{e}_x, \\mathbf{e}_y\\}\\) is only one example of an orthonormal basis for \\(\\mathbb{R}^2\\). It’s called the standard basis, since it’s the basis whose vectors point along the usual positive x and y axes.\nExpressing any vector in terms of its basis is just projecting the vector down onto each of the basis axes. Let’s do a quick example. Let \\(\\mathbf{v}=(1.25,2)\\) be a vector. Decomposed into the standard basis we just get\n\\[\\mathbf{v} = 1.25 \\mathbf{e}_x + 2 \\mathbf{e}_y.\\]\nGraphically this just looks as follows. We’ve already seen a plot like this, except this time I’m including the basis vectors \\(\\mathbf{e}_x\\) and \\(\\mathbf{e}_y\\) explicitly. Notice that the two basis vectors form a \\(90^\\circ\\) angle, i.e. they’re perpendicular. I’ll show in a moment that this is implied by the fact that \\(\\mathbf{e}_x \\cdot \\mathbf{e}_y = 0\\).\n\n\nCode\nv = np.array([1.25, 2])\nex = np.array([1, 0])\ney = np.array([0, 1])\nplot_vectors(\n    [v, v[0] * ex, v[1] * ey, ex, ey], colors=['red', 'steelblue', 'steelblue', 'black', 'black'], \n    ticks_every=1, zorders=[0, 1, 2, 3, 4, 5], headwidth=5,\n    text_offsets=[[0,0], [0,0.2], [0.05,0], [-0.2,0.2], [0.05,0]],\n    title='$\\mathbf{v}=v_x \\mathbf{e}_x + v_y \\mathbf{e}_y$', \n    labels=['$\\mathbf{v}$', '$v_x \\mathbf{e}_x$', '$v_y \\mathbf{e}_y$', '$\\mathbf{e}_x$', '$\\mathbf{e}_y$'])\n\n\n\n\n\n\n\n\n\nOf course, I already said the standard basis isn’t the only orthonormal basis for \\(\\mathbb{R}^2\\) we could choose. Here’s an example of another one that would work equally well. Let \\(\\mathbf{e}_a=\\frac{1}{\\sqrt{2}} (1,1)\\) and \\(\\mathbf{e}_b=\\frac{1}{\\sqrt{2}} (-1,1)\\). Notice that both vectors have unit length, and they’re orthogonal since \\(\\mathbf{e}_a \\cdot \\mathbf{e}_b = 0\\). Thus, they form an orthonormal basis for \\(\\mathbb{R}^2\\). In this basis, \\(\\mathbf{v}=(1.25, 2)\\) would be written\n\\[\\mathbf{v} = (\\mathbf{v} \\cdot \\mathbf{e}_a) \\mathbf{e}_a + (\\mathbf{v} \\cdot \\mathbf{e}_b) \\mathbf{e}_b \\approx 2.298 \\mathbf{e}_a + 0.530 \\mathbf{e}_b.\\]\nThis is a very different representation for \\(\\mathbf{v}\\). Nevertheless, the two basis vectors are still perpendicular to each other. You can see a plot of this below.\nThere are infinitely many orthonormal bases for \\(\\mathbb{R}^2\\). Just take any two perpendicular vectors in the plane and normalize them to unit length and they’ll form a valid orthonormal basis.\n\n\nCode\nv = np.array([1.25, 2])\nea = np.array([1, 1]) / np.sqrt(2)\neb = np.array([-1, 1]) / np.sqrt(2)\nvectors = [v, np.dot(v, ea) * ea, np.dot(v, eb) * eb, ea, eb]\nplot_vectors(\n    vectors, ticks_every=1, zorders=[0, 1, 5, 3, 4, 2], headwidth=7,\n    colors=['red', 'steelblue', 'steelblue', 'black', 'black'],\n    text_offsets=[[0, 0], [0.03, 0], [-0.3, -0.65], [-0.1, -0.48], [-0.2, 0.15]],\n    title='$\\mathbf{v}=v_a \\mathbf{e}_a + v_b \\mathbf{e}_b$', \n    labels=['$\\mathbf{v}$', '$v_a \\mathbf{e}_a$', '$v_b \\mathbf{e}_b$', '$\\mathbf{e}_a$', '$\\mathbf{e}_b$'])"
  },
  {
    "objectID": "notebooks/vectors.html#cosine-similarity",
    "href": "notebooks/vectors.html#cosine-similarity",
    "title": "6  Vector Spaces",
    "section": "6.7 Cosine Similarity",
    "text": "6.7 Cosine Similarity\nJust like we can express the length of a vector using the dot product, it turns out we can also express the angle between any two vectors in the plane using the dot product. If \\(\\theta\\) is the angle between two vectors \\(\\mathbf{v}\\) and \\(\\mathbf{w}\\), it turns out the dot product is given by\n\\[\\mathbf{v} \\cdot \\mathbf{w} = ||\\mathbf{v}|| \\cdot ||\\mathbf{w}|| \\cos \\theta.\\]\nNote that both sides of this equation are scalars since the dot product is a scalar and the product of norms is a scalar. If you’re good at trigonometry, you can convince yourself this formula must be true by projecting \\(\\mathbf{v}\\) onto \\(\\mathbf{w}\\) similar to the way we did projections onto the x and y axes before. The difference this time is that the component of \\(\\mathbf{v}\\) in the direction of \\(\\mathbf{w}\\) is not \\(v_x\\) or \\(v_y\\) anymore, but instead \\(||\\mathbf{v}|| \\cos \\theta\\).\nYou can see two special cases of this formula by looking at what happens when the two vectors are parallel or perpendicular. If the two vectors are parallel, then \\(\\theta = 0^\\circ, 180^\\circ\\), so \\(\\cos \\theta = \\pm 1\\), so \\(\\mathbf{v} \\cdot \\mathbf{w} = \\pm ||\\mathbf{v}|| \\cdot ||\\mathbf{w}||\\). More importantly, if the two vectors are perpendicular, then \\(\\theta = 90^\\circ, 270^\\circ\\), so \\(\\cos \\theta = 0\\), so \\(\\mathbf{v} \\cdot \\mathbf{w} = 0\\). That is, perpendicular vectors are orthogonal. They mean the same thing.\nIt’s more common to express this formula with \\(\\cos \\theta\\) on one side and the vector terms on the other so you can solve for the angle (or more commonly just the cosine of the angle). In this case, we have \\[\\cos \\theta = \\frac{\\mathbf{v} \\cdot \\mathbf{w}}{||\\mathbf{v}|| \\cdot ||\\mathbf{w}||}.\\]\nWhat matters more than anything is what this formula says and how to use it. Suppose, for example, you want to find the angle between the two vectors \\(\\mathbf{v} = (1,1)\\) and \\(\\mathbf{w} = (0, -1)\\). Then you’d have\n\\[\\begin{align*}\n\\mathbf{v} \\cdot \\mathbf{w} &= 1 \\cdot 0 + 1 \\cdot (-1) = -1, \\\\\n||\\mathbf{v}|| &= \\sqrt{1^2 + 1^2} = \\sqrt{2}, \\\\\n||\\mathbf{w}|| &= \\sqrt{0^2 + (-1)^2} = 1.\n\\end{align*}\\]\nPlugging this into the cosine formula gives,\n\\[\n\\cos \\theta = \\frac{-1}{\\sqrt{2}} \\quad \\Longrightarrow \\quad \\theta = \\cos^{-1}\\bigg(\\frac{-1}{\\sqrt{2}}\\bigg) = 135^\\circ.\n\\]\nYou can verify this is correct by plotting the two vectors and confirming that they’re about \\(135^\\circ\\) from each other, which corresponds to about 1.25 quarter turns around a circle. It’s interesting to note that the dot product will only be negative when the angle between the two vectors is obtuse, i.e. more than \\(90^\\circ\\), which is of course the case here.\n\n\nCode\nv = np.array([1, 1])\nw = np.array([0, -1])\nplot_vectors([v, w], title='$\\mathbf{v} \\cdot \\mathbf{w} = ||\\mathbf{v}||||\\mathbf{w}|| \\cos \\\\theta$', \n             text_offsets=[[0, 0], [0.1, 0]], ticks_every=0.5, xlim=(-1, 2), ylim=(-1.5, 1.5),\n             labels=['$\\mathbf{v}$', '$\\mathbf{w}$'], colors=['red', 'steelblue'], headwidth=7)\n\n\n\n\n\n\n\n\n\nIn machine learning, this formula for \\(\\cos \\theta\\) is called the cosine similarity. The reason for this is that the dot product itself is a measure of how similar two vectors are. To see why, consider two special cases:\n\nThe two vectors are parallel: This is as large as the dot product between two vectors can get in absolute value. The vectors are as similar as they can be in a sense. Up to a scalar multiple, they contain the same information.\nThe two vectors are perpendicular: This is as small as the dot product between two vectors can get in absolute value. The two vectors are as different as they can be in a sense. They share pretty much no information. Information about one vector tells you basically nothing about the other.\n\nThe cosine similarity is a function of two input vectors \\(\\mathbf{v}\\) and \\(\\mathbf{w}\\). Since we don’t actually care about the angle \\(\\theta\\) usually, we’ll more often denote the cosine similarity using a notation like \\(\\cos(\\mathbf{v},\\mathbf{w})\\) to make it clear it’s a function of its two input vectors,\n\\[\\cos(\\mathbf{v},\\mathbf{w}) = \\frac{\\mathbf{v} \\cdot \\mathbf{w}}{||\\mathbf{v}|| \\cdot ||\\mathbf{w}||}.\\]\nNote the cosine similarity is just a normalized dot product, since dividing by the norms forces \\(-1 \\leq \\cos(\\mathbf{v},\\mathbf{w}) \\leq 1\\). It thus captures the same idea of similarity that the dot product does, but it’s more useful when the lengths of vectors get out of control. This is particularly likely to happen in high dimensions, when \\(n >> 2\\). This is the so-called “curse of dimensionality”. We’ll come back to this idea in future lessons.\nHere’s a quick implementation of the cosine similarity function using numpy. There’s no built-in function to do it, but it’s easy enough to implement by making judicious use of the np.dot function. It should give the same answer found above for \\(\\cos \\theta\\), which is \\(-\\frac{1}{\\sqrt{2}} \\approx -0.707\\).\n\n\nCode\ndef cosine_similarity(v, w):\n    return np.dot(v, w) / np.sqrt(np.dot(v, v) * np.dot(w, w))\n\nprint(f'cos(v, w) = {cosine_similarity(v, w)}')\n\n\ncos(v, w) = -0.7071067811865475\n\n\nAlgorithm Analysis: Like the dot product, this function runs in \\(O(n)\\) time. There are three independent dot product operations happening here, each adding \\(O(n)\\) FLOPS. Since the outputs of dot products are scalars, the multiply and divide only add one FLOP each. The square root isn’t obvious, but you can assume it takes some constant number of FLOPS as well. The total must therefore be \\(O(n)\\)."
  },
  {
    "objectID": "notebooks/vectors.html#other-norms",
    "href": "notebooks/vectors.html#other-norms",
    "title": "6  Vector Spaces",
    "section": "6.8 Other Norms",
    "text": "6.8 Other Norms\nIt turns out that the norm I defined above is only one way to measure the length of a vector. It’s the most natural way to do so sense it corresponds to your intuitive notions of length, which itself relates to the Pythagorean Theorem. There are other ways to quantify vector length as well that aren’t as intuitive. Because they do sometimes show up in machine learning I’ll briefly mention a couple of these here.\nThe norm I’ve covered is called the 2-norm. It’s called this because it involves squares and square roots. We can write it in the form\n\\[||\\mathbf{v}|| = ||\\mathbf{v}||_2 = \\big(v_x^2 + v_y^2 \\big)^{1/2}.\\]\nIt turns out we can replace the twos with any other positive number \\(p>1\\) to get generalized norms, called p-norms,\n\\[||\\mathbf{v}||_p = \\big(v_x^p + v_y^p \\big)^{1/p}.\\]\nThe p-norms cover a large class of norms, since any \\(1 \\leq p \\leq \\infty\\) can define a valid norm. The 2-norm, as you’d guess, occurs when \\(p=2\\). A couple of other norms that show up in machine learning are the 1-norm when \\(p=1\\), and the infinity norm when \\(p=\\infty\\). For 2-dimensional vectors, these norms are\n\\[\\begin{align*}\n||\\mathbf{v}||_1 &= |v_x| + |v_y|, \\\\\n||\\mathbf{v}||_\\infty &= \\max\\big(|v_x|, |v_y|\\big).\n\\end{align*}\\]\nHere’s an example. I’ll calculate the \\(p=1, 2, \\infty\\) norms for the vector \\(\\mathbf{v}=(1,-2)\\). We have,\n\\[\\begin{align*}\n||\\mathbf{v}||_1 &= |1| + |-2| = 1 + 2 = 3, \\\\\n||\\mathbf{v}||_2 &= \\sqrt{1^2 + (-2)^2} = \\sqrt{1 + 4} = \\sqrt{5} \\approx 2.236, \\\\\n||\\mathbf{v}||_\\infty &= \\max\\big(|1|, |-2|\\big) = \\max(1, 2) = 2.\n\\end{align*}\\]\nNotice that \\(||\\mathbf{v}||_1 \\geq ||\\mathbf{v}||_2 \\geq ||\\mathbf{v}||_\\infty\\). This is a general fact.\nIt’s a little hard right now to describe why these norms are useful in machine learning since we don’t currently have the context. Just know that these norms do come up sometimes. I’ll go into more depth on the uses of these different norms as we apply them. In practice though, we’ll probably work with the regular 2-norm maybe 90% of the time.\nIn numpy, you can calculate any \\(p\\)-norm using the function np.linalg.norm(v, ord=p). Here’s an example.\n\n\nCode\nv = np.array([1, -2])\nprint(f'1-Norm of v: {np.linalg.norm(v, ord=1)}')\nprint(f'2-Norm of v: {np.linalg.norm(v, ord=2)}')\nprint(f'Infinity-Norm of v: {np.linalg.norm(v, ord=np.inf)}')\n\n\n1-Norm of v: 3.0\n2-Norm of v: 2.23606797749979\nInfinity-Norm of v: 2.0"
  },
  {
    "objectID": "notebooks/vectors.html#linear-maps",
    "href": "notebooks/vectors.html#linear-maps",
    "title": "6  Vector Spaces",
    "section": "6.9 Linear Maps",
    "text": "6.9 Linear Maps\nSo where do matrices fit into all this vector space stuff? It turns out that matrices correspond to functions that send vectors to new vectors. These kinds of functions are called linear maps. A linear map is any vector-valued function \\(\\mathbf{w} = \\mathbf{F}(\\mathbf{v})\\) that satisfies the principle of superposition: For any scalars \\(a,b\\) and any vectors \\(\\mathbf{u}, \\mathbf{v}\\),\n\\[\\mathbf{F}(a\\mathbf{u} + b\\mathbf{v}) = a\\mathbf{F}(\\mathbf{u}) + b\\mathbf{F}(\\mathbf{v}).\\]\nSaid differently, a linear map is a function in which we can always split the function up over sums and factor out scalar constants. For vectors in the plane, linear maps are exactly the functions that map a vector \\(\\mathbf{v}=(v_x,v_y)\\) in the plane to another vector \\(\\mathbf{w}=(w_x,w_y)\\) in the plane.\nTo see how linear maps relate to matrices, let’s look at a vector in terms of the standard basis, \\(\\mathbf{v}=x\\mathbf{e}_x + y\\mathbf{e}_y\\), and see how it behaves under the map \\(\\mathbf{w} = \\mathbf{F}(\\mathbf{v})\\). Using the principle of superposition, we have\n\\[\\begin{align*}\n\\mathbf{w} &= \\mathbf{F}(\\mathbf{v}) \\\\\n&= \\mathbf{F}(x\\mathbf{e}_x + y\\mathbf{e}_y) \\\\\n&= x \\mathbf{F}(\\mathbf{e}_x) + y \\mathbf{F}(\\mathbf{e}_y). \\\\\n&= \\begin{pmatrix} \\mathbf{F}(\\mathbf{e}_x) & \\mathbf{F}(\\mathbf{e}_y) \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix}. \\\\\n\\end{align*}\\]\nEvidently, if we define a \\(2 \\times 2\\) matrix \\(\\mathbf{A}\\) as the matrix whose column vectors are \\(\\mathbf{F}(\\mathbf{e}_x)\\) and \\(\\mathbf{F}(\\mathbf{e}_y)\\), then we just get \\(\\mathbf{w} = \\mathbf{A} \\mathbf{v}\\). That is, a linear map is completely equivalent to matrix-vector multiplication by some matrix, that matrix just being the matrix of column vectors that the basis vectors get mapped to.\nAs an example, suppose we had the following linear map and wanted to figure out its corresponding matrix,\n\\[(v_x + 2v_y, 3v_x + 4v_y) = \\mathbf{F}\\big((v_x, v_y)\\big).\\]\nWhat we can do is see how \\(\\mathbf{F}(\\mathbf{v})\\) acts on the standard basis vectors. Evidently,\n\\[\n\\mathbf{F}\\big((1, 0)\\big) = (1 + 0, 3 + 0) = (1, 3), \\quad\n\\mathbf{F}\\big((0, 1)\\big) = (0 + 2, 0 + 4) = (2, 4).\n\\]\nTreating these as column vectors, we can stack them to get the \\(2 \\times 2\\) matrix\n\\[\n\\mathbf{A} =\n\\begin{pmatrix}\n1 & 2 \\\\\n3 & 4 \\\\\n\\end{pmatrix}.\n\\]\nThus, for this example, the linear map is given by\n\\[\n\\mathbf{F}(\\mathbf{v}) = \\mathbf{A} \\mathbf{v} =\n\\begin{pmatrix}\n1 & 2 \\\\\n3 & 4 \\\\\n\\end{pmatrix}\n\\begin{pmatrix}\nv_x \\\\\nv_y \\\\\n\\end{pmatrix}.\n\\]\nSince both the input and output vectors are in the plane, we can visualize them on the same plot. Let’s look at what the above example does, for example, to \\(\\mathbf{v}=(1,1)\\). Algebraically, the output should be the vector \\(\\mathbf{w}=(3, 7)\\). Geometrically, this linear map seems to be doing two things, stretching the vector out by a factor of \\(\\frac{||\\mathbf{w}||}{||\\mathbf{v}||} \\approx 5.4\\), and rotating it counterclockwise by an angle of \\(\\cos(\\mathbf{v}, \\mathbf{w}) \\approx 53^\\circ\\). It’s a general fact that we can decompose a linear map into a combination of scalings and rotations. I’ll talk more about that in the next lesson.\n\n\nCode\nA = np.array([[1, 2], [3, 4]])\nv = np.array([1, 1]).reshape(-1, 1)\nplot_vectors([v.flatten(), (A @ v).flatten()], colors=['black', 'red'],\n             labels=['$\\mathbf{{v}}$', '$\\mathbf{{A}}\\mathbf{{v}}$'], text_offsets=[[0.01, -0.1], [-0.1, 0.05]],\n             title='$\\mathbf{{w}} = \\mathbf{{A}}\\mathbf{{v}}$',  xlim=(0, 5), ylim=(0, 8))\n\n\n\n\n\n\n\n\n\nThis plot doesn’t really tell us that much about the linear map though. It only says how the map acts on a single vector \\(\\mathbf{v}=(1,1)\\). What would be really illuminating is to see how the map acts on a bunch of different vectors all over the plane. We can do that using what’s called a vector field plot. Here’s a vector field plot of the linear map from the previous example.\n\n\nCode\nA = np.array([[1, 2], [3, 4]])\nplot_vector_field(A, alpha=0.8, title=f'Vector Field of $F(v)=Av$,\\n$A=${A.tolist()}')\n\n\n\n\n\n\n\n\n\nAs you can see, a vector field plot is a plot of a bunch of arrows. Each arrow is understood to be the output vector of the map at that point. The point itself is understood to be the input vector, which is never plotted. For example, at the point \\(\\mathbf{v}=(1,1)\\) its output vector \\(\\mathbf{w}=(3,7)\\) is shown as an arrow starting at the point \\((1,1)\\) and ending at the point \\((1+3, 7+1)\\). We do this for a bunch of points all over the plot so we can see how the map is affecting different inputs. So we can see all these arrows, we typically scale the length of the output vector down so we can see all the other arrows too. In the plot above, the vectors are scaled down by a factor of 25 or so. Other than that, they’re all plotted at the correct angles and relative scales to each other.\nAnother way to read a vector field plot is as a kind of flow map. The idea is to pick a point and follow the arrows away from that point to see where they lead. This creates a kind of flow line. Does the flow line move out to infinity, into the origin, or does it do something else like rotate around? The sizes of the vectors tells you something about how fast the flow is moving. The smaller the arrows the slower the flow. The larger the arrows the faster the flow.\nIf you look more carefully at the vector field I just plotted, perhaps the most obvious thing you’ll see is the diagonal line running from the upper left to the lower right. Along this characteristic line, the vectors seem to all be getting scaled down by the linear map. That is, the flow along that line is really slow. However, if you look at the vectors off that line, they all seem to be pointing outward along the opposite diagonal. The opposite diagonal is evidently another characteristic line, in that all the other flows seem to be pointing in that direction, going faster as they get farther out from the origin. These characteristic lines are called eigenspaces. They’re a fundamental property of the linear map itself. I’ll get into that stuff more in the next lesson.\nLet’s now briefly look at some other linear maps. The first one is the identity map, \\(\\mathbf{w} = \\mathbf{v}\\). The identity map always maps a vector to itself. It’s not hard to see what the corresponding matrix is. The name gives it away. Since \\(\\mathbf{e}_x = \\mathbf{F}(\\mathbf{e}_x)\\) and \\(\\mathbf{e}_y = \\mathbf{F}(\\mathbf{e}_y)\\), the corresponding matrix is the \\(2 \\times 2\\) identity matrix\n\\[\\mathbf{I} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ \\end{pmatrix}.\\]\nGraphically, the identity map just leaves a vector where it is. It neither rotates it nor scales it.\nHere’s what the vector field plot looks like. Notice that all the output vectors are mapping radially outward, and getting larger the further you get away from the origin. It’s harder to tell from the plot, but each output vector is also the same length as the input vectors, which of course aren’t shown.\n\n\nCode\nI = np.array([[1, 0], [0, 1]])\nplot_vector_field(I, alpha=0.8, title=f'Vector Field of $F(v)=Iv$,\\n$I=${I.tolist()}')\n\n\n\n\n\n\n\n\n\nAnother special linear map is the inverse map. If \\(\\mathbf{w} = \\mathbf{F}(\\mathbf{v})\\) is a linear map, its inverse map is\n\\[\\mathbf{v} = \\mathbf{F}^{-1}(\\mathbf{w}).\\]\nLike the name suggests, the inverse map just undoes the original map, in the sense that\n\\[\\mathbf{F}^{-1}(\\mathbf{F}(\\mathbf{v})) = \\mathbf{v}.\\]\nIf \\(\\mathbf{F}(\\mathbf{v}) = \\mathbf{A}\\mathbf{v}\\) is the matrix representation of the original map, then it should be obvious that \\(\\mathbf{F}^{-1}(\\mathbf{w}) = \\mathbf{A}^{-1}\\mathbf{w}\\) is the matrix representation of the inverse map. That is, the matrix corresponding to the inverse map is just the inverse of the matrix corresponding to the original map.\nHere’s what the vector field plot looks like for the example from before with\n\\[\n\\mathbf{F}(\\mathbf{v}) = \\mathbf{A} \\mathbf{v} =\n\\begin{pmatrix}\n1 & 2 \\\\\n3 & 4 \\\\\n\\end{pmatrix}\n\\begin{pmatrix}\nv_x \\\\\nv_y \\\\\n\\end{pmatrix}.\n\\]\nThe inverse map should be given by\n\\[\n\\mathbf{F}^{-1}(\\mathbf{w}) = \\mathbf{A}^{-1} \\mathbf{w} =\n\\begin{pmatrix}\n-2 & 1 \\\\\n\\frac{3}{2} & -\\frac{1}{2} \\\\\n\\end{pmatrix}\n\\begin{pmatrix}\nw_x \\\\\nw_y \\\\\n\\end{pmatrix}.\n\\]\nNotice how in this case the characteristic lines are the same, but their behavior has flipped. With \\(\\mathbf{A}\\), the upper left to lower right diagonal made vectors smaller while the other diagonal made them bigger. But \\(\\mathbf{A}^{-1}\\) seems to do the exact opposite. Also, if you look more carefully, you’ll see the direction of the arrows has reversed too, with them all flowing in instead of going out.\n\n\nCode\nA = np.array([[1, 2], [3, 4]])\nA_inv = np.linalg.inv(A)\nplot_vector_field(A_inv, alpha=0.8, \n    title=f'Vector Field of $F^{{-1}}(v)=A^{{-1}}v$,\\n$A^{{-1}}=${A_inv.round(2).tolist()}')\n\n\n\n\n\n\n\n\n\nThe behavior of linear maps is perhaps the real reason why matrix multiplication is so important to linear algebra as opposed to, say, element-wise multiplication. To see why, suppose we have two linear maps in the plane, \\(\\mathbf{u} = \\mathbf{F}(\\mathbf{w})\\) and \\(\\mathbf{w} = \\mathbf{G}(\\mathbf{v})\\). What happens if we compose these two maps to get\n\\[\\mathbf{u} = \\mathbf{F}(\\mathbf{G}(\\mathbf{v}))?\\]\nSince both linear maps must have some matrix representation, we can write \\(\\mathbf{u} = \\mathbf{A}\\mathbf{w}\\) and \\(\\mathbf{w} = \\mathbf{B} \\mathbf{v}\\), where \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) are two matrices whose values are determined by how their respective maps act on the standard basis \\(\\mathbf{e}_x, \\mathbf{e}_y\\). If we thus compose the two maps, we evidently get\n\\[\\mathbf{u} = (\\mathbf{A} \\mathbf{B}) \\mathbf{v}.\\]\nThat is, the composition of two linear maps is equivalent to matrix multiplication. Notice the fact that matrix multiplication doesn’t commute also implies that composition of linear maps doesn’t commute either.\nWhile linear maps are interesting for linear algebra, in machine learning we’re more often interested in affine maps. An affine map is just a linear map shifted by some constant vector,\n\\[\\mathbf{F}(\\mathbf{v}) = \\mathbf{A}\\mathbf{v} + \\mathbf{b}.\\]\nIn machine learning, the constant vector \\(\\mathbf{b}\\) is usually called a bias vector. Graphically, the only real difference between an affine map and a linear map is that vectors will not just get rotated and scaled, but translated by \\(\\mathbf{b}\\).\nAs an example, consider the affine map\n\\[\n\\mathbf{F}(\\mathbf{v}) = \\mathbf{A} \\mathbf{v} + \\mathbf{b} =\n\\begin{pmatrix}\n1 & 2 \\\\\n3 & 4 \\\\\n\\end{pmatrix}\n\\begin{pmatrix}\nv_x \\\\\nv_y \\\\\n\\end{pmatrix} +\n\\begin{pmatrix}\n1 \\\\\n1 \\\\\n\\end{pmatrix}.\n\\]\nIn this case, \\(\\mathbf{b}=(1,1)\\). Graphically, you can imagine this map doing the same \\(\\mathbf{A} \\mathbf{v}\\) mapping from before, but translating the tail of each vector by \\(\\mathbf{b}=(1,1)\\). For example, if we take \\(\\mathbf{v}=(1,0)\\) the affine map would look as follows.\n\n\nCode\nA = np.array([[1, 2], [3, 4]])\nb = np.array([[1], [1]])\nv = np.array([[1], [0]])\nvectors = [x.flatten() for x in [v, A @ v, A @ v + b, b]]\nplot_vectors(\n    vectors, xlim=(-1, 3), ylim=(-1, 5), headwidth=5, colors=['black', 'blue', 'red', 'green'],\n    labels=['$\\mathbf{{v}}$', '$\\mathbf{{A}}\\mathbf{{v}}$', \n            '$\\mathbf{{A}}\\mathbf{{v}} + \\mathbf{{b}}$', '$\\mathbf{{b}}$'], \n    text_offsets=[[0, 0.1], [0.6, -0.8], [-1.7, -1.3], [0.1, -0.2]], \n    tails=[[0, 0], [b[0][0], b[1][0]], [0, 0], [0, 0]],\n    zorders=[0, 1, 2, 3],\n    title='Affine Map: $\\mathbf{F}(\\mathbf{v}) = \\mathbf{A}\\mathbf{v} + \\mathbf{b}$')\n\n\n\n\n\n\n\n\n\nIt’s a little hard to see what’s going on in the vector field plot of this affine map, but again all it’s doing is shifting the output vectors by \\(\\mathbf{b}\\). Notice the characteristic lines and values don’t really seem to change. They’re a property of \\(\\mathbf{A}\\), and I didn’t change \\(\\mathbf{A}\\) in this example.\n\n\nCode\nplot_vector_field(A, b=b, alpha=0.8, title=f'Vector Field of $F(v)=Av+b$,\\n$A=${A.tolist()}, $b=${b.tolist()}')"
  },
  {
    "objectID": "notebooks/vectors.html#higher-dimensional-vector-spaces",
    "href": "notebooks/vectors.html#higher-dimensional-vector-spaces",
    "title": "6  Vector Spaces",
    "section": "6.10 Higher-Dimensional Vector Spaces",
    "text": "6.10 Higher-Dimensional Vector Spaces\nIt may seem like everything I’ve said is special for the case of two dimensions, but it’s really not. Every single thing I’ve said extends exactly how you’d expect to vectors of arbitrary size \\(n\\). The only difference now is that you can’t visualize the stuff anymore. You just have to trust the math. I’ll restate all of the definitions from above here, but for \\(n\\)-dimensional vector spaces instead.\nA vector of size \\(n\\) can be defined as a 1-dimensional array of real numbers \\(x_0,x_1,x_2,\\cdots,x_{n-1}\\),\n\\[\\mathbf{x} = (x_0,x_1,x_2,\\cdots,x_{n-1}).\\]\nVectors can be added together, and multiplied by scalars. Vector addition is defined element-wise. If \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are two vectors, then\n\\[\\mathbf{x} + \\mathbf{y} = (x_0+y_0, x_1+y_1, \\cdots, x_{n-1}+y_{n-1}).\\]\nTo keep a running example through this section, I’ll use numpy to create two vectors \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) each of size \\(n=10\\). Here’s their vector sum.\n\n\nCode\nx = np.array([1, 2, 3, 4, 5, 5, 4, 3, 2, 1])\ny = np.array([1, 0, -1, 0, 1, 0, -1, 0, 1, 0])\n\nprint(f'x + y = {x + y}')\n\n\nx + y = [2 2 2 4 6 5 3 3 3 1]\n\n\nScalar multiplication is defined similarly. If \\(c \\in \\mathbb{R}\\) is some scalar and \\(\\mathbf{x}\\) is some vector, then\n\\[c\\mathbf{x} = (cx_0,cx_1,\\cdots,cx_{n-1}).\\]\n\n\nCode\nc = 5\nprint(f'c * x = {c * x}')\n\n\nc * x = [ 5 10 15 20 25 25 20 15 10  5]\n\n\nVectors of size \\(n\\) live in the \\(n\\)-dimensional vector space \\(\\mathbb{R}^n\\). By definition, any linear combination of two vectors must also live in the same vector space. That is, if \\(\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^n\\) are two vectors and \\(a,b \\in \\mathbb{R}\\) are two scalars, then \\(a \\mathbf{x} + b \\mathbf{y} \\in \\mathbb{R}^n\\).\nThe dot product or inner product between two vectors \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) of size \\(n\\) is defined as their sum product, i.e.\n\\[\\mathbf{x} \\cdot \\mathbf{y} = x_0y_0 + x_1y_1 + \\cdots + x_{n-1}y_{n-1}.\\]\n\n\nCode\nprint(f'x . y = {np.dot(x, y)}')\n\n\nx . y = 1\n\n\nThe norm (technically the 2-norm) of a vector is defined as the square root of its dot product with itself, i.e.\n\\[||\\mathbf{x}|| = ||\\mathbf{x}||_2 = \\sqrt{\\mathbf{x} \\cdot \\mathbf{x}} = \\sqrt{x_0^2 + x_1^2 + \\cdots + x_{n-1}^2}.\\]\nThis is just the \\(n\\)-dimensional generalization of the Pythagorean Theorem. We can also consider other \\(p\\) norms as well. In particular, the cases when \\(p=1\\) and \\(p=\\infty\\) sometimes show up in applications,\n\\[\\begin{align*}\n||\\mathbf{x}||_1 &= \\sum_{i=0}^{n-1} |x_i| = |x_0| + |x_1| + \\cdots + |x_{n-1}|, \\\\\n||\\mathbf{x}||_\\infty &= \\max_{i=0,\\cdots,n-1} |x_i| = \\max\\big(|x_0|, |x_1|, \\cdots, |x_{n-1}|\\big).\n\\end{align*}\\]\nIt will always be the case that \\(||\\mathbf{x}||_1 \\geq ||\\mathbf{x}||_2 \\geq ||\\mathbf{x}||_\\infty\\).\n\n\nCode\nprint(f'1-Norm of x: {np.linalg.norm(x, ord=1)}')\nprint(f'2-Norm of x: {np.linalg.norm(x, ord=2)}')\nprint(f'Infinity-Norm of x: {np.linalg.norm(x, ord=np.inf)}')\n\n\n1-Norm of x: 30.0\n2-Norm of x: 10.488088481701515\nInfinity-Norm of x: 5.0\n\n\nThe distance \\(d(\\mathbf{x}, \\mathbf{y})\\) between two vectors \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) is just the norm of their difference vector,\n\\[d(\\mathbf{x}, \\mathbf{y}) = ||\\mathbf{x}-\\mathbf{y}|| = \\sum_{i=0}^{n-1} \\sqrt{(x_i-y_i)^2} = \\sqrt{(x_0-y_0)^2 + (x_1-y_1)^2 + \\cdots + (x_{n-1}-y_{n-1})^2}.\\]\nWe can define the angle between any two vectors \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) of size \\(n\\) by making use of the same identity for the dot product, which still holds in \\(n\\) dimensions,\n\\[\\mathbf{x} \\cdot \\mathbf{y} = ||\\mathbf{x}|| \\cdot ||\\mathbf{y}|| \\cos \\theta.\\]\nUsing this identity, we can define the cosine similarity \\(\\cos(\\mathbf{x}, \\mathbf{y})\\) by solving for \\(\\cos \\theta\\),\n\\[\\cos(\\mathbf{x}, \\mathbf{y}) = \\frac{\\mathbf{x} \\cdot \\mathbf{y}}{||\\mathbf{x}|| \\cdot ||\\mathbf{y}||}.\\]\nThe dot product is a measure of how similar two vectors are, and the cosine similarity is a normalized measure of how similar two vectors are, since dividing by the norms forces \\(-1 \\leq \\cos \\theta \\leq 1\\).\n\n\nCode\nprint(f'cos(x, y) = {cosine_similarity(x, y)}')\n\n\ncos(x, y) = 0.04264014327112208\n\n\nA set of vectors \\(\\mathbf{x}_0, \\mathbf{x}_1, \\cdots, \\mathbf{x}_{k-1}\\) is linearly independent if no one vector is a linear combination of the rest,\n\\[\\mathbf{x}_j \\neq \\sum_{i \\neq j} c_i \\mathbf{x}_j.\\]\nIf one vector is a linear combination of the rest, they’re linearly dependent. If there are exactly \\(n\\) linear independent vectors in the set, it’s called a basis.\nWe can define the standard basis on \\(\\mathbb{R}^n\\) with the following complete set of size \\(n\\) unit vectors,\n\\[\\begin{align*}\n\\mathbf{e}_0 &= (1, 0, 0, \\cdots, 0), \\\\\n\\mathbf{e}_1 &= (0, 1, 0, \\cdots, 0), \\\\\n\\vdots \\ &= \\qquad \\vdots \\\\\n\\mathbf{e}_{n-1} &= (0, 0, 0, \\cdots, 1).\n\\end{align*}\\]\nThe standard basis is an orthonormal basis since each vector is a unit vector and they’re all mutually orthogonal, i.e.\n\\[\n\\mathbf{e}_i \\cdot \\mathbf{e}_j = \\delta_{ij} =\n\\begin{cases}\n1 & i = j, \\\\\n0 & i \\neq j.\n\\end{cases}\n\\]\nNotation: The symbol \\(\\delta_{ij}\\) is called the Kronecker delta. It’s just a shorthand way of writing something is \\(1\\) if \\(i=j\\) and \\(0\\) if \\(i \\neq j\\).\n\n\nCode\nn = 10\ne = [ei.flatten().astype(int) for ei in np.eye(n)]\nprint(f'e3 = {e[3]}')\nprint(f'e8 = {e[8]}')\nprint(f'e3 . e3 = {np.dot(e[3], e[3])}')\nprint(f'e3 . e8 = {np.dot(e[3], e[8])}')\n\n\ne3 = [0 0 0 1 0 0 0 0 0 0]\ne8 = [0 0 0 0 0 0 0 0 1 0]\ne3 . e3 = 1\ne3 . e8 = 0\n\n\nIf a basis is orthonormal, any vector \\(\\mathbf{x}\\) can be decomposed into a linear combination of the basis elements by taking the dot product \\(\\mathbf{x} \\cdot \\mathbf{e}_i\\). For the standard basis, these just give the vector components \\(x_i\\),\n\\[\\mathbf{x} = \\sum_{i=0}^{n-1} (\\mathbf{x} \\cdot \\mathbf{e}_i) \\mathbf{e}_i = \\sum_{i=0}^{n-1} x_i \\mathbf{e}_i = x_0 \\mathbf{e}_0 + x_1 \\mathbf{e}_1 + \\cdots x_{n-1} \\mathbf{e}_{n-1}.\\]\nEach term \\(x_i \\mathbf{e}_i\\) in the sum corresponds to the projection of \\(\\mathbf{x}\\) onto the \\(i\\)th axis. Each axis in \\(\\mathbb{R}^n\\) is still a single line, but now there are \\(n\\) of these axis lines, all perpendicular to each other.\nA linear map is a vector-valued function \\(\\mathbf{y}=\\mathbf{F}(\\mathbf{x})\\) between vector spaces that preserves the linear structure of the spaces. In general, \\(\\mathbf{x} \\in \\mathbb{R}^m\\) and \\(\\mathbf{y} \\in \\mathbb{R}^n\\) need not be in the same vector spaces. Either way, a linear map can always be expressed as a matrix-vector equation \\(\\mathbf{y}=\\mathbf{A}\\mathbf{x}\\), where \\(\\mathbf{A}\\) is some \\(m \\times n\\) matrix whose entries are determined by how the map acts on the standard basis vectors, i.e. for each column \\(i=0,1,\\cdots,n-1\\) we have\n\\[\\mathbf{A}_{:, i} = \\mathbf{F}(\\mathbf{e}_i).\\]\nMore generally, an affine map is a linear map shifted by some bias vector \\(\\mathbf{b} \\in \\mathbb{R}^m\\). Affine maps can always be expressed as a shifted matrix-vector equation, \\(\\mathbf{y}=\\mathbf{A}\\mathbf{x} + \\mathbf{b}\\).\nApplication: Roughly speaking a neural network is just a composite function of successive affine maps. The only real difference with a neural network is that in between each affine map we apply a non-linear activation function to make the output do more interesting things. Most of the time nowadays the activation function is just the ReLU function \\(\\text{ReLU}(\\mathbf{z})=\\max(\\mathbf{0}, \\mathbf{z})\\). All it does is sets any negative entries to zero. For example, here’s what a “single hidden layer neural network” might look like for a regression problem,\n\\[\\begin{align*}\n\\mathbf{z} &= \\mathbf{A}\\mathbf{x} + \\mathbf{b}, \\\\\n\\mathbf{a} &= \\text{ReLU}(\\mathbf{z}), \\\\\n\\mathbf{y} &= \\mathbf{B}\\mathbf{a} + \\mathbf{c}. \\\\\n\\end{align*}\\]\nThe main thing about a neural network though is that the values in the matrix and bias vectors are learned from the training data. That is, they’re tuned specifically to make sure the neural network approximates the true input-output relationship behavior in the data as well as possible.\nJust as with linear maps in the plane, linear maps in higher dimensions always preserve lines. Not just lines in fact, but planes and hyperplanes as well. These generalizations of lines are called linear subspaces. Linear subspaces will always be hyperplanes in \\(n\\)-dimensional space that pass through the origin. Think of them as planes passing through the origin, but in more dimensions. If the hyperplane spanned by \\(\\mathbf{x}_0, \\mathbf{x}_1, \\cdots, \\mathbf{x}_{k-1}\\) is some \\(k\\)-dimensional linear subspace of \\(\\mathbb{R}^n\\), then its image under the linear map will be a new \\(k\\)-dimensional linear subspace in \\(\\mathbb{R}^m\\) (if \\(k \\leq m\\), otherwise it’ll just be the full vector space \\(\\mathbb{R}^m\\) itself). Any linear combination of vectors in a given subspace will stay inside that subspace. It’s closed under vector space operations. For all practical purposes it’s a new vector space \\(\\mathbb{R}^k\\) unto itself."
  },
  {
    "objectID": "notebooks/matrices.html#properties-of-matrices",
    "href": "notebooks/matrices.html#properties-of-matrices",
    "title": "7  Matrix Algebra",
    "section": "7.1 Properties of Matrices",
    "text": "7.1 Properties of Matrices\n\n7.1.1 Matrix Spaces\nJust like vectors, matrices can be thought of as objects in their own matrix space. A matrix space is just a vector space, except it has two dimensions \\(m\\) and \\(n\\). We’ll denote the matrix space of \\(m \\times n\\) matrices with the symbol \\(\\mathbb{R}^{m \\times n}\\). Just like vector spaces, matrix spaces must be closed under linear combinations. If \\(\\mathbf{A}, \\mathbf{B} \\in \\mathbb{R}^{m \\times n}\\) are two matrices, then any matrix linear combination \\(\\mathbf{C} = a\\mathbf{A} + b\\mathbf{B}\\) must also be a valid \\(m \\times n\\) matrix in \\(\\mathbb{R}^{m \\times n}\\). This means matrices behave the same way under addition and scalar multiplication as vectors do.\nWhile this fact should be kind of obvious by now, here’s an example anyway. I’ll choose \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) to both be \\(2 \\times 2\\) here. Adding them together or scalar multiplying them should also obviously give a matrix that’s \\(2 \\times 2\\), since everything is element-wise.\n\n\nCode\na = 5\nA = np.array(\n    [[1, 1], \n     [1, 1]])\nB = np.array(\n    [[1, -1], \n     [-1, 1]])\nprint(f'{a}A = \\n{5 * A}')\nprint(f'A + B = \\n{A + B}')\n\n\n5A = \n[[5 5]\n [5 5]]\nA + B = \n[[2 0]\n [0 2]]\n\n\nSince every matrix corresponds to a linear map \\(\\mathbf{F}(\\mathbf{x}) = \\mathbf{A}\\mathbf{x}\\), the space of matrices also corresponds to the space of linear maps from vectors \\(\\mathbf{x} \\in \\mathbb{R}^n\\) to vectors \\(\\mathbf{y} \\in \\mathbb{R}^m\\). Recall that the composition of linear maps is equivalent to matrix multiplication. If \\(\\mathbf{F}(\\mathbf{y}) = \\mathbf{A}\\mathbf{y}\\) and \\(\\mathbf{G}(\\mathbf{x}) = \\mathbf{B}\\mathbf{x}\\) are two linear maps, then their composition is equivalent to the matrix product of the two maps,\n\\[\\mathbf{z}=\\mathbf{F}(\\mathbf{G}(\\mathbf{x})) = \\mathbf{A}\\mathbf{B}\\mathbf{x}.\\]\nThe composition, and hence the matrix multiplication operation, only makes sense when the two matrices are compatible, i.e. \\(\\mathbf{A} \\in \\mathbb{R}^{m \\times n}\\) and \\(\\mathbf{B} \\in \\mathbb{R}^{n \\times p}\\). It also follows from this relationship to linear maps (which are of course just functions) that matrix multiplication is associative, i.e. we can put parenthesis wherever we like,\n\\[\\mathbf{A}\\mathbf{B}\\mathbf{C} = (\\mathbf{A}\\mathbf{B})\\mathbf{C} = \\mathbf{A}(\\mathbf{B}\\mathbf{C}).\\]\nDo remember, however, that matrix multiplication (and function composition) doesn’t commute, i.e. \\(\\mathbf{A}\\mathbf{B} \\neq \\mathbf{B}\\mathbf{A}\\), even when the two matrices are compatible.\n\n\n7.1.2 Transposes\nRecall that every matrix \\(\\mathbf{A} \\in \\mathbb{R}^{m \\times n}\\) has a transpose matrix \\(\\mathbf{A}^\\top \\in \\mathbb{R}^{n \\times m}\\) that’s defined as the same matrix, but with the indices swapped,\n\\[(A^\\top)_{i,j} = A_{j,i}.\\]\nHere’s a quick example for a \\(2 \\times 3\\) matrix \\(\\mathbf{A}\\).\n\n\nCode\nA = np.array(\n    [[1, 2, 3], \n     [4, 5, 6]])\nprint(f'A^T = \\n{A.T}')\n\n\nA^T = \n[[1 4]\n [2 5]\n [3 6]]\n\n\nWhat happens if we multiply two transposed matrices? Suppose \\(\\mathbf{A}\\) is \\(m \\times n\\) and \\(\\mathbf{B}\\) is \\(n \\times p\\). Then \\(\\mathbf{A}\\mathbf{B}\\) is \\(m \\times p\\). That means its transpose \\((\\mathbf{A}\\mathbf{B})^\\top\\) should be \\(p \\times m\\). But \\(\\mathbf{A}^\\top\\) is \\(n \\times m\\) and \\(\\mathbf{B}^\\top\\) is \\(p \\times n\\). This implies that the transpose of the product can only make sense if it’s the product of the transposes, but in opposite order so the shapes match up right,\n\\[(\\mathbf{A}\\mathbf{B})^\\top = \\mathbf{B}^\\top \\mathbf{A}^\\top.\\]\nThis is not really a proof of this fact. If you want a proof, what you’ll want to do is look at the individual elements of each side, and show the equation must be true element-by-element. I won’t bore you with this. I’ll just give you an example with numpy so you can see they have to be equal. I’ll take \\(\\mathbf{A}\\) to be \\(3 \\times 2\\) and \\(\\mathbf{B}\\) to be \\(2 \\times 3\\), which means \\((\\mathbf{A}\\mathbf{B})^\\top\\) should be \\(2 \\times 2\\). Recall you can transpose a matrix in numpy using A.T or np.transpose(A).\n\n\nCode\nA = np.array(\n    [[1, 2, 3], \n     [4, 5, 6]])\nB = np.array(\n    [[-1, -2], \n     [-3, -4], \n     [-5, -6]])\nprint(f'(AB)^T = \\n{(A @ B).T}')\nprint(f'B^T A^T = \\n{B.T @ A.T}')\n\n\n(AB)^T = \n[[-22 -49]\n [-28 -64]]\nB^T A^T = \n[[-22 -49]\n [-28 -64]]\n\n\n\n\n7.1.3 Inverses\nWhen a matrix is square, i.e. \\(\\mathbf{A}\\) is \\(n \\times n\\), we can think of it as mapping vectors to other vectors in the same vector space \\(\\mathbb{R}^n\\). The identity map (the “do nothing” map) always maps a vector to itself. It corresponds to the \\(n \\times n\\) identity matrix\n\\[\n\\mathbf{I} =\n\\begin{pmatrix}\n1 & 0 & 0 & \\cdots & 0 \\\\\n0 & 1 & 0 & \\cdots & 0 \\\\\n0 & 0 & 1 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & 0 & \\cdots & 1 \\\\\n\\end{pmatrix}.\n\\]\nHere’s an example. I’ll use np.eye(n) to generate the identity matrix for \\(n=5\\).\n\n\nCode\nI = np.eye(5)\nprint(f'I = \\n{I}')\n\n\nI = \n[[1. 0. 0. 0. 0.]\n [0. 1. 0. 0. 0.]\n [0. 0. 1. 0. 0.]\n [0. 0. 0. 1. 0.]\n [0. 0. 0. 0. 1.]]\n\n\nRecall the inverse of a square matrix \\(\\mathbf{A}\\) is the matrix \\(\\mathbf{A}^{-1}\\) satisfying\n\\[\\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{I}.\\]\nThe inverse matrix \\(\\mathbf{A}^{-1}\\) will exist exactly when the determinant of \\(\\mathbf{A}\\) is nonzero, i.e. \\(\\text{det}(\\mathbf{A}) \\neq 0\\). If the determinant is zero, then the matrix is singular, and no inverse can be found no matter how hard you look for one.\nRecall that in numpy you can invert a square matrix using np.linalg.inv(A). It’s usually not a good idea to do so because inverting a matrix is numerically unstable, but you can in principle. The inverse calculation runs in \\(O(n^3)\\) time just like multiplication.\nHere’s an example where \\(\\mathbf{A}\\) is \\(2 \\times 2\\). You can already see from this example the numerical loss of precision creeping in, since neither \\(\\mathbf{A}^{-1}\\mathbf{A}\\) nor \\(\\mathbf{A}\\mathbf{A}^{-1}\\) exactly yield the identity matrix.\n\n\nCode\nA = np.array(\n    [[1, 2], \n     [3, 4]])\nA_inv = np.linalg.inv(A)\nprint(f'A^(-1) = \\n{A_inv}')\nprint(f'A^(-1) A = \\n{A_inv @ A}')\nprint(f'A A^(-1) = \\n{A @ A_inv}')\n\n\nA^(-1) = \n[[-2.   1. ]\n [ 1.5 -0.5]]\nA^(-1) A = \n[[1.00000000e+00 0.00000000e+00]\n [1.11022302e-16 1.00000000e+00]]\nA A^(-1) = \n[[1.0000000e+00 0.0000000e+00]\n [8.8817842e-16 1.0000000e+00]]\n\n\nJust like with the transpose, we can ask what happens if we try to invert the product of two matrices. You can convince yourself that the same kind of rule holds: the inverse of a product is the product of the inverses in reverse order,\n\\[(\\mathbf{A}\\mathbf{B})^{-1} = \\mathbf{B}^{-1} \\mathbf{A}^{-1}.\\]\nHere’s a \\(2 \\times 2\\) “proof” of this fact.\n\n\nCode\nA = np.array(\n    [[1, 2], \n     [3, 4]])\nB = np.array(\n    [[1, 0], \n     [1, 1]])\nA_inv = np.linalg.inv(A)\nB_inv = np.linalg.inv(B)\nAB_inv = np.linalg.inv(A @ B)\nprint(f'(AB)^(-1) = \\n{AB_inv}')\nprint(f'B^(-1) A^(-1) = \\n{B_inv @ A_inv}')\n\n\n(AB)^(-1) = \n[[-2.   1. ]\n [ 3.5 -1.5]]\nB^(-1) A^(-1) = \n[[-2.   1. ]\n [ 3.5 -1.5]]\n\n\nI encourage you to check this result using the fact I derived from the last lesson for \\(2 \\times 2\\) matrices,\n\\[\n\\mathbf{A} =\n\\begin{pmatrix}\na & b \\\\\nc & d \\\\\n\\end{pmatrix} \\quad \\Longrightarrow \\quad\n\\mathbf{A}^{-1} = \\frac{1}{ad-bc}\n\\begin{pmatrix}\nd & -b \\\\\n-c & a \\\\\n\\end{pmatrix}.\n\\]\nIt also turns out that the transpose and inverse operations commute with each other,\n\\[(\\mathbf{A}^\\top)^{-1} = (\\mathbf{A}^{-1})^\\top.\\]\nRecall the pseudoinverse of a matrix is the generalization of the inverse to rectangular matrices. If \\(\\mathbf{A}\\) is an \\(m \\times n\\) matrix, the pseudoinverse is defined by,\n\\[\n\\mathbf{A}^+ =\n\\begin{cases}\n(\\mathbf{A}^\\top \\mathbf{A})^{-1} \\mathbf{A}^\\top, & m > n \\\\\n\\mathbf{A}^\\top (\\mathbf{A} \\mathbf{A}^\\top)^{-1}, & m < n. \\\\\n\\end{cases}\n\\]\nFor the pseudoinverse to exist in either case, the smallest dimension needs to have all linearly independent vectors. Some properties of the matrix inverse also hold for the pseudoinverse,\n\\[(\\mathbf{A}\\mathbf{B})^+ = \\mathbf{B}^+ \\mathbf{A}^+,\\] \\[(\\mathbf{A}^\\top)^+ = (\\mathbf{A}^+)^\\top.\\]\nSome properties don’t though. For example, the pseudoinverse isn’t a two-sided inverse anymore. It’s a left inverse if \\(m > n\\), and a right inverse if \\(m < n\\).\n\n\n7.1.4 Determinant and Trace\nNotice something with the \\(2 \\times 2\\) matrix above. Since \\(\\text{det}(\\mathbf{A}) = ad - bc\\), we can evidently write\n\\[\\mathbf{A}^{-1} = \\frac{1}{\\text{det}(\\mathbf{A})} \\mathbf{\\tilde A},\\]\nwhere \\(\\mathbf{\\tilde A}\\) is some kind of matrix related to \\(\\mathbf{A}\\). The properties of \\(\\mathbf{\\tilde A}\\) aren’t important (it’s called the adjugate if you’re curious). But this general fact turns out to be true for any \\(n \\times n\\) matrix, except the formula for the determinant gets a lot more complicated. What’s important is that \\(\\mathbf{A}^{-1}\\) is inversely proportional to the determinant, i.e.\n\\[\\mathbf{A}^{-1} \\propto \\frac{1}{\\text{det}(\\mathbf{A})}.\\]\nThat’s why we can’t allow \\(\\text{det}(\\mathbf{A}) = 0\\), because then \\(\\mathbf{A}^{-1}\\) blows up due to the division by zero. Now, I’ve already said \\((\\mathbf{A}\\mathbf{B})^{-1} = \\mathbf{B}^{-1} \\mathbf{A}^{-1}\\). If then\n\\[\\mathbf{A}^{-1} \\propto \\frac{1}{\\text{det}(\\mathbf{A})}, \\quad \\mathbf{B}^{-1} \\propto \\frac{1}{\\text{det}(\\mathbf{B})},\\]\nwe evidently must have that\n\\[(\\mathbf{AB})^{-1} \\propto \\frac{1}{\\text{det}(\\mathbf{AB})} \\propto \\mathbf{B}^{-1} \\mathbf{A}^{-1} \\propto \\frac{1}{\\text{det}(\\mathbf{A}) \\cdot \\text{det}(\\mathbf{B})}.\\]\nIt thus seems to be the case that the determinant of a product is proportional to the product of the determinants. This proportionality turns out to be exact, i.e.\n\\[\\text{det}(\\mathbf{A}\\mathbf{B}) = \\text{det}(\\mathbf{A}) \\cdot \\text{det}(\\mathbf{B}) = \\text{det}(\\mathbf{B}) \\cdot \\text{det}(\\mathbf{A}).\\]\nIn general, the determinant of an \\(n \\times n\\) matrix \\(\\mathbf{A}\\) is a nasty \\(n\\) degree multivariate polynomial of the elements of \\(\\mathbf{A}\\). There’s no reliably easy way to calculate it except for small \\(n\\) matrices. In numpy, you can use np.linalg.det(A) to calculate the determinant, but just as with inverses, this is a numerically unstable operation, and so should be avoided where possible. Moreover, it runs in \\(O(n^3)\\) time, which is just as slow as matrix multiplication.\nHere’s an example. I’ll verify this “product rule” for determinants using two \\(3 \\times 3\\) matrices. The determinant of both matrices turns out to be \\(6\\), which means their product should have determinant \\(36\\).\n\n\nCode\nA = np.array(\n    [[3, 0, 0],\n     [1, 2, 0],\n     [1, 1, 1]])\nB = np.array(\n    [[1, 1, 1],\n     [0, 2, 1],\n     [0, 0, 3]])\ndet_A = np.linalg.det(A)\ndet_B = np.linalg.det(B)\ndet_AB = np.linalg.det(A @ B)\nprint(f'det(A) = {det_A}')\nprint(f'det(B) = {det_B}')\nprint(f'det(AB) = {det_AB}')\n\n\ndet(A) = 6.0\ndet(B) = 6.0\ndet(AB) = 36.0\n\n\nNotice in both cases the determinant happens to be the product of the diagonal elements\n\\[\\text{det}(\\mathbf{A}) = \\text{det}(\\mathbf{B}) = 1 \\cdot 2 \\cdot 3 = 6.\\]\nI rigged the result to come out this way. It’s not always true. It’s only true when a matrix is either lower triangular (the elements above the diagonal are all zero), upper triangular (the elements below the diagonal are all zero), or diagonal (the elements off the diagonal are all zero). In this example, \\(\\mathbf{A}\\) was lower triangular and \\(\\mathbf{B}\\) was upper triangular. I chose both to have the same diagonal elements (in different order) on purpose.\nMore generally, if \\(\\mathbf{A}\\) is diagonal or upper/lower triangular, then\n\\[\\text{det}(\\mathbf{A}) = \\prod_{i=0}^{n-1} A_{i,i} = A_{0,0} A_{1,1} \\cdots A_{n-1,n-1}.\\]\nIt’s not yet obvious, but we can always “change” a square matrix \\(\\mathbf{A}\\) into one of these three kinds of matrices, and then calculate the determinant of \\(\\mathbf{A}\\) this way. There are a few ways to do this. I’ll cover these when I get to matrix factorizations below.\nSome other properties of the determinant that you can verify are,\n\n\\(\\text{det}(\\mathbf{I}) = 1\\).\n\\(\\text{det}(\\mathbf{A}^\\top) = \\text{det}(\\mathbf{A})\\).\n\\(\\text{det}(\\mathbf{A}^{-1}) = \\frac{1}{\\text{det}(\\mathbf{A})}\\).\n\\(\\text{det}(c\\mathbf{A}) = c^n\\text{det}(\\mathbf{A})\\).\n\nThe determinant is one important way to get a scalar out of a matrix. Another useful scalar is the trace, which is far simpler to calculate. The trace of a matrix \\(\\mathbf{A}\\) is the sum of its diagonal elements, usually written\n\\[\\text{tr}(\\mathbf{A}) = \\sum_{i=0}^{n-1} A_{i,i} = A_{0,0} + A_{1,1} + \\cdots + A_{n-1,n-1}.\\]\nUnlike the determinant, the trace doesn’t split up over products. It instead splits over addition,\n\\[\\text{tr}(\\mathbf{A} + \\mathbf{B}) = \\text{tr}(\\mathbf{A}) + \\text{tr}(\\mathbf{B}).\\]\nThis is very easy to verify from the fact that the sum is element-wise, so \\(\\sum (A+B)_{i,i} = \\sum A_{i,i} + \\sum B_{i,i}\\).\nSome other fairly trivial properties the trace satisfies are,\n\n\\(\\text{tr}(\\mathbf{I}) = n\\).\n\\(\\text{tr}(\\mathbf{A}^\\top) = \\text{tr}(\\mathbf{A})\\).\n\\(\\text{tr}(c\\mathbf{A}) = c\\text{tr}(\\mathbf{A})\\).\n\\(\\text{tr}(\\mathbf{A}\\mathbf{B}) = \\text{tr}(\\mathbf{B}\\mathbf{A})\\).\n\nHere’s a “proof” of the last result on the same \\(3 \\times 3\\) matrices above. In numpy, you can calculate the trace using np.trace. It’s not unstable like the determinant is, and it’s fast to calculate since it’s only summing the \\(n\\) diagonal terms, which is \\(O(n)\\) time.\n\n\nCode\ntr_AB = np.trace(A @ B)\ntr_BA = np.trace(B @ A)\nprint(f'tr(AB) = {tr_AB}')\nprint(f'tr(BA) = {tr_BA}')\n\n\ntr(AB) = 13\ntr(BA) = 13\n\n\nIt’s kind of obvious what the determinant is good for. It tells you how invertible a matrix is. But what does the trace tell you? It turns out both the trace and the determinant also tell you something important about the scale of the matrix. We’ll see this in more depth below when we talk about eigenvalues.\n\n\n7.1.5 Linear Independence and Rank\nWe can always think of a matrix in terms of its column vectors. If \\(\\mathbf{A}\\) is \\(m \\times n\\), it has \\(n\\) column vectors \\(\\mathbf{A}_{:, 0}, \\mathbf{A}_{:, 1}, \\cdots, \\mathbf{A}_{:, n-1}\\) each of size \\(m\\). Concatenated together in order, the column vectors form the matrix itself,\n\\[\n\\mathbf{A} =\n\\begin{pmatrix}\n\\mathbf{A}_{:, 0} & \\mathbf{A}_{:, 1} & \\cdots & \\mathbf{A}_{:, n-1}\n\\end{pmatrix}.\n\\]\nIt turns out these column vectors also tell us how invertible a matrix is, but in a more general and useful way than the determinant does. Roughly speaking, a matrix is invertible if we can’t write any one column vector as a function of the other column vectors. This is just the definition of linear independence.\nRecall a set of vectors \\(\\mathbf{x}_0, \\mathbf{x}_1, \\cdots, \\mathbf{x}_{k-1}\\) is linearly independent if no one vector is a linear combination of the rest,\n\\[\\mathbf{x}_j \\neq \\sum_{i \\neq j} c_i \\mathbf{x}_j.\\]\nIf one vector is a linear combination of the rest, they’re linearly dependent.\nAn \\(n \\times n\\) matrix \\(\\mathbf{A}\\) is invertible if and only if its column vectors are all linearly independent. Equivalently, the column vectors span an \\(n\\)-dimensional vector space. To see why this is true, let’s look at a \\(2 \\times 2\\) matrix \\(\\mathbf{A}\\) with column vectors \\(\\mathbf{a}=\\binom{a}{b}\\) and \\(\\mathbf{b}=\\binom{c}{d}\\),\n\\[\n\\mathbf{A} = \\begin{pmatrix} \\mathbf{a} & \\mathbf{b} \\end{pmatrix} =\n\\begin{pmatrix}\na & b \\\\\nc & d \\\\\n\\end{pmatrix}.\n\\]\nNow, if \\(\\mathbf{a}\\) and \\(\\mathbf{b}\\) are linearly dependent, then \\(\\mathbf{b}\\) must be a scalar multiple of \\(\\mathbf{a}\\), say \\(\\mathbf{b} = \\beta \\mathbf{a}\\). Then \\(\\mathbf{A}\\) would look like\n\\[\n\\mathbf{A} = \\begin{pmatrix} \\mathbf{a} & \\beta \\mathbf{a} \\end{pmatrix} =\n\\begin{pmatrix}\na & \\beta a \\\\\nc & \\beta c \\\\\n\\end{pmatrix}.\n\\]\nThis means its determinant would be \\(\\text{det}(\\mathbf{A}) = \\beta ac - \\beta ac = 0\\), which of course means \\(\\mathbf{A}\\) can’t be invertible.\nGraphically, saying the column vectors are linearly dependent is saying they’ll map any vector onto the same subspace. For the \\(2 \\times 2\\) case, that means any vector \\(\\mathbf{v}\\) hit by \\(\\mathbf{A}\\) will get mapped onto the same line, no matter what \\(\\mathbf{v}\\) you pick. The matrix is collapsing, or projecting, the vector space down to a lower-dimensional subspace.\nHere’s a plot of this idea. I’ll make \\(\\mathbf{A}\\) have two linearly dependent columns, then plot its action on several different vectors, plotted in black. Acting on these by \\(\\mathbf{A}\\) will map them to the red vectors, which all lie on the same line in the plane. They’re all collapsing onto the same subspace, evidently the line \\(y=-x\\).\n\n\nCode\nbeta = 1.5\na0 = np.array([1, -1]).reshape(-1, 1)\na1 = beta * a0\nA = np.hstack([a0, a1])\nv = np.array([1, 1]).reshape(-1, 1)\nw = np.array([-1, 0]).reshape(-1, 1)\nu = np.array([1, -3]).reshape(-1, 1)\nvectors = [x.flatten() for x in [v, A @ v, w, A @ w, u, A @ u]]\n\nplot_vectors(vectors, colors=['black', 'red'] * 3, title='Linearly Dependence',\n             labels=['$\\mathbf{v}$', '$\\mathbf{A}\\mathbf{v}$'] + [''] * 4,\n             text_offsets=[[0, 0]] * 6, headwidth=5)\n\n\n\n\n\n\n\n\n\nThe number of linearly independent column vectors a matrix has is called its rank, written \\(\\text{rank}(\\mathbf{A})\\). Clearly it’ll always be the case that \\(\\text{rank}(\\mathbf{A}) \\leq n\\). When \\(\\text{rank}(\\mathbf{A}) = n\\) exactly the matrix is called full rank. Only full rank square matrices are invertible.\nHere’s an example. I’ll use np.linalg.matrix_rank(A) to calculate the rank of the above \\(2 \\times 2\\) example. Since \\(\\text{rank}(\\mathbf{A})=1<2\\), the matrix \\(\\mathbf{A}\\) must be singular, as I’ve of course already shown.\n\n\nCode\nrank = np.linalg.matrix_rank(A)\nprint(f'rank(A) = {rank}')\n\n\nrank(A) = 1\n\n\nThe idea of rank also extends to rectangular matrices. An \\(m \\times n\\) matrix \\(\\mathbf{A}\\) is called full rank if the vectors in its minimum dimension are linearly independent. That is, if \\(m < n\\), the row vectors must all be linearly independent. And if \\(m > n\\) the column vectors must all be linearly independent.\n\n\n7.1.6 Outer Products\nWe’ll frequently be interested in low rank matrices, which are matrices whose rank is much much less than the dimension, i.e. \\(\\text{rank}(\\mathbf{A}) \\ll n\\). As we’ll see, low rank matrices are special because they can efficiently compress the information contained in a matrix, which often allows us to represent data more efficiently, or clean up data by denoising away “unnecessary” dimensions. In fact, approximating a matrix with a lower rank matrix is the whole idea behind dimension reduction, one of the core areas of unsupervised learning.\nThe most useful low-rank matrices are the outer products of two vectors. If \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are size \\(n\\) vectors, define their outer product by\n\\[\n\\mathbf{x} \\mathbf{y}^\\top =\n\\begin{pmatrix}\nx_0 y_0 & x_0 y_1 & \\cdots & x_0 y_{n-1} \\\\\nx_1 y_0 & x_1 y_1 & \\cdots & x_1 y_{n-1} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nx_{n-1} y_0 & x_{n-1} y_1 & \\cdots & x_{n-1} y_{n-1} \\\\\n\\end{pmatrix}.\n\\]\nNotice each column vector \\(\\mathbf{A}_{:, j}\\) of the outer product matrix is linearly proportional to the first column \\(\\mathbf{A}_{:, 0}\\), since\n\\[\\mathbf{A}_{:, j} = \\mathbf{x} y_j = \\mathbf{x} y_0 \\frac{y_j}{y_0} = \\frac{y_j}{y_0} \\mathbf{A}_{:, 0}.\\]\nThis means that only one column vector is linearly independent, which implies \\(\\text{rank}(\\mathbf{x} \\mathbf{y}^\\top)=1\\). The outer product is evidently rank-1, and hence highly singular. You’d never be able to invert it. But it is useful as we’ll see soon.\nHere’s an example of an outer product calculation. You can either calculate x @ y.T directly or use np.outer(x, y). Since both vectors are size \\(3\\), the outer product should be a \\(3 \\times 3\\) matrix with rank-1.\n\n\nCode\nx = np.array([1, 2, 3])\ny = np.array([3, 2, 1])\nouter = np.outer(x, y)\nprint(f'xy^T = \\n{outer}')\nprint(f'rank(xy^T) = {np.linalg.matrix_rank(outer)}')\n\n\nxy^T = \n[[3 2 1]\n [6 4 2]\n [9 6 3]]\nrank(xy^T) = 1\n\n\nYou can think of the outer product matrix as a kind of projection matrix. It always projects vectors onto the same one-dimensional line in \\(\\mathbb{R}^n\\). Why? Suppose \\(\\mathbf{v}\\) is some vector. If we hit it with the outer product matrix \\(\\mathbf{x} \\mathbf{y}^\\top\\), using the fact matrix multiplication is associative, we get\n\\[(\\mathbf{x} \\mathbf{y}^\\top) \\mathbf{v} = \\mathbf{x} (\\mathbf{y}^\\top \\mathbf{v}) = (\\mathbf{y} \\cdot \\mathbf{v}) \\mathbf{x}.\\]\nThat is, \\(\\mathbf{v}\\) just gets projected onto the space spanned by the vector \\(\\mathbf{x}\\). Evidently the other outer product vector \\(\\mathbf{y}\\) determines how long the projection vector will be. Here’s a visual representation of this idea for 2-dimensional vectors. Take\n\\[\\begin{align*}\n\\mathbf{x} &= (1, 1) \\\\\n\\mathbf{y} &= (1, -1) \\\\\n\\mathbf{v}_0 &= (-1, 2) \\quad &\\Longrightarrow \\quad (\\mathbf{y} \\cdot \\mathbf{v}_0) \\mathbf{x} &= (-3, -3) \\\\\n\\mathbf{v}_1 &= (2, 0) \\quad &\\Longrightarrow \\quad (\\mathbf{y} \\cdot \\mathbf{v}_1) \\mathbf{x} &= (2, 2) \\\\\n\\mathbf{v}_2 &= (2, -1) \\quad &\\Longrightarrow \\quad (\\mathbf{y} \\cdot \\mathbf{v}_2) \\mathbf{x} &= (3, 3). \\\\\n\\end{align*}\\]\nApplying the outer product \\(\\mathbf{x} \\mathbf{y}^\\top\\) to each \\(\\mathbf{v}_i\\) should project each vector onto the space spanned by \\(\\mathbf{x}=(1, 1)\\), which is just the line \\(y=x\\). Notice the projections are all proportional to \\((1, 1)\\), as they should be. In the plot below, each vector and its projection have the same color. The outer product vectors \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are shown in black.\n\n\nCode\nx = np.array([1, 1]).reshape(-1, 1)\ny = np.array([1, -1]).reshape(-1, 1)\nvs = [np.array([-1, 2]).reshape(-1, 1), \n      np.array([2, 0]).reshape(-1, 1), \n      np.array([2, -1]).reshape(-1, 1)]\nws = [(x @ y.T) @ v for v in vs]\nvectors = [vector.flatten() for vector in vs + ws + [x, y]]\nplot_vectors(\n    vectors, colors=['salmon', 'limegreen', 'steelblue'] * 2 + ['black', 'black'], headwidth=5, width=0.01,\n    labels=['$\\mathbf{v}_0$', '$\\mathbf{v}_1$', '$\\mathbf{v}_2$'] + [''] * 3 + ['$\\mathbf{x}$', '$\\mathbf{y}$'],\n    text_offsets = [[0, 0.2], [0, 0.2], [0.1, -0.3]] + [[0,0]] * 3 + [[-0.4, 0.15], [0, -0.3]], ticks_every=1,\n    title='Outer Product Projections', zorders=[0, 5, 1, 2, 4, 3, 4, 6, 7], xlim=(-3.5, 3.5), ylim=(-3.5, 3.5))"
  },
  {
    "objectID": "notebooks/matrices.html#special-matrices",
    "href": "notebooks/matrices.html#special-matrices",
    "title": "7  Matrix Algebra",
    "section": "7.2 Special Matrices",
    "text": "7.2 Special Matrices\nThere are many classes of matrices that have various special properties. I’ll quickly introduce a few that’ll be of interest to us in machine learning.\n\n7.2.1 Diagonal Matrices\nProbably the most basic class of matrices are the diagonal matrices. A diagonal matrix is an \\(m \\times n\\) matrix \\(\\mathbf{D}\\) whose elements are only non-zero on the diagonals, i.e. \\(D_{i,j} = 0\\) if \\(i \\neq j\\). For example, the following \\(3 \\times 3\\) matrix is diagonal since its only non-zero values lie on the diagonal,\n\\[\n\\mathbf{D} =\n\\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 2 & 0 \\\\\n0 & 0 & 3 \\\\\n\\end{pmatrix}.\n\\]\nWe’ve already seen an important diagonal matrix a few times, the identity matrix \\(\\mathbf{I}\\). The identity matrix is the diagonal matrix whose diagonal entries are all ones. It’s common to short-hand a diagonal matrix by just specifying its diagonal entries as a vector. In this notation, we’d use the short-hand\n\\[\\mathbf{D} = \\text{diag}(1,2,3).\\]\nto refer to the matrix in the above example. It means exactly the same thing, we’re just only specifying the diagonal elements. This is also the easiest way to define a diagonal matrix in numpy, by using np.diag. Notice that a diagonal matrix contains \\(n^2\\) elements, but we only need to specify \\(n\\) of them to fully determine what the matrix is (i.e. the diagonal elements themselves).\nIn a sense, a diagonal matrix can only scale a vector it acts on, not rotate it or reflect it. This is because multiplying diagonal matrix with a vector is equivalent to element-wise multiplying the diagonal elements with the vector, which causes each vector component to get stretched by some amount. For example, if \\(\\mathbf{x}=(1,1,1)\\), when the above example \\(\\mathbf{D}\\) acts on it, we’d get\n\\[\n\\mathbf{D}\\mathbf{x} =\n\\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 2 & 0 \\\\\n0 & 0 & 3 \\\\\n\\end{pmatrix}\n\\begin{pmatrix}\n1 \\\\\n1 \\\\\n1 \\\\\n\\end{pmatrix} =\n\\begin{pmatrix}\n1 \\\\\n2 \\\\\n3 \\\\\n\\end{pmatrix} =\n\\begin{pmatrix}\n1 \\\\\n2 \\\\\n3 \\\\\n\\end{pmatrix} \\circ\n\\begin{pmatrix}\n1 \\\\\n1 \\\\\n1 \\\\\n\\end{pmatrix}.\n\\]\nHere’s an example of how to define a diagonal matrix in numpy using np.diag. I’ll define the same matrix as the above example, and then act on the same vector to show it just scales the entries.\n\n\nCode\nD = np.diag([1, 2, 3])\nx = np.array([1, 1, 1]).reshape(-1, 1)\nprint(f'D = diag(1,2,3) = \\n{D}')\nprint(f'Dx = {(D @ x).flatten()}')\n\n\nD = diag(1,2,3) = \n[[1 0 0]\n [0 2 0]\n [0 0 3]]\nDx = [1 2 3]\n\n\n\n\n7.2.2 Symmetric Matrices\nAnother special class of matrices important to machine learning is the symmetric matrix. A symmetric matrix is a square matrix \\(\\mathbf{S}\\) that equals its own transpose, i.e. \\(\\mathbf{S}^\\top = \\mathbf{S}\\). They’re called symmetric matrices because their lower diagonals and upper diagonals are mirror images. Symmetric matrices can be thought of as the matrix equivalent of a real number.\nFor example, consider the matrix \\[\n\\mathbf{S} =\n\\begin{pmatrix}\n1 & -1 & -2 \\\\\n-1 & 2 & 1 \\\\\n-2 & 1 & 3 \\\\\n\\end{pmatrix}.\n\\]\nThis matrix is symmetric since the upper diagonal and lower diagonal are the same, i.e. \\(S_{i,j} = S_{j,i}\\). Symmetric matrices are very important as we’ll see. They’re the matrix generalization of the idea of a real number.\nSince the lower diagonal and upper diagonal of a symmetric matrix always equal, we only need to specify what the diagonal and upper diagonal are to fully determine the matrix. If \\(\\mathbf{S}\\) contains \\(n^2\\) entries, only \\[n + \\frac{1}{2}(n^2 - n) = \\frac{1}{2}n(n+1)\\]\nof those elements are actually unique. This fact can be used to shave a lot of time off of algorithms involving symmetric matrices. In numpy, you can check a matrix \\(\\mathbf{S}\\) is symmetric by checking that it equals its transpose. Due to numerical roundoff, you may want to wrap the condition inside np.allclose.\n\n\nCode\nS = np.array([\n    [1, -1, -2],\n    [-1, 2, 1],\n    [-2, 1, 3]])\nis_symmetric = lambda A: np.allclose(A, A.T)\nis_symmetric(S)\n\n\nTrue\n\n\n\n\n7.2.3 Upper and Lower Triangular Matrices\nClosely related to diagonal matrices are lower and upper triangular matrices. An \\(m \\times n\\) matrix \\(\\mathbf{L}\\) is lower-triangular if the entries in its upper diagonal are zero, i.e. \\(L_{i,j} = 0\\) when \\(i < j\\). Similarly, an \\(m \\times n\\) matrix \\(\\mathbf{U}\\) is upper-triangular if the entries in its lower diagonal are zero, i.e. \\(U_{i,j} = 0\\) when \\(i > j\\). I’ve already showed an example of these when I covered determinants. Here they are again,\n\\[\n\\mathbf{L} =\n\\begin{pmatrix}\n3 & 0 & 0 \\\\\n1 & 2 & 0 \\\\\n1 & 1 & 1 \\\\\n\\end{pmatrix}, \\qquad \\mathbf{U} =\n\\begin{pmatrix}\n1 & 1 & 1 \\\\\n0 & 2 & 1 \\\\\n0 & 0 & 3 \\\\\n\\end{pmatrix}.\n\\]\nUpper and lower triangular (and diagonal) matrices are useful because it’s easy to invert them and calculate their determinants. Just like symmetric matrices, only \\(\\frac{1}{2}n(n+1)\\) unique elements are needed to fully specify these matrices since an entire off-diagonal is all zeros.\n\n\n7.2.4 Orthogonal Matrices\nThe next class of matrices I’ll introduce are more subtle, but very important geometrically. These are the orthogonal matrices. An orthogonal matrix is an \\(n \\times n\\) matrix \\(\\mathbf{Q}\\) whose transpose is its inverse, i.e.\n\\[\\mathbf{Q}^\\top = \\mathbf{Q}^{-1} \\quad \\text{or} \\quad \\mathbf{Q}^\\top \\mathbf{Q}=\\mathbf{I}.\\]\nAs an example, consider the following matrix,\n\\[\n\\mathbf{Q} = \\frac{1}{\\sqrt{2}}\n\\begin{pmatrix}\n1 & 1 \\\\\n1 & -1 \\\\\n\\end{pmatrix} =\n\\begin{pmatrix}\n\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\\\\n\\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}} \\\\\n\\end{pmatrix}.\n\\]\nWe can check \\(\\mathbf{Q}\\) is orthogonal by checking it satisfies the condition \\(\\mathbf{Q}^\\top \\mathbf{Q}=\\mathbf{I}\\),\n\\[\n\\mathbf{Q}^\\top \\mathbf{Q} =\n\\frac{1}{\\sqrt{2}}\n\\begin{pmatrix}\n1 & 1 \\\\\n1 & -1 \\\\\n\\end{pmatrix}\n\\frac{1}{\\sqrt{2}}\n\\begin{pmatrix}\n1 & 1 \\\\\n1 & -1 \\\\\n\\end{pmatrix} =\n\\frac{1}{2}\n\\begin{pmatrix}\n2 & 0 \\\\\n0 & 2 \\\\\n\\end{pmatrix} =\n\\begin{pmatrix}\n1 & 0 \\\\\n0 & 1 \\\\\n\\end{pmatrix} = \\mathbf{I}.\n\\]\nNotice from this example that the column vectors \\(\\mathbf{q}_0, \\mathbf{q}_1\\) form an orthonormal basis for \\(\\mathbb{R}^2\\), since\n\\[\\mathbf{q}_0 \\cdot \\mathbf{q}_1 = 0, \\quad \\mathbf{q}_0 \\cdot \\mathbf{q}_0 = \\mathbf{q}_1 \\cdot \\mathbf{q}_1 = 1.\\]\nThis is a general fact. The column vectors of an orthogonal matrix \\(\\mathbf{Q}\\) form a complete set of orthonormal basis vectors for \\(\\mathbb{R}^n\\). Conversely, we can always form an orthogonal matrix by first finding an orthonormal basis and then creating column vectors out of the basis vectors. This is usually the way orthogonal matrices are constructed in practice using algorithms like the Gram-Schmidt Algorithm.\nIt’s not at all obvious, but the fact that the column vectors of \\(\\mathbf{Q}\\) form an orthonormal basis constrains the number of unique elements \\(\\mathbf{Q}\\) is allowed to have. Requiring each \\(\\mathbf{q}_i\\) means \\(n\\) total elements are already determined. The further requirement that the column vectors be mutually orthogonal determines another \\(\\frac{1}{2}n(n-1)\\). This means \\(\\mathbf{Q}\\) only has \\(n^2 - n - \\frac{1}{2}n(n-1) = \\frac{1}{2}n(n-1)\\) unique elements. For example, when \\(\\mathbf{Q}\\) is \\(2 \\times 2\\) it only has \\(\\frac{1}{2}2(2-1)=1\\) unique element. The other \\(3\\) are all determined by that one element. This unique element can be thought of as a rotation angle. I’ll come back to this in a minute.\nAn important fact about orthogonal matrices is that they preserve the dot products between vectors. If \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are two vectors, then\n\\[(\\mathbf{Q} \\mathbf{x}) \\cdot (\\mathbf{Q}\\mathbf{y}) = \\mathbf{x} \\cdot \\mathbf{y}.\\]\nThis follows from the fact that \\((\\mathbf{Q} \\mathbf{x})^\\top (\\mathbf{Q} \\mathbf{y}) = \\mathbf{x}^\\top \\mathbf{Q}^\\top\\mathbf{Q}\\mathbf{y} = \\mathbf{x}^\\top \\mathbf{I} \\mathbf{y} = \\mathbf{x}^\\top \\mathbf{y}\\). Since the dot product encodes the notions of length and angle, this fact implies that orthogonal matrices can’t change the lengths of vectors, nor the angles between vectors. Orthogonal matrices preserve the geometry of the vector space.\nThis fact suggests some deep intuition about what orthogonal matrices do. If they can’t change the lengths of vectors or the angles between them, then all they can do is rotate vectors or reflect them across some line. In fact, it turns out any \\(2 \\times 2\\) orthogonal matrix can be written in the form\n\\[\n\\mathbf{Q} =\n\\begin{pmatrix}\n\\cos \\theta & \\mp \\sin \\theta \\\\\n\\sin \\theta & \\pm \\cos \\theta \\\\\n\\end{pmatrix},\n\\]\nwhere \\(\\theta\\) is some angle (expressed in radians). When the right column vector is \\(\\binom{-\\sin\\theta}{\\cos\\theta}\\), \\(\\mathbf{Q}\\) is a pure rotation matrix. It will rotate any vector in the plane by an angle \\(\\theta\\), counterclockwise if \\(\\theta > 0\\), and clockwise if \\(\\theta < 0\\). When the right column vector is \\(\\binom{\\sin\\theta}{-\\cos\\theta}\\), \\(\\mathbf{Q}\\) becomes a reflection matrix; it’ll reflect vectors about the line at an angle of \\(\\frac{\\theta}{2}\\) with the x-axis. The combination of these two together can generate any 2D rotation or reflection.\nHere’s a visual of this idea. I’ll take the unit vector \\(\\mathbf{e}_x=(1,0)\\) and use \\(\\mathbf{Q}\\) to rotate it by some angle, in this case \\(\\theta = 45^\\circ\\). Note the need to convert the angle to radians by multiplying the angle in degrees by \\(\\frac{\\pi}{180}\\). You should be able to confirm that the red vector is indeed the black vector \\(\\mathbf{e}_x\\) rotated counterclockwise by \\(45^\\circ\\) to the new vector \\(\\mathbf{Q}\\mathbf{e}_x = 2^{-1/2}(1,1)\\). The factor of \\(2^{-1/2}\\) appears to keep the vector normalized to unit length.\n\n\nCode\ntheta_degrees = 45\ntheta = theta_degrees * np.pi / 180\nQ = np.array([\n    [np.cos(theta), -np.sin(theta)], \n    [np.sin(theta), np.cos(theta)]])\nex = np.array([1, 0]).reshape(-1, 1)\nQex = Q @ ex\nplot_vectors([ex.flatten(), Qex.flatten()], colors=['black', 'red'], title=f'${theta_degrees}^\\circ$ Rotation',\n             labels=['$\\mathbf{e}_x$', '$\\mathbf{Q}\\mathbf{e}_x$'], text_offsets=[[-0.1, 0.1], [0, 0]],\n             ticks_every=1, xlim=(-0.5, 1.5), ylim=(-0.5, 1.5))\n\n\n\n\n\n\n\n\n\nI’ll finish this section by noting that orthogonal matrices always have determinant \\(\\pm 1\\). You can see this by applying the determinant product formula to \\(\\mathbf{Q}^\\top \\mathbf{Q}=\\mathbf{I}\\),\n\\[1 = \\text{det}(\\mathbf{I}) = \\text{det}(\\mathbf{Q}^\\top \\mathbf{Q}) = \\text{det}(\\mathbf{Q}^\\top) \\cdot \\text{det}(\\mathbf{Q}) = \\big(\\text{det}(\\mathbf{Q})\\big)^2,\\]\nwhich implies \\(\\text{det}(\\mathbf{Q}) = \\pm 1\\). This evidently divides orthogonal matrices into two distinct classes:\n\n\\(\\text{det}(\\mathbf{Q}) = +1\\): These are the orthogonal matrices that correspond to pure rotations.\n\\(\\text{det}(\\mathbf{Q}) = -1\\): These are the orthogonal matrices that correspond to reflections.\n\nI’ll verify that the rotation matrix I just plotted indeed has determinant \\(+1\\).\n\n\nCode\nprint(f'det(Q) = {np.linalg.det(Q)}')\n\n\ndet(Q) = 1.0\n\n\n\n\n7.2.5 Block Matrices\nSometimes a matrix might look kind of diagonal or triangular, but not exactly. For example, consider the following matrix,\n\\[\n\\mathbf{A} =\n\\begin{pmatrix}\n1 & 2 & 0 & 0 \\\\\n3 & 4 & 0 & 0 \\\\\n0 & 0 & 5 & 6 \\\\\n0 & 0 & 7 & 8 \\\\\n\\end{pmatrix}.\n\\]\nThis matrix looks kind of diagonal, but not exactly. Notice, however, that we can think about this matrix as being composed of four sub-matrices, or blocks,\n\\[\n\\mathbf{A}_{0,0} =\n\\begin{pmatrix}\n1 & 2 \\\\\n3 & 4 \\\\\n\\end{pmatrix}, \\quad\n\\mathbf{A}_{0,1} =\n\\begin{pmatrix}\n0 & 0 \\\\\n0 & 0 \\\\\n\\end{pmatrix}, \\quad\n\\mathbf{A}_{1,0} =\n\\begin{pmatrix}\n0 & 0 \\\\\n0 & 0 \\\\\n\\end{pmatrix}, \\quad\n\\mathbf{A}_{1,1} =\n\\begin{pmatrix}\n5 & 6 \\\\\n7 & 8 \\\\\n\\end{pmatrix}.\n\\]\nIf we think of \\(\\mathbf{A}\\) in terms of these 4 blocks, we can write it simply as\n\\[\n\\mathbf{A} =\n\\begin{pmatrix}\n1 & 2 & 0 & 0 \\\\\n3 & 4 & 0 & 0 \\\\\n0 & 0 & 5 & 6 \\\\\n0 & 0 & 7 & 8 \\\\\n\\end{pmatrix} =\n\\begin{pmatrix}\n\\begin{pmatrix} 1 & 2 \\\\ 3 & 4 \\\\ \\end{pmatrix} & \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\\\ \\end{pmatrix} \\\\\n\\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\\\ \\end{pmatrix} & \\begin{pmatrix} 5 & 6 \\\\ 7 & 8 \\\\ \\end{pmatrix} \\\\\n\\end{pmatrix} =\n\\begin{pmatrix}\n\\mathbf{A}_{0,0} & \\mathbf{A}_{0,1} \\\\\n\\mathbf{A}_{1,0} & \\mathbf{A}_{1,1} \\\\\n\\end{pmatrix}.\n\\]\nWhere the braces are doesn’t really effect anything other than how the elements are indexed. What matters is we can express this \\(4 \\times 4\\) matrix as a \\(2 \\times 2\\) block matrix that semantically represents the exact same matrix. Notice that in block form \\(\\mathbf{A}\\) is now a diagonal matrix. We call a matrix that can be blocked into diagonal form like this block diagonal. Block diagonal matrices are the most useful of the block matrices. If a block matrix is upper or lower triangular, we’d call it a block upper triangular or block lower triangular matrix, respectively.\nWhen matrices are in block form, you can manipulate them exactly the way you would if they weren’t. The only difference is that you have to remember matrix multiplication doesn’t commute. For example, we could write \\(\\mathbf{A}^2 = \\mathbf{A}\\mathbf{A}\\) in block form as\n\\[\n\\mathbf{A}^2 = \\mathbf{A}\\mathbf{A} =\n\\begin{pmatrix}\n\\mathbf{A}_{0,0} & \\mathbf{A}_{0,1} \\\\\n\\mathbf{A}_{1,0} & \\mathbf{A}_{1,1} \\\\\n\\end{pmatrix}\n\\begin{pmatrix}\n\\mathbf{A}_{0,0} & \\mathbf{A}_{0,1} \\\\\n\\mathbf{A}_{1,0} & \\mathbf{A}_{1,1} \\\\\n\\end{pmatrix} =\n\\begin{pmatrix}\n\\mathbf{A}_{0,0}\\mathbf{A}_{0,0} + \\mathbf{A}_{0,1}\\mathbf{A}_{1,0}  & \\mathbf{A}_{0,0}\\mathbf{A}_{0,1} + \\mathbf{A}_{0,1}\\mathbf{A}_{1,1} \\\\\n\\mathbf{A}_{1,0}\\mathbf{A}_{0,0} + \\mathbf{A}_{1,1}\\mathbf{A}_{1,0} & \\mathbf{A}_{1,0}\\mathbf{A}_{0,1} + \\mathbf{A}_{1,1}\\mathbf{A}_{1,1} \\\\\n\\end{pmatrix}.\n\\]\nThis would give the exact same answer as multiplying \\(\\mathbf{A}^2 = \\mathbf{A}\\mathbf{A}\\) in regular form, except we’d have extra braces floating around that we can ignore.\nNow, you might ask why we even care about blocking matrices like this. Probably the most important reason we care is hardware. Computer memory is typically divided into a sequence of fixed-sized blocks. When we want to operate on an array, the system has to go into memory and fetch where those array values are stored, perform the array operation, and then place the answer back into memory. During the fetch step, it will take a long time if the array elements are located far away from each other in different blocks. For this reason, programs tend to place array elements nearby each other in memory. But when one block runs out, the program has to go find a new block of memory to place the other elements. This suggests that if we want to efficiently fetch array elements from memory, we should do so block by block. That is, we should find a way to partition the array so each block of the array fits in the same block of memory. If we do this, we can perform operations much faster than we would if the program had to search new blocks every time it needed to perform an operation.\nThe best example of this is matrix multiplication. While it might take \\(O(n^3)\\) time to multiply two arrays in theory, don’t forget that asymptotic notation has a hidden constant term out front that we ignore. In real life, that constant can make a big difference. If we try to multiply two matrices without doing any blocking, we’d have a much larger constant than if we first blocked the matrices into blocks that would fit in one block of memory. In fact, this is what the LAPACK routines behind functions like np.matmul do. They don’t just naively multiply two matrices by running over a triple loop. They first block both matrices into sub-matrix blocks that fit efficiently in memory, and then run the triple loop block-style before putting everything back together. It’s for this reason more than anything else that numpy array methods run much faster than anything you’d write in python. When it comes to array operations, numpy and the LAPACK routines will swim laps around anything you’d code up yourself.\n\n\n7.2.6 Sparse Matrices\nA very useful class of matrices in applications are the sparse matrices. Sparse matrices are defined by the property that most of their entries are zero. Only a sparse number of elements are non-zero. When a matrix is sparse, we can often more efficiently store its elements using a different data structure that only keeps track of the non-zero elements and where they occur in the matrix. We can then define matrix algorithms in a way that they only operate on the non-zero entries, which can considerably speed up computation.\nFor example, suppose an \\(n \\times n\\) matrix \\(\\mathbf{A}\\) is sparse, with \\(k \\ll n\\) non-zero entries. If we wanted to multiply \\(\\mathbf{A}\\) with itself, it would usually take \\(O(n^3)\\) time and require \\(O(n^2)\\) words of memory. But, since \\(\\mathbf{A}\\) is sparse, we could multiply it with itself in \\(O(k^3)\\) and use only \\(O(k)\\) words of memory. The speedup comes from the fact that we only need to keep track of the non-zero elements when adding and multiplying elements in the matrix, which means we only need to keep track of the \\(k\\) non-zero elements in the multiplication.\nFor ordinary sized matrices, treating them as sparse doesn’t really benefit you much. It’s when matrices get huge that sparse methods can be useful. One example of this that comes up in machine learning is when we want to represent a corpus of text as a matrix of data. In that case, each row would be a document of text, and each column would be a word in the vocabulary of all possible words. Vocabulary sizes can get huge, often millions of words. If you have, say, 10,000 documents, that means you’d have a 10,000 by 1,000,000 matrix of data, which is pretty huge. Fortunately, any one document only contain a handful of words in the total vocabulary. This means the data matrix is sparse, and we can efficiently manipulate it using sparse methods.\nNumpy doesn’t have any direct methods to work with sparse matrices, but scipy does. To define a matrix as sparse in scipy, use scipy.sparse.csr_matrix. This will encode a sparse matrix using a CSR matrix, which is one of several ways to efficiently represent a sparse matrix. Once we’ve encoded a matrix as sparse, we can more or less use any of the operations we’re used to. To convert a sparse matrix back to a normal, dense matrix, use A.todense(). Here’s an example. I’ll convert the following matrix into sparse form,\n\\[\n\\mathbf{A} =\n\\begin{pmatrix}\n1 & 0 & 0 & 0 \\\\\n0 & 2 & 0 & 0 \\\\\n3 & 0 & 0 & 4 \\\\\n0 & 5 & 0 & 6 \\\\\n\\end{pmatrix}.\n\\]\nNotice how it’s only keeping track of the non-zero values and where in the matrix they occur.\n\n\nCode\nfrom scipy.sparse import csr_matrix\n\nA = np.array([\n    [1, 0, 0, 0], \n    [0, 2, 0, 0], \n    [3, 0, 0, 4], \n    [0, 5, 0, 6]])\nA_sparse = csr_matrix(A)\nprint(f'A_sparse = \\n{A_sparse}')\n\n\nA_sparse = \n  (0, 0)    1\n  (1, 1)    2\n  (2, 0)    3\n  (2, 3)    4\n  (3, 1)    5\n  (3, 3)    6\n\n\n\n\nCode\nA_dense = A_sparse.todense()\nprint(f'A_dense = \\n{A_dense}')\n\n\nA_dense = \n[[1 0 0 0]\n [0 2 0 0]\n [3 0 0 4]\n [0 5 0 6]]"
  },
  {
    "objectID": "notebooks/matrices.html#matrix-factorizations",
    "href": "notebooks/matrices.html#matrix-factorizations",
    "title": "7  Matrix Algebra",
    "section": "7.3 Matrix Factorizations",
    "text": "7.3 Matrix Factorizations\nGiven any two compatible matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\), we can get a third matrix \\(\\mathbf{C}\\) by matrix multiplication, \\(\\mathbf{C} = \\mathbf{A}\\mathbf{B}\\). Now suppose we wanted to go the other way. Given a matrix \\(\\mathbf{C}\\), how can we factor it back out into a product \\(\\mathbf{A}\\mathbf{B}\\)? This is the idea behind matrix factorization. In practice, we’re interested in factoring a matrix into a product of special types of matrices that are easier to work with, like symmetric, diagonal, or orthogonal matrices.\n\n7.3.1 LU Factorization\nProbably the most basic matrix factorization is the LU Factorization. LU factorization factors an \\(m \\times n\\) matrix \\(\\mathbf{A}\\) into a product of a lower triangular matrix \\(\\mathbf{L}\\) and an upper triangular matrix \\(\\mathbf{U}\\), \\[\\mathbf{A} = \\mathbf{L}\\mathbf{U}.\\]\nThe LU factorization is most useful for solving a system of linear equations. If \\(\\mathbf{A}\\mathbf{x}=\\mathbf{b}\\), we can do an LU factorization of \\(\\mathbf{A}\\) and write the system as \\(\\mathbf{LUx} = \\mathbf{b}\\). This can then be solved by breaking it into two steps, known as forward substitution and back substitution,\n\nForward substitution: Solve \\(\\mathbf{Ly} = \\mathbf{b}\\) for \\(\\mathbf{y}\\).\nBack Substitution: Solve \\(\\mathbf{Ux} = \\mathbf{y}\\) for \\(\\mathbf{x}\\).\n\nThese two steps are easy to do since each system can be solved by substitution, working from the “tip” of the triangle down. The LU factorization is essentially what matrix solvers like np.linalg.solve do to solve linear systems.\nOf course, the question still remains how to actually factor \\(\\mathbf{A}\\) into \\(\\mathbf{L}\\mathbf{U}\\). I won’t describe the algorithm to do this, or any matrix factorization really, since their inner workings aren’t that relevant to machine learning. If you’re curious, LU factorization is done using some variant of an algorithm known as Gaussian Elimination. Note the LU factorization in general is a cubic time algorithm, i.e. \\(O(n^3)\\) if \\(\\mathbf{A}\\) is \\(n \\times n\\).\nThe LU factorization can also be used to compute the determinant of a square matrix. Since \\(\\mathbf{L}\\) and \\(\\mathbf{U}\\) are triangular, their determinant is just the product of their diagonals. Using the product rule for determinants then gives\n\\[\\text{det}(\\mathbf{A}) = \\text{det}(\\mathbf{LU}) = \\text{det}(\\mathbf{L}) \\cdot \\text{det}(\\mathbf{U}) = \\prod_{i=0}^{n-1} L_{i,i} \\cdot U_{i,i}.\\]\nThe LU factorization can also be used to compute the inverse of a square matrix. The idea is to solve the matrix system of equations\n\\[\\mathbf{A} \\mathbf{X} = \\mathbf{I},\\]\nassuming \\(\\mathbf{X}=\\mathbf{A}^{-1}\\) are the \\(n^2\\) unknown variables you’re solving for. This system can be solved by using the same technique of forward substitution plus back substitution. Note that solving for both the determinant and inverse this way each takes \\(O(n^3)\\) time due to the LU decomposition. This is one reason why you should probably avoid calculating these quantities explicitly unless you really need them.\nStrangely, numpy doesn’t have a built-in LU factorization solver, but scipy does using scipy.linalg.lu. It factors a matrix into not two, but three products, \\(\\mathbf{A}=\\mathbf{PLU}\\). The \\(\\mathbf{P}\\) is a permutation matrix. It just accounts for the fact that sometimes you need to swap the rows before doing the LU factorization. I won’t go into that. Here’s the LU factorization of the above example matrix. I’ll also verify that \\(\\mathbf{A}=\\mathbf{LU}\\).\n\n\nCode\nfrom scipy.linalg import lu\n\nA = np.array([[1, 1], \n              [1, -1]])\nP, L, U = lu(A)\nprint(f'L = \\n{L}')\nprint(f'U = \\n{U}')\nprint(f'LU = \\n{L @ U}')\n\n\nL = \n[[1. 0.]\n [1. 1.]]\nU = \n[[ 1.  1.]\n [ 0. -2.]]\nLU = \n[[ 1.  1.]\n [ 1. -1.]]\n\n\n\n\n7.3.2 QR Factorization\nAnother useful factorization is to factor an \\(m \\times n\\) matrix \\(\\mathbf{A}\\) into a product of an \\(m \\times m\\) orthogonal matrix \\(\\mathbf{Q}\\) and an \\(m \\times n\\) upper triangular matrix \\(\\mathbf{R}\\),\n\\[\\mathbf{A} = \\mathbf{QR}.\\]\nThe QR factorization is useful if we want to create an orthonormal basis out of the column vectors of \\(\\mathbf{A}\\), since \\(\\mathbf{Q}\\) will give a complete set of basis vectors built from orthogonalizing \\(\\mathbf{A}\\). It’s also useful for calculating other random things of interest. Like LU factorization, it can be used to calculate determinants, since\n\\[\\text{det}(\\mathbf{A}) = \\text{det}(\\mathbf{QR}) = \\text{det}(\\mathbf{Q}) \\cdot \\text{det}(\\mathbf{R}) = 1 \\cdot \\text{det}(\\mathbf{R}) = \\prod_{i=0}^{n-1} R_{i,i}.\\]\nIt can also be used to find the inverse matrix. Use the fact that \\(\\mathbf{A}^{-1} = (\\mathbf{QR})^{-1} = \\mathbf{R}^{-1} \\mathbf{Q}^\\top\\), since \\(\\mathbf{Q}\\) is orthogonal. The matrix \\(\\mathbf{R}^{-1}\\) can be calculated efficiently via back-substitution since \\(\\mathbf{R}\\) just a triangular matrix. Both the determinant and inverse calculation again take \\(O(n^3)\\) time because the QR factorization does.\nIn practice, the QR factorization is done using algorithms like the Gram-Schmidt method or Householder reflections. Just like LU factorization, QR factorization is in general an \\(O(n^3)\\) algorithm. In numpy, you can get the QR factorization using np.linalg.qr(A).\nThe exact QR factorization I described is technically called the full QR factorization, since it orthogonalizes all of the columns, even if \\(\\mathbf{A}\\) isn’t full rank. Usually by default the algorithms only orthogonalize the first \\(r=\\text{rank}(\\mathbf{A})\\) columns. If you want to return the full QR factorization in numpy, you need to pass in the keyword argument mode = 'complete'.\nHere’s the full QR factorization of the same matrix from before.\n\n\nCode\nA = np.array([[1, 1], \n              [1, -1]])\nQ, R = np.linalg.qr(A, mode='complete')\nprint(f'Q = \\n{Q.round(10)}')\nprint(f'R = \\n{R.round(10)}')\nprint(f'QR = \\n{Q @ R}')\n\n\nQ = \n[[-0.70710678 -0.70710678]\n [-0.70710678  0.70710678]]\nR = \n[[-1.41421356  0.        ]\n [ 0.         -1.41421356]]\nQR = \n[[ 1.  1.]\n [ 1. -1.]]\n\n\n\n\n7.3.3 Spectral Decomposition\nThe spectral decomposition is a way to factor a symmetric matrix \\(\\mathbf{S}\\) into a product of an orthonormal matrix \\(\\mathbf{X}\\) and a diagonal matrix \\(\\mathbf{\\Lambda}\\),\n\\[\\mathbf{S} = \\mathbf{X \\Lambda X}^\\top.\\]\nThe matrix \\(\\mathbf{\\Lambda}\\) is called the eigenvalue matrix, and \\(\\mathbf{X}\\) is the eigenvector matrix. The diagonal entries of \\(\\mathbf{\\Lambda}\\) are called the eigenvalues of \\(\\mathbf{S}\\), denoted \\(\\lambda_i\\),\n\\[\\mathbf{\\Lambda} = \\text{diag}(\\lambda_0, \\lambda_1, \\cdots, \\lambda_n).\\]\nThe column vectors of \\(\\mathbf{X}\\) are called the eigenvectors of \\(\\mathbf{S}\\), denoted \\(\\mathbf{x}_i\\),\n\\[\\mathbf{X} = \\begin{pmatrix} \\mathbf{x}_0 & \\mathbf{x}_1 & \\cdots & \\mathbf{x}_{n-1} \\end{pmatrix}.\\]\nEigenvalues and eigenvectors arise from trying to find special “characteristic” lines in the vector space \\(\\mathbb{R}^n\\) that stay fixed when acted on by \\(\\mathbf{S}\\). Let \\(\\mathbf{x}\\) be the unit vector along one of these lines. Saying \\(\\mathbf{S}\\) can’t rotate \\(\\mathbf{x}\\) is equivalent to saying it can only scale \\(\\mathbf{x}\\) by some value \\(\\lambda\\). Finding these special characteristic lines is thus equivalent to solving the equation\n\\[\\mathbf{S}\\mathbf{x} = \\lambda \\mathbf{x}\\]\nfor \\(\\lambda\\) and \\(\\mathbf{x}\\). The vector \\(\\mathbf{x}\\) is the eigenvector (German for “characteristic vector”). The scalar \\(\\lambda\\) is its corresponding eigenvalue (German for “characteristic value”). We can rewrite this equation as \\((\\mathbf{S} - \\lambda \\mathbf{I})\\mathbf{x} = \\mathbf{0}\\), where \\(\\mathbf{0}\\) is the zero vector. Taking the determinant of \\(\\mathbf{S} - \\lambda \\mathbf{I}\\) and insisting it must be singular gives a polynomial equation, called the characteristic equation, that can (in principle) be solved for the eigenvalue \\(\\lambda\\),\n\\[\\text{det}(\\mathbf{S} - \\lambda \\mathbf{I}) = 0.\\]\nFor example, if \\(\\mathbf{S}\\) is a symmetric \\(2 \\times 2\\) matrix, we have\n\\[\n\\mathbf{S} =\n\\begin{pmatrix}\na & b \\\\\nb & d \\\\\n\\end{pmatrix} \\quad \\Longrightarrow \\quad\n\\mathbf{S} - \\lambda \\mathbf{I} =\n\\begin{pmatrix}\na-\\lambda & b \\\\\nb & d-\\lambda \\\\\n\\end{pmatrix} \\quad \\Longrightarrow \\quad\n\\text{det}(\\mathbf{S} - \\lambda \\mathbf{I}) = (a - \\lambda)(d - \\lambda) - b^2 = \\lambda^2 - (a + d)\\lambda + (ad-b^2) = 0.\n\\]\nNotice that \\(\\text{tr}(\\mathbf{S}) = a + d\\) and \\(\\text{det}(\\mathbf{S}) = ad-b^2\\), so the characteristic equation in this special \\(2 \\times 2\\) cases reduces to\n\\[\\lambda^2 - \\text{tr}(\\mathbf{S})\\lambda + \\text{det}(\\mathbf{S}) = 0.\\]\nThis is a quadratic equation whose solution is the two eigenvalues \\(\\lambda_0, \\lambda_1\\). Once the eigenvalues are known, they can be plugged back into the linear equation \\((\\mathbf{S} - \\lambda \\mathbf{I})\\mathbf{x} = \\mathbf{0}\\) to solve for the eigenvectors \\(\\mathbf{x}_0, \\mathbf{x}_1\\), e.g. using LU factorization.\nJust to put some numbers in, take the following specific \\(2 \\times 2\\) matrix\n\\[\n\\mathbf{S} =\n\\begin{pmatrix}\n2 & 1 \\\\\n1 & 2 \\\\\n\\end{pmatrix}.\n\\]\nSince \\(\\text{tr}(\\mathbf{S})=2+2=4\\) and \\(\\text{det}(\\mathbf{S})=2 \\cdot 2 - 1 \\cdot 1 = 3\\), the characteristic equation is\n\\[\\lambda^2 - 4\\lambda + 3 = 0 \\quad \\Longrightarrow \\quad (\\lambda-1)(\\lambda - 3) = 0 \\quad \\Longrightarrow \\quad \\lambda=1, 3.\\]\nThe eigenvalues for this matrix are thus \\(\\lambda_0 = 3\\) and \\(\\lambda_1 = 1\\). Note it’s conventional to order the eigenvalues from largest to smallest, though it isn’t required. The eigenvectors are gotten by solving the two systems\n\\[\n(\\mathbf{S} - \\lambda_0 \\mathbf{I})\\mathbf{x}_0 = \\mathbf{0} \\quad \\Longrightarrow \\quad\n\\begin{pmatrix}\n2-3 & 1 \\\\\n1 & 2-3 \\\\\n\\end{pmatrix}\n\\begin{pmatrix}\nx_0 \\\\\ny_0 \\\\\n\\end{pmatrix} =\n\\begin{pmatrix}\n0 \\\\\n0 \\\\\n\\end{pmatrix} \\quad \\Longrightarrow \\quad\n\\mathbf{x}_0 =\n\\frac{1}{\\sqrt{2}}\n\\begin{pmatrix}\n1 \\\\\n1 \\\\\n\\end{pmatrix} \\approx\n\\begin{pmatrix}\n0.707 \\\\\n0.707 \\\\\n\\end{pmatrix},\n\\]\n\\[\n(\\mathbf{S} - \\lambda_1 \\mathbf{I})\\mathbf{x}_1 = \\mathbf{0} \\quad \\Longrightarrow \\quad\n\\begin{pmatrix}\n2-1 & 1 \\\\\n1 & 2-1 \\\\\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\\ny_1 \\\\\n\\end{pmatrix} =\n\\begin{pmatrix}\n0 \\\\\n0 \\\\\n\\end{pmatrix} \\quad \\Longrightarrow \\quad\n\\mathbf{x}_1 =\n\\frac{1}{\\sqrt{2}}\n\\begin{pmatrix}\n1 \\\\\n-1 \\\\\n\\end{pmatrix} \\approx\n\\begin{pmatrix}\n0.707 \\\\\n-0.707 \\\\\n\\end{pmatrix}.\n\\]\nYou can easily check that \\(\\mathbf{x}_0\\) and \\(\\mathbf{x}_1\\) are orthogonal. Note the eigenvectors here have been normalized so \\(||\\mathbf{x}_0||=||\\mathbf{x}_1||=1\\). This isn’t required, but it’s the most common convention to ensure the eigenvector matrix \\(\\mathbf{X}\\) is a properly orthogonal.\nHere’s a plot of what this looks like. I’ll show that \\(\\mathbf{v}_0=\\sqrt{2}\\mathbf{x}_0=(1,1)\\) gets scaled by a factor of \\(\\lambda_0=3\\) when acted on by \\(\\mathbf{S}\\). Similarly, I’ll show that \\(\\mathbf{v}_1=\\sqrt{2}\\mathbf{x}_1=(1,-1)\\) gets scaled by a factor of \\(\\lambda_1=1\\) (i.e. not at all) when acted on by \\(\\mathbf{S}\\). Importantly, notice that \\(\\mathbf{S}\\) doesn’t rotate either vector. They stay along their characteristic lines, or eigenspaces, which in this example are the lines \\(y=\\pm x\\).\n\n\nCode\nS = np.array([\n    [2, 1], \n    [1, 2]])\nv0 = np.array([1, 1]).reshape(-1, 1)\nSv0 = S @ v0\nv1 = np.array([1, -1]).reshape(-1, 1)\nSv1 = S @ v1\nvectors = [x.flatten() for x in [v0, Sv0, v1, Sv1]]\nplot_vectors(\n    vectors, colors=['black', 'red', 'black', 'blue'], xlim=(-1, 4), ylim=(-2, 4), zorders=[1, 0, 2, 3], \n    labels=['$\\mathbf{v}_0$', '$\\mathbf{S}\\mathbf{v}_0$', '$\\mathbf{v}_1$', '$\\mathbf{S}\\mathbf{v}_1$'],\n    text_offsets=[[-0.45, 0.25], [0.05, 0.15], [0.1, -0.5], [0.05, 0.3]], \n    title='Eigenspaces of $\\mathbf{S}$')\n\n\n\n\n\n\n\n\n\nA result I won’t prove, called the spectral theorem, guarantees that the eigenvalues of a symmetric matrix will be real-valued, and that the eigenvectors will form an orthonormal basis for \\(\\mathbb{R}^n\\). This is why \\(\\mathbf{X}\\) ends up being an orthogonal matrix. The fact that the eigenvalues have to be real is why we can think of symmetric matrices as the matrix generalization of a real number.\nThe spectral decomposition \\(\\mathbf{S} = \\mathbf{X \\Lambda X}^\\top\\) is just a matrix way of writing the individual equations \\(\\mathbf{S}\\mathbf{x} = \\lambda \\mathbf{x}\\). Grouping the eigenvectors and eigenvalues into matrices, we can write these equations in one go as \\(\\mathbf{S}\\mathbf{X} = \\mathbf{\\Lambda} \\mathbf{X}\\), which is just the spectral decomposition.\nBack to our working example, putting the eigenvalues and eigenvectors into their respective matrices gives\n\\[\n\\mathbf{\\Lambda} =\n\\begin{pmatrix}\n3 & 0 \\\\\n0 & 1 \\\\\n\\end{pmatrix}, \\qquad\n\\mathbf{X} =\n\\frac{1}{\\sqrt{2}}\n\\begin{pmatrix}\n1 & 1 \\\\\n1 & -1 \\\\\n\\end{pmatrix}.\n\\]\nThat is, the symmetric matrix \\(\\mathbf{S}\\) factorizes into the spectral decomposition\n\\[\n\\mathbf{S} = \\mathbf{X \\Lambda X}^\\top =\n\\frac{1}{\\sqrt{2}}\n\\begin{pmatrix}\n1 & 1 \\\\\n1 & -1 \\\\\n\\end{pmatrix}\n\\begin{pmatrix}\n3 & 0 \\\\\n0 & 1 \\\\\n\\end{pmatrix}\n\\frac{1}{\\sqrt{2}}\n\\begin{pmatrix}\n1 & 1 \\\\\n1 & -1 \\\\\n\\end{pmatrix}.\n\\]\nWe can find the spectral decomposition of a symmetric matrix in numpy using np.linalg.eigh(S). Note that np.linalg.eig(S) will also work, but eigh is more efficient for symmetric matrices than eig. In either case, they return a pair of arrays, the first being the diagonals of \\(\\mathbf{\\Lambda}\\), the second being \\(\\mathbf{X}\\). I’ll also verify that the spectral decomposition indeed gives \\(\\mathbf{S}\\).\n\n\nCode\nS = np.array([[2, 1], \n              [1, 2]])\nlambdas, X = np.linalg.eigh(S)\nLambda = np.diag(lambdas)\nprint(f'Lambda = \\n{Lambda}')\nprint(f'X = \\n{X}')\nprint(f'X Lambda X^T = \\n{X @ Lambda @ X.T}')\n\n\nLambda = \n[[1. 0.]\n [0. 3.]]\nX = \n[[-0.70710678  0.70710678]\n [ 0.70710678  0.70710678]]\nX Lambda X^T = \n[[2. 1.]\n [1. 2.]]\n\n\nNotice something from the example I just worked. It turns out that \\(\\text{tr}(\\mathbf{S}) = 4 = \\lambda_0 + \\lambda_1\\) and \\(\\text{det}(\\mathbf{S}) = 3 = \\lambda_0 \\lambda_1\\). This fact turns out to always be true for \\(n \\times n\\) symmetric matrices, namely if \\(\\mathbf{S}\\) has eigenvalues \\(\\lambda_0, \\lambda_1, \\cdots, \\lambda_{n-1}\\), then\n\\[\\begin{align*}\n\\text{tr}(\\mathbf{S}) &= \\sum_{i=0}^{n-1} \\lambda_i = \\lambda_0 + \\lambda_1 + \\cdots + \\lambda_{n-1}, \\\\\n\\text{det}(\\mathbf{S}) &= \\prod_{i=0}^{n-1} \\lambda_i = \\lambda_0 \\cdot \\lambda_1 \\cdots \\lambda_{n-1}.\n\\end{align*}\\]\nThis fact implies that \\(\\mathbf{S}\\) will be invertible if and only if all the eigenvalues are non-zero, since otherwise we’d have \\(\\text{det}(\\mathbf{S})=0\\).\nGiven how important the spectral decomposition is to many applications, there are a lot of different algorithms for finding it, each with its own trade-offs. One popular algorithm for doing so is the QR algorithm. Roughly speaking, the QR algorithm works as follows:\n\nStart with \\(\\mathbf{S}_0 = \\mathbf{S}\\).\nFor some number of iterations \\(t=0,1,\\cdots, T-1\\) do the following:\n\nCalculate the QR factorization of \\(\\mathbf{S}_t\\): \\(\\mathbf{Q}_{t+1}, \\mathbf{R}_{t+1} = \\text{qr}(\\mathbf{S}_t)\\).\nUpdate \\(\\mathbf{S}_t\\) by reversing the factorization order: \\(\\mathbf{S}_{t+1} = \\mathbf{R}_{t+1} \\mathbf{Q}_{t+1}\\).\n\nTake \\(\\mathbf{\\Lambda} \\approx \\mathbf{S}_{T-1}\\) and \\(\\mathbf{X} \\approx \\mathbf{Q}_{T-1}\\).\n\nDue to the QR factorizations and matrix multiplications, this algorithm will be \\(O(n^3)\\) at each step, which all together gives a time complexity of \\(O(Tn^3)\\). It’s not at all obvious from what I’ve said why the QR algorithm even works. In fact, to work well it requires a few small modifications I won’t go into.\n\n\n7.3.4 Positive Definiteness\nThe eigenvalues of a symmetric matrix \\(\\mathbf{S}\\) are important because they in some sense specify how much \\(\\mathbf{S}\\) tends to stretch vectors in different directions. Most important for machine learning purposes though is the sign of the eigenvalues. The sign of the eigenvalues of a symmetric matrix essentially determine how hard it is to optimize a given function. This is especially relevant in machine learning, since training a model is all about optimizing the loss function of a model’s predictions against the data.\nA symmetric matrix \\(\\mathbf{S} \\in \\mathbb{R}^{n \\times n}\\) whose eigenvalues are all non-negative is called positive semi-definite, sometimes expressed with the shorthand \\(\\mathbf{S} \\succcurlyeq 0\\). Positive semi-definite matrices are important because they’re essentially the matrix generalization of a non-negative number \\(s \\geq 0\\).\nIf the eigenvalues of \\(\\mathbf{S}\\) are all strictly positive, the matrix is called positive definite, sometimes expressed with the shorthand \\(\\mathbf{S} \\succ 0\\). Positive definite matrices are the matrix generalization of a strictly positive number \\(s > 0\\). Clearly any positive definite matrix is also positive semi-definite.\nBy writing \\(\\mathbf{S}=\\mathbf{X \\Lambda X}^\\top\\) and expanding everything out, it’s not hard to show that if \\(\\mathbf{S}\\) is positive semi-definite, and \\(\\mathbf{x} \\in \\mathbb{R}^n\\) is any non-zero vector, then\n\\[\\mathbf{x}^\\top \\mathbf{S} \\mathbf{x} \\geq 0.\\]\nA similar rule holds for positive definite matrices, except it’s \\(\\mathbf{x}^\\top \\mathbf{S} \\mathbf{x} > 0\\) instead.\nExpressions of the form \\(\\mathbf{x}^\\top \\mathbf{S} \\mathbf{x}\\) are called quadratic forms. They’ll always be scalars since all they are is a dot product \\(\\mathbf{x} \\cdot \\mathbf{S} \\mathbf{x}\\). This is why a positive semi-definite matrix extends the idea of a nonnegative number \\(s\\), since \\(s\\) would satisfy \\(xsx=sx^2 \\geq 0\\) for any \\(x \\neq 0\\). Similarly for the positive definite case, a positive \\(s\\) would satisfy \\(xsx > 0\\) for any \\(x \\neq 0\\).\nNote that two of the most important matrices in machine learning, the Hessian and the covariance matrix, are both positive semi-definite, so these things do come up in applications. As you’d probably guess, the easiest way to determine if a symmetric matrix is positive definite or semi-definite is to just calculate the eigenvalues and check their signs. For example, I showed before that the matrix\n\\[\n\\mathbf{S} =\n\\begin{pmatrix}\n2 & 1 \\\\\n1 & 2 \\\\\n\\end{pmatrix}\n\\]\nhas eigenvalues \\(\\lambda = 3, 1\\). Since both of these are positive, \\(\\mathbf{S}\\) is positive definite. It’s also positive semi-definite since they’re both non-negative. To check if a matrix is positive definite, for example, in numpy, you can do something like the following. Modify the inequality accordingly for the other types.\n\n\nCode\ndef is_positive_definite(S):\n    eigvals = np.linalg.eigvals(S)\n    return np.all(eigvals > 0)\n\nS = np.array([[2, 1], \n              [1, 2]])\nis_positive_definite(S)\n\n\nTrue\n\n\nSuppose \\(a \\neq 0\\) is some non-negative number. We know we can take its square root to get another non-negative number \\(\\sqrt{a} \\neq 0\\). Positive semi-definite matrices have a similar property. If \\(\\mathbf{S} \\succcurlyeq 0\\), then we can find a “square root” matrix \\(\\mathbf{R}\\) such that\n\\[\\mathbf{S} = \\mathbf{R} \\mathbf{R}^\\top.\\]\nIt turns out though that matrices have many possible square roots, not just one. For this reason, we might as well choose an \\(\\mathbf{R}\\) that has some convenient form. One useful form is to assume that \\(\\mathbf{R}\\) is triangular. When we do this, we get what’s called the Cholesky Factorization. If \\(\\mathbf{S}\\) is positive semi-definite, we’ll factor \\(\\mathbf{S}\\) into as,\n\\[\\mathbf{S} = \\mathbf{L} \\mathbf{L}^\\top,\\]\nwhere \\(\\mathbf{L}\\) is some lower triangular matrix, which also means \\(\\mathbf{L}^\\top\\) is upper triangular.\nHere’s an example. Let’s try to find the Cholesky factorization of the same symmetric matrix from before,\n\\[\n\\mathbf{S} =\n\\begin{pmatrix}\n2 & 1 \\\\\n1 & 2 \\\\\n\\end{pmatrix}.\n\\]\nTake \\(\\mathbf{L}\\) to be\n\\[\n\\mathbf{L} =\n\\begin{pmatrix}\n\\sqrt{2} & 0 \\\\\n\\frac{1}{\\sqrt{2}} & \\sqrt{\\frac{3}{2}} \\\\\n\\end{pmatrix} \\approx\n\\begin{pmatrix}\n1.414 & 0 \\\\\n0.707 & 1.225 \\\\\n\\end{pmatrix}.\n\\]\nThen\n\\[\n\\mathbf{L}\\mathbf{L}^\\top =\n\\begin{pmatrix}\n\\sqrt{2} & 0 \\\\\n\\frac{1}{\\sqrt{2}} & \\sqrt{\\frac{3}{2}} \\\\\n\\end{pmatrix}\n\\begin{pmatrix}\n\\sqrt{2} & \\frac{1}{\\sqrt{2}} \\\\\n0 & \\sqrt{\\frac{3}{2}} \\\\\n\\end{pmatrix} =\n\\begin{pmatrix}\n2 & 1 \\\\\n1 & 2 \\\\\n\\end{pmatrix} =\n\\mathbf{S},\n\\]\nhence \\(\\mathbf{L}\\) is the Cholesky factor or square root of \\(\\mathbf{S}\\). How I came up with \\(\\mathbf{L}\\) here isn’t important. We can do the Cholesky factorization in numpy using np.linalg.cholesky. Let’s check my answer above is correct. Looks like it is.\n\n\nCode\nS = np.array([[2, 1], \n              [1, 2]])\nL = np.linalg.cholesky(S)\nprint(f'L = \\n{L}')\nprint(f'L L^T = \\n{L @ L.T}')\n\n\nL = \n[[1.41421356 0.        ]\n [0.70710678 1.22474487]]\nL L^T = \n[[2. 1.]\n [1. 2.]]\n\n\nIn practice, the Cholesky factorization is calculated using variants on the same algorithms used to calculate the LU factorization. Indeed, the Cholesky and LU factorizations have a lot in common. Both methods factor a matrix into a product of lower and upper triangular matrices. The major difference is that Cholesky only needs to find one lower triangular matrix \\(\\mathbf{L}\\), not two. Like LU factorization, Cholesky runs in \\(O(n^3)\\) time, but only uses half the FLOPS that LU does.\nThis means positive semi-definite matrices have more efficient algorithms than general matrices do. For example, suppose you wanted to solve a linear system \\(\\mathbf{A}\\mathbf{x}=\\mathbf{b}\\). If you knew \\(\\mathbf{A}\\) was positive semi-definite, you could solve the system at half the cost by calculating the Cholesky factorization \\(\\mathbf{A}=\\mathbf{L}\\mathbf{L}^\\top\\), and then using substitution to solve for \\(\\mathbf{x}\\). Similar approaches apply for other matrix quantities, like the inverse or determinant.\n\n\n7.3.5 Singular Value Decomposition\nThe spectral decomposition is mostly useful for square symmetric matrices. Yet, the properties of eigenvalues and eigenvectors seem to be incredibly useful for understanding how a matrix behaves. They say something useful about the characteristic scales and directions of a matrix and its underlying linear operator. It turns out we can generalize the spectral decomposition to arbitrary matrices, but with some slight modifications. This modified factorization is called the singular value decomposition, or SVD for short.\nSuppose \\(\\mathbf{A}\\) is some arbitrary \\(m \\times n\\) matrix. It turns out we can always factor \\(\\mathbf{A}\\) into a product of the form\n\\[\\mathbf{A} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^\\top,\\]\nwhere \\(\\mathbf{U}\\) is an \\(m \\times m\\) orthogonal matrix called the left singular matrix, \\(\\mathbf{V}\\) is a different \\(n \\times n\\) orthogonal matrix called the left singular matrix, and \\(\\mathbf{\\Sigma}\\) is an \\(m \\times n\\) diagonal matrix called the singular value matrix.\nThe singular value matrix \\(\\mathbf{\\Sigma}\\) is a rectangular diagonal matrix. This means the diagonal will only have \\(k=\\min(m, n)\\) entries. The diagonal entries are called the singular values of \\(\\mathbf{A}\\), usually denoted \\(\\sigma_0, \\sigma_1, \\cdots, \\sigma_{k-1}\\). Unlike eigenvalues, singular values are required to be non-negative.\nThe column vectors of \\(\\mathbf{U}\\) and \\(\\mathbf{V}\\) are called the left and right singular vectors respectively. Since both matrices are orthogonal, their singular vectors will form an orthonormal basis for \\(\\mathbb{R}^m\\) and \\(\\mathbb{R}^n\\) respectively.\nNotice that whereas with the spectral composition \\(\\mathbf{S} = \\mathbf{X} \\mathbf{\\Lambda} \\mathbf{X}^\\top\\) has only a single orthogonal matrix \\(\\mathbf{X}\\), the SVD has two different orthogonal matrices \\(\\mathbf{U}\\) and \\(\\mathbf{V}\\) to worry about, and each one is a different size. Also, while \\(\\mathbf{\\Lambda}\\) can contain eigenvalues of any sign, \\(\\mathbf{\\Sigma}\\) can only contain singular values that are nonnegative.\nNonetheless, the two factorizations are related by the following fact: The singular values of \\(\\mathbf{A}\\) are the eigenvalues of the symmetric matrix \\(\\mathbf{S} = \\mathbf{A}^\\top \\mathbf{A}\\). Not only that, they’re also the eigenvalues of the transposed symmetric matrix \\(\\mathbf{S}^\\top = \\mathbf{A} \\mathbf{A}^\\top\\). This fact gives one way you could actually calculate the SVD. The singular value matrix \\(\\mathbf{\\Sigma}\\) will just be the eigenvalue matrix of \\(\\mathbf{S}\\) (and \\(\\mathbf{S}^\\top\\)). The left singular matrix \\(\\mathbf{U}\\) will be the eigenvector matrix of \\(\\mathbf{S}\\). The right singular matrix \\(\\mathbf{V}\\) will be the eigenvector matrix of \\(\\mathbf{S}^\\top\\). Very roughly speaking, this is what many SVD algorithms use, e.g. by applying the QR algorithm on both \\(\\mathbf{S}\\) and \\(\\mathbf{S}^\\top\\).\nCalculating the SVD by hand is much more of a pain than the spectral decomposition is because you have to do it twice, once on \\(\\mathbf{S}\\) and once on \\(\\mathbf{S}^\\top\\). I’ll spare you the agony of this calculation, and just use numpy to calculate the SVD of the following matrix,\n\\[\n\\mathbf{A} =\n\\begin{pmatrix}\n1 & 1 \\\\\n1 & 0 \\\\\n1 & -1 \\\\\n\\end{pmatrix}.\n\\]\nWe can use np.linalg.svd(A) to calculate the SVD of \\(\\mathbf{A}\\). It’ll return a triplet of arrays, in order \\(\\mathbf{U}\\), the diagonal of \\(\\mathbf{\\Sigma}\\), and \\(\\mathbf{V}^T\\). Note to get the full \\(\\mathbf{\\Sigma}\\) you can’t just use np.diag since \\(\\mathbf{\\Sigma}\\) won’t be square here. You have to add a row of zeros after to make the calculation work out. I’ll do this just using a loop and filling in the diagonals manually.\nNotice that the two singular values are positive, \\(\\sigma_0 = \\sqrt{3} \\approx 1.732\\) and \\(\\sigma_1 = \\sqrt{2} \\approx 1.414\\). In this example, the right singular matrix \\(\\mathbf{V}\\) is just \\(\\text{diag}(-1, 1)\\), which is clearly orthogonal. The left singular matrix \\(\\mathbf{U}\\) is a little harder to see, but it’s also orthogonal. Finally, the product \\(\\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^\\top\\) indeed gives \\(\\mathbf{A}\\).\n\n\nCode\nA = np.array([\n    [1, 1],\n    [1, 0],\n    [1, -1]])\nm, n = A.shape\nk = min(m, n)\nU, sigma, Vt = np.linalg.svd(A)\nSigma = np.zeros((m, n))\nfor i in range(k):\n    Sigma[i, i] = sigma[i]\nUSVt = U @ Sigma @ Vt\nprint(f'U = \\n{U.round(10)}')\nprint(f'Sigma = \\n{Sigma.round(10)}')\nprint(f'V = \\n{Vt.T.round(10)}')\nprint(f'U Sigma V^T = \\n{USVt.round(10)}')\n\n\nU = \n[[-0.57735027  0.70710678  0.40824829]\n [-0.57735027  0.         -0.81649658]\n [-0.57735027 -0.70710678  0.40824829]]\nSigma = \n[[1.73205081 0.        ]\n [0.         1.41421356]\n [0.         0.        ]]\nV = \n[[-1.  0.]\n [-0.  1.]]\nU Sigma V^T = \n[[ 1.  1.]\n [ 1.  0.]\n [ 1. -1.]]\n\n\nTo give you an intuition is to what the SVD is doing, suppose \\(\\mathbf{x} \\in \\mathbb{R}^n\\) is some size-\\(n\\) vector. Suppose we want to operate on \\(\\mathbf{x}\\) with \\(\\mathbf{A}\\) to get a new vector \\(\\mathbf{v} = \\mathbf{A}\\mathbf{x}\\). Writing \\(\\mathbf{A} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^\\top\\), we can do this operation in a sequence of three successive steps:\n\nCalculate \\(\\mathbf{y} = \\mathbf{V}^\\top \\mathbf{x}\\): The output is also a size-\\(n\\) vector \\(\\mathbf{y} \\in \\mathbb{R}^n\\). Since \\(\\mathbf{V}\\) is orthogonal, this action can only rotate (or reflect) \\(\\mathbf{x}\\) by some angle in space.\nCalculate \\(\\mathbf{z} = \\mathbf{\\Sigma}\\mathbf{y}\\): The output is now a size-\\(k\\) vector \\(\\mathbf{z} \\in \\mathbb{R}^k\\). Since \\(\\mathbf{\\Sigma}\\) is diagonal, it can only stretch \\(\\mathbf{y}\\) along the singular directions of \\(\\mathbf{V}\\), not rotate it.\nCalculate \\(\\mathbf{v} = \\mathbf{U}\\mathbf{z}\\): The output is now a size-\\(m\\) vector \\(\\mathbf{v} \\in \\mathbb{R}^m\\). Since \\(\\mathbf{U}\\) is orthogonal, this action can only rotate (or reflect) \\(\\mathbf{z}\\) by some angle in space.\n\nThe final output is thus a vector \\(\\mathbf{v} = \\mathbf{A}\\mathbf{x}\\) that first got rotated in \\(\\mathbb{R}^n\\), then scaled in \\(\\mathbb{R}^k\\), then rotated again in \\(\\mathbb{R}^m\\). So you can visualize this better let’s take a specific example. To make everything show up on one plot I’ll choose a \\(2 \\times 2\\) matrix, so \\(m=n=k=2\\), for example\n\\[\n\\mathbf{A} =\n\\begin{pmatrix}\n1 & 2 \\\\\n1 & 1 \\\\\n\\end{pmatrix}.\n\\]\nThe singular values to this matrix turn out to be \\(\\sigma_0 \\approx 2.618\\) and \\(\\sigma_1 \\approx 0.382\\). What I’m going to do is randomly sample a bunch of unit vectors \\(\\mathbf{x}\\), then apply the successive operations above to each vector. The original vectors \\(\\mathbf{x}\\) are shown in red, the vectors \\(\\mathbf{y} = \\mathbf{V}^\\top \\mathbf{x}\\) in blue, the vectors \\(\\mathbf{z} = \\mathbf{\\Sigma}\\mathbf{y}\\) in green, and finally the vectors \\(\\mathbf{v} = \\mathbf{U}\\mathbf{z}\\) in black. Notice that the red vectors just kind of fill in the unit circle, since they’re all unit vectors of length one. The blue vectors also fill in the unit circle, since \\(\\mathbf{V}^\\top\\) can only rotate vectors, not stretch them. The green vectors then get stretched out into an elliptical shape due to \\(\\mathbf{\\Sigma}\\). The distortion of the ellipse depends on the “distortion ratio” \\(\\frac{\\sigma_0}{\\sigma_1} \\approx 6.85\\). This means one axis gets stretched about \\(6.85\\) times as much as the other. Finally, since \\(\\mathbf{U}\\) can only rotate vectors, the black vectors then rotate these stretched vectors into their final position.\n\n\nCode\nA = np.array([\n    [1, 2],\n    [1, 1]])\nm, n = A.shape\nk = min(m, n)\nU, sigma, Vt = np.linalg.svd(A)\nSigma = np.diag(sigma)\nprint(f'Sigma = \\n{Sigma.round(10)}')\n\n\nSigma = \n[[2.61803399 0.        ]\n [0.         0.38196601]]\n\n\n\n\nCode\nplot_svd(A)\n\n\n\n\n\n\n\n\n\nThe “distortion ratio” \\(\\frac{\\sigma_0}{\\sigma_1}\\) mentioned above can actually be used as a measure of how invertible a matrix is. It’s called the condition number, denoted \\(\\kappa\\). For a general \\(n \\times n\\) matrix, the condition number is defined as the ratio of the largest to the smallest singular value,\n\\[\\kappa = \\frac{\\sigma_0}{\\sigma_{k-1}}.\\]\nThe higher the condition number is, the harder it is to invert \\(\\mathbf{A}\\). A condition number of \\(\\kappa=1\\) is when the singular values are the same. These are easiest to invert. Matrices with low \\(\\kappa\\) are called called well-conditioned matrices. The identity matrix has \\(\\kappa=1\\), for example. If one of the singular values is \\(0\\) then \\(\\kappa\\) will be infinite, meaning the matrix isn’t invertible at all. Matrices with high \\(\\kappa\\) are called ill-conditioned matrices. For this reason, the condition number is very often used in calculations when it’s important to make sure that \\(\\mathbf{A}\\) isn’t singular or close to singular. In numpy, you can calculate the condition number of a matrix directly by using np.linalg.cond(A).\n\n\n7.3.6 Low-Rank Approximations\nThe SVD is useful for many reasons. In fact, it’s probably the single most useful factorization in all of applied linear algebra. One reason this is true is because every matrix has one. When in doubt, if you can’t figure out how to do something with a matrix, you can take its SVD and try to work with those three matrices one-by-one. While that’s nice, the more useful application of the SVD to machine learning is that it’s a good way to compress or denoise data. To see why we need to look at the SVD in a slightly different way.\nSuppose \\(\\mathbf{A}\\) is some \\(m \\times n\\) matrix. Suppose \\(\\mathbf{u}_0, \\mathbf{u}_1, \\cdots, \\mathbf{u}_{m-1}\\) are the column vectors of \\(\\mathbf{U}\\), and \\(\\mathbf{v}_0, \\mathbf{v}_1, \\cdots, \\mathbf{v}_{n-1}\\) are the column vectors of \\(\\mathbf{V}\\). Suppose \\(\\sigma_0, \\sigma_1, \\cdots, \\sigma_{k-1}\\) are the singular values of \\(\\mathbf{A}\\), by convention ordered from largest to smallest. Then writing out the SVD in terms of the column vectors, and multiplying everything out matrix multiplication style, we have\n\\[\n\\mathbf{A} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^\\top =\n\\begin{pmatrix}\n\\mathbf{u}_0 & \\mathbf{u}_1 & \\cdots & \\mathbf{u}_{m-1}\n\\end{pmatrix}\n\\text{diag}\\big(\\sigma_0, \\sigma_1, \\cdots, \\sigma_{k-1}\\big)\n\\begin{pmatrix}\n\\mathbf{v}_0^\\top \\\\ \\mathbf{v}_1^\\top \\\\ \\cdots \\\\ \\mathbf{v}_{n-1}^\\top\n\\end{pmatrix} =\n\\sum_{i=0}^{k-1} \\sigma_i \\mathbf{u}_i \\mathbf{v}_i^\\top =\n\\sigma_0 \\mathbf{u}_0 \\mathbf{v}_0^\\top + \\sigma_1 \\mathbf{u}_1 \\mathbf{v}_1^\\top + \\cdots + \\sigma_{k-1} \\mathbf{u}_{k-1} \\mathbf{v}_{k-1}^\\top.\n\\]\nThat is, we can write \\(\\mathbf{A}\\) as a sum of outer products over the singular vectors, each weighted by its singular value. That’s fine. But why is it useful? All I did was re-write the SVD in a different form, after all. The gist of it is that we can use this formula to approximate \\(\\mathbf{A}\\) by a lower-dimensional matrix. Supposing we only kept the first \\(d < k\\) terms of the right-hand side and dropped the rest, we’d have\n\\[\\mathbf{A} \\approx \\mathbf{U}_d \\mathbf{\\Sigma}_d \\mathbf{V}_d^\\top = \\sigma_0 \\mathbf{u}_0 \\mathbf{v}_0^\\top + \\sigma_1 \\mathbf{u}_1 \\mathbf{v}_1^\\top + \\cdots + \\sigma_{d-1} \\mathbf{u}_{d-1} \\mathbf{v}_{d-1}^\\top.\\]\nThis approximation will be a rank-\\(d\\) matrix again of size \\(m \\times n\\). It’s rank \\(d\\) because it’s a sum of \\(d\\) “independent” rank-1 matrices. When \\(d \\ll k\\), this is called the low-rank approximation. While this approximation is low rank it still has size \\(m \\times n\\). It’s the inner dimensions that got cut from \\(k\\) to \\(d\\), not the outer dimensions. To get a true low-dimensional approximation, we need to multiply both sides by \\(\\mathbf{V}_d\\),\n\\[\\mathbf{A}_d =  \\mathbf{A} \\mathbf{V}_d = \\mathbf{U}_d \\mathbf{\\Sigma}_d.\\]\nWe’re now approximating the \\(m \\times n\\) matrix \\(\\mathbf{A}\\) with an \\(m \\times d\\) matrix I’ll call \\(\\mathbf{A}_d\\). Said differently, we’re compressing the \\(n\\) columns of \\(\\mathbf{A}\\) down to just \\(d \\ll n\\) columns. Note that we’re not dropping the last \\(n-d\\) columns, we’re building new columns that best approximate all of the old columns.\nLet’s try to understand why low rank approximations are useful, and that they indeed do give good approximations to large matrices. To do so, consider the following example. I’m going to load some data from a well-known dataset in machine learning called MNIST. It’s a dataset of images of handwritten digits. When the low-rank approximation is applied to data, it’s called principle components analysis, or PCA. PCA is probably the most fundamental dimension reduction algorithm, a way of compressing high-dimensional data into lower-dimensional data.\nEach image is size \\(28 \\times 28\\), which flatten out into \\(n = 28 \\cdot 28 = 784\\) dimensions. I’ll load \\(m=1000\\) random samples from the MNIST dataset. This will create a matrix \\(\\mathbf{A}\\) of shape \\(1000 \\times 784\\). I’ll go ahead and calculate the SVD to get \\(\\mathbf{U}\\), \\(\\mathbf{\\Sigma}\\), and \\(\\mathbf{V}^\\top\\). In this case, \\(k=\\min(m,n)=784\\), so these matrices will have sizes \\(1000 \\times 1000\\), \\(1000 \\times 784\\), and \\(784 \\times 784\\) respectively. As I mentioned before, numpy only returns the non-zero diagonals of \\(\\mathbf{\\Sigma}\\), which is a size \\(k=784\\) vector of the singular values. Thankfully, that’s all we’ll need here.\n\n\nCode\nm = 1000\nA = sample_mnist(size=m)\nU, sigma, Vt = np.linalg.svd(A)\nA.shape, U.shape, sigma.shape, Vt.shape\nprint(f'A.shape = {A.shape}')\nprint(f'U.shape = {U.shape}')\nprint(f'sigma.shape = {sigma.shape}')\nprint(f'Vt.shape = {Vt.shape}')\n\n\nA.shape = (1000, 784)\nU.shape = (1000, 1000)\nsigma.shape = (784,)\nVt.shape = (784, 784)\n\n\nThink of each row of \\(\\mathbf{A}\\) as representing a single image in the dataset, and each column of \\(\\mathbf{A}\\) as representing a single pixel of the image.\nSince these are images, I might as well show you what they look like. To do that, just pick a random row from the matrix. Each row will be a flattened image. To turn it into an image, we can just reshape the row to have shape \\(28 \\times 28\\), then plot it using plt.imshow. Below, I’m picking off the first row, which turns out to be an image of a handwritten \\(0\\).\n\n\nCode\nimg = A[0, :].reshape(28, 28)\nplt.imshow(img, cmap='Greys')\nplt.axis('off')\nplt.show()\n\n\n\n\n\n\n\n\n\nLet’s start by taking \\(d=2\\). Why? Because when \\(d=2\\) we can plot each image as a point in the xy-plane! This suggests a powerful application of the low-rank approximation, to visualize high-dimensional data. To calculate \\(\\mathbf{A}_d\\), we’ll need to truncate \\(\\mathbf{U}\\), \\(\\mathbf{\\Sigma}\\), and \\(\\mathbf{V}^\\top\\). To make the shapes come out right, we’ll want to drop the first \\(d\\) columns of \\(\\mathbf{U}\\) and the first \\(d\\) rows of \\(\\mathbf{V}^\\top\\). Once we’ve got these, we can calculate \\(\\mathbf{A}_d\\), which in this case will be size \\(1000 \\times 2\\).\n\n\nCode\nd = 2\nU_d, sigma_d, Vt_d = U[:, :d], sigma[:d], Vt[:d, :]\nA_d = A @ Vt_d.T\nprint(f'U_d.shape = {U_d.shape}')\nprint(f'sigma_d = {sigma_d}')\nprint(f'Vt_d.shape = {Vt_d.shape}')\nprint(f'A_d.shape = {A_d.shape}')\n\n\nU_d.shape = (1000, 2)\nsigma_d = [197.89062659  66.60026657]\nVt_d.shape = (2, 784)\nA_d.shape = (1000, 2)\n\n\nNow we have \\(m=1000\\) “images”, each with \\(d=2\\) “variables”. This means we can plot them in the xy-plane, taking \\(x\\) to be the first column A_d[:, 0], and \\(y\\) to be the second column A_d[:, 1]. Here’s a scatter plot of all images projected down to 2 dimensions. I can’t make out any patterns in the plot, and you probably can’t either. But at least we’ve found an interesting and sometimes useful way to visualize high-dimensional data.\n\n\nCode\nplt.scatter(A_d[:, 0], A_d[:, 1], s=1, alpha=0.8)\nplt.xticks([])\nplt.yticks([])\nplt.title(f'{m} MNIST Images')\nplt.show()\n\n\n\n\n\n\n\n\n\nHow good is our approximation? We can use the singular values to figure this out. In the low rank approximation, we’re keeping \\(d\\) singular values and dropping the remaining \\(k-d\\). Throwing away those remaining singular values is throwing away information about our original matrix \\(\\mathbf{A}\\). To figure out how much information we’re keeping in our approximation, we can just look at the ratio of the sum of singular values kept to the total sum of all singular values,\n\\[R_d = \\frac{\\sigma_0 + \\sigma_1 + \\cdots + \\sigma_{d-1}}{\\sigma_0 + \\sigma_1 + \\cdots + \\sigma_{k-1}}.\\]\nThis ratio is sometimes called the explained variance for reasons I’ll get into in a future lesson.\nIn the rank-2 case I just worked out, this ratio turns out to be \\(R_2 = \\frac{\\sigma_0 + \\sigma_1}{\\sum \\sigma_i} \\approx 0.087\\). That is, this rank-2 approximation is preserving about 8.7% of the information in the original data.\n\n\nCode\nR_d = np.sum(sigma_d) / np.sum(sigma)\nprint(f'R_d = {R_d}')\n\n\nR_d = 0.08740669535517863\n\n\nThat’s pretty bad. We can do better. Let’s take \\(d=100\\) and see how well that does. Of course, we won’t be able to plot the data in the xy-plane anymore, but it’ll better represent the original data. We’re now at \\(R_d \\approx 0.643\\), which means we’re preserving about 64.3% of the information in the original data, and we’re doing it using only \\(\\frac{100}{784} \\approx 0.127\\), or 12.7% of the total columns of \\(\\mathbf{A}\\).\n\n\nCode\nd = 100\nU_d, sigma_d, Vt_d = U[:, :d], sigma[:d], Vt[:d, :]\nA_d = A @ Vt_d.T\nprint(f'A_d.shape = {A_d.shape}')\n\n\nA_d.shape = (1000, 100)\n\n\n\n\nCode\nR_d = np.sum(sigma_d) / np.sum(sigma)\nprint(f'R_d = {R_d}')\n\n\nR_d = 0.6433751746962163\n\n\nAnother way to see how good our compression is is to “unproject” the compressed images and plot them. To unproject \\(\\mathbf{A}_d\\), just multiply on the right again by \\(\\mathbf{V}^\\top\\) to get the original \\(m \\times n\\) matrix approximation again,\n\\[\\mathbf{A} \\approx \\mathbf{A}_d \\mathbf{V}^\\top.\\]\nOnce I’ve done that, I can just pluck a random row from the approximation, resize it, and plot it using plt.imshow, just like before. Notice this time we can still clearly see the handwritten \\(0\\), but it’s a bit grainer than it was before. The edges aren’t as sharp. Nevertheless, we can still make out the digit pretty solidly.\n\n\nCode\nimg = (A_d @ Vt_d)[0, :].reshape(28, 28)\nplt.imshow(img, cmap='Greys')\nplt.axis('off')\nplt.show()\n\n\n\n\n\n\n\n\n\nBut why is this approach good for compression anyway? After all, we still have to unproject the rows back into the original \\(m \\times n\\) space. Maybe think about it this way. If you just stored the full matrix \\(\\mathbf{A}\\), you’d have tot store \\(m \\cdot n\\) total numbers. In this example, that’s \\(1000 \\cdot 784 = 784000\\) numbers you’d have to store in memory.\nBut suppose now we do the low rank approximation. What we can then do is just store \\(\\mathbf{A}_d\\) and \\(\\mathbf{V}\\) instead. That means we’d instead store \\(m \\cdot d + d \\cdot n\\) total numbers. In our example, that comes out to \\(1000 \\cdot 100 + 100 \\cdot 784 = 100000 + 78400 = 178400\\), which is only \\(\\frac{178400}{784000} \\approx 0.227\\) or 22.7% of the numbers we’d have to store otherwise. We’ve thus compressed our data by a factor of about \\(\\frac{1}{0.227} \\approx 4.4\\). That’s a 4.4x compression of the original images.\nNow, this kind of PCA compression isn’t perfect, or lossless, since we can’t recover the original images exactly. But we can still recover the most fundamental features of the image, which in this case are the handwritten digits. This kind of compression is lossy, since it irreversibly throws away some information in the original data. Yet, it still maintains enough information to be useful in many settings."
  },
  {
    "objectID": "notebooks/tensors.html",
    "href": "notebooks/tensors.html",
    "title": "8  Tensor Algebra",
    "section": "",
    "text": "Talk about what a tensor is by defining a dyad, extending it to higher rank, and defining an arbitrary tensor as a linear combination of dyads in a tensor space\nTalk about linearity, tensor contraction, outer products, trace (contracting two indices)\nTalk about the Hessian of a vector-valued function as a rank-3 tensor\nFind ML-relevant topics to cover where tensors arise"
  },
  {
    "objectID": "notebooks/multivariate-calculus.html#partial-differentials",
    "href": "notebooks/multivariate-calculus.html#partial-differentials",
    "title": "9  Multivariate Calculus",
    "section": "9.1 Partial Differentials",
    "text": "9.1 Partial Differentials\nIn univariate calculus we were interested in functions of the form \\(y=f(x)\\), where \\(y\\) is a single output that depends continuously on a single input \\(x\\). We can extend differentiation to higher dimensions as well. To keep things simple, let’s start with bivariate functions of the form \\(z = f(x,y)\\). The output \\(z\\) will now depend continuously on two input variables \\(x\\) and \\(y\\).\nRecall the graph of a bivariate function is a two-dimensional surface. For example, here’s a plot of the function\n\\[z = x^2 + y^2.\\]\nThe graph of this function is an upward-facing bowl centered at the origin \\((0,0,0)\\).\n\n\nCode\nx = np.linspace(-10, 10, 100)\ny = np.linspace(-10, 10, 100)\nf = lambda x, y: x**2 + y**2\n\nplot_function_3d(x, y, f, title='$z=x^2+y^2$', titlepad=10, labelpad=5, ticks_every=[4, 4, 50], dist=12,\n                 figsize=(6, 5))\n\n\n\n\n\n\n\n\n\nSuppose we have some bivariate function \\(z=f(x,y)\\). We ultimately want to figure out how does a small change in both inputs create small changes in the output. It’s simpler to start with the case where we only change one input at a time, while imagining the other one is constant.\nLet’s start by pretending \\(y\\) is constant and only \\(x\\) is allowed to vary. Then for all practical purposes \\(z\\) is a function of \\(x\\) alone. To make this easier to visualize I’ll write \\(z = f(x, \\color{red}{y}\\color{black})\\) to represent this idea. Variables in black will be allowed to vary, and variables in red are assumed to be held constant.\nLet me also go ahead and introduce the notion of a partial differential. In situations where we’re only varying one input variable while holding the rest fixed, it’s common to use a special notation for the differentials. Instead of writing \\(dz\\) for the differential of \\(z\\), we’d write \\(\\partial z\\) (pronounced “partial z”) to make it clear we’re only varying one input. If the input we’re varying is \\(x\\), we might write \\(\\partial_x z\\) to make this extra clear. If we’re varying \\(y\\), we’d write \\(\\partial_y z\\). Since the input differentials \\(dx\\) and \\(dy\\) don’t depend on anything we can denote them either way. That is, \\(\\partial x = dx\\) and \\(\\partial y = dy\\).\nNow, suppose we change \\(x\\) by some infinitesimal amount \\(\\partial x\\). Then \\(z\\) will change by an amount\n\\[\\partial_x z = f(x + \\partial x, \\color{red}{y}\\color{black}) - f(x, \\color{red}{y}\\color{black}).\\]\nRemember, \\(z=f(x, \\color{red}{y}\\color{black})\\) is a univariate function of \\(x\\) alone. As long as you’ve got that clear in your mind, it’s okay to write the function as \\(z=f(x, y)\\) and mentally remember \\(y\\) is fixed. Dividing both sides by \\(\\partial x\\) gives some kind of derivative, which we call the partial derivative with respect to \\(x\\),\n\\[\\frac{\\partial z}{\\partial x} = \\frac{f(x+\\partial x, \\color{red}{y}\\color{black}) - f(x, \\color{red}{y}\\color{black})}{\\partial x}.\\]\nWhen writing the partial derivative, we tend to lazily write \\(\\partial z\\) instead of \\(\\partial_x z\\) since the denominator makes it clear what we’re varying. Just as with the ordinary derivative, the partial derivative says how much the output will vary in response to small changes in the input, except with the caveat that we’re only varying \\(x\\) and keeping \\(y\\) fixed.\nWe can of course do exactly the same thing with the variables reversed, varying \\(y\\) and fixing \\(x\\). In that case, we’d have \\(z = f(\\color{red}{x}\\color{black}, y)\\). If we change \\(y\\) by an infinitesimal amount \\(\\partial y\\), then \\(z\\) changes by an amount\n\\[\\partial_y z = f(\\color{red}{x}\\color{black}, y + \\partial y) - f(\\color{red}{x}\\color{black}, y).\\]\nDividing everything by \\(\\partial y\\) gives the partial derivative with respect to \\(y\\),\n\\[\\frac{\\partial z}{\\partial y} = \\frac{f(\\color{red}{x}\\color{black}, y + \\partial y) - f(\\color{red}{x}\\color{black}, y)}{\\partial y}.\\]\nThe partial derivative with respect to \\(y\\) says how much \\(z\\) will change in response to small changes in \\(y\\) alone, at a given fixed value of \\(x\\).\nIt’s a good idea to go ahead and do a couple of examples. Despite all the weird notation floating around, calculating partial derivatives symbolically isn’t hard. All you do is differentiate the function with respect to the given variable while pretending the other one is a constant. Since it’s just univariate differentiation, all the differentiation rules we learned before carry over as is.\nAs a simple example, take the function I plotted above, \\(z = x^2 + y^2\\). In that case, if we treat \\(y\\) as constant and vary \\(x\\), then the partial differential of \\(z\\) is\n\\[\\partial_x z = f(x + \\partial x, \\color{red}{y}\\color{black}) - f(x, \\color{red}{y}\\color{black}) = \\big((x + \\partial x)^2 + \\color{red}{y^2}\\color{black}{\\big) - \\big(x^2 + }\\color{red}{y^2}\\color{black}{\\big) \\approx 2x \\partial x,}\\]\nDividing both sides by \\(\\partial x\\) we get the partial derivative with respect to \\(x\\),\n\\[\\frac{\\partial z}{\\partial x} = 2x.\\]\nSimilarly, if we vary \\(y\\) and hold \\(x\\) fixed, the partial differential of \\(z\\) is\n\\[\\partial_y z = f(\\color{red}{x}\\color{black}, y + \\partial y) - f(\\color{red}{x}\\color{black}, y) = \\big(\\color{red}{x^2}\\color{black} + (y + \\partial y)^2 \\big) - \\big(\\color{red}{x^2}\\color{black} + y^2\\big) \\approx 2y \\partial y.\\]\nDividing both sides by \\(\\partial y\\), we get the partial derivative with respect to \\(y\\),\n\\[\\frac{\\partial z}{\\partial y} = 2y.\\]\nTo take a more complicated example, suppose we had the function\n\\[z = e^x \\sin 5y - \\frac{4y}{x}.\\]\nUsing the differential rules directly, the partial differentials turn out to be\n\\[\n\\partial_x z = \\bigg(e^x \\sin 5y + \\frac{4y}{x^2}\\bigg) \\partial x, \\quad\n\\partial_y z = \\bigg(5e^x \\cos 5y - \\frac{4}{x}\\bigg) \\partial y.\n\\]\nDividing both sides by \\(\\partial x\\) and \\(\\partial y\\) respectively, the partial derivatives are then\n\\[\n\\frac{\\partial z}{\\partial x} = e^x \\sin 5y + \\frac{4y}{x^2}, \\quad\n\\frac{\\partial z}{\\partial y} = 5e^x \\cos 5y - \\frac{4}{x}.\n\\]\nWe can calculate partial derivatives in sympy the same way we did ordinary derivatives. The only difference is we need to define two input symbols, and we need to be careful which one we differentiate the function with respect to. Here’s a sympy calculation of the partial derivatives from the previous example.\n\n\nCode\nx, y = sp.symbols('x y')\nz = sp.exp(x) * sp.sin(5 * y) - 4 * y / x\ndzdx = z.diff(x)\ndzdy = z.diff(y)\n\nprint(f'z = {z}')\nprint(f'∂z/∂x = {dzdx}')\nprint(f'∂z/∂y = {dzdy}')\n\n\nz = exp(x)*sin(5*y) - 4*y/x\n∂z/∂x = exp(x)*sin(5*y) + 4*y/x**2\n∂z/∂y = 5*exp(x)*cos(5*y) - 4/x\n\n\nWe can calculate partial derivatives numerically just like we could ordinary derivatives. The exact same caveats apply. We’d want to choose \\(\\partial x\\) and \\(\\partial y\\) to be small, but not too small to avoid numerical roundoff. Below I’ll define two functions diff_x(f, x, y, dx) and diff_y(f, x, y, dy) to calculate the \\(x\\) and \\(y\\) partials respectively.\nAs an example, I’ll numerically differentiate the function \\(z=x^2+y^2\\) at the point \\((1,1)\\). In this case, both partial derivatives should be exactly \\(2\\). If dx and dy are both 1e-5, the numerical estimate should agree with the exact answer to within an error of 1e-5.\n\n\nCode\ndef diff_x(f, x, y, dx=1e-5):\n    dz = f(x + dx, y) - f(x, y)\n    return dz / dx\n\ndef diff_y(f, x, y, dy=1e-5):\n    dz = f(x, y + dy) - f(x, y)\n    return dz / dy\n\nf = lambda x, y: x**2 + y**2\nx, y = (1, 1)\nprint(f'∂z/∂x = {diff_x(f, x, y)}')\nprint(f'∂z/∂y = {diff_y(f, x, y)}')\n\n\n∂z/∂x = 2.00001000001393\n∂z/∂y = 2.00001000001393\n\n\nAs in the univariate case, a bivariate function needs to satisfy certain conditions in order to be differentiable. First, it needs to be continuous. We’d say a bivariate function \\(z=f(x,y)\\) is continuous at a point \\((a,b)\\) if, whenever \\(x\\) is infinitesimally close to \\(a\\) and \\(y\\) is infinitesimally close to \\(b\\), the output \\(z=f(x,y)\\) is infinitesimally close to \\(f(a,b)\\). Informally, this just says if \\(x \\approx a\\) and \\(y \\approx b\\), then \\(z \\approx f(a,b)\\). The function itself is called continuous if it’s continuous at every possible point \\((a,b)\\) in the plane. This is just the bivariate way of saying the function’s graph can’t have any jumps or sudden breaks in it anywhere.\nTo be differentiable, the quotients forming both partial derivatives need to be well-defined. Graphically this just says the function’s graph also can’t any sharp kinks in it in either the \\(x\\) direction or the \\(y\\) direction.\nAs in the univariate case, just about all the bivariate functions we’re interested in are continuous and differentiable, including sums, powers, products, exponents, logarithms, trig functions, and quotients. Here are a few examples of continuous and differentiable bivariate functions:\n\n\\(z = x + y\\) for all \\(x, y\\).\n\\(z = 10x - y^2\\) for all \\(x, y\\).\n\\(z = e^{x - y}\\) for all \\(x, y\\).\n\\(z = \\log(10xy - 4) \\sin x + \\cos y\\) whenever \\(10xy-4 > 0\\).\n\\(z = \\frac{10}{5x - y}\\) whenever \\(5x-y \\neq 0\\)."
  },
  {
    "objectID": "notebooks/multivariate-calculus.html#total-differentials",
    "href": "notebooks/multivariate-calculus.html#total-differentials",
    "title": "9  Multivariate Calculus",
    "section": "9.2 Total Differentials",
    "text": "9.2 Total Differentials\nThe partial differentials only say how much the output changes in response to one variable changing. What if both variables are allowed to change? Suppose again we have a function \\(z=f(x,y)\\), but this time we change both \\(x\\) by \\(dx\\) and \\(y\\) by \\(dy\\). The total amount \\(z\\) would change evidently must be\n\\[dz = f(x+dx, y+dy) - f(x, y).\\]\nSince we’re allowing both inputs to vary now we can use the regular \\(dz\\) notation for the differential. We’d call \\(dz\\) the total differential of \\(z\\).\nLet’s take a quick example. Take again the quadratic function \\(z=x^2 + y^2\\). If we follow the above rule, then we’d have\n\\[dz = f(x+dx, y+dy) - f(x, y) = \\big((x + dx)^2 + (y + dy)^2\\big) - \\big(x^2 + y^2\\big) = 2xdx + 2ydy + dx^2 + 2dxdy + dy^2.\\]\nIf we assume \\(dx\\) and \\(dy\\) are both infinitesimal then we can neglect the quadratic terms, in which case we’d get\n\\[dz = 2xdx + 2ydy.\\]\nNotice something interesting. The term containing \\(dx\\) is just the partial differential \\(\\partial_x z\\), and the term containing \\(dy\\) is just the partial differential \\(\\partial_y z\\),\n\\[dz = \\partial_x z + \\partial_y z.\\]\nWe can write the same thing in terms of the partial derivatives too,\n\\[dz = \\frac{\\partial z}{\\partial x}dx + \\frac{\\partial z}{\\partial y}dy.\\]\nIt turns out this fact is true for any differentiable function \\(z=f(x,y)\\). To see why, notice that by adding and subtracting \\(f(x+dx, y)\\) to \\(dz\\), we’d have\n\\[dz = f(x+dx, y+dy) - f(x, y) = \\big(f(x+dx, y+dy) - f(x+dx, y) \\big) + \\big(f(x+dx, y) - f(x,y) \\big).\\]\nWe can think of this differential as a sum of two pieces. The second piece is just the partial differential \\(\\partial_x z\\) at the point \\((x,y)\\). The first piece is just the partial differential \\(\\partial_y z\\) at the point \\((x+dx,y)\\), but since \\(dx\\) is infinitesimal this will be approximately the partial differential at \\((x,y)\\) too. That is, we can always write the total differential \\(dz\\) in the form\n\\[dz = \\partial_x z + \\partial_y z = \\frac{\\partial z}{\\partial x}dx + \\frac{\\partial z}{\\partial y}dy.\\]\nEvidently, we can think of an arbitrary small change in \\(z\\) as being composed of two independent terms: One term that says how much \\(z\\) will change in response to only \\(x\\) varying, and another term that says how much \\(z\\) will change in response to only \\(y\\) varying. This is why I covered partial differentials first. We can build any total differential out of a sum of partial differentials. They’re in some sense a basis in the linear algebraic sense."
  },
  {
    "objectID": "notebooks/multivariate-calculus.html#tangent-planes",
    "href": "notebooks/multivariate-calculus.html#tangent-planes",
    "title": "9  Multivariate Calculus",
    "section": "9.3 Tangent Planes",
    "text": "9.3 Tangent Planes\nLet’s now try to get a visual understanding what these terms mean. Recall in the univariate case that we used the formula \\(dy = \\frac{dy}{dx} dx\\) to get a formula for the tangent line to the function \\(y=f(x)\\) at the point \\(x=a\\),\n\\[f(x) \\approx f(a) + \\frac{d}{dx} f(a) \\cdot (x-a).\\]\nThe tangent line was the best approximation of the function \\(y=f(x)\\) near the point \\(x=a\\), and the slope of the tangent line was just the derivative taken at the point \\(a\\). We can do a similar thing in the bivariate case.\nSuppose we’re interested in approximating the function \\(z=f(x,y)\\) around some point \\((a,b)\\). Provided \\(x\\) is close to \\(a\\) and \\(y\\) is close to \\(b\\), we can use the total differential formula to write\n\\[f(x,y) \\approx f(a,b) + \\frac{\\partial}{\\partial x}f(a,b) \\cdot (x-a) + \\frac{\\partial}{\\partial x}f(a,b) \\cdot (y-b).\\]\nSince the partial derivative terms \\(\\frac{\\partial}{\\partial x}f(a,b)\\) and \\(\\frac{\\partial}{\\partial y}f(a,b)\\) are just constants that depend only on \\(a\\) and \\(b\\), what we’re looking at is the formula for a plane. It’s called the tangent plane. The tangent plane is just the plane in 3D space that hugs the surface of \\(z=f(x,y)\\) at the point \\(\\big(a,b,f(a,b)\\big)\\).\nRecall for the quadratic function \\(z=x^2 + y^2\\) had total differential \\(dz = 2xdx + 2ydy\\). Suppose we were interested in the tangent plane of the function at the point \\((1,1)\\). Since\n\\[f(1,1) = 1^2 + 1^2 = 2, \\quad \\frac{\\partial}{\\partial x}f(1,1) = 2 \\cdot 1 = 2, \\quad \\frac{\\partial}{\\partial y}f(1,1) = 2 \\cdot 1 = 2,\\]\nthe tangent plane would just be given by\n\\[z = 2 + 2(x-1) + 2(y-1).\\]\nHere’s a 3D plot of the surface of \\(z=x^2 + y^2\\) along with this tangent plane. Notice how the green plane is hugging tangent to the blue surface at a single point, namely the exact point \\((1,1,2)\\). The black curves shown on the blue surface correspond to the curves where \\(x\\) or \\(y\\) are constant, respectively. The black curves shown on the green surface correspond to the tangent lines of those same curves. The slope of those lines represents the value of that partial derivative at the point \\((1, 1, 2)\\).\n\n\nCode\nf = lambda x, y: x**2 + y**2\ndfdx = lambda x, y: (2 * x, 2 * y)\n\na = b = 1\n\nx = np.linspace(-2, 2, 100)\ny = np.linspace(-2, 2, 100)\n\nf_tangent = lambda x, y: dfdx(a, b)[0] * (x - a) + dfdx(a, b)[1] * (y - b) + f(a, b)\n\nplot_function_3d(x, y, [f, f_tangent], labelpad=5, ticks_every=[1, 1, 4], dist=12, figsize=(6, 6),\n                 xlim=(-2, 2), ylim=(-2, 2), zlim=(-10, 10), elev=30, azim=-45,\n                 points=[(a, b, f(a, b))], colors=['blue', 'green'], alpha=0.4,\n                 curves = [(x, 0 * x + b, f(x, b**2)), \n                           (0 * y + a, y, f(a**2, y)),\n                           (x, 0 * x + b, f_tangent(x, b**2)),\n                           (0 * y + a, y, f_tangent(a**2, y))\n                          ],\n                 title=f'Tangent Plane to $z=x^2+y^2$ at ${(1, 1, 2)}$', titlepad=0)\n\n\n\n\n\n\n\n\n\nAs long as we’re near the point \\((a,b)\\), the function \\(z=f(x,y)\\) is well approximated by its tangent plane. Said differently, we’ve managed to linearize the function near the point \\((a,b)\\). As long as \\((x,y)\\) is close to \\((a,b)\\), we can for all practical purposes pretend the point is on the tangent plane. Anytime you see a plane or a line like this, you should immediately think linear algebra. Indeed, we’re on the cusp of seeing where calculus and linear algebra meet."
  },
  {
    "objectID": "notebooks/multivariate-calculus.html#the-gradient",
    "href": "notebooks/multivariate-calculus.html#the-gradient",
    "title": "9  Multivariate Calculus",
    "section": "9.4 The Gradient",
    "text": "9.4 The Gradient\nSo far we’ve talked about partial derivatives and total differentials, but we’ve yet to talk about a total derivative. Let’s look again at the formula for the total differential,\n\\[dz = \\frac{\\partial z}{\\partial x}dx + \\frac{\\partial z}{\\partial y}dy.\\]\nWhat would it even mean in this case to talk about a derivative as a ratio of total differentials? To make headway here we should step back and stare at this equation. If you look carefully, you’ll see this equation is screaming at us to use vector notation. It kind of looks like a dot product between two vectors.\nIf we define the following two vectors in the plane,\n\\[d\\mathbf{x} = (dx, dy), \\quad \\mathbf{g} = \\mathbf{g}(\\mathbf{x}) = \\bigg(\\frac{\\partial z}{\\partial x}, \\frac{\\partial z}{\\partial y}\\bigg),\\]\nthen we can write the total differential as the dot product \\(dz = \\mathbf{g} \\cdot d\\mathbf{x}\\). Notice the analogy between this equation and the one for the univariate differential \\(dy = \\frac{dy}{dx} dx\\). Evidently, the vector \\(\\mathbf{g}\\) acts like some kind of vector derivative. It’s called the gradient vector, or usually just the gradient. By abusing notation a little bit, we’ll sometimes write the gradient as a ratio of differentials by pretending we can divide by \\(d\\mathbf{x}\\),\n\\[\\frac{dz}{d\\mathbf{x}} = \\mathbf{g} = \\bigg(\\frac{\\partial z}{\\partial x}, \\frac{\\partial z}{\\partial y}\\bigg),\\]\nDoing this makes it obvious that we can go back and forth between the total differential \\(dz\\) and the gradient \\(\\mathbf{g}\\) by “multiplying” both sides by \\(d\\mathbf{x}\\).\nAnother popular notation for the gradient is to use the del operator \\(\\nabla\\) to write the gradient as \\(\\nabla z\\) or \\(\\nabla f(\\mathbf{x})\\). I personally prefer to avoid this notation in most cases since, on top of introducing yet another strange symbol, \\(\\nabla z\\) doesn’t much help you remember what it is or what it does.\nThis new vector derivative suggests we should step back and think about multivariate calculus from the point of view of vectors instead of multiple variables. Instead of imagining a bivariate function as a function of the form \\(z = f(x,y)\\), let’s instead imagine it as a vector input function \\(z = f(\\mathbf{x})\\), where \\(\\mathbf{x} = (x, y)\\) is a vector in the plane. It means exactly the same thing, except we’re now overloading the same function \\(z=f(\\cdot)\\) to map 2-dimensional vector inputs \\(\\mathbf{x}\\) to scalar outputs \\(z\\).\nLet’s work a couple of examples. Take the quadratic function \\(z = x^2 + y^2\\) again. We already showed its two partials are \\(2x\\) and \\(2y\\). Putting these into a vector gives the gradient, \\(\\mathbf{g} = (2x, 2y)\\). It’s interesting to think about this example in terms of vectors. Notice that this quadratic function is just the squared norm of \\(\\mathbf{x}\\),\n\\[z = x^2 + y^2 = \\mathbf{x} \\cdot \\mathbf{x} = ||\\mathbf{x}||^2.\\]\nWhat we’ve evidently just shown is the gradient of the squared norm \\(z = ||\\mathbf{x}||^2\\) is \\(\\mathbf{g} = 2\\mathbf{x}\\). Notice how similar this looks to the scalar case, where the derivative of \\(y=x^2\\) is \\(\\frac{dy}{dx} = 2x\\).\nNow let’s take the function \\(z = ax + by\\). This is easy. The gradient is just \\(\\mathbf{g} = (a, b)\\). Again, what’s more interesting is thinking in terms of vectors. Let \\(\\mathbf{a} = (a,b)\\). Then we can write \\(z = \\mathbf{a} \\cdot \\mathbf{x}\\). What we’ve just shown then is the gradient of \\(\\mathbf{a} \\cdot \\mathbf{x}\\) is just \\(\\mathbf{a}\\). Again, compare with the scalar case, where the derivative of \\(y=ax\\) is just \\(\\frac{dy}{dx} = a\\).\nYou can use sympy to compute gradients in a couple different ways. One way is just to calculate each of the partial derivatives one-by-one like I did before. Another way is to define a matrix symbol and take its derivative. Here’s an example of how to do this using the previous example.\n\n\nCode\nx = sp.MatrixSymbol('x', 2, 1)\na = sp.MatrixSymbol('a', 2, 1)\n\nz = a.T @ x\ngrad = z.diff(x)\nprint(f'z = {z}')\nprint(f'g = {grad}')\n\n\nz = a.T*x\ng = a\n\n\nTo compute the gradient numerically, we just need to compute all the partial derivatives and put them into a vector. Since I already wrote two functions diff_x and diff_y to calculate the partial derivatives, all we need to do is call those two functions and put the outputs in a vector. I’ll create a function called diff(f, x, y, dx, dy) to do this. When we get more into vector notation I’ll clean it up a little so it generalizes to higher dimensions.\nI’ll again use the example of \\(z = x^2 + y^2\\) to calculate the gradient numerically at the point \\((1,1)\\). Again, nothing special here. We’re just putting the partial derivatives in a vector and returning it.\n\n\nCode\ndef grad(f, x, y, dx=1e-5, dy=1e-5):\n    dzdx = diff_x(f, x, y, dx)\n    dzdy = diff_y(f, x, y, dy)\n    gradient = np.array([dzdx, dzdy])\n    return gradient\n    \nf = lambda x, y: x**2 + y**2\n\ngradient = grad(f, 1, 1)\nprint(f'g = {gradient}')\n\n\ng = [2.00001 2.00001]\n\n\nWe can also write the tangent plane equation in vector notation. If \\(\\mathbf{a} = (a,b)\\) and \\(\\mathbf{x} \\approx \\mathbf{a}\\), then the tangent plane of \\(z=f(\\mathbf{x})\\) at \\(\\mathbf{a}\\) is given by\n\\[f(\\mathbf{x}) \\approx f(\\mathbf{a}) + \\mathbf{g}(\\mathbf{a})^\\top (\\mathbf{x} - \\mathbf{a}).\\]\nThe notation \\(\\mathbf{g}(\\mathbf{a})\\) just means to take the gradient and evaluate it at \\(\\mathbf{x} = \\mathbf{a}\\). From this point of view we can think of the gradient as a linear map that maps vector differences \\(d\\mathbf{x} \\approx (\\mathbf{x} - \\mathbf{a})\\) to scalar differences \\(dz \\approx f(\\mathbf{x}) - f(\\mathbf{a})\\).\nNow, consider again an arbitrary function \\(z = f(\\mathbf{x})\\). Suppose we perturbed the input \\(\\mathbf{x}\\) in some arbitrary direction by an amount \\(d\\mathbf{x}\\). This would have to change the output \\(z\\) by an amount\n\\[dz = f(\\mathbf{x} + d\\mathbf{x}) - f(\\mathbf{x}).\\]\nBut I just showed this same change is given by the dot product of the gradient vector \\(\\mathbf{g}\\) with \\(d\\mathbf{x}\\),\n\\[dz = \\mathbf{g} \\cdot d\\mathbf{x}.\\]\nThis suggests a way to define the gradient as a kind of ratio of differentials. Notice that by factoring out its norm we can write the differential vector \\(d\\mathbf{x}\\) as a scalar times a unit vector in the \\(d\\mathbf{x}\\) direction,\n\\[d\\mathbf{x} = ||d\\mathbf{x}|| \\mathbf{e}_{dx}.\\]\nIf we divide both sides of the total differential by the scalar norm \\(||d\\mathbf{x}||\\), we evidently get\n\\[\\mathbf{g} \\cdot \\mathbf{e}_{dx} = \\frac{dz}{||d\\mathbf{x}||} = \\frac{f(\\mathbf{x} + d\\mathbf{x}) - f(\\mathbf{x})}{||d\\mathbf{x}||}.\\]\nSaid differently, the projection of the gradient \\(\\mathbf{g}\\) in the direction of \\(d\\mathbf{x}\\) is just the ratio of the total differential over the norm of \\(d\\mathbf{x}\\). This projection is sometimes called the directional derivative of \\(z\\) in the direction of \\(d\\mathbf{x}\\). The directional derivative is what we get if we ask, “What is the partial derivative of a function along an arbitrary line in the plane, not necessarily the x or y axes?” If \\(\\mathbf{e_u}\\) is any unit vector, the directional derivative in the \\(u\\)-direction is just\n\\[\\frac{\\partial z}{\\partial u} = \\mathbf{g} \\cdot \\mathbf{e}_u.\\]\nThe main reason I bring up the directional derivative is because it gives us a useful way to interpret the gradient graphically. Recall we can write the dot product of any two vectors in terms of the angle between them. If \\(\\theta\\) is the angle between the vectors \\(\\mathbf{g}\\) and \\(\\mathbf{e}_u\\), we evidently have\n\\[\\frac{\\partial z}{\\partial u} = ||\\mathbf{g}|| \\cdot ||\\mathbf{e}_u|| \\cos\\theta = ||\\mathbf{g}|| \\cos\\theta.\\]\nThe right-hand side will be highest when \\(\\cos\\theta = 1\\), which occurs when \\(\\mathbf{g}\\) is parallel with \\(\\mathbf{e}_u\\). This means the directional derivative will be highest when it’s pointing in the same direction as the gradient. Since derivatives are slopes, saying the directional derivative is highest in the direction of the gradient is the same thing as saying the slope of the curve is steepest in that direction. That is, the gradient points in the direction of steepest ascent of \\(z=f(\\mathbf{x})\\), i.e. the direction that’s sloping up the fastest at a given point.\nFor example, take the quadratic \\(z = x^2 + y^2\\) again. Its gradient is \\(\\mathbf{g} = (2x, 2y)\\). Which way does this point? Well, since \\(\\mathbf{g} = 2\\mathbf{x}\\), it clearly points in the direction of \\(\\mathbf{x}\\). That’s radially outward from the origin. If we look at the plot of this surface again, it’s clear that the function is increasing steepest as we move radially outward at any given point.\nHere’s a plot of this function again, but showing the gradient at the point \\((1,1)\\) as a red vector. Notice how the arrow is pointing radially outward from the point, or “up the bowl”.\n\n\nCode\nf = lambda x, y: x**2 + y**2\ndfdx = lambda x, y: (2 * x, 2 * y)\n\na, b = 1, 1\nx = np.linspace(-3, 3, 100)\ny = np.linspace(-3, 3, 100)\n\nf_tangent = lambda x, y: 2 * a * (x - a) + 2 * b * (y - b) + 2\ngradient = dfdx(a, b)\n\n# plot_tangent_plane(x, y, a, b, f, f_tangent, dfdx, plot_grad=True, grad_scale=2, title=f'')\nplot_function_3d(x, y, f, labelpad=5, ticks_every=[1, 1, 5], dist=12, figsize=(6, 6),\n                 xlim=(-3, 3), ylim=(-3, 3), zlim=(-5, 20), elev=50, azim=-60, titlepad=10,\n                 points=[(a, b, f(a, b))], title=f'Gradient Vector at {(a, b)}', alpha=0.8,\n                 # curves = [(x, 0 * x + b, f(x, b**2)), (0 * y + a, y, f(a**2, y))],\n                 arrows=[[[a, b, f(a, b)], [gradient[0] / 2, gradient[1] / 2, 0]]])\n\n\n\n\n\n\n\n\n\nThis may be easier to visualize if we look at the contour plot instead. Here’s the contour plot of the same surface, shown in blue. The tangent plane contours are shown in green. The gradient at \\((1,1)\\) is the red vector. The interesting thing to notice here is that the gradient is pointing perpendicular to the contour lines of both the surface and the tangent plane.\nIf you think about the contour plot as a topographical map, following the gradient vector at each point would take you upward along the steepest possible path, probably not what you want if you’re a hiker, but it’s definitely what you want if you’re trying to get to the peak of a surface as quickly as possible.\n\n\nCode\nf = lambda x, y: x**2 + y**2\ndfdx = lambda x, y: (2 * x, 2 * y)\n\na, b = 1, 1\nx = np.linspace(-4, 4, 100)\ny = np.linspace(-4, 4, 100)\n\nf_tangent = lambda x, y: 2 * a * (x - a) + 2 * b * (y - b) + 2\ngradient = dfdx(a, b)\nlevels = [[2, 4, 8, 14, 20], [0, 2, 4, 6, 8, 10, 12]]\ncolors = [['steelblue'] * len(levels), ['green']]\nplot_contour(x, y, [f, f_tangent], points=[(a, b)], arrows=[[(a, b), (gradient[0], gradient[1])]], figsize=(4, 4),\n             ticks_every=[1, 1], show_clabels=True, alphas=[0.6, 0.9], colors=colors, levels=levels,\n             title=f'Gradient Vector at {(a, b)}')\n\n\n\n\n\n\n\n\n\nSince the gradient can be thought of as a vector-valued function that maps vectors \\(\\mathbf{x}\\) to new vectors \\(\\mathbf{g}\\), we can also think of the gradient as a vector field. Here’s a vector field plot of the gradient of \\(z=x^2 + y^2\\) for different choices of \\(\\mathbf{x}\\). Notice how all the gradients are pointing outwards. This follows from the fact that \\(\\mathbf{g} = 2\\mathbf{x}\\) for this function. The arrows are also getting bigger the further we get away from the origin. If you follow any flow of arrows, you’ll move in the direction of steepest ascent.\n\n\nCode\nplot_vector_field(dfdx, xlim=(-3, 3), ylim=(-3, 3), n_points=20, color='red', alpha=1,\n                      scale=None, figsize=(4, 3), title=f'$g=2x$ as a Vector Field')\n\n\n\n\n\n\n\n\n\nNotice something. Sense the gradient vectors all point up out of the bowl. The negative gradients must all point in the opposite direction, i.e. down into the bowl. If the positive gradient points in the direction of steepest ascent, then the negative gradient must point in the direction of steepest descent. Following the negative gradient takes you towards the bottom of the bowl as fast as possible. It’s hard to overstate how important this fact is to machine learning. The fundamental optimizer used to train most machine learning models is gradient descent. I’ll get back to this in a lot more detail in future lessons."
  },
  {
    "objectID": "notebooks/multivariate-calculus.html#the-hessian",
    "href": "notebooks/multivariate-calculus.html#the-hessian",
    "title": "9  Multivariate Calculus",
    "section": "9.5 The Hessian",
    "text": "9.5 The Hessian\nIn basic calculus we could also talk about derivatives of derivatives. The second derivative of a univariate function \\(y=f(x)\\) is just the derivative of the derivative, i.e.\n\\[\\frac{d^2 y}{dx^2} = \\frac{d}{dx} \\frac{dy}{dx}.\\]\nWe can do the same thing in multivariate calculus, but we have to be careful about what we mean by the second derivative of a function.\nSince partial derivatives are just univariate derivatives, we can take second partial derivatives in the usual way. The only difference is that we can now have mixed partial derivatives. If \\(z=f(x,y)\\) is a bivariate function, it will have not \\(2\\), but \\(2^2=4\\) second partial derivatives,\n\\[\\frac{\\partial^2 z}{\\partial x^2}, \\quad \\frac{\\partial^2 z}{\\partial x \\partial y}, \\quad \\frac{\\partial^2 z}{\\partial y \\partial x}, \\quad \\frac{\\partial^2 z}{\\partial y^2}.\\]\nTo work an example, let’s again look at the function\n\\[z = e^x \\sin 5y - \\frac{4y}{x}.\\]\nWe already saw that the first partial derivatives are given by\n\\[\n\\frac{\\partial z}{\\partial x} = e^x \\sin 5y + \\frac{4y}{x^2}, \\quad\n\\frac{\\partial z}{\\partial y} = 5e^x \\cos 5y - \\frac{4}{x}.\n\\]\nWe can get the second partial derivatives by differentiating both of these, each with respect to both \\(x\\) and \\(y\\),\n\\[\\begin{align*}\n\\frac{\\partial^2 z}{\\partial x^2} &= \\frac{\\partial}{\\partial x} \\frac{\\partial z}{\\partial x} = e^x \\sin 5y - \\frac{8y}{x^3}, &\n\\frac{\\partial^2 z}{\\partial y \\partial x} &= \\frac{\\partial}{\\partial y} \\frac{\\partial z}{\\partial x} = 5 e^x \\cos 5y + \\frac{4}{x^2}, \\\\\n\\frac{\\partial^2 z}{\\partial x \\partial y} &= \\frac{\\partial}{\\partial x} \\frac{\\partial z}{\\partial y} = 5e^x \\cos 5y + \\frac{4}{x^2}, &\n\\frac{\\partial^2 z}{\\partial y^2} &= \\frac{\\partial}{\\partial y} \\frac{\\partial z}{\\partial y} = -25e^x \\sin 5y.\n\\end{align*}\\]\nOf course, sympy can calculate these for you. For example, to calculate the mixed partial \\(\\frac{\\partial^2 z}{\\partial y \\partial x}\\), just call z.diff(x).diff(y). Notice how the ordering of the method call is backwards from the partial derivative notation. Be careful about that.\n\n\nCode\nx, y = sp.symbols('x y')\nz = sp.exp(x) * sp.sin(5 * y) - 4 * y / x\n\nprint(f'z = {z}')\nprint(f'∂∂z/∂x∂x = {z.diff(x).diff(x)}, \\t ∂∂z/∂y∂x = {z.diff(y).diff(x)}')\nprint(f'∂∂z/∂x∂y = {z.diff(x).diff(y)}, \\t ∂∂z/∂y∂y = {z.diff(y).diff(y)}')\n\n\nz = exp(x)*sin(5*y) - 4*y/x\n∂∂z/∂x∂x = exp(x)*sin(5*y) - 8*y/x**3,   ∂∂z/∂y∂x = 5*exp(x)*cos(5*y) + 4/x**2\n∂∂z/∂x∂y = 5*exp(x)*cos(5*y) + 4/x**2,   ∂∂z/∂y∂y = -25*exp(x)*sin(5*y)\n\n\nI laid the second partials in this example out as a \\(2 \\times 2\\) grid on purpose to suggest that we should probably organize them into a \\(2 \\times 2\\) matrix. This matrix is called the Hessian matrix, or just the Hessian for short. I’ll denote it by the symbol \\(\\mathbf{H}\\),\n\\[\n\\mathbf{H} = \\mathbf{H}(\\mathbf{x}) =\n\\begin{pmatrix}\n\\frac{\\partial^2 z}{\\partial x^2} & \\frac{\\partial^2 z}{\\partial y \\partial x} \\\\\n\\frac{\\partial^2 z}{\\partial x \\partial y} & \\frac{\\partial^2 z}{\\partial y^2}\n\\end{pmatrix}.\n\\]\nIn the previous example the Hessian matrix turned out to be\n\\[\n\\mathbf{H} =\n\\begin{pmatrix}\ne^x \\sin 5y - \\frac{8y}{x^3} & 5 e^x \\cos 5y + \\frac{4}{x^2} \\\\\n5 e^x \\cos 5y + \\frac{4}{x^2} & -25e^x \\sin 5y \\\\\n\\end{pmatrix}.\n\\]\nIf you stare at the off diagonals of this example, you’ll see this matrix is symmetric, i.e. \\(\\mathbf{H}^\\top = \\mathbf{H}\\). This is a more or less general fact. As long as the second partial derivatives are all continuous, their Hessian matrix will be symmetric. This will generally be the case for us in practice.\nTo calculate the Hessian numerically, we’d need to calculate all the second partial derivatives and put them into a matrix. Here’s the simplest way one might do this for a bivariate function. The function I’ll use looks just like the one for the gradient, except now we need to take derivatives twice. I’ll use it to calculate the Hessian of the quadratic \\(z = x^2 + y^2\\) at the point \\((1,1)\\). The exact answer should be \\(\\mathbf{H} = \\text{diag}(2, 2)\\). Taking both \\(dx\\) and \\(dy\\) to be 1e-4, to within an error of around 1e-4 the answer seems right.\n\n\nCode\ndef hessian(f, x, y, dx=1e-4, dy=1e-4):\n    dfdx = lambda x, y: diff_x(f, x, y, dx)\n    dfdy = lambda x, y: diff_y(f, x, y, dy)\n    d2zdx2 = diff_x(dfdx, x, y, dx)\n    d2zdxdy = diff_x(dfdy, x, y, dx)\n    d2zdydx = diff_y(dfdx, x, y, dy)\n    d2zdy2 = diff_y(dfdy, x, y, dy)\n    hess = np.array([[d2zdx2, d2zdydx], [d2zdxdy, d2zdy2]])\n    return hess\n    \nf = lambda x, y: x**2 + y**2\nhess = hessian(f, 1, 1)\nprint(f'H = \\n{hess}')\n\n\nH = \n[[1.99999999 0.        ]\n [0.         1.99999999]]\n\n\nIt may not be completely obvious, but the Hessian is indeed the multivariate generalization of the second derivative. I’ll show why in the next section. Because of this, I’ll sometimes denote the Hessian by pretending we can divide by \\(d\\mathbf{x}^2\\) and writing\n\\[\\frac{d^2 z}{d\\mathbf{x}^2} = \\mathbf{H}.\\]\nSince we have the first and second derivatives now, we can talk about the second-order approximation of a function. If \\(z = f(\\mathbf{x})\\), and \\(\\mathbf{x}\\) is close to some vector \\(\\mathbf{a}\\), then the function’s best quadratic approximation is given by\n\\[f(\\mathbf{x}) \\approx f(\\mathbf{a}) + \\mathbf{g}(\\mathbf{a})^\\top (\\mathbf{x} - \\mathbf{a}) + \\frac{1}{2}(\\mathbf{x} - \\mathbf{a})^\\top \\mathbf{H}(\\mathbf{a}) (\\mathbf{x} - \\mathbf{a}).\\]\nRemember, \\(\\mathbf{g}(\\mathbf{a})\\) and \\(\\mathbf{H}(\\mathbf{a})\\) mean, take the gradient and Hessian and evaluate them at the point \\(\\mathbf{a}\\). This means they’ll be arrays of numbers, not arrays of functions. Instead of a plane, this approximation will try to approximate the function near \\(\\mathbf{a}\\) with a quadratic surface. The interesting thing in the multivariate case is that a quadratic surface need not even be bowl-shaped. We can also have quadratic surfaces that look like a saddle, for example\n\\[z = x^2 - y^2.\\]\n\n\nCode\nx = np.linspace(-3, 3, 100)\ny = np.linspace(-3, 3, 100)\nf = lambda x, y: x**2 - y**2\n\nplot_function_3d(x, y, f, title='$z=x^2-y^2$', titlepad=0, labelpad=5, ticks_every=[1, 1, 2], dist=12,\n                 figsize=(6, 5), elev=30, azim=60)\n\n\n\n\n\n\n\n\n\nThe presence of the quadratic form \\((\\mathbf{x} - \\mathbf{a})^\\top \\mathbf{H}(\\mathbf{a}) (\\mathbf{x} - \\mathbf{a})\\) should suggest what’s going on here. Recall that a quadratic form is closely related to the eigenvalues of a matrix. In particular, the sign of the eigenvalues say something about the sign of the quadratic form. In the scalar case, the sign of \\(h\\) in an expression like \\((x-a)h(x-a) = h \\cdot (x-a)^2\\) says whether the quadratic will bowl upwards of downwards. In that case there are only two possibilities, the function will bowl up or down. In the bivariate case there are now three depending on the sign of the eigenvalues of \\(\\mathbf{H}(\\mathbf{a})\\):\n\nBoth eigenvalues are positive: In this case, \\(\\mathbf{H}(\\mathbf{a})\\) is positive definite. This means the quadratic surface will be an upward bowl shape. The prototypical example of this kind of quadratic is \\(z = x^2 + y^2\\).\nBoth eigenvalues are negative: In this case, \\(\\mathbf{H}(\\mathbf{a})\\) is negative definite. This means the quadratic surface will be a downward bowl shape. The prototypical example of this kind of quadratic is \\(z = -(x^2 + y^2)\\).\nOne eigenvalue is positive, the other is negative: In this case, \\(\\mathbf{H}(\\mathbf{a})\\) is neither, or indefinite. This means the quadratic surface will be a saddle shape. The prototypical examples of this kind of quadratic are \\(z = x^2 - y^2\\) or \\(z = -x^2 + y^2\\).\nOne of the eigenvalues is zero: In this case, we can’t say much about what the function is doing at \\(\\mathbf{a}\\) using only a quadratic approximation. We’d have to consider higher order terms to get an idea.\n\nUsually, which of the three cases we fall into depends on the choice of the point \\(\\mathbf{a}\\). For one choice of \\(\\mathbf{a}\\), \\(\\mathbf{H}(\\mathbf{a})\\) might be positive definite. For another it might be negative definite. For another it might be indefinite. For some simple functions though, the sign of the eigenvalues don’t depend on \\(\\mathbf{a}\\). These types of functions are special.\nIf \\(\\mathbf{H}(\\mathbf{a})\\) is positive semi-definite for any point \\(\\mathbf{a}\\), we’d say the function \\(z=f(\\mathbf{x})\\) is a convex function. Convex functions will always look more or less like upward-sloping bowls. Similarly, if \\(\\mathbf{H}(\\mathbf{a})\\) is negative semi-definite for all points \\(\\mathbf{a}\\), we’d say the function \\(z=f(\\mathbf{x})\\) is a concave function. Concave functions will always look more or less like downward-sloping bowls.\nAs an example, consider again the quadratic function \\(z = x^2 + y^2\\). The Hessian of this simple function is just\n\\[\n\\mathbf{H} =\n\\begin{pmatrix}\n2 & 0 \\\\\n0 & 2 \\\\\n\\end{pmatrix} = 2 \\mathbf{I}.\n\\]\nThis means its two eigenvalues are both \\(\\lambda = 2 > 0\\). Notice the Hessian doesn’t depend on \\(x\\) or \\(y\\) at all. It’s constant. Taken together, this means \\(\\mathbf{H}\\) is positive definite for all \\(\\mathbf{x}\\), and so the function \\(z = x^2 + y^2\\) is a convex function.\nAs another example, suppose we instead had \\(z = -(x^2 + y^2)\\). In that case, we’d have\n\\[\n\\mathbf{H} =\n\\begin{pmatrix}\n-2 & 0 \\\\\n0 & -2 \\\\\n\\end{pmatrix} = -2 \\mathbf{I},\n\\]\nso both eigenvalues are \\(\\lambda = -2 < 0\\). Again, the Hessian is constant, which in this case means \\(\\mathbf{H}\\) is negative definite for all \\(\\mathbf{x}\\). Thus, \\(z = -(x^2+y^2)\\) is a concave function. Notice that this is just the negative of the previous example. In fact, the negative of any convex function will be concave, and vice versa, since negating a function just flips the signs of the Hessian’s eigenvalues.\nAs a final example, suppose we had \\(z = x^2 - y^2\\). In that case, we’d have\n\\[\n\\mathbf{H} =\n\\begin{pmatrix}\n2 & 0 \\\\\n0 & -2 \\\\\n\\end{pmatrix} = \\text{diag}(2, -2).\n\\]\nThe eigenvalues in this case are the diagonal entries, \\(\\lambda = 2, -2\\). Again, the Hessian is constant, but the eigenvalues have different signs, which means \\(\\mathbf{H}\\) is indefinite for all \\(\\mathbf{x}\\). This function is thus neither convex nor concave.\nIf you like, you can use sympy to calculate these eigenvectors for you. Here’s a sympy calculations of the eigenvalues from the previous example. Of course, this is overkill in this case since the eigenvalues of a diagonal matrix are just the diagonal entries, but it will do it anyway.\n\n\nCode\nH = sp.Matrix([[2, 0], [0, -2]])\neigs = H.eigenvals()\nprint(f'eigenvalues = {list(eigs.keys())}')\n\n\neigenvalues = [2, -2]"
  },
  {
    "objectID": "notebooks/multivariate-calculus.html#the-jacobian",
    "href": "notebooks/multivariate-calculus.html#the-jacobian",
    "title": "9  Multivariate Calculus",
    "section": "9.6 The Jacobian",
    "text": "9.6 The Jacobian\nWhen it comes to differentiation, we’ve gone about as far as we can go with scalar-valued bivariate functions of the form \\(z = f(x, y)\\). We’ll now step up one level, not by going to three input variables as you might think, but by going to two output variables. That is, we’ll consider pairs of bivariate functions\n\\[\\begin{align*}\nz &= f(x, y), \\\\\nu &= g(x, y). \\\\\n\\end{align*}\\]\nFunctions like this are called vector-valued functions. They output vector values instead of scalar values. To see why, let’s write it in vector form by defining an input vector \\(\\mathbf{x} = (x, y)\\), an output vector \\(\\mathbf{y} = (z, u)\\), and a vector function \\(\\mathbf{f}(\\mathbf{x}) = \\big(f(x,y), g(x,y) \\big)\\). Then we can write the pair of bivariate functions as\n\\[\\mathbf{y} = \\mathbf{f}(\\mathbf{x}).\\]\nHere’s an example of a vector-valued function,\n\\[\\begin{align*}\nz &= x^2 + y^2, \\\\\nu &= e^{x + y}. \\\\\n\\end{align*}\\]\nWe can also write it in vector notation as\n\\[\n\\mathbf{y} =\n\\begin{pmatrix}\nz \\\\\nu \\\\\n\\end{pmatrix} =\n\\begin{pmatrix}\nx^2 + y^2 \\\\\ne^{x + y} \\\\\n\\end{pmatrix} =\n\\mathbf{f}(\\mathbf{x}).\n\\]\nWe can try to visualize a vector-valued function, but it does start to get harder. When it’s just a pair of bivariate functions, we can just visualize two surface in space. One will be the surface \\(z=f(x,y)\\), the other the surface \\(u=g(x,y)\\). Here’s a plot for the above example. The blue surface is the same familiar bowl of \\(z = x^2 + y^2\\). The orange surface is the 2D exponential \\(u = e^{x + y}\\), which as you’d predict blows up fast when \\(x+y\\) is positive.\n\n\nCode\nx = np.linspace(-2, 2, 100)\ny = np.linspace(-2, 2, 100)\nf = lambda x, y: x**2 + y**2\ng = lambda x, y: np.exp(x + y)\n\nplot_function_3d(x, y, [f, g], title='$z=x^2+y^2, u=e^{x+y}$', titlepad=10, labelpad=5, alpha=0.7,\n                 ticks_every=[1, 1, 10], dist=12, zlim=(0, 30), figsize=(6, 5))\n\n\n\n\n\n\n\n\n\nFor all practical purposes you can just treat these vector-valued functions as a pair of bivariate functions and operate on them one at a time. For example, we can take total differentials of \\(z\\) and \\(u\\) separately to get\n\\[\\begin{align*}\ndz &= \\frac{\\partial z}{\\partial x}dx + \\frac{\\partial z}{\\partial y} dy, \\\\\ndu &= \\frac{\\partial u}{\\partial x}dx + \\frac{\\partial u}{\\partial y} dy. \\\\\n\\end{align*}\\]\nEach of these functions also has its own gradient, namely\n\\[\n\\frac{dz}{d\\mathbf{x}} = \\bigg(\\frac{\\partial z}{\\partial x}, \\frac{\\partial z}{\\partial y} \\bigg), \\quad\n\\frac{du}{d\\mathbf{x}} = \\bigg(\\frac{\\partial u}{\\partial x}, \\frac{\\partial u}{\\partial y} \\bigg).\n\\]\nAs you’d expect, using these gradients, we can write the total differentials as\n\\[\\begin{align*}\ndz &= \\frac{dz}{d\\mathbf{x}} \\cdot d\\mathbf{x}, \\\\\ndu &= \\frac{du}{d\\mathbf{x}} \\cdot d\\mathbf{x}. \\\\\n\\end{align*}\\]\nNow’s where it gets more interesting. What do we mean by the derivative of a vector-valued function? It’s evidently not just the gradient, because now we’ve got two of them, one for \\(z\\) and one for \\(u\\). But if you stare at the total differentials, you’ll see that yet again we can stack things into arrays. If we can define,\n\\[\nd\\mathbf{y} =\n\\begin{pmatrix}\ndz \\\\\ndu \\\\\n\\end{pmatrix}, \\quad\n\\mathbf{J} = \\mathbf{J}(\\mathbf{x}) =\n\\bigg (\\frac{dz}{d\\mathbf{x}}, \\frac{du}{d\\mathbf{x}} \\bigg ),\n\\]\nthen we should be able to write the total differentials as a single vector equation,\n\\[d\\mathbf{y} =\n\\begin{pmatrix}\ndz \\\\\ndu \\\\\n\\end{pmatrix} =\n\\begin{pmatrix}\n\\frac{dz}{d\\mathbf{x}}^\\top \\\\\n\\frac{du}{d\\mathbf{x}}^\\top \\\\\n\\end{pmatrix}\n\\begin{pmatrix}\ndx \\\\\ndy \\\\\n\\end{pmatrix} =\n\\begin{pmatrix}\n\\frac{dz}{d\\mathbf{x}}^\\top d\\mathbf{x} \\\\\n\\frac{du}{d\\mathbf{x}}^\\top d\\mathbf{x} \\\\\n\\end{pmatrix} =\n\\mathbf{J} d\\mathbf{x}.\\]\nWe do have to be careful though, because this time the array \\(\\mathbf{J}\\) is no longer a vector. It’s a stack of two column vectors, namely the gradients of \\(z\\) and \\(u\\). This means \\(\\mathbf{J}\\) is a \\(2 \\times 2\\) matrix. It’s called the Jacobian matrix, or just the Jacobian,\n\\[\n\\mathbf{J} = \\mathbf{J}(\\mathbf{x}) =\n\\begin{pmatrix}\n\\frac{dz}{d\\mathbf{x}}^\\top \\\\\n\\frac{du}{d\\mathbf{x}}^\\top \\\\\n\\end{pmatrix} =\n\\begin{pmatrix}\n\\frac{\\partial z}{\\partial x} & \\frac{\\partial z}{\\partial y} \\\\\n\\frac{\\partial u}{\\partial x} & \\frac{\\partial u}{\\partial y} \\\\\n\\end{pmatrix}.\n\\]\nLet’s do a quick example. Take the previous vector-valued function. In that case we’d have\n\\[\n\\mathbf{J}(\\mathbf{x}) =\n\\begin{pmatrix}\n\\frac{\\partial z}{\\partial x} & \\frac{\\partial u}{\\partial x} \\\\\n\\frac{\\partial z}{\\partial y} & \\frac{\\partial u}{\\partial y} \\\\\n\\end{pmatrix} =\n\\begin{pmatrix}\n2x & 2y \\\\\ne^{x + y} & e^{x + y} \\\\\n\\end{pmatrix}.\n\\]\nNotice from this example that unlike the Hessian of a scalar-valued function, the Jacobian is not symmetric \\(\\mathbf{J}^\\top \\neq \\mathbf{J}\\). This will generally be the case. Why should it be? After all, \\(z=f(x,y)\\) and \\(u=g(x,y)\\) are two completely different functions.\nAgain, the total differential \\(d\\mathbf{y} = \\mathbf{J} d\\mathbf{x}\\) suggests that the Jacobian is the derivative of a vector-valued function. For this reason, I’ll again pretend we can divide by \\(d\\mathbf{x}\\) and sometimes write the Jacobian as\n\\[\\frac{d\\mathbf{y}}{d\\mathbf{x}} = \\mathbf{J}.\\]\nNotice how this looks almost exactly like the old scalar derivative \\(\\frac{dy}{dx}\\). The only difference is that now both \\(\\mathbf{y}\\) and \\(\\mathbf{x}\\) are vectors, and their ratio is a matrix. As with the gradient, you can think of the Jacobian as a linear map, except this time it’s a map from vectors to vectors. It sends input differential vectors \\(d\\mathbf{x}\\) to output differential vectors \\(d\\mathbf{y} = \\mathbf{J}d\\mathbf{x}\\).\nCalculating the Jacobian numerically is again pretty easy. Just calculate the gradients of \\(z\\) and \\(u\\) separately and stack them into a matrix. Here’s an example. I’ll numerically calculate the Jacobian of the above example at the point \\(\\mathbf{x} = (1,1)\\).\n\n\nCode\ndef jacobian(f, g, x, y, dx=1e-5):\n    grad_z = grad(f, x, y, dx=dx)\n    grad_u = grad(g, x, y, dx=dx)\n    jacob = np.vstack([grad_z, grad_u])\n    return jacob\n    \nf = lambda x, y: x**2 + y**2\ng = lambda x, y: np.exp(x + y)\n\na, b = 1, 1\njacob = jacobian(f, g, a, b)\nprint(f'J{a, b} = \\n{jacob}')\n\n\nJ(1, 1) = \n[[2.00001    2.00001   ]\n [7.38909304 7.38909304]]\n\n\nAs with scalar-valued function \\(z = f(\\mathbf{x})\\), we can linearize a vector-valued function \\(\\mathbf{y} = \\mathbf{f}(\\mathbf{x})\\) about any point \\(\\mathbf{a}\\) provided \\(\\mathbf{x}\\) is near \\(\\mathbf{a}\\),\n\\[\\mathbf{f}(\\mathbf{x}) \\approx \\mathbf{f}(\\mathbf{a}) + \\mathbf{J}(\\mathbf{a}) (\\mathbf{x} - \\mathbf{a}).\\]\nThe result of linearization this time though won’t be a tangent plane, but rather two tangent planes, one for each function. For example, with the previous function, linearizing near the point \\(\\mathbf{a} = (1,1)\\) would give\n\\[\n\\begin{pmatrix}\nz \\\\\nu \\\\\n\\end{pmatrix} \\approx\n\\begin{pmatrix}\n1^2 + 1^2 \\\\\ne^{1+1} \\\\\n\\end{pmatrix} +\n\\begin{pmatrix}\n2 \\cdot 1 & 2 \\cdot 1 \\\\\ne^{1 + 1} & e^{1 + 1} \\\\\n\\end{pmatrix}\n\\begin{pmatrix}\nx-1 \\\\\ny-1 \\\\\n\\end{pmatrix} =\n\\begin{pmatrix}\n2 + 2(x-1) + 2(y-1) \\\\\ne^2 +  e^2(x-1) + e^2(y-1)\\\\\n\\end{pmatrix}.\n\\]\nLet’s look at one last example. Suppose we had a scalar-valued function \\(z = f(x,y)\\). Then its gradient \\(\\mathbf{g}(\\mathbf{x})\\) must itself be a vector-valued function, namely the function\n\\[\n\\mathbf{y} = \\mathbf{g}(\\mathbf{x}) =\n\\begin{pmatrix}\n\\color{red}{\\frac{\\partial z}{\\partial x}} \\\\\n\\color{blue}{\\frac{\\partial z}{\\partial y}} \\\\\n\\end{pmatrix}.\n\\]\nThis means we can take the Jacobian of the gradient,\n\\[\\mathbf{J}(\\mathbf{x}) = \\frac{d}{d\\mathbf{x}} \\mathbf{g}(\\mathbf{x}) =\n\\begin{pmatrix}\n\\frac{\\partial}{\\partial x}\\color{red}{\\frac{\\partial z}{\\partial x}} & \\frac{\\partial}{\\partial y}\\color{red}{\\frac{\\partial z}{\\partial x}} \\\\\n\\frac{\\partial}{\\partial x}\\color{blue}{\\frac{\\partial z}{\\partial y}} & \\frac{\\partial}{\\partial y}\\color{blue}{\\frac{\\partial z}{\\partial y}} \\\\\n\\end{pmatrix} =\n\\begin{pmatrix}\n\\frac{\\partial^2 z}{\\partial x^2} & \\frac{\\partial^2 z}{\\partial y \\partial x} \\\\\n\\frac{\\partial^2 z}{\\partial x \\partial y} & \\frac{\\partial^2 z}{\\partial y^2} \\\\\n\\end{pmatrix} = \\mathbf{H}(\\mathbf{x}).\n\\]\nThat is, the Jacobian of the gradient is just the Hessian. This is the sense in which the Hessian is the multivariate generalization of the second derivative. What about the Hessian of a vector-valued function though? I haven’t talked about that. It turns out that the Hessian in that case isn’t a matrix anymore, but rather a rank-3 tensor. I’ll talk more about those in a future lesson.\nIf you haven’t noticed, multivariate calculus seems to come with its own confusing zoo of notation and terminology. Instead of just “derivatives”, we now have “partial derivatives”, “gradients”, and “Jacobians”. Instead of “second derivatives”, we have “second partial derivatives” and “Hessians”. Worse, there are a lot of different notations around to denote each of these things. All this stuff can be confusing. It’s the way it is largely for historical reasons. In practice, people often are lazier about these things. In fact, in machine learning, people often refer to any first derivative as a gradient, and any second derivative as a Hessian."
  },
  {
    "objectID": "notebooks/multivariate-calculus.html#differentiation-rules",
    "href": "notebooks/multivariate-calculus.html#differentiation-rules",
    "title": "9  Multivariate Calculus",
    "section": "9.7 Differentiation Rules",
    "text": "9.7 Differentiation Rules\nSince partial differentials and derivatives are ordinary univariate derivatives, they naturally inherit all of the standard differentiation rules from ordinary calculus. I’ll list the rules for the partial derivatives with respect to \\(x\\), but they apply to \\(y\\) too, as well as both partial differentials.\n\n\n\n\n\n\n\n\nName\nRule\nExample\n\n\nLinearity\n\\(\\frac{\\partial}{\\partial x}(au + bv) = a\\frac{\\partial u}{\\partial x} + b\\frac{\\partial v}{\\partial x}\\)\n\\(\\frac{\\partial}{\\partial x}(2x^2 y + 5y^2\\log x) = 2y\\frac{\\partial}{\\partial x}x^2 + 5y^2\\frac{\\partial}{\\partial x}\\log x = 4xy + \\frac{5y^2}{x}\\)\n\n\nProduct Rule\n\\(\\frac{\\partial}{\\partial x}(uv)=u\\frac{\\partial v}{\\partial x} + v\\frac{\\partial u}{\\partial x}\\)\n\\(\\frac{\\partial}{\\partial x}(x e^x \\log y) = x \\log y \\frac{\\partial}{\\partial x}e^x + e^x \\log y \\frac{\\partial}{\\partial x} x = xe^x \\log y + e^x \\log y\\)\n\n\nQuotient Rule\n\\(\\frac{\\partial}{\\partial x}\\big(\\frac{u}{v}\\big) = \\frac{v\\frac{\\partial u}{\\partial x}-u\\frac{\\partial v}{\\partial x}}{v^2}\\)\n\\(\\frac{\\partial}{\\partial x} \\frac{\\cos x e^y}{x^2 + y} = \\frac{(x^2+y)e^y\\frac{\\partial}{\\partial x}\\cos x-\\cos x e^y \\frac{\\partial}{\\partial x}(x^2+y)}{(x^2+y)^2} = \\frac{-(x^2+y)e^y \\sin x - 2x e^y \\cos x}{(x^2+y)^2}\\)\n\n\nChain Rule\n\\(\\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial w}\\frac{\\partial w}{\\partial x}\\)\n\\(\\frac{\\partial}{\\partial x} e^{\\sin x} \\log(10y) = \\log(10y) \\frac{\\partial}{\\partial w} e^w \\frac{\\partial}{\\partial x}\\sin x = e^{\\sin x} \\cos(x) \\log(10y)\\)\n\n\nInverse Rule\n\\(\\frac{\\partial x}{\\partial z} = \\big(\\frac{\\partial z}{\\partial x}\\big)^{-1}\\)\n\\(z = 5x + y \\quad \\Longrightarrow \\quad \\frac{\\partial x}{\\partial z} = \\big(\\frac{\\partial z}{\\partial x}\\big)^{-1} = \\frac{1}{5}\\)\n\n\n\nEach of these rules can be extended to gradients and Jacobians as well, with some modifications to account for the fact that we’re now dealing with vectors and matrices instead of scalars.\nLinearity extends easily. Since vectors and matrices are linear objects, linearity extends right over with no modifications. For example, if \\(\\mathbf{u}=\\mathbf{f}(\\mathbf{x})\\) and \\(\\mathbf{v}=\\mathbf{g}(\\mathbf{x})\\) are two vector-valued functions, and \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) are two constant matrices with compatible shapes, then the Jacobian satisfies\n\\[\\frac{d}{d\\mathbf{x}} (\\mathbf{A}\\mathbf{u} + \\mathbf{B}\\mathbf{v}) = \\mathbf{A}\\frac{d\\mathbf{u}}{d\\mathbf{x}} + \\mathbf{B}\\frac{d\\mathbf{v}}{d\\mathbf{x}}.\\]\nThe product rule carries over almost identically too, except we have to be careful about the order of multiplication since matrix multiplication doesn’t commute. For example, the gradient of the dot product of two vector-valued functions is\n\\[\\frac{d}{d\\mathbf{x}} \\mathbf{u} \\cdot \\mathbf{v} = \\frac{d\\mathbf{v}}{d\\mathbf{x}} \\mathbf{u} + \\frac{d\\mathbf{u}}{d\\mathbf{x}} \\mathbf{v}.\\]\nSince the gradient of a scalar is a vector, the right-hand side must be a vector, which means the derivatives (i.e. the Jacobian matrices) must go to the left of each vectors so that their product is a vector.\nThe quotient rule doesn’t really make sense for vector-valued functions since we’re not allowed to divide vectors. For scalar-valued functions we’re okay. For example, the gradient of \\(u=f(x,y)\\) over \\(v=g(x,y)\\) is given by\n\\[\\frac{d}{d\\mathbf{x}} \\bigg(\\frac{u}{v}\\bigg) = \\frac{1}{v^2} \\bigg(v \\frac{du}{d\\mathbf{x}} - u\\frac{dv}{d\\mathbf{x}} \\bigg).\\]\nSince the gradient of a scalar must be a vector, the right-hand side must be a vector. Indeed it is, since the two gradients \\(\\frac{du}{d\\mathbf{x}}\\) and \\(\\frac{dv}{d\\mathbf{x}}\\) are vectors.\nThe inverse rule involves dividing by a derivative. For a gradient that wouldn’t make sense since we’d be dividing by a vector. For a Jacobian though it does make sense as long as the Jacobian is an invertible square matrix. For the \\(2 \\times 2\\) Jacobian I defined before, provided that we can invert \\(\\mathbf{y} = \\mathbf{f}(\\mathbf{x})\\), we’d have\n\\[\\frac{d\\mathbf{x}}{d\\mathbf{y}} = \\bigg(\\frac{d\\mathbf{y}}{d\\mathbf{x}} \\bigg)^{-1}.\\]\nThe chain rule is important enough to machine learning that I want to talk about it in a little more detail. Suppose \\(z = f(x,y)\\) is some function. In terms of its partial derivatives, its total differential is just\n\\[dz = \\frac{\\partial z}{\\partial x} dx + \\frac{\\partial z}{\\partial y} dy.\\]\nSuppose that \\(x\\) and \\(y\\) depend on some third variable \\(u\\). Then we can find the total derivative \\(\\frac{dz}{du}\\) by dividing both sides of the total differential by \\(du\\),\n\\[dz = \\frac{\\partial z}{\\partial x} \\frac{dx}{du} + \\frac{\\partial z}{\\partial y} \\frac{dy}{du}.\\]\nSuppose though that \\(x\\) and \\(y\\) depend on not only \\(u\\), but also some other variable \\(v\\). That is, \\((x, y) = g(u, v)\\). If we wanted to know the partial derivative of \\(z\\) with respect to \\(u\\), holding \\(v\\) constant, all we’d need to do is convert \\(dz\\) to \\(\\partial z = \\partial_u z\\) and divide both sides by \\(\\partial u\\) to get\n\\[\\frac{\\partial z}{\\partial u} = \\frac{\\partial z}{\\partial x} \\frac{\\partial x}{\\partial u} + \\frac{\\partial z}{\\partial y} \\frac{\\partial y}{\\partial u}.\\]\nSimilarly, if we wanted to know the partial derivative of \\(z\\) with respect to \\(v\\), holding \\(u\\) constant, we’d convert convert \\(dz\\) to \\(\\partial z = \\partial_v z\\) and divide by \\(\\partial v\\),\n\\[\\frac{\\partial z}{\\partial v} = \\frac{\\partial z}{\\partial x} \\frac{\\partial x}{\\partial v} + \\frac{\\partial z}{\\partial y} \\frac{\\partial y}{\\partial v}.\\]\nWe can express this in a cleaner notation by turning everything into vectors and matrices. If we let \\(\\mathbf{x} = (x,y)\\) and \\(\\mathbf{u} = (u,v)\\), then the partials \\(\\frac{\\partial z}{\\partial u}\\) and \\(\\frac{\\partial z}{\\partial v}\\) are just the gradient vector \\(\\frac{dz}{d\\mathbf{u}}\\). The right-hand side is a matrix-vector product of the Jacobian matrix \\(\\frac{d\\mathbf{x}}{d\\mathbf{u}}\\) with the gradient vector \\(\\frac{dz}{d\\mathbf{x}}\\). Again, we have to be careful about the order of multiplication by putting the matrix to the left. The chain rule in vector form is thus\n\\[\\frac{dz}{d\\mathbf{u}} = \\frac{d\\mathbf{x}}{d\\mathbf{u}} \\frac{dz}{d\\mathbf{x}}.\\]\nWhat about if on top of all this \\(\\mathbf{u}\\) was a vector-valued function of some other vector \\(\\mathbf{w}\\), and we wanted to know the gradient \\(\\frac{dz}{d\\mathbf{w}}\\)? No problem. We just need to include another Jacobian matrix \\(\\frac{d\\mathbf{u}}{d\\mathbf{w}}\\). Now, of course, we run into an issue of which order to multiply the two Jacobian matrices. It turns out we should multiply any new Jacobian matrices on the left, so we’d have\n\\[\\frac{dz}{d\\mathbf{w}} = \\frac{d\\mathbf{u}}{d\\mathbf{w}} \\frac{d\\mathbf{x}}{d\\mathbf{u}} \\frac{dz}{d\\mathbf{x}}.\\]\nThis can be extended, or chained, as many times as we’d like. Suppose we had some complicated scalar-valued function of \\(L+1\\) compositions\n\\[\\begin{align*}\n\\mathbf{a}_1 &= \\mathbf{f}_1(\\mathbf{x}), \\\\\n\\mathbf{a}_2 &= \\mathbf{f}_2(\\mathbf{a}_1), \\\\\n\\vdots & \\qquad \\vdots \\\\\n\\mathbf{a}_{L-1} &= \\mathbf{f}_{L-1}(\\mathbf{a}_{L-2}) \\\\\n\\mathbf{y} &= \\mathbf{f}_L(\\mathbf{a}_{L-1}). \\\\\nz &= g(\\mathbf{y}). \\\\\n\\end{align*}\\]\nThen the multivariate chain rule would say\n\\[\\frac{dz}{d\\mathbf{x}} =  \\frac{d\\mathbf{a}_1}{d\\mathbf{x}} \\frac{d\\mathbf{a}_2}{d\\mathbf{a}_1} \\cdots \\frac{d\\mathbf{a}_{L-1}}{d\\mathbf{a}_{L-2}} \\frac{d\\mathbf{y}}{d\\mathbf{a}_{L-1}} \\frac{dz}{d\\mathbf{y}}.\\]\nIt may not be at all obvious, but I’ve essentially just outlined for you how you’d derive the important backpropagation algorithm of deep learning. Of course, this was just done for bivariate functions, but it extends to functions of arbitrary dimension as well, even when each of the \\(\\mathbf{f}(\\mathbf{a})\\) are different sizes. The only real difference is that with neural networks we’d think of each function \\(\\mathbf{f}(\\mathbf{a})\\) as a layer in a neural network, which is just a complicated function composition that maps inputs \\(\\mathbf{x}\\) to outputs \\(\\mathbf{y} = \\mathbf{f}_L(\\mathbf{a}_{L-1})\\). The scalar-valued function \\(z = g(\\mathbf{y})\\) is the loss function, which measures how good \\(\\mathbf{y}\\) is at predicting the true output in the data. By calculating the gradient \\(\\frac{dz}{d\\mathbf{x}}\\) and using it to move in the direction of steepest descent we can minimize the loss by updating the parameters in the network until we’ve hit the minimum, roughly speaking.\nBackpropagation gives a much more efficient way to calculate the gradient of a long chain of composite functions than using ordinary numerical differentiation. The reason is we’re able to use information from the later gradients to help compute the earlier gradients, which saves on computation. In general, using the chain rule to calculate the gradient of a scalar-valued function like this is called autodifferentiation. This is how deep learning frameworks like Pytorch or Tensorflow or Jax calculate gradients.\nI’ll close up this section by listing out some of the gradients and Jacobians we might be interested in in machine learning. Don’t try to memorize these. Just look them over and keep them in mind for later reference.\n\n\n\n\n\n\n\n\nName\nDerivative\nScalar Equivalent\n\n\nLinearity (scalar-valued)\n\\(\\frac{d}{d\\mathbf{x}}(au + bv) = a\\frac{du}{d\\mathbf{x}} + b\\frac{dv}{d\\mathbf{x}}\\)\n\\(\\frac{d}{dx}(au + bv) = a\\frac{du}{dx} + b\\frac{dv}{dx}\\)\n\n\nLinearity (vector-valued)\n\\(\\frac{d}{d\\mathbf{x}}(\\mathbf{A}\\mathbf{u} + \\mathbf{B}\\mathbf{v}) = \\mathbf{A}\\frac{d\\mathbf{u}}{d\\mathbf{x}} + \\mathbf{B}\\frac{d\\mathbf{v}}{d\\mathbf{x}}\\)\n\\(\\frac{d}{dx}(au + bv) = a\\frac{du}{dx} + b\\frac{dv}{dx}\\)\n\n\nProduct Rule (scalar-valued)\n\\(\\frac{d}{d\\mathbf{x}}(uv) = u\\frac{dv}{d\\mathbf{x}} + v\\frac{du}{d\\mathbf{x}}\\)\n\\(\\frac{d}{dx}(uv) = u\\frac{dv}{dx} + v\\frac{du}{dx}\\)\n\n\nProduct Rule (dot products)\n\\(\\frac{d}{d\\mathbf{x}}(\\mathbf{u}^\\top \\mathbf{v}) = \\mathbf{u}^\\top \\frac{d\\mathbf{v}}{d\\mathbf{x}} + \\mathbf{v}^\\top \\frac{d\\mathbf{u}}{d\\mathbf{x}}\\)\n\\(\\frac{d}{dx}(uv) = u\\frac{dv}{dx} + v\\frac{du}{dx}\\)\n\n\nChain Rule (both scalar-valued)\n\\(\\frac{dz}{d\\mathbf{x}} = \\frac{dz}{dy} \\frac{dy}{d\\mathbf{x}}\\)\n\\(\\frac{dz}{dx} = \\frac{dz}{dy} \\frac{dy}{dx}\\)\n\n\nChain Rule (scalar-valued, vector-valued)\n\\(\\frac{dz}{d\\mathbf{x}} = \\big(\\frac{dz}{d\\mathbf{y}}\\big)^\\top \\frac{d\\mathbf{y}}{d\\mathbf{x}}\\)\n\\(\\frac{dz}{dx} = \\frac{dz}{dy} \\frac{dy}{dx}\\)\n\n\nChain Rule (both vector-valued)\n\\(\\frac{d\\mathbf{z}}{d\\mathbf{x}} = \\frac{d\\mathbf{z}}{d\\mathbf{y}} \\frac{d\\mathbf{y}}{d\\mathbf{x}}\\)\n\\(\\frac{dz}{dx} = \\frac{dz}{dy} \\frac{dy}{dx}\\)\n\n\nConstant Function (scalar-valued)\n\\(\\frac{d}{d\\mathbf{x}} c = 0\\)\n\\(\\frac{d}{dx} c = 0\\)\n\n\nConstant Function (vector-valued)\n\\(\\frac{d}{d\\mathbf{x}} \\mathbf{c} = \\mathbf{0}\\)\n\\(\\frac{d}{dx} c = 0\\)\n\n\nSquared Two-Norm\n\\(\\frac{d}{d\\mathbf{x}} ||\\mathbf{x}||_2^2 = \\frac{d}{d\\mathbf{x}} \\mathbf{x}^\\top \\mathbf{x} = 2 \\mathbf{x}\\)\n\\(\\frac{d}{dx} x^2 = 2x\\)\n\n\nLinear Combination\n\\(\\frac{d}{d\\mathbf{x}} \\mathbf{c}^\\top \\mathbf{x} = \\mathbf{c}\\)\n\\(\\frac{d}{dx} cx = c\\)\n\n\nSymmetric Quadratic Form\n\\(\\frac{d}{d\\mathbf{x}} \\mathbf{x}^\\top \\mathbf{S} \\mathbf{x} = 2 \\mathbf{S} \\mathbf{x}\\)\n\\(\\frac{d}{dx} sx^2 = 2sx\\)\n\n\nAffine Function\n\\(\\frac{d}{d\\mathbf{x}} (\\mathbf{A}\\mathbf{x} + \\mathbf{b}) = \\mathbf{A}^\\top\\) or \\(\\frac{d}{d\\mathbf{x}} (\\mathbf{x}^\\top \\mathbf{A} + \\mathbf{b}) = \\mathbf{A}\\)\n\\(\\frac{d}{dx} (ax+b) = a\\)\n\n\nSquared Error Function\n\\(\\frac{d}{d\\mathbf{x}} ||\\mathbf{A}\\mathbf{x}-\\mathbf{b}||_2^2 = 2\\mathbf{A}^\\top (\\mathbf{A}\\mathbf{x}-\\mathbf{b})\\)\n\\(\\frac{d}{dx} (ax-b)^2 = 2a(ax-b)\\)\n\n\nCross Entropy Function\n\\(\\frac{d}{d\\mathbf{x}} (-\\mathbf{c}^\\top \\log \\mathbf{x}) = -\\frac{\\mathbf{c}}{\\mathbf{x}}\\) (element-wise division)\n\\(\\frac{d}{dx} (-c \\log x) = -\\frac{c}{x}\\)\n\n\nReLU Function\n\\(\\frac{d}{d\\mathbf{x}} \\max(\\mathbf{0}, \\mathbf{x}) = \\text{diag}(\\mathbf{x} \\geq \\mathbf{0})\\) (element-wise \\(\\geq\\))\n\\(\\frac{d}{dx} \\max(0, x) = \\text{$1$ if $x \\geq 0$ else $0$}\\)\n\n\nSoftmax Function\n\\(\\frac{d}{d\\mathbf{x}} \\text{softmax}(\\mathbf{x}) = \\text{diag}(\\mathbf{y}) - \\mathbf{y} \\mathbf{y}^\\top\\) where \\(\\mathbf{y} = \\text{softmax}(\\mathbf{x})\\)\n\n\n\n\nYou can calculate gradients and Jacobians in sympy, though in my opinion it can be kind of painful except in the simplest cases. Here’s an example, where I’ll calculate the Jacobian of the squared error function \\(||\\mathbf{A}\\mathbf{x}-\\mathbf{b}||_2^2\\).\nAside: There is also a nice online tool that lets you do this somewhat more easily.\n\n\nCode\nm = sp.Symbol('m')\nn = sp.Symbol('n')\nA = sp.MatrixSymbol('A', m, n)\nx = sp.MatrixSymbol('x', n, 1)\nb = sp.MatrixSymbol('b', m, 1)\n\ny = (A * x - b).T * (A * x - b)\ngradient = y.diff(x)\nprint(f'dy/dx = gradient')\n\n\ndy/dx = gradient"
  },
  {
    "objectID": "notebooks/multivariate-calculus.html#volume-integration",
    "href": "notebooks/multivariate-calculus.html#volume-integration",
    "title": "9  Multivariate Calculus",
    "section": "9.8 Volume Integration",
    "text": "9.8 Volume Integration\nSo far I’ve only really talked about how to differentiate of bivariate functions. I’ve said nothing about how to integrate them. Indeed, for machine learning purposes differentiation is far more important to know in detail than integration is. Just as multivariate functions have different kinds of differentials and derivatives, they have different kinds of integrals as well. There are volume integrals, line integrals, surface integrals, and so on. For now I’ll focus only on volume integrals. These come up when dealing with multivariate probability distributions, as we’ll see later. This choice to skip other types of integration means I’ll be skipping over a lot of fundamental facts of vector calculus that aren’t as relevant in machine learning.\nLet’s begin by recalling how we defined the integral of a univariate function \\(y = f(x)\\). We defined the (definite) integral as the (signed) area under the curve of \\(f(x)\\) in the region \\(a \\leq x \\leq b\\). We did this by approximating the exact area with the area of a bunch of rectangles of height \\(f(x_n)\\) and width \\(dx\\). The area became exact as we allowed the number of rectangles \\(N\\) to become infinitely large and their widths \\(dx = \\frac{b-a}{N}\\) to become infinitesimally small in proportion,\n\\[A = \\int_a^b f(x) dx = \\sum_{n=0}^{N-1} f(x_n) dx.\\]\nTo calculate these areas efficiently we introduced the Fundamental Theorem of Calculus. This said that, provided we can find a function \\(F(x)\\) whose derivative is the original function \\(f(x)\\), then the area under the curve is given by\n\\[\\int_a^b f(x) dx = F(b) - F(a).\\]\nLet’s now try to extend these ideas to bivariate functions. With bivariate functions, we’ll no longer be talking about the area under a curve, but instead about the volume under a surface. We’ll approximate the volume under a surface not with rectangles, but with rectangular prisms. Instead of integrating between two endpoints, we’ll instead have to integrate over a two-dimensional region in the xy-plane.\nSuppose we want to calculate the volume under the surface of \\(z = f(x, y)\\) inside of some region \\((x, y) \\in R\\). The region could be any shape in the xy-plane. It could be a rectangle, a circle, a figure eight, whatever. What we’ll imagine breaking the region \\(R\\) up into a bunch of little rectangles of width \\(dx\\) and height \\(dy\\). Each little rectangle will evidently have an area of \\(dA = dx \\cdot dy\\). Clearly, if we add up the area of all these little rectangles, we just get the area of the region \\(R\\). When \\(dA\\) is infinitesimal, this is an integral for the area \\(A\\),\n\\[A = \\int_R dA = \\int_R dx dy.\\]\nGreat, but this just gives us the area of the region \\((x, y) \\in R\\) in the plane. What we wanted was the volume under the surface of \\(z = f(x, y)\\) in 3D space. Here’s how we can get that. Take one of the tiny rectangle areas \\(dA\\) and multiply it by the value of the function evaluated at any point \\((x_n, y_m)\\) inside that rectangle. This will give the volume \\(dV = f(x_n, y_m) dA\\) of a rectangular prism whose base is \\(dA\\) and whose height is \\(f(x_n, y_m)\\). To get the total volume under the surface, just add all these up. Provided \\(dA\\) is infinitesimal, the total volume \\(V\\) will be given by an integral,\n\\[V = \\int_R f(x, y) dA = \\sum_{n=0}^{N-1}\\sum_{m=0}^{M-1} f(x_n, y_m) dA.\\]\nThis integral is called the volume integral of \\(z = f(x, y)\\) over the region \\(R\\). It’s just the sum of the volume of a bunch of infinitely thin rectangular prisms.\nHere’s an example so you can better see what’s going on. I’ll approximate the volume under the surface of \\(z = x^2 + y^2\\) inside the region \\(R = [0,1]^2\\). That is, inside the region where \\(0 \\leq x \\leq 1\\) and \\(0 \\leq y \\leq 1\\). This region is called the unit square. I’ll approximate the volume using \\(N \\cdot M = 10 \\cdot 10 = 100\\) rectangular prisms each of base area \\(dA = 0.1 \\cdot 0.1 = 0.01\\). The pink surface is the surface itself and the rectangular prisms are shown in blue.\n\n\nCode\nx = np.linspace(0, 1, 100)\ny = np.linspace(0, 1, 100)\nf = lambda x, y: x**2 + y**2\nplot_approximating_prisms(x, y, f, dx=0.1, dy=0.1, azim=-30, elev=30, dist=12, figsize=(6, 6), titlepad=0, \n                          labelpad=8, ticks_every=None, title='$N\\cdot M=100$ Rectangular Prisms')\n\n\n\n\n\nIt looks like the approximate volume from this calculation is about \\(V \\approx 0.57\\). It’s hard to know at this moment how close this is to the true value, but based on the plot it seems we may be underestimating the true volume a little bit. We’ll come back to this in a second.\nThe area \\(dA\\) is called the area element. Since \\(dA = dxdy\\), we can also write the volume integral as\n\\[\\int_R f(x, y) dA = \\iint_R f(x, y) dx dy.\\]\nNotice the use of one integral sign on the left, and two on the right. This is saying more than it looks like. On the left, we’re doing a single integral for the area \\(A\\). On the right we’re doing a double integral, first over \\(x\\), then over \\(y\\). From this point on I’ll only focus on the case when \\(R = [a, b] \\times [c, d]\\) is a rectangle. That is, \\(a \\leq x \\leq b\\) and \\(c \\leq y \\leq d\\). In that case, we can write the the double integral as\n\\[\\int_{[a, b] \\times [c, d]} f(x, y) dA = \\int_c^d \\bigg( \\int_a^b f(x, y) dx \\bigg) dy.\\]\nNotice that the right-hand side is just two univariate integrals, an inside integral over \\(x\\) from \\(x=a\\) to \\(x=b\\), and then an outside integral over \\(y\\) from \\(y=c\\) to \\(y=d\\). This gives us a way to actually calculate the volume under a surface. We first do the inner integral over \\(x\\), treating \\(y\\) as constant, and then do the outer integral over \\(y\\) to get a numerical value for \\(V\\). For each univariate integral all the usual integration rules apply.\nLet’s see if we can calculate the exact volume under the example in the above plot, namely the quadratic function \\(z = x^2 + y^2\\) on the unit square \\(R = [0,1]^2\\). To make it clear which integral is which, I’ll highlight the \\(x\\) integral and variables in blue, and the \\(y\\) integral and variables in red. We have,\n\\[\\begin{align*}\nV &= \\int_{[0,1]^2} (\\color{steelblue}{x^2} \\color{black}{+} \\color{red}{y^2}\\color{black}{) dA} \\\\\n&= \\color{red}{\\int_0^1} \\color{black}{\\bigg(}\\color{steelblue}{\\int_0^1} (\\color{steelblue}{x^2} \\color{black}{+} \\color{red}{y^2}) \\color{steelblue}{dx} \\color{black}{\\bigg)} \\color{red}{dy} \\\\\n&= \\color{red}{\\int_0^1} \\color{black}{\\bigg[}\\frac{1}{3} \\color{steelblue}{x^3} \\color{black}{+} \\color{steelblue}{x}\\color{red}{y^2} \\color{black}{\\bigg]}_{\\color{steelblue}{x=0}}^{\\color{steelblue}{x=1}} \\color{red}{dy} \\\\\n&= \\color{red}{\\int_0^1} \\color{black}{\\bigg(\\frac{1}{3}} + \\color{red}{y^2} \\color{black}{\\bigg)} \\color{red}{dy} \\\\\n&= \\color{black}{\\bigg[\\frac{1}{3}}\\color{red}{y} \\color{black}{+ \\frac{1}{3}} \\color{red}{y^3} \\color{black}{\\bigg]}_{\\color{red}{y=0}}^{\\color{red}{y=1}} \\\\\n&= \\color{black}{\\frac{2}{3} \\approx 0.667.} \\\\\n\\end{align*}\\]\nEvidently, the exact volume under the surface \\(z = x^2 + y^2\\) on the unit square is just \\(V = \\frac{2}{3} \\approx 0.667\\). It seems like our approximation from before was indeed underestimating the true volume a decent bit. Here’s how to calculate the same volume integral with sympy. To integrate \\(z = f(x,y)\\) over a rectangle \\([a,b] \\times [c,d]\\), use the command sp.integrate(z, (x, a, b), (y, c, d)). Note the integral will be calculated in the order you specify.\n\n\nCode\nx, y = sp.symbols('x y')\nz = x**2 + y**2\nvolume = sp.integrate(z, (x, 0, 1), (y, 0, 1))\nprint(f'V = {volume}')\n\n\nV = 2/3\n\n\nNumerically integrating a volume integral is similar to the way it was for an area integral. We need to define a grid of points \\((x_n, y_m)\\), use those to get the prisms heights \\(f(x_n, y_m)\\), multiply those heights by \\(dA = dx \\cdot dy\\) to get the volume of the rectangular prisms, and then sum all the volumes up to get the full volume. In general, such an algorithm will run in \\(O(N \\cdot M)\\) time and have a \\(\\mathcal{O}(dx \\cdot dy)\\) error.\nIn the example below I’ll calculate the same example volume integral using dx=1e-4 and dy=1e-4. That implies there will be\n\\[N \\cdot M = \\frac{b-a}{dx} \\cdot \\frac{d-c}{dy} = 10^4 \\cdot 10^4 = 10^8\\]\ntotal subregions we’ll need to calculate the prism volumes of and then sum over. As you can see, this is already a pretty huge number which will make even this simple calculation run pretty slow. You can imagine how much worse it’ll get for higher-dimensional integrals. In fact, we’d almost never want to calculate volume integrals this way. There are better ways to do it, for example via random sampling schemes like Monte-Carlo integration.\n\n\nCode\ndef integrate(f, a, b, c, d, dx=1e-4, dy=1e-4):\n    N = int((b - a) / dx)\n    M = int((d - c) / dy)\n    dA = dx * dy\n    x_range = np.linspace(a + dx/2, b - dx/2, N)\n    y_range = np.linspace(c + dy/2, d - dy/2, M)\n    X, Y = np.meshgrid(x_range, y_range)\n    Z = f(X, Y)\n    prisms = Z * dA\n    integral = np.sum(prisms)\n    return integral\n\n\nf = lambda x, y: x**2 + y**2\na, b = 0, 1\nc, d = 0, 1\nvolume = integrate(f, a, b, c, d)\nprint(f'V = {volume}')\n\n\nV = 0.6666666650000003\n\n\nThe volume integral inherits the same linearity property that the area integral. We can break it up over sums and factor out any multiplied constants,\n\\[\\int_{R} \\big(a f(x, y) + b g(x, y)\\big) dA = a \\int_{R} f(x, y) dA + b \\int_{R} g(x, y) dA.\\]\nJust like we could split a univariate integral up by breaking the integration region \\([a,b]\\) into pieces, we can do the same thing with volume integrals by breaking the region \\(R\\) up into pieces. If we break \\(R\\) up into two non-overlapping subregions \\(R_1\\) and \\(R_2\\), we can write\n\\[\\int_{R} f(x,y) dA = \\int_{R_1} f(x,y) dA + \\int_{R_2} f(x,y) dA.\\]\nAs long as one of the function inside the volume integral doesn’t blow up when integrated with respect to \\(x\\) or \\(y\\) we can swap the integrals and get the same answer for the volume. For a rectangular region, this just says\n\\[\\int_{[a, b] \\times [c, d]} f(x, y) dA = \\int_c^d \\bigg( \\int_a^b f(x, y) dx \\bigg) dy = \\int_a^b \\bigg( \\int_c^d f(x, y) dy \\bigg) dx.\\]\nThis makes sense, as a double integral is really just a double sum, and we can swap the order of sums as long as the values inside the sum don’t blow up.\nRecall that for a univariate function \\(y = f(x)\\), we could change variables by letting \\(x = g(u)\\) and integrating over \\(u\\),\n\\[\\int f(x) dx = \\int f(g(u)) \\frac{dx}{du} du.\\]\nWe can do something similar for a bivariate function \\(z = f(x,y)\\). Suppose we found it difficult to integrate this function with respect to \\(x\\) and \\(y\\). What we could do is try to find two other variables \\(u\\) and \\(v\\) that make things easier to integrate. Suppose they’re related via some vector-valued function \\((x, y) = g(u, v)\\) or in vector notation \\(\\mathbf{x} = g(\\mathbf{u})\\). We’d like to be able to write something like\n\\[f(\\mathbf{x}) = f(g(\\mathbf{u})) \\cdot \\frac{d\\mathbf{x}}{d\\mathbf{u}},\\]\nbut this isn’t quite right. The left-hand side is a scalar, while the right-hand side is a matrix since it’s multiplying a Jacobian. It turns out the right thing to do is multiply not by the Jacobian, but by the absolute value of its determinant,\n\\[f(\\mathbf{x}) = f(g(\\mathbf{u})) \\cdot \\bigg | \\det \\bigg(\\frac{d\\mathbf{x}}{d\\mathbf{u}} \\bigg) \\bigg | = f(g(\\mathbf{u})) \\cdot \\big|\\det(\\mathbf{J})\\big|.\\]\nUsing this rule we can integrate over a region \\(R\\) by instead integrating over its transformed region \\(g^{-1}(R) = \\{ u, v : (x, y) = g(u, v) \\}\\),\n\\[\\int_R f(x, y) dxdy = \\int_{g^{-1}(R)} f\\big(g(u,v)\\big) \\cdot \\big|\\det(\\mathbf{J})\\big| \\ du dv.\\]\nThis trick is particularly useful when \\(R\\) is some complicated non-rectangular region in \\(xy\\)-space, but we can easily find a transformation \\((x, y) = g(u, v)\\) that makes \\(g^{-1}(R)\\) rectangular in \\(uv\\)-space. The formula \\(dA = \\big|\\det(\\mathbf{J})\\big| \\ du dv\\) is in fact the most general form of the area element we can write down.\nA classic example of a change of variables is “proving” that the area of a circle of radius \\(s\\) is \\(A = 2\\pi s\\). Suppose we wanted to find the area inside the circle \\(R = \\{x, y : x^2 + y^2 = s^2 \\}\\). If we just let \\(f(x,y) = 1\\), we can get this area \\(A\\) by doing a volume integral,\n\\[A = \\int_R dA = \\iint_R dx dy.\\]\nThis would be pretty complicated to do in \\(xy\\) coordinates, but there’s a change of variables we can do to make this integral super easy, namely polar coordinates. Define a transformation function \\((x, y) = g(r, \\theta)\\) by\n\\[x = r \\cos \\theta, \\quad y = r \\sin \\theta.\\]\nWe require that \\(r \\geq 0\\) and \\(0 \\leq \\theta \\leq 2\\pi\\). It may not be obvious, but this is a valid invertible function. You can go back and forth between \\((x,y)\\) and \\((r,\\theta)\\) for every point except the origin. This follows from the fact that \\(r\\) just represents the distance of \\((x,y)\\) from the origin, and \\(\\theta\\) represents the angle between the vector \\(\\mathbf{x} = (x,y)\\) and the \\(x\\) basis vector \\(\\mathbf{e}_x = (1,0)\\).\nAnyway, the Jacobian of this transformation is given by taking all the partial derivatives of \\(\\mathbf{x} = g(\\mathbf{u})\\),\n\\[\\mathbf{J} =\n\\begin{pmatrix}\n\\frac{\\partial x}{\\partial r} & \\frac{\\partial y}{\\partial r} \\\\\n\\frac{\\partial x}{\\partial \\theta} & \\frac{\\partial y}{\\partial \\theta} \\\\\n\\end{pmatrix} =\n\\begin{pmatrix}\n\\cos\\theta & \\sin\\theta \\\\\n-r\\sin\\theta & r\\cos\\theta \\\\\n\\end{pmatrix} \\quad \\Longrightarrow \\quad\n\\big|\\det(\\mathbf{J})\\big| = \\big | r\\cos^2 \\theta + r\\sin^2 \\theta \\big | = r.\n\\]\nHere I just used the trig identity \\(\\cos^2 \\theta + \\sin^2 \\theta = 1\\). Evidently, I’ve shown that the area element in polar coordinates is given by\n\\[dA = dx dy = r dr d\\theta.\\]\nNow, to do the integration we need to figure out what \\(g^{-1}(R)\\) is. Here let’s use some intuition. Since \\(r\\) represents a radius, and we’re trying to find the area inside a circle of radius \\(s\\), it seems like \\(r\\) should run from \\(0\\) to \\(s\\). Since \\(\\theta\\) is an angle, to sweep over the area of a whole circle we should let the angle go all the way around, from \\(0^\\circ\\) to \\(360^\\circ\\). In radians that means \\(\\theta\\) goes from \\(0\\) to \\(2\\pi\\).\nThus, we finally have the integral in polar coordinates. The integration is pretty easy. Since \\(f(x,y)=1\\), we can just break this up into a product of independent integrals, one over \\(r\\), and the other one over \\(\\theta\\). The area of a circle of radius \\(s\\) is thus given by\n\\[A = \\int_R dA = \\int_0^{2\\pi} \\bigg(\\int_0^s r dr \\bigg) d\\theta = \\bigg(\\int_0^s r dr \\bigg) \\bigg(\\int_0^{2\\pi} d\\theta \\bigg) = \\frac{1}{2} r^2 \\ \\bigg |_{r=0}^s \\cdot \\theta \\ \\bigg |_{\\theta = 0}^{2\\pi} = 2\\pi s.\\]\nUnfortunately, symbolic packages like sympy aren’t usually smart enough to recognize and perform a change of variables on their own. That’s an insight you usually have to do yourself before integrating. Once you’ve found a good change of variables, you can plug and chug as usual in the new variable system.\n\n\nCode\nr, theta, s = sp.symbols('r theta s')\narea = sp.integrate(1, (r, 0, s), (theta, 0, 2 * sp.pi))\nprint(f'A = {area}')\n\n\nA = 2*pi*s"
  },
  {
    "objectID": "notebooks/multivariate-calculus.html#calculus-in-higher-dimensions",
    "href": "notebooks/multivariate-calculus.html#calculus-in-higher-dimensions",
    "title": "9  Multivariate Calculus",
    "section": "9.9 Calculus In Higher Dimensions",
    "text": "9.9 Calculus In Higher Dimensions\nYet again, it may seem like very little I’ve said would generalize the dimensions higher than two, but in fact almost all of it does. I was careful to formulate the language in a way so that it would, particularly by using vector notation where I could. The main difference, again, is that with higher dimensional functions we can’t easily visualize what’s going on anymore.\nSuppose we have a scalar-valued function \\(z = f(x_0, x_1, \\cdots, x_{n-1})\\) that depends on \\(n\\) input variables \\(x_0, x_1, \\cdots, x_{n-1}\\). We can also think of this as a vector-input function by defining \\(\\mathbf{x} = (x_0, x_1, \\cdots, x_{n-1})\\) and writing \\(z = f(\\mathbf{x})\\). Now, suppose we nudge one of the \\(x_i\\) by some infinitesimal amount \\(\\partial x_i = d x_i\\). Then \\(z = f(\\mathbf{x})\\) will get nudged by an amount\n\\[\\partial_i z = f(x_0, x_1, \\cdots, x_i + \\partial x_i, \\cdots, x_{n-1}) - f(x_0, x_1, \\cdots, x_i, \\cdots, x_{n-1}).\\]\nThis is called the partial differential of \\(z = f(\\mathbf{x})\\) with respect to \\(x_i\\). Notice all we did was nudge \\(x_i\\) by a tiny amount \\(\\partial x_i\\). All the other variables stayed fixed, hence the term “partial”. This expression looks kind of messy. We can write it more compactly using vector notation by noting that \\(\\mathbf{x} + \\partial x_i \\mathbf{e}_i\\) is the vector whose components are \\(x_0, x_1, \\cdots, x_i + \\partial x_i, \\cdots, x_{n-1}\\). Then we just have\n\\[\\partial_i z = f(\\mathbf{x} + \\partial x_i \\mathbf{e}_i) - f(\\mathbf{x}).\\]\nIf we divide both sides by \\(\\partial x_i\\) we get the partial derivative of the function with respect to \\(x_i\\),\n\\[\\frac{\\partial z}{\\partial x_i} = \\frac{f(\\mathbf{x} + \\partial x_i \\mathbf{e}_i) - f(\\mathbf{x})}{\\partial x_i} = \\frac{f(x_0, x_1, \\cdots, x_i + \\partial x_i, \\cdots, x_{n-1}) - f(x_0, x_1, \\cdots, x_i, \\cdots, x_{n-1})}{\\partial x_i}.\\]\nIn general we want to know how \\(z\\) changes when all the inputs are arbitrarily changed, not how \\(z\\) changes when only a single variable changes. Suppose \\(\\mathbf{x}\\) is changed by some size \\(n\\) infinitesimal vector \\(d\\mathbf{x} = (dx_0, dx_1, \\cdots, dx_{n-1})\\). Then evidently \\(z = f(\\mathbf{x})\\) would change by an amount\n\\[dz = f(\\mathbf{x} + d\\mathbf{x}) - f(\\mathbf{x}) = f(x_0 + dx_0, x_1 + dx_1, \\cdots, x_{n-1} + dx_{n-1}) - f(x_0, x_1, \\cdots, x_{n-1}).\\]\nThis is called the total differential of \\(z\\) with respect to \\(\\mathbf{x}\\). By the same logic we used for bivariate functions, the total differential also turns out to be a sum over all its partial differentials,\n\\[dz = \\sum_{i=0}^{n-1} \\partial_i z = \\partial_0 z + \\partial_1 z + \\cdots + \\partial_{n-1} z.\\]\nThis of course means that we can also write the total differential as a sum over the partial derivatives,\n\\[dz = \\sum_{i=0}^{n-1} \\frac{\\partial z}{\\partial x_i} dx_i = \\frac{\\partial z}{\\partial x_0} dx_0 + \\frac{\\partial z}{\\partial x_1} dx_1 + \\cdots + \\frac{\\partial z}{\\partial x_{n-1}} dx_{n-1}.\\]\nWe can write everything in vector notation by defining a vector of size \\(n\\) called the gradient vector,\n\\[\\frac{dz}{d\\mathbf{x}} = \\mathbf{g}(\\mathbf{x}) = \\bigg(\\frac{\\partial z}{\\partial x_0}, \\frac{\\partial z}{\\partial x_1}, \\cdots, \\frac{\\partial z}{\\partial x_{n-1}} \\bigg).\\]\nThe gradient vector is again the multivariate generalization of the total derivative, at least for scalar-valued functions. We can again write the total differential as a dot product of the gradient vector with the differential \\(d\\mathbf{x}\\),\n\\[dz = \\frac{dz}{d\\mathbf{x}} \\cdot d\\mathbf{x} = \\mathbf{g} \\cdot d\\mathbf{x}.\\]\nThis equation gives us a geometric interpretation of the gradient by defining a kind of \\(n\\)-dimensional version of the tangent plane, called a tangent hyperplane. It’s an \\(n\\)-dimensional hyperplane in \\(n+1\\) dimensional space. The tangent hyperplane to the function at a point \\(\\mathbf{a}\\) is given by setting the differential \\(d\\mathbf{x} = \\mathbf{x} - \\mathbf{a}\\) and solving for \\(f(\\mathbf{x})\\) to get\n\\[f(\\mathbf{x}) \\approx f(\\mathbf{a}) + \\mathbf{g}(\\mathbf{a})^\\top (\\mathbf{x} - \\mathbf{a}).\\]\nDon’t even try to visualize this. It’s just a conceptual thing. You can think of the tangent hyperplane as a best affine approximation to the function at the point \\(\\mathbf{a}\\). It’s the hyperplane that just touches tangent to the point \\(f(\\mathbf{a})\\) in \\(n+1\\) dimensional space.\nThe gradient vector \\(\\mathbf{g}(\\mathbf{a})\\) again points in the direction of steepest ascent of the function at the point \\(\\mathbf{x} = \\mathbf{a}\\). You can think of this as following from the fact that along contours of constant \\(z\\) we must have \\(dz = 0\\), which implies that \\(\\mathbf{g}\\) must be orthogonal to any infinitesimal changes \\(d\\mathbf{x}\\) along the contour. Of course, in higher-dimensions, a contour is really any \\(n\\)-dimensional surface of constant \\(z\\).\nWe can generalize the second derivative to higher dimensions as well by looking at the matrix of all partial derivatives of \\(z = f(\\mathbf{x})\\). Since there are \\(n\\) variables, there will be \\(n^2\\) possible second partials, most of which will be mixed partials. In all cases we’d care about the order of the mixed partials doesn’t matter. That is, partial derivatives commute. For example, the second partial with respect to \\(x_i\\) and \\(x_j\\) can be found by any of the following,\n\\[\\frac{\\partial^2 z}{\\partial x_i \\partial x_j} = \\frac{\\partial}{\\partial x_i} \\frac{\\partial z}{\\partial x_j} = \\frac{\\partial}{\\partial x_j} \\frac{\\partial z}{\\partial x_i} = \\frac{\\partial^2 z}{\\partial x_j \\partial x_i}.\\]\nPutting all of these second partials into a matrix gives the \\(n \\times n\\) matrix we call the Hessian,\n\\[\n\\frac{d^2z}{d\\mathbf{x}^2} = \\mathbf{H}(\\mathbf{x}) =\n\\begin{pmatrix}\n\\frac{\\partial^2 z}{\\partial x_0^2} & \\frac{\\partial^2 z}{\\partial x_1 \\partial x_0} & \\cdots & \\frac{\\partial^2 z}{\\partial x_{n-1} \\partial x_0} \\\\\n\\frac{\\partial^2 z}{\\partial x_0 \\partial x_1} & \\frac{\\partial^2 z}{\\partial x_1^2} & \\cdots & \\frac{\\partial^2 z}{\\partial x_{n-1} \\partial x_1} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial^2 z}{\\partial x_0 \\partial x_{n-1}} & \\frac{\\partial^2 z}{\\partial x_1 \\partial x_{n-1}} & \\cdots & \\frac{\\partial^2 z}{\\partial x_{n-1}^2}\n\\end{pmatrix}.\n\\]\nSince the second partials all commute, the Hessian will be symmetric, \\(\\mathbf{H}^\\top = \\mathbf{H}\\). The Hessian is the multivariate generalization of the second derivative in the sense that it’s the Jacobian of the gradient. As with any \\(n \\times n\\) symmetric matrix, the Hessian is guaranteed to have \\(n\\) real eigenvalues \\(\\lambda_0, \\lambda_1, \\cdots, \\lambda_{n-1}\\). The sign of these \\(n\\) eigenvalues tells you the curvature of the function at a point. This follows from the second-order approximation to the function at a point \\(\\mathbf{a}\\), given by\n\\[f(\\mathbf{x}) \\approx f(\\mathbf{a}) + \\mathbf{g}(\\mathbf{a})^\\top (\\mathbf{x} - \\mathbf{a}) + \\frac{1}{2}(\\mathbf{x} - \\mathbf{a})^\\top \\mathbf{H}(\\mathbf{a}) (\\mathbf{x} - \\mathbf{a}).\\]\nGeometrically, you can think of this function as defining a kind of hyper quadratic function. The term \\((\\mathbf{x} - \\mathbf{a})^\\top \\mathbf{H}(\\mathbf{a}) (\\mathbf{x} - \\mathbf{a})\\) is a quadratic form that determines the shape of this quadratic function. The sign of the eigenvalues tells you which directions will slope downwards and which will slope upwards.\nIf the eigenvalues are all positive then \\(\\mathbf{H}(\\mathbf{a})\\) will be positive definite, and the quadratic will bowl upwards. A function where \\(\\mathbf{H}(\\mathbf{a})\\) is positive definite for all \\(\\mathbf{a} \\in \\mathbb{R}^n\\) is called a convex function. Convex functions are very special to machine learning and optimization in general since, as we’ll see later, they have a unique global minimum, and there are very reliably efficient algorithms to find this minimum.\nIf the eigenvalues are all negative then \\(\\mathbf{H}(\\mathbf{a})\\) will be negative definite, and the quadratic will bowl downwards. Up to a minus sign, convex and concave functions are basically the same things, since if \\(f(\\mathbf{x})\\) is convex, then \\(-f(\\mathbf{x})\\) will be concave, and vice versa. Just as convex functions have a unique global minimum, concave functions have a unique global maximum. Outside of these two cases the eigenvalues will have mixed signs. Then, the function is called a non-convex function. Non-convex functions can bowl upwards in some directions and downward in others, like a saddle, except in \\(n\\) dimensions it can bowl downwards or upwards in multiple directions.\nThus far we’ve considered scalar-valued functions. Of course, we can consider vector-valued functions as well. Suppose we have a function \\(\\mathbf{y} = f(\\mathbf{x})\\) that maps \\(n\\) inputs \\(\\mathbf{x} = (x_0, x_1, \\cdots, x_{n-1})\\) inputs to \\(m\\) outputs \\(\\mathbf{y} = (y_0, y_1, \\cdots, y_{n-1})\\). Note that we need not require \\(m=n\\). We can have as many inputs as we like and as many outputs as we like. We can again think of each output \\(y_j\\) as a scalar-valued function that has a gradient \\(\\mathbf{g}_j\\). Putting all of these gradients into a matrix defines the \\(m \\times n\\) Jacobian matrix,\n\\[\n\\frac{d\\mathbf{y}}{d\\mathbf{x}} = \\mathbf{J}(\\mathbf{x}) =\n\\begin{pmatrix}\n\\frac{\\partial y_0}{\\partial x_0} & \\frac{\\partial y_0}{\\partial x_1} & \\cdots & \\frac{\\partial y_0}{\\partial x_{n-1}} \\\\\n\\frac{\\partial y_1}{\\partial x_0} & \\frac{\\partial y_1}{\\partial x_1} & \\cdots & \\frac{\\partial y_1}{\\partial x_{n-1}} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial y_{m-1}}{\\partial x_0} & \\frac{\\partial y_{m-1}}{\\partial x_1} & \\cdots & \\frac{\\partial y_{m-1}}{\\partial x_{n-1}}\n\\end{pmatrix}.\n\\]\nThe Jacobian matrix is the most general multivariate generalization of the ordinary derivative. It just generalizes the gradient to vector-valued functions. Notice that in general the Jacobian won’t even be a square matrix, much less symmetric. The total differential for a vector-valued function \\(\\mathbf{y} = f(\\mathbf{x})\\) can be written in the usual way by using matrix-vector multiplication,\n\\[d\\mathbf{y} = \\frac{d\\mathbf{y}}{d\\mathbf{x}} d\\mathbf{x} = \\mathbf{J} d\\mathbf{x}.\\]\nEvidently, the Jacobian can be thought of as a linear map that maps \\(n\\)-dimensional differential vectors \\(d\\mathbf{x}\\) to \\(m\\)-dimensional vectors \\(d\\mathbf{y}\\). As with the gradient, we can use the Jacobian to create an affine approximation to the function at a point \\(\\mathbf{a}\\),\n\\[f(\\mathbf{x}) \\approx f(\\mathbf{a}) + \\mathbf{J}(\\mathbf{a}) (\\mathbf{x} - \\mathbf{a}).\\]\nInstead of defining a single \\(n\\)-dimensional hyperplane, this function will define \\(m\\) such hyperplanes at once, one for each component of \\(\\mathbf{y}\\).\nAll of the differentiation rules given for bivariate functions carry over into higher dimensions as well. Higher-dimensional gradients and Jacobians satisfy their own versions of linearity, the product rule, the chain rule, and the inverse and quotient rules. Again, we have to keep in mind the compatibility of dimensions when figuring out which way to order terms.\nHere are some functions to calculate partial derivatives, the gradient, the Jacobian, and the Hessian for higher-dimensional functions. I’ll fill this in more later…\n\n\nCode\ndef diff(f, x, i, dx=1e-5):\n    # O(n) time, O(dx) convergence\n    e = np.zeros(len(x))\n    e[i] = 1\n    dzdxi = (f(x + dx * e) - f(x)) / dx\n    return dzdxi\n\ndef grad(f, x, dx=1e-5):\n    # O(n^2) time, O(n * dx) convergence\n    n = len(x)\n    partials = [diff(f, x, i, dx=dx) for i in range(n)]\n    return np.array(partials)\n\ndef jacobian(f, x, dx=1e-5):\n    # O(m * n^2) time, O(m * n * dx) convergence\n    m = len(f)\n    grads = [grad(f[j], x, dx=dx) for j in range(m)]\n    jacob = np.vstack(grads)\n    return jacob\n\ndef hessian(f, x, dx=1e-4):\n    # O(n^3) time, O(n^2 * dx^2) convergence\n    grad_fn = lambda x: grad(f, x, y, dx=dx)\n    hess = jacob(grad_fn, x, dx=dx)\n    return hess\n\n\nFinally, we can generalize the volume integral to higher dimensions as well. Now though, \\(z = f(x_0,x_1,\\cdots,x_{n-1})\\) defines an \\(n\\)-dimensional hypersurface in \\(n+1\\) dimensional space, and we seek to calculate the hypervolume under this hypersurface over an \\(n\\)-dimensional region \\(R\\). We proceed by trying to approximate this hypervolume by using hyperrectangles with infinitesimal base hyperarea \\(dA = dx_0 dx_1 \\cdots dx_{n-1}\\) and heights \\(f(x_0,x_1,\\cdots,x_{n-1})\\). Each of these hyperrectangles will have an infinitesimal hypervolume \\(dV = f(x_0,x_1,\\cdots,x_{n-1}) dA\\).\nIf we partition the region \\(R\\) into a grid of points and calculate the hypervolumes of each of these points and sum them all up, we get the hypervolume \\(V\\) under the function, which we call the hypervolume integral of \\(f(x_0,x_1,\\cdots,x_{n-1})\\) over \\(R\\),\n\\[V = \\int_{R} f(x_0,x_1,\\cdots,x_{n-1}) \\ dA = \\sum_{\\text{all hyperrectangles}} f(x_0,x_1,\\cdots,x_{n-1}) \\ dA.\\]\nWhen the region \\(R = [a_0, b_0] \\times [a_1, b_1] \\times \\cdots \\times [a_{n-1}, b_{n-1}]\\), the region itself is a hyperrectangle. In that case, we can break up the hypervolume integral into a sequence of iterated univariate integrals,\n\\[\\int_{R} f(x_0,x_1,\\cdots,x_{n-1}) \\ dA = \\int_{a_0}^{b_0} \\int_{a_1}^{b_1} \\cdots \\int_{a_{n-1}}^{b_{n-1}} f(x_0,\\cdots,x_{n-1}) \\ dx_0dx_1\\cdots dx_{n-1}.\\]\nAs with ordinary volume integrals, we’d do these from the inside out. Provided the function doesn’t blow up inside the region we can write the sequence of integrals in any order and get the same answer. All of the rules for the bivariate integral I mentioned extend to these hypervolume integrals as well. The only real difference is we’re now integrating over \\(n\\) variables, not just two.\nI won’t bother including code to numerical integrate these higher-dimensional hypervolume integrals since they’d be ungodly slow to calculate. In practice, methods like Monte Carlo integration are used to calculate these integrals when \\(n\\) is bigger than like 2 or 3."
  },
  {
    "objectID": "notebooks/optimization.html",
    "href": "notebooks/optimization.html",
    "title": "10  Optimization",
    "section": "",
    "text": "Talk about what optimization is from a practical point of view\nDefine the univariate optimization problem, focusing mostly on the minimum of unconstrained cost functions\nDerive Newton’s method from the version for root finding\nShow how gradient descent is just Newton’s method when the Hessian is a scalar, the learning rate\nTalk about higher-dimensional optimization, focusing first on convex functions and then talking about non-convex optimization via gradient descent to find a local stationary point (usually a saddlepoint)\n\nFor the purposes of machine learning, by far the most important application of differentiation and calculus in general is to optimization. Optimization is the problem of finding the “best” values with respect to some function. Usually in machine learning, by “best” we mean finding the minimum value of a loss function, which is a function that measures agreement between a model’s prediction and the data it sees. Finding the minimum value of the loss function essentially means we’ve found the best weights for our model, the ones that give the highest accuracy on the data.\nAn interesting fact is that for a reasonably smooth function, its minimum value will always be at a point where the derivative is zero. To see why, consider our tangent line plot of \\(y=x^2\\) from before. What happens if we set our point of interest to be \\(x_0=0\\)? Clearly that’s the minimum of this function. At this point, the tangent line hugs the parabola horizontally, which means it’s a point where the slope is zero.\nf = lambda x: x ** 2\ndfdx = lambda x: 2 * x\nx0 = 0\ny0 = f(x0)\nx = np.arange(-10, 10, 0.1)\nf_tangent = lambda x: y0 + dfdx(x0) * (x - x0) plot_function(x, (f, f_tangent), (-5, 5), (-2, 10), title=f’Tangent of \\(y=x^2\\) at \\({(x0,y0)}\\)’)\nThis same fact also holds for the maximum of a function as well. Not just the maximum, but any other point where the function is flat, called saddle points. As an example, the origin is a saddle-point of the function \\(y=x^3\\). These general points where the derivative is zero (min, max, or saddle point) are called stationary points.\nIn machine learning we usually care most about the minimum. I’ll just mention that we can formulate any maximum problem as a minimum problem by just multiplying the function by -1, which flips the function upside down, turning any maxima into minima.\nNow, suppose we have a univariate function \\(y=f(x)\\). The problem of (unconstrained) optimization is to find a point \\(x^*\\) such that \\(y^* = f(x^*)\\) is the minimum value of \\(f(x)\\), i.e.  \\[y^* = \\min f(x) \\leq f(x) \\text{ for all } x.\\] The special point \\(x^*\\) that minimizes the function is called the argmin, written \\[x^* = \\text{argmin } f(x).\\]\nI need to mention a subtle point. What do I mean when I say “the minimum”? When I say \\(y^* \\leq f(x)\\) for all \\(x\\), which \\(x\\) values am I talking about? This means we’re really only talking about the minimum over some range of \\(x\\) values. We have to specify what that range is. If the range is the whole real line, it really is the minimum, usually called the global minimum. If it’s over some subset of the real line it may not be the global minimum since we’re not looking at every \\(x\\). It’s only the minimum in our region of interest. This sort of region-specific minimum is called a local minimum.\nWhile this seems like a subtle point, it is an important one in machine learning. Some algorithms, like deep learning algorithms, can only reliably find a local minimum. Finding the global minimum can be harder unless there’s only one minimum to begin with. These simple functions are called convex functions. Our above example of \\(y=x^2\\) is a convex function. It only has one minimum, and the function just slopes up around it on both sides in a bowl shape. Deep learning loss functions on the other hand are nasty, wiggly things with lots of bumps and valleys. Such functions are called non-convex functions. In general they’ll have lots of local minima.\nSo back to the fact about the derivative being zero at the minimum, what we “proved” by example is that at the point \\(x^*\\) we should have \\[\\frac{d}{dx}f(x^*)=0.\\] Another useful way to state the same fact is to think in terms of infinitesimals: At \\(x^*\\), any infinitesimal perturbation \\(dx\\) won’t change the value of the function at all, \\(f(x^*+dx) = f(x^*)\\). This is just another way of stating that \\(dy=0\\) at \\(x^*\\). The fact that small perturbations don’t change the function’s value is unique to minima and other stationary points.\nLet’s verify this fact with the same example \\(y=x^2\\) by looking at small perturbations around \\(x=0\\). Since \\(f(0)=0\\) is a minimum, any perturbation should just give \\(0\\) as well. Choosing a \\(dx\\) of 1e-5, we can see that the function’s perturbed value \\(f(0+dx)\\) is only about 1e-10, essentially negligible since \\(dx^2 \\approx 0\\) for infinitesimals. This won’t be true for any other value of \\(x\\), e.g. \\(x=1\\), which has a much larger change of 2e-5, which is on the order of \\(dx\\), as expected.\ndx = 1e-5\nf(0 + dx) - f(0)\nf(1 + dx) - f(1)\nPretty much everything I’ve said on optimization extends naturally to higher dimensions. That’s why I went into so much detail on the simple univariate case. It’s easier to explain and visualize. To extend to \\(n\\) dimensions we basically just need to convert inputs into vectors and derivatives into gradients. Other than this the formulas all look basically the same.\nSuppose we have now a scalar-valued multivariate function \\(z=f(\\mathbf{x})=f(x_1,\\cdots,x_n)\\). The problem of (unconstrained) optimization is to find a vector \\(\\mathbf{x}^* \\in \\mathbb{R}^n\\) such that \\(z^* = f(\\mathbf{x}^*)\\) is the minimum value of \\(f(\\mathbf{x})\\), i.e.  \\[z^* = \\min f(\\mathbf{x}) \\leq f(\\mathbf{x}) \\text{ for all } \\mathbf{x} \\in \\mathbb{R}^n.\\] The vector \\(\\mathbf{x}^*\\) that minimizes the function is called the argmin, written \\[\\mathbf{x}^* = \\text{argmin } f(\\mathbf{x}).\\]\nJust as the derivative is zero at the minimum in the univariate case, the gradient is the zero vector at the minimum in the multivariate case, \\[\\frac{d}{d\\mathbf{x}}f(\\mathbf{x^*})=\\mathbf{0}.\\] Another way of stating the same fact is that at the minimum \\(f(\\mathbf{x^*} + d\\mathbf{x}) = f(\\mathbf{x^*})\\) for any infinitesimal perturbation vector \\(d\\mathbf{x}\\). Equivalently, \\(dz=0\\).\n\n10.0.1 Gradient Descent\nSo if the minimum is so important how do we actually find the thing? For simple functions like \\(y=x^2\\) we can do it just by plotting the function, or by trial and error. We can also do it analytically by solving the equation \\(\\frac{dy}{dx}\\big|_{x^*}=0\\) for \\(x^*\\). But for complicated functions, or functions we can’t exactly write down, this isn’t feasible. We need an algorithmic way to do it.\nLet’s try something simple. Since the derivative at \\(x\\) tells us the slope of the function at \\(x\\), it’s in some sense telling us how far we are away from the minimum. Suppose we perturb \\(x\\) to \\(dx\\). Then \\(y=f(x)\\) gets perturbed to \\(y+dy=f(x+dx)\\). Now, observe the almost trivial fact that \\[dy = \\frac{dy}{dx}dx.\\] So if \\(\\frac{dy}{dx}\\) is large, small changes in \\(x\\) will result in large changes in \\(y\\). Similarly, if \\(\\frac{dy}{dx}\\) is small, then small changes in \\(x\\) will result in small changes in \\(y\\). But we demonstrated above that if we’re near the minimum we know that changes in \\(y\\) will be tiny if \\(dx\\) is small. Thus, the derivative serves as a kind of “how close are we to the minimum” metric.\nBut that’s not all the derivative tells us. Since the sign of the derivative indicates which way the slope is slanting, it also tells us which direction the minimum is in. If you’re at a point on the function, the minimum will always be in the direction that’s sloping downward from you. Since the slope slants upward in the direction of the sign of the derivative, and we want to move downward the other way, the minimum will be in the direction of the negative of the derivative.\nMore formally, suppose we want to find the minimum of \\(y=f(x)\\). To start, we’ll pick a point \\(x_0\\) at random. Doesn’t matter too much how. Pick a step size, we’ll call it \\(\\alpha\\). This will multiply the derivative and tell us how big of a step to take towards the minimum (more on why this is important in a second). Now, we’ll take a step towards the minimum \\[x_1 = x_0 - \\alpha \\frac{dy}{dx}\\bigg|_{x_0}.\\] This puts us at a new point \\(x_1\\), which will be closer to the argmin \\(x^*\\) if our step size is small enough. Now do it again, \\[x_2 = x_1 - \\alpha \\frac{dy}{dx}\\bigg|_{x_1}.\\] And again, \\[x_3 = x_2 - \\alpha \\frac{dy}{dx}\\bigg|_{x_2}.\\] Keep doing this over and over. Stop when the points aren’t changing much anymore, i.e. when \\(|x_{n+1}-x_n|<\\varepsilon\\) for some small tolerance \\(\\varepsilon\\). Then we can say that the argmin is \\(x^* \\approx x_n\\), and the minimum is \\(y^* \\approx f(x_n)\\). Done.\nThis simple algorithm to find the (local) minimum by starting at a random point and steadily marching in the direction of the derivative is called gradient descent. With some relatively minor modifications here and there, gradient descent is how many machine learning algorithms are trained, including essentially all deep learning algorithms. It’s very possibly the most important algorithm in machine learning.\nIn machine learning, running an optimizer like gradient descent is usually called training. You can kind of imagine optimization as trying to teach something to a model. The condition of being at the minimum is analogous to the model learning whatever task it is you’re trying to teach it. The thing we’re minimizing in this case is the loss function, which is hand-picked essentially to measure how well the model is learning the given task.\nThe step size \\(\\alpha\\) is so important in machine learning that it’s given a special name, the learning rate. It in essence controls how quickly a model learns, or trains. I’ll use this terminology for \\(\\alpha\\) going forward.\nHere’s what the algorithm looks like as a python function gradient_descent. It will take as arguments the function f we’re trying to minimize, the function for its derivative or gradient grad_fn, the initial point x0, the learning rate alpha. I’ll also pass in two optional arguments, max_iter and eps, where max_iter is how many iterations to run gradient descent in the worst case, and eps is the tolerance parameter to indicate when to stop.\ndef gradient_descent(f, grad_fn, x0, alpha, max_iter=1000, eps=1e-5):\n    x_prev = x0  # initialize the algorithm\n    for i in range(max_iter):\n        x_curr = x_prev - alpha * grad_fn(x_prev)  # gradient descent step\n        if np.abs(x_curr - x_prev) < eps:  # if changes are smaller than eps we're done, return x*\n            print(f'converged after {i} iterations')\n            return x_curr\n        x_prev = x_curr\n    print(f'failed to converge in {max_iter} iterations')  # else warn and return x* anyway\n    return x_curr\nLet’s run this algorithm on our simple example \\(y=x^2\\). Recall its derivative function is \\(\\frac{dy}{dx}=2x\\). I’ll choose an initial point \\(x_0=5\\) and a learning rate of \\(\\alpha=0.8\\). The optional arguments won’t change.\nWe can see that gradient descent in this case converges (i.e. finishes) after only 27 iterations. It predicts an argmin of about \\(x^* \\approx 3 \\cdot 10^{-6}\\) and a minimum of about \\(y^* \\approx 9 \\cdot 10^{12}\\). Since both are basically \\(0\\) (the true value for both) to within one part in \\(10^{-5}\\) we seem to have done pretty well here.\nFeel free to play around with different choices of the learning rate alpha to see how that affects training time and convergence. Getting a good feel for gradient descent is essential for a machine learning practitioner.\nf = lambda x: x ** 2\ngrad_fn = lambda x: 2 * x\nx0 = 5\nalpha = 0.8\nx_min = gradient_descent(f, grad_fn, x0, alpha)\ny_min = f(x_min)\nprint(f'estimated argmin: {x_min}')\nprint(f'estimated min: {y_min}')\nWhile I’ve shown the math and code for gradient descent, we’ve still yet to get a good intuition for what the algorithm is doing. For this I’ll turn to a visualization. What I’m going to do is plot the function curve in black, and on top of it show each step of gradient descent. Each red dot on the curve of the function will indicate the point \\((x_n,y_n)\\) at step \\(n\\) of the algorithm. Successive steps will be connected by a red line. Each red line will show which points the algorithm jumps from and to at each step. Starting and ending points will be annotated as well.\nTo do this I’ll use a helper function plot_gradient_descent, which takes in the same arguments as gradient_descent as well as a few more arguments that do some styling of the plot. Internally, all this function is doing is running gradient descent on the given arguments, then plotting the functions, dots, and line segments described.\nI’ll start by showing what gradient descent is doing on the exact same example as above. The curve of course is just a parabola sloping upward from the origin. The starting point is just \\((x_0,f(x_0))=(5,25)\\). After running for \\(N=30\\) iterations the algorithm basically settles down to \\((x_N,f(x_N)) \\approx (0,0)\\). Notice what’s happening in between though. Imagine you dropped a marble into a bowl at the starting point. After landing, the marble bounces across the bowl several times as it settles down around the origin, where it rolls around less and less until it eventually dissipates all its kinetic energy and settles down at the bottom of the bowl.\nplot_gradient_descent(f=f, grad_fn=grad_fn, x0=x0, alpha=alpha, n_iters=30, \n                      title=f'$y=x^2$,  $\\\\alpha={alpha}$,  $N={30}$,  $x_0={x0}$')\nTo illustrate what the learning rate is doing, and how important it is to tune it well, let’s try the same problem in two other cases: a really high learning rate, and a really low learning rate. I’ll start with a high learning rate of \\(\\alpha=1.1\\). I’ll run the algorithm this time for \\(N=20\\) iterations.\nPay particular attention in this case to the start and end labels. Evidently choosing a high learning rate caused the algorithm not to spiral down towards the minimum, but to spiral up away from the minimum! This is the hallmark of choosing too large a learning rate. The algorithm won’t converge at all. It’ll just keep shooting further and further away from the minimum.\nalpha = 1.1\nN = 20\nplot_gradient_descent(f=f, grad_fn=grad_fn, x0=x0, alpha=alpha, n_iters=10, \n                      title=f'$y=x^2$,  $\\\\alpha={alpha}$,  $N={30}$,  $x_0={x0}$')\nLet’s now look at a low learning rate of \\(\\alpha=0.01\\). I’ll run this one for \\(N=150\\) iterations. Notice now that the algorithm is indeed converging towards the minimum, but it’s doing it really, really slowly. It’s not bouncing around the bowl at all, but rather slowly crawling down in small steps. This is the hallmark of using too low a learning rate. The algorithm will converge, but it’ll do so really, really slowly, and you’ll need to train for a lot of iterations.\nalpha = 0.01\nN = 150\nplot_gradient_descent(f=f, grad_fn=grad_fn, x0=x0, alpha=alpha, n_iters=N,\n                      title=f'$y=x^2$,  $\\\\alpha={alpha}$,  $N={N}$,  $x_0={x0}$')\nThings may seem all fine and good. We have an algorithm that seems like it can reliably find the minimum of whatever function we give it, at least in the univariate case. Unfortunately, there are a few subtleties involved that I’ve yet to mention. It turns out that the function I picked, \\(y=x^2\\) is a particularly easy function to minimize. It’s a convex function. Not all functions behave that nicely. Practically no loss function in deep learning does.\nIf a function is non-convex (i.e. not bowl-shaped) it can have multiple minima. This means that you can’t be sure gradient descent will pick out the global minimum if you run it. Which minimum it settles in will depend on your choice of initial point \\(x_0\\), the learning rate \\(\\alpha\\), and perhaps even the number of iterations \\(N\\) you run the algorithm.\nThis isn’t the only problem, or even the worst problem. Perhaps the worst problem is saddle points. If there are saddle points in the function, gradient descent may well settle down on one of those instead of any of the minima. Here’s an example of this. Let’s look at the function \\(y=x^3 + (x+1)^4\\). Its derivative function turns out to be \\(\\frac{dy}{dx}=3x^2 + 4(x+1)^3\\). Check WolframAlpha if you don’t believe me.\nNow, suppose we want to find the minimum of this function. Not knowing any better, we pick an initial point \\(x_0=3\\), and just to be safe we pick a small learning rate \\(\\alpha=0.001\\). Let’s run gradient descent now for \\(N=500\\) iterations. Surely that’s enough to find the minimum, right?\nEvidently not. The true minimum seems to be somewhere around the point \\((-2.8, -12)\\). The algorithm didn’t settle down anywhere near this point. It settled around the origin \\((0,0)\\). So what happened? If you look closely, you’ll see it got stuck in a flat spot, i.e. a saddle point. No matter how many iterations you run gradient descent with this learning rate, it will never leave this flat spot. It’s stuck.\nf = lambda x: x ** 3 + (x + 1) ** 4\ngrad_fn = lambda x: 3 * x ** 2 + 4 * (x + 1) ** 3\nx0 = 3\nN = 500\nalpha=0.001\nplot_gradient_descent(f, grad_fn, x0, alpha=alpha, n_iters=N, xlim=(-4, 2), ylim=(-15, 50), \n                      title=f'$y=x^3 + (x-1)^4$,  $\\\\alpha={alpha}$,  $N={N}$,  $x_0={x0}$')\nAll isn’t necessarily lost. What happens if we pick a higher learning rate to let the algorithm bounce around the function a little bit before slowing down? Let’s pick \\(\\alpha=0.03\\) now and run for the same number of iterations. Now it looks like we’re doing just fine. Gradient descent was able to bounce across the flat spot and settle down at the other side.\nalpha=0.03\nN = 100\nplot_gradient_descent(f, grad_fn, x0, alpha=alpha, n_iters=N, xlim=(-6, 4), ylim=(-15, f(3) + 20), \n                      annotate_start_end=True,\n                      title=f'$y=x^3 + (x-1)^4$,  $\\\\alpha={alpha}$,  $N={N}$,  $x_0={x0}$')\nThis example was meant to show that saddle points can be a real issue. Gradient descent will not tell you if the point it found is a minimum or a saddle point, it’ll just stop running and spit out a value. You thus need to be careful about things like this when running gradient descent on real-life functions. It’s even worse in higher dimensions, where it turns out that almost all stationary points will be saddle points, and very few will be minima or maxima.\nFor these reasons, it’s common in machine learning to not use a tolerance condition like \\(|x_{n}-x_{n-1}| < \\varepsilon\\). Instead we just specify some number of iterations \\(N\\) and run the algorithm \\(N\\) times. Basically, we want to give the algorithm a chance to get out of a flat spot if it gets stuck in one for some reason. Said differently, if a function is not convex, and most in machine learning are not convex, the notion of convergence doesn’t necessarily mean that much since we don’t even know if we’re at a minimum or not.\nThe gradient descent algorithm works exactly the same as in the univariate case, except we now use the gradient vector instead of the derivative at each step. Here’s the algorithm in steps: 1. Initialize a starting vector \\(\\mathbf{x}_0\\). 2. For \\(N\\) iterations, perform the gradient descent update \\[\\mathbf{x}_n = \\mathbf{x}_{n-1} - \\alpha \\frac{dz}{d\\mathbf{x}}\\bigg|_{\\mathbf{x}=\\mathbf{x}_{n-1}}.\\] 3. Converge either when some convergence criterion is satisfied, \\(||\\mathbf{x}_n-\\mathbf{x}_{n-1}||_2 \\leq \\varepsilon\\), or when some maximum number of iterations \\(N\\) is reached. 4. Return \\(\\mathbf{x}_N\\). The best guess for the argmin is \\(\\mathbf{x}^* \\approx \\mathbf{x}_N\\), and for the minimum is \\(z^* \\approx f(\\mathbf{x}_N)\\).\nAside: I’ll quickly note that gradient descent isn’t the only minimization algorithm. Some other algorithms worth noting use not just the first derivative in their updates, but also the second derivative. Examples include algorithms like Newton’s Method and LBFGS. The second derivative provides information about the curvature of the function, which can speed up convergence by making the learning rate adaptive. While these second-order algorithms are useful in some areas of machine learning, it usually turns out to be far too computationally expensive to calculate the second derivative (also called the Hessian) of a function in high dimensions. Perhaps the main reason gradient descent is used in machine learning is because it provides a good tradeoff between its speed of convergence and computational performance.\nThis pretty much covers everything I wanted to talk about regarding optimization, the most important application of calculus to machine learning. In future lessons we’ll spend more time talking about gradient descent as well as its more modern variants like SGD and Adam."
  },
  {
    "objectID": "notebooks/probability.html#randomness",
    "href": "notebooks/probability.html#randomness",
    "title": "11  Basic Probability",
    "section": "11.1 Randomness",
    "text": "11.1 Randomness\nProbability is a calculus for modeling random processes. There are things we just can’t predict with certainty given the information we have available. Stuff that we can’t predict with certainty we call random, or noise, or non-deterministic. Stuff we can predict with certainty we call deterministic or certain. Here are some examples of these two kinds of processes. The questions in the deterministic column have exact answers, while those in the random column do not.\n\n\n\n\n\n\n\nDeterministic Process\nRandom Process\n\n\n\n\nDoes \\(2+2=4\\)?\nWill it rain today?\n\n\nWhat is the capital of France?\nWhat is the result of rolling a pair of dice?\n\n\nHow many sides does a square have?\nWhat is the next card in a shuffled deck?\n\n\nWhat is the value of pi?\nWhat is the stock price of Apple tomorrow?\n\n\nWhat is the boiling point of water at sea level?\nWhat is the winning number for next week’s lottery?\n\n\n\nDeterministic processes aren’t terribly interesting. They either will occur with certainty, or they won’t. Random processes might occur. To quantify what we mean by might we’ll introduce the notion of probability. You can think of probability as a function mapping questions like “Will it rain today?” to a number between \\(0\\) and \\(1\\) that indicates our “degree of belief” in whether that question is true,\n\\[0 \\leq \\mathbb{Pr}(\\text{Will it rain today?}) \\leq 1.\\]\nThe question inside this probability function is called an event. An event is anything that might occur. Mathematically speaking, an event is a set that lives in some abstract sample space of all possible outcomes.\nWhen we’re certain an event will occur we say it has probability one, or a 100% chance of happening. When we’re certain an event will not occur we say it has probability zero, or a 0% chance of happening. These extremes are deterministic processes. Random processes are anything in between. For the question “Will it rain today?”, we might say there is a 20% chance of rain, in which case we believe \\(\\mathbb{Pr}(\\text{Will it rain today?}) = 0.2\\).\nA common theme we’ll see in machine learning is that we’re interested in mapping arbitrary data structures like strings to numerical data structures that we can do mathematical calculations with, like floats or arrays. In this particular example, it’s convenient to map the question “Will it rain today?” to a binary variable I’ll call \\(x\\), \\[\nx =\n\\begin{cases}\n1, & \\text{It will rain today} \\\\\n0, & \\text{It will not rain today}.\n\\end{cases}\n\\]\nThen asking for \\(\\mathbb{Pr}(\\text{Will it rain today?})\\) is the same thing as asking “what is the probability that \\(x=1\\)”, or equivalently, what is \\(\\mathbb{Pr}(x=1)\\)? Saying we believe there’s a 20% chance of rain today is equivalent to saying we believe there is a 20% chance that \\(x=1\\), i.e. \\(\\mathbb{Pr}(x=1)=0.2\\).\nVariables like \\(x\\) are called random variables. They’re a way of encoding random events numerically via some kind of encoding convention like I just used. It’s much more convenient to work with random variables than events or questions since we can now use all our usual mathematical tools like calculus and linear algebra to understand random processes.\nTo understand how random variables work, it’s often helpful to think of them as the outputs of random number generators. These are algorithms that generate, or sample, random numbers from some given distribution. Unlike regular functions, where a given input will always produce a definite output, a random number generator can (and usually will) produce different outputs every single time the same input is passed in.\nThe canonical example of a random number generator is called rand. It’s an algorithm for uniformly generating (pseudo) random real numbers \\(0 \\leq x \\leq 1\\). Every time we call rand we’ll get a different number with no clear pattern.\nHere’s an example. I’ll call rand via the numpy function np.random.rand a bunch of times and print the first 10 outputs. Notice how all over the place they seem to be. The only thing we know is they’re between zero and one.\n\n\nCode\nx = np.random.rand(100)\nx[:12]\n\n\narray([0.5488135 , 0.71518937, 0.60276338, 0.54488318, 0.4236548 ,\n       0.64589411, 0.43758721, 0.891773  , 0.96366276, 0.38344152,\n       0.79172504, 0.52889492])\n\n\nThink of a random variable informally as being some variable \\(x\\) whose values are determined by a function \\(x=f(n)\\), except the function can’t make up its mind or follow a pattern. On one sampling we might get \\(x=f(0)=0.548\\). Next, \\(x=f(1)=0.715\\). Next, \\(x=f(2)=0.603\\). Etc. We can’t force \\(x\\) to take on a definite value. It jumps around with no clear pattern.\n\n\nCode\nplt.scatter(range(len(x)), x)\nplt.xlabel('n')\nplt.ylabel('x')\nplt.title('$x = f(n)$')\nplt.show();\n\n\n\n\n\nSince random variable outputs jump around like this we need a different way to visualize them than just thinking of them as points on the number line. The most useful way to visualize random variables is using a histogram. To create a histogram, we sample a random variable a whole bunch of times, and plot a count of how many times the variable takes on each given value. We then show these counts in a bar chart with the heights indicating the counts for each value.\nIn matplotlib we can plot histograms of an array of samples x using the function plt.hist(x). Here’s an example. I’ll sample 100 values from rand and put them in an array x, then plot the histogram.\n\n\nCode\nx = np.random.rand(100)\nplt.hist(x)\nplt.show();\n\n\n\n\n\nNotice that we just sampled \\(100\\) different values, but we don’t see \\(100\\) different bars. That’s because histograms don’t plot bars for all values. First, the values get binned into some number of equally spaced subintervals, called bins, then the counts that get plotted are the counts of values inside each bin. In this case, the histogram divides the samples into \\(10\\) equally spaced bins. If you look carefully you should see \\(10\\) bars in the plot. We can change the number of bins by passing in a keyword bins specifying how many bints to take.\nSince I’ll be using histograms a lot in this lesson I’m going to write a helper function plot_histogram to bundle up the code to plot them nicely. Instead of using plt.hist, however, I’ll use the seaborn library’s sns.histplot, which creates much nicer looking histograms. Seaborn is an extension library of matplotlib made specifically for making nicer plots of data. Ignore the is_discrete argument for now. I’ll use it in the next section.\n\n\nCode\ndef plot_histogram(x, is_discrete=False, title='', **kwargs):\n    if is_discrete:\n        sns.histplot(x, discrete=True, shrink=0.8, **kwargs)\n        unique = np.unique(x)\n        if len(unique) < 15:\n            plt.xticks(unique)\n    else:\n        sns.histplot(x, **kwargs)\n    plt.title(title)\n    plt.show()\n\n\nIt’s still kind of hard to see if the \\(100\\) rand samples have any kind of pattern in the above histogram plot. Let’s now sample 10,000 numbers from rand and see if we can find one.\n\n\nCode\nx = np.random.rand(10000)\nplot_histogram(x, bins=10, title=f'rand({10000})')\n\n\n\n\n\nIt should be increasingly clear now that what’s going on is that rand is sampling numbers between 0 and 1 with equal probability. Each bin should contain roughly \\(\\frac{10000}{10}=1000\\) counts, since there are \\(10000\\) samples and \\(10\\) bins. Said differently, the values in each bin should have a \\(\\frac{1}{10}=0.1\\) probability of being sampled. For example, the values in the left-most bin, call it \\(I_0 = [0, 0.1]\\) should have\n\\[\\mathbb{Pr}(x \\in I_0) = \\mathbb{Pr}(0 \\leq x \\leq 0.1) = 0.1.\\]\nThis type of “flat”, equal probability sampling is called uniform random sampling.\nYou may be questioning that it’s indeed the case that each bin is truly getting sampled as much as the other bins. After all, the plot still clearly shows their heights vary a bit. Some bins have slightly more values than others do. We can look at how many counts are in the bin using np.histogram, which also defaults to \\(10\\) bins. You can see some bins have as many as \\(1037\\) values, some as few as \\(960\\) values.\n\n\nCode\nbin_counts, _ = np.histogram(x)\nbin_counts\n\n\narray([1025, 1036,  999,  981, 1037,  989,  956,  996,  976, 1005])\n\n\n\n11.1.0.1 Aside: Estimating the Fluctuation in Bin Counts\nThis variation in the bin counts is really due to the fact that we’re only sampling a finite number of values. To get true uniform sampling, where all bins have the same counts, we’d have to sample an infinitely large number of times.\nHere’s a rule of thumb for how much the bin counts should be expected to fluctuate as a function of the sample size. If \\(N\\) is the number of samples, and each bin \\(k\\) contains \\(N_k\\) counts (i.e. its bar height is \\(N_k\\)), then you can expect the counts to fluctuate above and below \\(N_k\\) by about\n\\[\\sigma_k = \\sqrt{N_k\\bigg(1 - \\frac{N_k}{N}\\bigg)}.\\]\nSaid differently, the counts should be expected to roughly lie in a range \\(N_k \\pm \\sigma_k\\). This notation means the same thing as saying the counts should roughly speaking lie in the range \\([N_k - \\sigma_k, N_k + \\sigma_k]\\). By “roughly”, I mean sometimes bins can have counts outside this range, but it’s uncommon.\nIn the above example, there are \\(N=10000\\) samples, and each bin has about \\(N_k=1000\\) counts, so you should expect the counts to fluctuate by about\n\\[\\sigma_k = \\sqrt{1000\\bigg(1 - \\frac{1000}{10000}\\bigg)} = 30,\\]\nwhich means the counts should rougly lie in the range \\(1000 \\pm 30\\). This seems to be in line with what we’re seeing experimentally. Notice as the sample size \\(N \\rightarrow \\infty\\), the fluctuations \\(\\sigma_k \\rightarrow 0\\). We’ll see where this rule comes from later (hint: the binomial distribution).\nBack to random variables. Broadly speaking we can divide random variables into two classes of distributions:\n\ndiscrete distributions: random variables that can only take on a discrete set of values.\ncontinuous distributions: random variables that can take on any continuum of real values.\n\nI’ll start by talking about the discrete case since it’s easier to understand."
  },
  {
    "objectID": "notebooks/probability.html#discrete-probability",
    "href": "notebooks/probability.html#discrete-probability",
    "title": "11  Basic Probability",
    "section": "11.2 Discrete Probability",
    "text": "11.2 Discrete Probability\nDiscrete random variables are variables that can only take on a discrete range of values. Usually this range is a finite set like \\(\\{0,1\\}\\) or \\(\\{1,2,3,4,5,6\\}\\) or something like that. But they could have an infinite range too, for example the set \\(\\mathbb{N}\\) of all non-negative integers. Rand is not an example of a discrete random variable, since there the range is all of the interval \\([0,1]\\).\nHere are some examples of real life things that can be modeled by a discrete random variable:\n\nModeling the rolls of a die with faces \\(1,2,3,4,5,6\\).\nModeling values from flipping a coin taking on a value of heads or tails.\nModeling a hand of poker, where there are 5 cards each drawn from the same deck of 52 cards.\nModeling the outputs of data used to train a machine learning classification model.\nModeling the number of heads gotten from flipping a coin a whole bunch of times.\nModeling the number of people entering a building per hour.\n\n\n11.2.1 Motivation: Rolling a Die\nConsider a very simple toy problem: rolling a die (singular of dice). If you’ve never seen dice before, they’re white cubes with black dots on each face of the cube. Each face gets some number of black dots on it between 1 and 6. People like to “roll” these dice in games by shaking and tossing them onto the ground. The person with the highest score, i.e. the most number of dots facing upward, wins that round.\n\n\n🎲\n\n\nLet’s think a little bit about a single die. Suppose I want to roll a single die. Having not rolled the die yet, what should I “expect” the value to be when I roll the die? Call this score \\(x\\). The possible values I can have are just the number of dots on each face of the die, i.e. \\(1,2,3,4,5,6\\). This alone doesn’t tell me what the chance is that any given \\(x\\) turns up in a roll. We need some other information.\nPerhaps your common sense kicks in and you think, “Well clearly each number has an equal chance of showing up if you roll the die”. This is called the principle of indifference. In practice you’d usually be right. You’re saying that, since we don’t have any other information to go on, each number should have an equal chance of showing up on each roll. That is, on any given roll, the random variable \\(x\\) should take on each value \\(k=1,2,\\cdots,6\\) with probability,\n\\[p_k = \\mathbb{Pr}(x=k) = \\frac{1}{6}.\\]\nThis just says that the probability of rolling \\(x=1\\) is \\(p_1 = \\frac{1}{6}\\), the probability of rolling \\(x=2\\) is also \\(p_2 = \\frac{1}{6}\\), etc. Notice that these probabilities satisfy two properties that all probabilities must satisfy: 1. Each probability is non-negative: \\(p_k = \\frac{1}{6} \\geq 0\\), 2. The sum of all the possible probabilities is one: \\(\\sum_{k=1}^6 p_k = p_1 + p_2 + p_3 + p_4 + p_5 + p_6 = 6 \\cdot \\frac{1}{6} = 1\\).\nThese two properties are the defining characteristics of a probability. The second condition is just a mathematical way of saying that rolling the die must return some value \\(x \\in \\{1,2,3,4,5,6\\}\\). It can’t just make up some new value, or refuse to answer.\nAnyway, suppose I rolled the die \\(N=36\\) times and got the following values:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRoll\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n\n\n\n\nValue\n3\n4\n5\n4\n3\n1\n3\n6\n5\n2\n1\n5\n4\n2\n1\n1\n1\n6\n5\n6\n3\n5\n5\n3\n3\n6\n6\n1\n5\n4\n2\n2\n4\n6\n2\n4\n\n\n\nWe can make a histogram out of these and check the principle of indifference by verifying the bins are all of about the same height (at least as close to the same as only 30 rolls will allow). Note that I’m now using is_discrete=True here, which tells the helper function to give each unique \\(k\\) its own bin.\n\n\nCode\nx = [3, 4, 5, 4, 3, 1, 3, 6, 5, 2, 1, 5, 4, 2, 1, 1, 1, 6, 5, 6, 3, 5, 5, 3, 3, 6, 6, 1, 5, 4, 2, 2, 4, 6, 2, 4]\nplot_histogram(x, is_discrete=True, title='36 Die Rolls')\n\n\n\n\n\nGiven the fact that I only rolled \\(36\\) times, this histogram looks very uniform, giving a pretty strong hint that each value has an equal probability of being rolled. Since most bars have height \\(6\\), they correspond to probabilities of \\(\\frac{6}{36}=\\frac{1}{6}\\), which is what our common sense expected. Note the counts can fluctuate in this case in a range of about \\(6 \\pm 2\\). This is an example of a fair die.\nWhat if our common sense was incorrect? What if I rolled the die a bunch of times and found out some numbers occurred a lot more often than others? This would happen if the die were weighted unevenly, or loaded. In this case we’re left to assign some weight \\(N\\) to each number \\(k\\).\nTo determine what the right weights should be empirically, probably the easiest way would again be to roll the die a bunch of times and count how many times each value \\(k\\) occurs. Those counts will be your weights \\(N_k\\). These are just the heights of each bin in the histogram. To turn them into probabilities \\(p_k\\), divide by the total number of rolls, call it \\(N\\). The probabilities would then be given approximately by\n\\[p_k = \\mathbb{Pr}(x=k) \\approx \\frac{N_k}{N}.\\]\nThat is, the probability \\(p_k\\) is just a ratio of counts, the fraction of times \\(x=k\\) occurred in \\(N\\) counts. As \\(N \\rightarrow \\infty\\) this equality goes from approximate to exact. In fact, we could define the probability \\(p_k = \\mathbb{Pr}(x=k)\\) as the limit\n\\[p_k = \\mathbb{Pr}(x=k) = \\lim_{N \\rightarrow \\infty} \\frac{N_k}{N}.\\]\nThis is an alternate way of defining a probability, different from the “degree of belief” approach I used above. This is usually called the frequentist or objective approach. In this approach, probability is the frequency of the number of times an outcome occurs in an experiment, i.e. \\(\\frac{N_k}{N}\\). In contrast, the “degree of belief” perspective is called the Bayesian or subjective approach. Both approaches have their uses, so we’ll go back and forth between the two as it suits us.\nTo test if your die is loaded, what you can do is roll the die \\(N\\) trials and calculate the probabilities. If they’re all roughly equal to \\(1/6\\) like the example above then the die is fair. Otherwise it’s loaded. Suppose when I’d rolled the die I’d instead gotten the following outcomes:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRoll\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n\n\n\n\nValue\n4\n4\n5\n4\n3\n5\n3\n6\n5\n6\n1\n5\n4\n5\n6\n5\n1\n6\n5\n6\n3\n5\n5\n4\n3\n6\n6\n4\n5\n4\n2\n5\n4\n6\n2\n4\n\n\n\nLet’s plot the histogram of these outcomes and compare to the fair die case.\n\n\nCode\nx = [4, 4, 5, 4, 3, 5, 3, 6, 5, 6, 1, 5, 4, 5, 6, 5, 1, 6, 5, 6, 3, 5, 5, 4, 3, 6, 6, 4, 5, 4, 2, 5, 4, 6, 2, 4]\nplot_histogram(x, is_discrete=True, title='36 Die Rolls (Round 2)')\n\n\n\n\n\nNotice how now the outcomes are skewed towards higher values. This clearly doesn’t look uniform anymore since most of the counts aren’t in the expected range of \\(6 \\pm 2\\). The die has been “loaded to roll high”.\nUsing the experimental approach we can estimate what the probability of rolling each value is. To do that, we can just take each value \\(k\\) and sum up the number of times \\(x=k\\) and divide it by the total counts \\(N\\). This will return an array of probabilities, where each index \\(k\\) contains the entry \\(p_{k+1} = \\frac{N_k}{N}\\).\n\n\nCode\nsupport = np.unique(x)\nN = len(x)\nNk = [sum([x == k]) for k in support]\np = Nk / N\n[f\"Pr(x={i+1}) = {round(p[i], 3)}\" for i in range(len(p))]\n\n\n['Pr(x=1) = 0.056',\n 'Pr(x=2) = 0.056',\n 'Pr(x=3) = 0.111',\n 'Pr(x=4) = 0.25',\n 'Pr(x=5) = 0.306',\n 'Pr(x=6) = 0.222']\n\n\n\n\n11.2.2 General Case\nOf course, there’s nothing special about a die. We can define probabilities in exactly the same way for any discrete random variable. A random variable \\(x\\) is called discrete if it can take on one of \\(n\\) countable values \\(x_0,x_1,\\cdots,x_{n-1}\\). Suppose we run an experiment \\(n\\) times and observe the outcomes of \\(x\\) at each trial. If \\(x=x_k\\) for some number of counts \\(n_j\\), then the probability \\(x=x_k\\) is given by the limit of running the experiment infinitely many times,\n\\[p_k = \\mathbb{Pr}(x=k) = \\lim_{N \\rightarrow \\infty} \\frac{N_k}{N}.\\]\nThe set of values that \\(x\\) can take on are called the support of the random variable. For values outside the support, it’s assumed the probability is zero. As will always be true with probabilities, it’s still the case that each probability must be non-negative, and they must all sum to one,\n\\[p_k \\geq 0, \\quad \\sum_{k=0}^{n-1} p_k = 1.\\]\nWhile we have an experimental way to calculate probabilities now, it would be useful to define probabilities as functions of random variables so we can study them mathematically. These functions are called probability distributions. Suppose the probabilities \\(p_k\\) are given by some function \\(p(x)\\) mapping outcomes to probabilities. When this is true, we say \\(x\\) is distributed as \\(p(x)\\), written in short-hand as \\(x \\sim p(x)\\). If \\(x\\) is discrete, we call the function \\(p(x)\\) a probability mass function, or PMF for short.\nIn the simple case of the fair die, since each \\(p_k = \\frac{1}{6}\\), its PMF is just the simple constant function \\(p(x) = \\frac{1}{6}\\). This distribution is an example of the discrete uniform distribution. If \\(x\\) is a discrete random variable taking on one of \\(k\\) outcomes, and \\(x\\) is distributed as discrete uniform, then its probabilities are given by \\(p_k = \\frac{1}{n}\\) for all \\(k\\). In histogram language, all bins have approximately the same number of counts.\nIn the less simple case of the loaded die we had to estimate each probability empirically. Supposing we could calculate those probabilities exactly, the PMF for that particular loaded die would look like\n\\[\np(x) =\n\\begin{cases}\n0.056, & x = 1, \\\\\n0.056, & x = 2, \\\\\n0.111, & x = 3, \\\\\n0.250, & x = 4, \\\\\n0.306, & x = 5, \\\\\n0.220, & x = 6.\n\\end{cases}\n\\]\nThis is an example of a categorical distribution. Their histograms can look completely arbitrary. Each bin can contain as many counts as it likes. All that matters is that \\(k\\) is finite and all the probabilities sum to one. Any time you take a discrete uniform random variable and weigh the outcomes (e.g. by loading a die) you’ll create a categorical distribution.\nTypically each distribution will have one or more parameters \\(\\theta\\) that can be adjusted to change the shape or support of the distribution. Instead of writing \\(p(x)\\) for the PMF, when we want to be explicit about the parameters we’ll sometimes write \\(p(x; \\theta)\\). The semi-colon is used to say that any arguments listed after it are understood to be parameters, not function inputs. In this notation, parameters of a distribution are assumed to be known, non-random values. We’ll relax this requirement below, but assume parameters are non-random for now.\nFor example, the discrete uniform distribution has two parameters indicating the lowest and highest values in the support, called \\(a\\) and \\(b\\). We could thus express its PMF as \\(p(x;a,b)\\), which means “the probability of \\(x\\) given known parameters \\(a\\) and \\(b\\)”.\nUsing these parameters, it’s also common to use special symbols as a short-hand for common distributions. For example, the discrete uniform distribution with parameters \\(a\\) and \\(b\\) is often shortened to something like \\(DU(a,b)\\). If we want to say \\(x\\) is a discrete uniform random variable, we’d write \\(x \\sim DU(a,b)\\). You’ll also sometimes see people use the symbol to write the PMF as well, for example \\(DU(x;a,b)\\).\n\n\n11.2.3 Discrete Distributions\nSome discrete probability distributions occur so frequently that they get a special name. Each one tends to occur when modeling certain kinds of phenomena. Here are a few of the most common discrete distributions. I’ll just state them and summarize their properties for future reference.\n\n11.2.3.1 Discrete Uniform Distribution\n\nSymbol: \\(DU(a,b)\\)\nParameters: Integers \\(a, b\\), where \\(a\\) is the minimum and \\(b-1\\) is the maximum value in the support\nSupport: \\(x=a,a+1,\\cdots,b-1\\)\nProbability mass function: \\[p(x; a,b) = \\frac{1}{b-a}, \\ \\text{ for } x = a, a+1, \\cdots, b-1.\\]\nCumulative distribution function: \\[\nP(x; a,b) =\n\\begin{cases}\n0 & x < a, \\\\\n\\frac{\\text{int}(x) - a}{b-a}, & a \\leq x \\leq b, \\\\\n1 & x \\geq 1.\n\\end{cases}\n\\]\nRandom number generator: np.random.randint(a, b)\nNotes:\n\nUsed to model discrete processes that occur with equal weight, or are suspected to (the principle of indifference)\nExample: The fair die, taking \\(a=1, b=7\\) gives \\(x \\sim D(1,7)\\) with \\(p(x) = \\frac{1}{7-1} = \\frac{1}{6}\\)\n\n\n\n\nCode\na = 1\nb = 7\nx = np.random.randint(a, b, size=100000)\nplot_histogram(x, is_discrete=True, stat='probability', title=f'$DU({a},{b})$ PMF')\n\n\n\n\n\n\n\n11.2.3.2 Bernoulli Distribution\n\nSymbol: \\(\\text{Ber}(\\text{p})\\)\nParameters: The probability of success \\(0 \\leq \\text{p} \\leq 1\\)\nSupport: \\(x=0,1\\)\nProbability mass function: \\[\np(x; \\text{p}) = \\text{p}^x (1-\\text{p})^{1-x} =\n\\begin{cases}\n1-\\text{p} & x = 0, \\\\\n\\text{p} & x = 1.\n\\end{cases}\n\\]\nCumulative distribution function: \\[\nP(x; \\text{p}) =\n\\begin{cases}\n0 & \\text{if } x < 0 \\\\\n1-p & \\text{if } 0 \\leq x < 1 \\\\\n1 & \\text{if } x \\geq 1.\n\\end{cases}.\n\\]\nRandom number generator: np.random.choice([0, 1], p=[1 - p, p])\nNotes:\n\nUsed to model binary processes where the probability of success can be estimated\nExample: Flipping a fair coin, where \\(\\text{tails} = 0\\), \\(\\text{heads} = 1\\), and \\(\\text{p}=\\frac{1}{2}\\)\nUsed for binary classification. Given an input \\(\\mathbf{x}\\) with some binary output \\(y=0,1\\). If \\(\\text{p}=\\hat y\\), then \\(y \\sim \\text{Ber}(\\hat y)\\).\nSpecial case of the binomial distribution where \\(n=1\\): \\(\\text{Ber}(\\text{p}) = \\text{Bin}(1, \\text{p})\\).\n\n\n\n\nCode\np = 0.7\nx = np.random.choice([0, 1], p=[1 - p, p], size=1000)\nplot_histogram(x, is_discrete=True, stat='probability', title=f'$Ber({p})$ PMF')\n\n\n\n\n\n\n\n11.2.3.3 Categorical Distribution\n\nSymbol: \\(\\text{Cat}(p_0,p_1,\\cdots,p_{k-1})\\) or \\(\\text{Cat}(\\mathbf{p})\\)\nParameters: \\(k\\) non-negative real numbers \\(p_j\\) that sum to one, each representing the probability of getting \\(x_j\\)\n\nCommonly written as a vector \\(\\mathbf{p} = (p_0,p_1,\\cdots,p_{k-1})\\)\n\nSupport: \\(x = 0, 1, \\cdots, k-1\\)\nProbability mass function: \\[\np(x; \\mathbf{p}) = \\begin{cases}\np_0 & x = 0, \\\\\np_1 & x = 1, \\\\\n\\vdots & \\vdots \\\\\np_{k-1} & x = k-1.\n\\end{cases}\n\\]\nCumulative distribution function: \\[\nP(x; \\mathbf{p}) =\n\\begin{cases}\n0 & \\text{if } x \\leq x_0 \\\\\np_0 & \\text{if } x_0 \\leq x \\leq x_1 \\\\\np_0 + p_1 & \\text{if } x_1 \\leq x \\leq x_2 \\\\\np_0 + p_1 + p_2 & \\text{if } x_2 \\leq x \\leq x_3 \\\\\n\\vdots & \\vdots \\\\\n1 & \\text{if } x \\geq x_{n-1}.\n\\end{cases}\n\\]\nRandom number generator: np.random.choice(np.arange(k), p=p)\nNotes:\n\nUsed to model categorical processes where a finite number of classes can occur with arbitrary probabilities\nUsed for multiclass classification. Given an input \\(\\mathbf{x}\\) with outputs in one of \\(k\\) classes \\(y=0,1,\\cdots,k-1\\). If \\(\\mathbf{p}=\\mathbf{\\hat y}\\), then \\(\\mathbf{y} \\sim \\text{Cat}(\\mathbf{\\hat y})\\).\nGeneralization of the Bernoulli distribution, allowing for \\(k\\) distinct outcomes instead of just \\(2\\).\nModels the values rolled from a die when \\(k=6\\).\n\n\n\n\nCode\np = [0.2, 0.5, 0.3]\nx = np.random.choice(np.arange(len(p)), p=p, size=1000)\nplot_histogram(x, is_discrete=True, stat='probability', title=f'$Cat{tuple(p)}$ PMF')\n\n\n\n\n\n\n\n11.2.3.4 Binomial Distribution\n\nSymbol: \\(\\text{Bin}(n, \\text{p})\\)\nParameters: The number of trials \\(n=1,2,3,\\cdots\\) and probability \\(0 \\leq \\text{p} \\leq 1\\) of success of each trial\nSupport: \\(x = 0, 1, \\cdots, n\\)\nProbability mass function: \\[p(x; n,\\text{p}) = \\binom{n}{x} \\text{p}^{x} (1-\\text{p})^{n-x}, \\ \\text{for} \\ x=0,1,\\cdots,n, \\ \\text{where} \\ \\binom{n}{x} = \\frac{n!}{x!(n-x)!}.\\]\nCumulative distribution function: \\[P(x; n,\\text{p}) = \\sum_{k=0}^{\\text{int}(x)} {n \\choose k} p^k(1-p)^{n-k}.\\]\nRandom number generator: np.random.binomial(n, p)\nNotes:\n\nUsed to model the number of successes from \\(n\\) independent binary processes (analogous to coin flips)\nExample: Flipping a fair coin \\(n\\) times and counting the number of heads\nGeneralization of the Bernoulli distribution. The sum of \\(n\\) independent Bernoulli variables is \\(\\text{Bin}(n, \\text{p})\\).\nThe number of counts in each bin of a histogram of independent samples can be modeled as a binomial random variable\n\n\n\n\nCode\nn = 10\np = 0.7\nx = np.random.binomial(n, p, size=1000)\nplot_histogram(x, is_discrete=True, stat='probability', title=f'$Bin{(n,p)}$ PMF')\n\n\n\n\n\n\n\n11.2.3.5 Poisson Distribution\n\nSymbol: \\(\\text{Poisson}(\\lambda)\\)\nParameters: A rate parameter \\(\\lambda \\geq 0\\)\nSupport: \\(x = 0, 1, 2, 3, \\cdots\\)\nProbability mass function: \\[p(x; \\lambda) = \\frac{\\lambda^x e^{-\\lambda}}{x!}, \\quad \\text{for} \\ x=0,1,2,3,\\cdots.\\]\nCumulative distribution function: \\[P(x; \\lambda) = e^{-\\lambda}\\sum_{k=0}^{\\text{int}(x)}\\frac{\\lambda^k}{k!}.\\]\nRandom number generator: np.random.poisson(lambda)\nNotes:\n\nUsed to model counting processes, like the number of calls coming into a call center, or the number of times a Geiger counter registers a click\nExample: The number of people walking through the door of a coffee shop per hour can be modeled as a Poisson distribution\n\n\n\n\nCode\nlambda_ = 4\nx = np.random.poisson(lambda_, size=1000)\nplot_histogram(x, is_discrete=True, stat='probability', title=f'$Poisson({lambda_})$ PMF')\n\n\n\n\n\n\n\n\n11.2.4 Probabilities of Multiple Outcomes\nWe’ve seen how to calculate the probabilities of any one outcome. The probability that \\(x=k\\) is given by \\(\\text{Pr}(x=k) = p(k)\\), where \\(p(k)\\) is the PMF. It’s natural to then ask how we can think about probabilities of multiple outcomes. For example, consider again the situation of rolling a fair die. Suppose we were interested in knowing what the probability was of rolling an even number, i.e. \\(x=2,4,6\\). How would we approach this? Your intuition suggests the right idea. We can just sum the probabilities of each outcome together,\n\\[\\mathbb{Pr}(x\\text{ is even}) = \\mathbb{Pr}(x=2,4,6) = p(2) + p(4) + p(6) = \\frac{1}{6} + \\frac{1}{6} + \\frac{1}{6} = \\frac{1}{2}.\\]\nThis same idea extends to any discrete set. Suppose we’re interested in the probability that some discrete random variable \\(x\\) takes on values in some set \\(E = \\{x_0, x_1, \\cdots, x_{m-1}\\}\\). Then all we need to do is some over the probabilities of all the outcomes in \\(E\\), i.e.\n\\[\\mathbb{Pr}(x \\in E) = \\sum_{k \\in E} p(k) = \\sum_{i=0}^{m-1} p(x_i) = p(x_0) + p(x_1) + \\cdots p(x_{m-1}).\\]\nWhen the set of interest is the entire support of \\(x\\), the right-hand side is just the sum the probability of all possible outcome, which is just one. Thus, we’ll always have \\(0 \\leq \\mathbb{Pr}(x \\in E) \\leq 1\\) for any set \\(E\\).\nThough we don’t really have to for discrete variables, it’s conventional to define another function \\(P(x)\\) called the cumulative distribution function, or CDF. It’s the probability \\(x \\in (-\\infty, x_0]\\) for some fixed value \\(x_0 \\in \\mathbb{R}\\),\n\\[P(x_0) = \\mathbb{Pr}(x \\leq x_0) = \\sum_{k \\leq x_0} p(k) = \\sum_{k=-\\infty}^{\\text{int}(x_0)} p(k),\\]\nwhere it’s understood that \\(p(k)=0\\) whenever \\(k\\) isn’t in the support of \\(x\\). Note the CDF is a real-valued function. We can ask about \\(P(x_0)\\) for any \\(x_0 \\in \\mathbb{R}\\), not just discrete values of \\(x_0\\).\nBut why should we care? It turns out if we know the CDF in some simple form, we can use it to calculate the probability \\(x\\) is in any other interval by differencing the CDF at the endpoints. Suppose we’re interested in the probability \\(a \\leq x \\leq b\\). If we know the CDF for a particular distribution in some simple form, we can just difference it to get the probability of being in the interval, i.e.\n\\[\\mathbb{Pr}(a \\leq x \\leq b) = \\mathbb{Pr}(x \\leq b) - \\mathbb{Pr}(x \\leq a) = P(b) - P(a).\\]\nThis fact is more useful for continuous distributions than discrete ones, since in the discrete case we can always just sum over the values, which is usually pretty quick to do.\n\n11.2.4.1 Application: Getting a Job\nHere’s a useful application where probabilities of multiple outcomes can sometimes come in handy. Suppose you’re applying to a bunch of jobs, and you want to know what is the probability that you’ll get at least one offer. Suppose you’ve applied to \\(n\\) jobs. For simplicity, assume each job has roughly the same probability \\(\\text{p}\\) of giving you an offer. Then each job application looks kind of like the situation of flipping a coin. If \\(x_i=1\\) you get an offer, if \\(x_i=0\\) you get rejected. We can thus think of each job application as a Bernoulli random variable \\(x_i \\sim \\text{Ber}(\\text{p})\\).\nNow, assume that the job applications are all independent of each other, so one company’s decision whether to give you an offer doesn’t affect another company’s decision to give you an offer. This isn’t perfectly true, but it’s reasonably true. In this scenario, the total number of offers \\(x\\) you get out of \\(n\\) job applications will then be binomially distributed, \\(x \\sim \\text{Bin}(n, \\text{p})\\).\nWe can use this fact to answer the question we started out with: What is the probability that you receive at least one offer? It’s equivalent to asking, if \\(x\\) is binomial, what is the probability that \\(x \\geq 1\\)? Now, since \\(x\\) is only supported on non-negative values, we have\n\\[\\begin{align*}\n\\mathbb{Pr}(x \\geq 1) &= \\mathbb{Pr}(x \\geq 0) - \\mathbb{Pr}(x=0) \\\\\n&= 1 - \\mathbb{Pr}(x=0) \\\\\n&= 1 - p(0;n,\\text{p}) \\\\\n&= 1 - \\binom{n}{0} \\text{p}^0 (1-\\text{p})^{n-0} \\\\\n&= 1 - \\frac{n!}{0!(n-0)!} (1-\\text{p})^n \\\\\n&= 1 - (1-\\text{p})^n.\n\\end{align*}\\]\nWe thus have a formula. The probability of receiving at least one job offer from applying to \\(n\\) jobs, assuming each gives an offer with probability \\(\\text{p}\\), and applications are independent of each other, is\n\\[\\mathbb{Pr}(\\text{at least one offer}) = 1 - (1-\\text{p})^n.\\]\nHere’s an example of how this formula can be useful. Suppose you believe you have a 10% chance of getting an offer from any one company you apply to, so \\(\\text{p}=0.1\\). If you apply to \\(n=10\\) jobs, you’ll have about a 34.86% chance of receiving at least one offer.\n\n\nCode\np = 0.1\nn = 10\nprob_offer = 1 - (1 - p) ** n\nprob_offer\n\n\n0.6513215599\n\n\nLet’s now ask how many jobs you’d have to apply to to give yourself at least a 90% chance of getting at least one job offer? Here’s what you can do. Let \\(O = \\mathbb{Pr}(\\text{at least one offer})\\), so \\(O = (1-p)^n\\). Set \\(O=0.9\\) and solve for \\(n\\). Then you’d have\n\\[\\begin{align*}\nO &= 1 - (1-p)^n \\\\\n(1-p)^n &= 1 - O \\\\\nn \\log(1-p) &= \\log(1 - O) \\\\\nn &= \\frac{\\log(1 - O)}{\\log(1 - p)}.\n\\end{align*}\\]\nPlugging in \\(p=0.1\\) and \\(O=0.9\\) gives \\(n \\approx 21.85\\). Thus, you’d need to apply to at least \\(n=22\\) jobs to have a decent chance of getting at least one offer. Here’s a plot of this idea. Each curve is a plot of \\(n=n(p)\\) for different choices of \\(O\\), in this case, 50%, 75%, 90%, and 99%.\n\n\nCode\np = np.linspace(0.01, 0.999, 100)\nO = [0.5, 0.75, 0.9, 0.99]\nfor o in O:\n    n = np.log(1 - o) / np.log(1 - p)\n    plt.plot(p, n, label=f'$O={round(o*100)}$%')\nplt.xticks(0.1 * np.arange(11))\nplt.ylim(0, 70)\nplt.title(\n    \"\"\"How many jobs would you have to \n    apply to to get at least one job offer\n    with confidence $O$?\"\"\".title(), fontsize=11)\nplt.xlabel('$p$')\nplt.ylabel('$n$')\nplt.grid(True, alpha=0.5)\nplt.legend()\nplt.show()\n\n\n\n\n\nThe moral of this story is that you have two ways to up your chances of getting a job offer: Up your chances of getting any one job (i.e. increase \\(p\\)), or apply to a lot more jobs (i.e. increase \\(n\\)). The more confident you want to be of getting an offer (i.e. \\(O\\)), the more jobs you’ll need to apply to. This same idea can be used to model the probability of at least one occurrence for any binary event similar to this."
  },
  {
    "objectID": "notebooks/probability.html#continuous-probability",
    "href": "notebooks/probability.html#continuous-probability",
    "title": "11  Basic Probability",
    "section": "11.3 Continuous Probability",
    "text": "11.3 Continuous Probability\nSo far we’ve covered discrete random variables, ones that take on a finite (or countably infinite) set of values. We can also consider random variables that take on a continuous range of values. For example, a continuous random variable \\(x\\) can take on values in the entire interval \\([0,1]\\), or the whole real line \\(\\mathbb{R} = (-\\infty, \\infty)\\). The key difference between continuous variables and discrete variables is that we have to think in terms of calculus now. Instead of points we’ll have infinitesimal areas. Instead of sums we’ll have integrals.\nIt may not be obvious to you that there are practical examples where continuous random variables would be useful. Here are some examples:\n\nModeling the behavior of random number generators like rand.\nModeling the total sales a business will do next quarter.\nModeling the time it takes for a customer to complete a purchase in an online store.\nModeling the amount of fuel consumed by a vehicle on a given day.\nModeling the height of waves in the ocean at a given time.\nModeling the length of a stay in a hospital by a typical patient.\nModeling the amount of rainfall in a specific region over a period of time.\nModeling the measured voltage of a car battery at any point in time.\n\nIn fact, any continuous variable you can think of could be treated as random depending on the situation. Even if a variable is completely deterministic, there may be situations where it’s helpful to think of it as random. The whole idea of Monte Carlo methods is based on this idea, in fact.\n\n11.3.1 Motivation: Rand Again\nI showed example of a continuous random variable already at the beginning of this lesson, when I introduced the idea of random number generators like rand. Rand is an example of a function that can (approximately) generate samples of a continuous random variable. In particular, it samples uniformly from the interval \\([0,1]\\). I already showed what its histogram looks like for a large number of samples. Here it is again.\n\n\nCode\nx = np.random.rand(10000)\nplot_histogram(x, bins=10, title=f'rand({10000})')\n\n\n\n\n\nLet’s now try to figure out how we should define the probability of values sampled from rand. In the discrete case, we were able to define probabilities by running an experiment (e.g. rolling a die a bunch of times). We could look at the ratio of the number of times \\(N_k\\) an outcome \\(k\\) occurred over the number of total trials \\(N\\). This made sense in the discrete case since we could reasonably well rely on each outcome \\(x=k\\) occurring enough times to get a meaningful count.\nThis approach doesn’t work well for continuous random variables. Suppose \\(x\\) is the random variable resulting from rand, uniform on the interval \\([0,1]\\). If I sample a single value from rand, there’s no reason to assume I’ll ever see that exact value again. There are uncountably infinitely many values to choose from in \\([0,1]\\), so I’m pretty much guaranteed to never see the same value twice. Instead of counting how many times each value occurs, what I can do is use the binning trick we saw with histograms. For example, I can divide \\([0,1]\\) up into ten subintervals (or bins)\n\\[I_0=[0,0.1], \\quad I_1=[0.1,0.2], \\quad I_3=[0.2,0.3], \\quad \\cdots, \\quad I_9=[0.9,1].\\]\nIf I sample one value from rand it’s guaranteed to be in one of these subintervals \\(I_k\\). If I sample a whole bunch of values from rand, say \\(N=1000\\), I should expect each \\(I_k\\) to contain about \\(N_k=100\\) counts (10% of the total since there are 10 bins). It thus seems to make perfect sense to define a probability on each \\(I_k\\),\n\\[\\mathbb{Pr}(x \\in I_k) = \\frac{N_k}{N} = \\frac{100}{1000} = \\frac{1}{10} = 0.1.\\]\n\n\nCode\nN = 10000\nM = 10\ndx = 1 / M\nx = np.random.rand(N)\nplot_histogram(x, bins=M, title=f'M=${M}$ subintervals of length $dx={dx}$')\n\n\n\n\n\nWe still want to approximate the discrete idea of having a probability \\(\\mathbb{Pr}(x=k)\\). How can we do it using this idea of subintervals? Enter calculus. What we can imagine doing is allowing each subinterval \\(I_k\\) to become infinitesimally small. Suppose we subdivide \\([0,1]\\) into \\(M\\) total subintervals each of infinitesimal length \\(dx\\), satisfying \\(M=\\frac{1}{dx}\\), i.e.\n\\[I_0=[0,dx], \\quad I_1=[dx, 2dx], \\quad I_2=[2dx, 3dx], \\quad \\cdots, \\quad I_{M-1}=[(M-1)dx, 1].\\]\nSuppose \\(x_0\\) is some point in one of these tiny intervals \\(I_k=[kdx, (k+1)dx]\\). Since each \\(I_k\\) is a very tiny interval, the probability that \\(x \\approx x_0\\) is pretty much exactly the same thing as the probability that \\(x \\in I_k\\). Let’s thus define the probability that \\(x \\approx x_0\\) as the probability that \\(x \\in I_k\\),\n\\[\\mathbb{Pr}(x \\approx x_0) = \\mathbb{Pr}(x \\in I_k) = \\lim_{N \\rightarrow \\infty} \\frac{N_k}{N}.\\]\nHere’s an approximate representation of this idea. I won’t be able to make \\(M=10^{300}\\) bins like I’d like, but I can at least make bins so you can see the point. I’ll need to generate a huge number of samples \\(N\\) so the histogram will populate. Notice each \\(N_k \\approx \\frac{N}{M} = 1000\\). That is,\n\\[\\mathbb{Pr}(x \\approx x_0) \\approx \\frac{N_k}{N} \\approx \\frac{N/M}{N} = \\frac{1}{M} = dx.\\]\nEvidently, the probability \\(x \\approx x_0\\) is infinitesimal, so very very tiny. This is why you’ll basically never sample the same value twice.\n\n\nCode\nN = 1000000\nM = N // 1000\ndx = 1 / M\nx = np.random.rand(N)\nplot_histogram(x, bins=M, title=f'M=${M}$ subintervals of length $dx={dx}$')\n\n\n\n\n\n\n\n11.3.2 General Case\nThe facts I’ve shown about rand extend to more general continuous random variables as well. Suppose \\(x\\) is supported on some interval \\([a,b]\\). It could even be infinite. Let’s divide this interval up into \\(M\\) tiny sub-intervals of length \\(dx\\), where \\(M\\) must satisfy \\(M = \\frac{b-a}{dx}\\),\n\\[I_0=[a,a+dx], \\quad I_1=[a+dx, a+2dx], \\quad I_2=[a+2dx, a+3dx], \\quad \\cdots, \\quad I_{M-1}=[a+(M-1)dx, b].\\]\nNow, run an experiment \\(N\\) times and count how many times outcomes occur, not for each \\(x\\), but for each subinterval \\(I_k=[a+kdx, a+(k+1)dx]\\). If \\(x_0 \\in I_k\\), that is, if \\(a+kdx \\leq x_0 \\leq a+(k+1)dx\\), then the probability that \\(x \\approx x_0\\) is defined by,\n\\[\\mathbb{Pr}(x \\approx x_0) = \\mathbb{Pr}(x \\in I_k) = \\lim_{N \\rightarrow \\infty} \\frac{N_k}{N}.\\]\nJust as with the uniform case before, it’s useful to think of the probability \\(\\mathbb{Pr}(x \\approx x_0)\\) as explicitly being proportional to the subinterval length \\(dx\\). In the uniform case it was just \\(\\mathbb{Pr}(x \\approx x_0)=dx\\) exactly. In the more general case, \\(\\mathbb{Pr}(x \\approx x_0)\\) may depend on the value of \\(x_0\\), so we need to weight the right-hand side by some non-negative weighting function \\(p(x) \\geq 0\\), so\n\\[\\mathbb{Pr}(x \\approx  x_0) = \\mathbb{Pr}(x \\in I_k) = p(x_0)dx.\\]\nThis weighting function \\(p(x)\\) is called the probability density function, or PDF for short. It’s the continuous analogue of the probability mass function from the discrete case (hence why I use the same notation). Unlike the discrete PMF, the PDF is not a probability all by itself. It’s a probability per infinitesimal unit \\(dx\\). That is, it’s a density. For this reason, the PDF need not sum to one. It only needs to be non-negative, i.e. all outputs \\(p(x_0)\\) should lie on or above the x-axis, never below it. But any one output \\(p(x_0)\\) can be arbitrarily large, even \\(\\infty\\)!\nWhat must be true is that all probabilities sum to one. Since each \\(\\mathbb{Pr}(x \\approx x_0)\\) is infinitesimal now, this means all probablities must integrate to one over the support of \\(x\\). If \\(x\\) is supported on \\([a,b]\\), then\n\\[\\mathbb{Pr}(a \\leq x \\leq b) = \\sum_{k=0}^{M-1} \\mathbb{Pr}(x \\in I_k) = \\int_a^b p(x)dx = 1.\\]\nThis means we can think of a PDF as being any non-negative function that integrates to one. In fact, any function that satisfies this property is a valid PDF for some continuous random variable.\nSpecifying the functional form of the PDF \\(p(x)\\) creates a continuous probability distribution. By specifying \\(p(x)\\), we’ve uniquely specified what the probabilities have to be for the variable \\(x\\). In the next section I’ll define some of the most common continuous distributions.\nJust as with discrete probabilities, we can get the probability that \\(x\\) is in any set by summing over all the values in that set. The only difference is we replace the sum with an integral over the set. For example, the probability that \\(c \\leq x \\leq d\\) is given by\n\\[\\mathbb{Pr}(c \\leq x \\leq d) = \\int_c^d p(x)dx.\\]\nWe can also define a cumulative distribution function \\(P(x)\\) for continuous probabilities in exactly the same way, except again replacing sums with integrals,\n\\[P(x_0) = \\mathbb{Pr}(x \\leq x_0) = \\int_{-\\infty}^{x_0} p(x')dx',\\]\nwhere it’s understood that \\(p(x')=0\\) whenever \\(x'\\) is outside the support of \\(x\\).\nIf we can obtain the CDF for a distribution, we can calculate the probability \\(x\\) is in any set without having to evaluate an integral. For example, if the set is again the interval \\([c,d]\\), then\n\\[\\mathbb{Pr}(c \\leq x \\leq d) = P(d) - P(a).\\]\nThis is just a restatement of the rule for definite integrals from the calculus lesson, if \\(f(x)=\\frac{d}{dx}F(x)\\), then\n\\[\\int_c^d f(x) dx = F(d) - F(c).\\]\nTo show a brief example, I’ll calculate the CDF of the rand distribution shown already, where \\(x\\) is uniform on \\([0,1]\\). I already showed that its PDF is just \\(p(x)=1\\) for all \\(0 \\leq x \\leq 1\\). Outside this interval \\(p(x)=0\\) everywhere. Using the PDF I can calculate the CDF by integrating. There are three cases to consider. If \\(x < 0\\), the CDF will just be \\(P(x)=0\\) since \\(p(x)=0\\). If \\(x > 1\\), \\(P(x) = 1\\) since we’re integrating over the whole support \\([0,1]\\). Otherwise, we’re integrating over some subinterval \\([0,x]\\), in which case \\(P(x)=x\\). That is,\n\\[\nP(x) = \\int_{-\\infty}^x p(x') dx' =\n\\begin{cases}\n0, & x < 0 \\\\\nx, & 0 \\leq x \\leq 1 \\\\\n1, & x > 1.\n\\end{cases}\\]\nHere’s a plot of both the PDF and CDF of rand. Notice the PDF is just the constant \\(p(x)=1\\) on \\([0,1]\\), whose area under the curve is just one, since the total probability must integrate to one. Also, notice how this same area is the exact same thing that the histogram tries to approximate. In fact, a histogram is just a discrete approximation to the area under a continuous PDF.\nFor the CDF, notice how the function starts at \\(P(x)=0\\) on the far left, and ramps up monotonically to \\(P(x)=1\\) as \\(x\\) increases. Every CDF will have this property. The only difference is what the ramp looks like. It’ll always be the case that \\(P(-\\infty)=0\\), \\(P(\\infty)=1\\), and some monotonic increasing curve connects these two extremes.\n\n\nCode\nx = np.linspace(0, 1, 100)\np = lambda x: np.ones(len(x))\nplot_function(x, p, xlim=(-0.5, 1.5), ylim=(-0.5, 1.5), set_ticks=True, title='Rand PDF')\n\n\n\n\n\n\n\nCode\nx = np.linspace(-1, 2, 100)\nP = lambda x: np.clip(x, 0, 1) ## quick way to define the piecewise CDF shown above\nplot_function(x, P, xlim=(-1, 2), ylim=(-0.5, 1.5), title='Rand CDF')\n\n\n\n\n\n\n\n11.3.3 Continuous Distributions\nAs with discrete distributions, some continuous distributions occur so frequently that they get a special name. Here are a few of the most common continuous distributions. I’ll just state them and summarize their properties for future reference.\n\n11.3.3.1 Uniform Distribution\n\nSymbol: \\(U(a,b)\\)\nParameters: The minimum \\(a\\) and maximum \\(b\\) values in the support\nSupport: \\(x \\in [a,b]\\)\nProbability density function: \\[p(x; a,b) = \\frac{1}{b-a}, \\ \\text{ for } a \\leq x \\leq b.\\]\nCumulative distribution function: \\[\nP(x; a, b) =\n\\begin{cases}\n0 & x < a, \\\\\n\\frac{x - a}{b-a}, & a \\leq x \\leq b, \\\\\n1 & x \\geq 1.\n\\end{cases}\n\\]\nRandom number generator: np.random.uniform(a, b)\nNotes:\n\nUsed to model continuous processes that occur with equal weight, or are suspected to (the principle of indifference)\nExample: The values sampled from rand, where \\(a=0\\) and \\(b=1\\), so \\(x \\sim U(0,1)\\).\nThe rand example \\(U(0,1)\\) is called the standard uniform distribution.\n\n\n\n\nCode\na, b = -2, 5\nx = np.linspace(a, b, 1000)\np = lambda x: 1 / (b - a) * np.ones(len(x))\nplot_function(x, p, xlim=(a - 0.5, b + 0.5), ylim=(-0.5 / (b - a), 1.5 / (b - a)), set_ticks=True,\n              title=f'$U({a},{b})$ PDF')\n\n\n\n\n\n\n\n11.3.3.2 Gaussian Distribution (Normal Distribution)\n\nSymbol: \\(\\mathcal{N}(\\mu, \\sigma^2)\\)\nParameters: The mean \\(\\mu \\in \\mathbb{R}\\) and variance \\(\\sigma^2 \\geq 0\\) of the distribution\nSupport: \\(x \\in \\mathbb{R}\\)\nProbability density function: \\[p(x; \\mu , \\sigma^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp{\\bigg(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\bigg)}.\\]\nCumulative distribution function: \\[P(x; \\mu , \\sigma^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\int_{-\\infty}^x \\exp{\\bigg(-\\frac{(x' - \\mu)^2}{2\\sigma^2}\\bigg)} dx'.\\]\nRandom number generator: np.random.normal(mu, sigma) (note it sigma is the square root of the variance \\(\\sigma^2\\))\nNotes:\n\nUsed to model the sum or mean of many continuous random variables, e.g. the distribution of unbiased measurements of some continuous quantity\nExample: The distribution of heights in a given population of people.\nUsed in machine learning to model the outputs of an L2 regression model. Given a input \\(\\mathbf{x}\\) with a continuous output \\(y\\), model \\(y = f(\\mathbf{x}) + \\varepsilon\\), where \\(\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)\\) is some random error term and \\(f(\\mathbf{x})\\) is some deterministic function to be learned. Then \\(y \\sim \\mathcal{N}(f(\\mathbf{x}),\\sigma^2)\\).\nThe special case when \\(\\mu=0, \\sigma^2=1\\) is called the standard Gaussian distribution, written \\(\\mathcal{N}(0,1)\\). Values sampled from a standard Gaussian are commonly denoted by \\(z\\). By convention, its PDF is denoted by \\(\\phi(z)\\) and its CDF by \\(\\Phi(z)\\).\nCan turn any Gaussian random variable \\(x\\) into a standard Gaussian or vice versa via the transformations \\[z = \\frac{x-\\mu}{\\sigma}, \\qquad x = \\sigma z + \\mu.\\]\nThe CDF of a Gaussian can’t be written in closed form since the Gaussian integral can’t be written in terms of elementary functions. Since the standard Gaussian CDF \\(\\Phi(z)\\) has a library implementation it’s most common to transform other Gaussian CDFs into standard form and then calculate that way. Use the function norm.cdf from scipy.stats to get the standard CDF function \\(\\Phi(z)\\).\n\n\n\n\nCode\nx = np.linspace(-10, 10, 1000)\np_gaussian = lambda x: 1 / np.sqrt(2 * np.pi) * np.exp(-1/2 * x**2)\nplot_function(x, p_gaussian, xlim=(-3, 3), ylim=(0, 0.5), set_ticks=False,\n              title=f'Standard Gaussian PDF')\n\n\n\n\n\n\n\nCode\nfrom scipy.stats import norm\n\nx = np.linspace(-3, 3, num=100)\nPhi = lambda x: norm.cdf(x)\nplot_function(x, Phi, xlim=(-3, 3), ylim=(0, 1), set_ticks=False, title='Standard Gaussian CDF')\n\n\n\n\n\n\n\n11.3.3.3 Laplace Distribution\n\nSymbol: \\(\\text{Laplace}(\\mu, s)\\)\nParameters: The mean \\(\\mu \\in \\mathbb{R}\\) and scale \\(s \\geq 0\\) of the distribution\nSupport: \\(x \\in \\mathbb{R}\\)\nProbability density function: \\[p(x; \\mu , s) = \\frac{1}{2s} \\exp\\bigg(-\\frac{|x-\\mu|}{s}\\bigg).\\]\nCumulative distribution function: \\[\nP(x; \\mu , s) =\n\\begin{cases}\n\\frac{1}{2} \\exp\\bigg(-\\frac{|x-\\mu|}{s}\\bigg), & x \\leq \\mu \\\\\n1 - \\frac{1}{2} \\exp\\bigg(-\\frac{|x-\\mu|}{s}\\bigg), & x > \\mu.\n\\end{cases}\n\\]\nRandom number generator: np.random.laplace(mu, s)\nNotes:\n\nUsed to model Gaussian-like situations where extreme values are somewhat more likely to occur than in a Gaussian. These are called outliers.\nExample: The distribution of finanical stock returns, where extreme returns are more likely than expected under a Gaussian distribution.\nUsed in machine learning to model the outputs of an L1 regression model. Given an input \\(\\mathbf{x}\\) with a continuous output \\(y\\), model \\(y = f(\\mathbf{x}) + \\varepsilon\\), where \\(\\varepsilon \\sim \\text{Laplace}(0, s)\\) is some random error term (that can be extreme-valued) and \\(f(\\mathbf{x})\\) is some deterministic function to be learned. Then the outputs are also Laplace distributed, with \\(y \\sim \\text{Laplace}(f(\\mathbf{x}), s)\\).\nThe special case when \\(\\mu=0, s=1\\) is called the standard Laplace distribution, written \\(\\text{Laplace}(0, 1)\\).\n\n\n\n\nCode\nx = np.linspace(-10, 10, 1000)\np_laplace = lambda x: 1 / (2 * np.pi) * np.exp(-np.abs(x))\nps = [p_gaussian, p_laplace]\nplot_function(x, ps, xlim=(-4, 4), ylim=(0, 0.5), set_ticks=False, labels=['Gaussian', 'Laplace'],\n             title='Gaussian vs Laplace PDFs')\n\n\n\n\n\n\n\n\n11.3.4 Cauchy Distribution\n\nSymbol: \\(\\text{Cauchy}(m, s)\\)\nParameters: The median \\(m \\in \\mathbb{R}\\) and scale \\(s > 0\\) of the distribution.\nSupport: \\(x \\in \\mathbb{R}\\)\nProbability density function: \\[p(x; m, s) = \\frac{1}{\\pi s} \\frac{1}{1 + \\big(\\frac{x-m}{s}\\big)^2}.\\]\nCumulative distribution function: \\[P(x; m, s) = \\frac{1}{\\pi} \\arctan \\bigg(\\frac{x-m}{s}\\bigg) + \\frac{1}{2}.\\]\nRandom number generator: s * np.random.standard_cauchy() + m\nNotes:\n\nUsed to model Gaussian-like situations where extreme values are highly likely to occur frequently.\nSuch a distribution is said to exhibit heavy-tailed behavior, since there’s a “heavy” amount of probability in the tails of the distribution, making extreme values likely to occur.\nExample: The distribution of computer program runtimes often exhibits heavy-tailed behavior.\nThe case when \\(m=0\\) and \\(s=1\\) is called the standard Cauchy distribution, denoted \\(\\text{Cauchy}(0, 1)\\).\nTechnically speaking, the mean of the Cauchy distribution doesn’t exist, so you have to use the median instead.\n\n\n\n\nCode\nx = np.linspace(-10, 10, 1000)\np_cauchy = lambda x: 1 / np.pi * 1 / (1 + x ** 2)\nps = [p_gaussian, p_laplace, p_cauchy]\nplot_function(x, ps, xlim=(-10, 10), ylim=(0, 0.5), set_ticks=False, labels=['Gaussian', 'Laplace', 'Cauchy'],\n             title='Gaussian vs Laplace vs Caucy PDFs')\n\n\n\n\n\n\n11.3.4.1 Exponential Distribution\n\nSymbol: \\(\\text{Exp}(\\lambda)\\)\nParameters: A rate parameter \\(\\lambda > 0\\)\nSupport: \\(x \\in [0,\\infty)\\)\nProbability density function: \\[p(x; \\lambda) = \\lambda e^{-\\lambda x}.\\]\nCumulative distribution function: \\[P(x; \\lambda) = 1 - e^{-\\lambda x}.\\]\nRandom number generator: np.random.exponential(lambda)\nNotes:\n\nUsed to model the time between two independent discrete events, assuming those events occur at a roughly constant rate.\nExample: The time between earthquakes in a given region, assuming earthquakes are rare and independent events.\nFrequently used to model the time between two Poisson distributed events. If the events are Poisson distributed and independent, then the time between any two events will be exponentially distributed.\n\n\n\n\nCode\nlambda_ = 1\nx = np.linspace(0, 20, 100)\np = lambda x: lambda_ * np.exp(-lambda_ * x)\nplot_function(x, p, xlim=(0, 20), ylim=(0, 1), set_ticks=False, title=f'$Exp({lambda_})$ PDF')"
  },
  {
    "objectID": "notebooks/multivariate-probability.html#multivariate-distributions",
    "href": "notebooks/multivariate-probability.html#multivariate-distributions",
    "title": "12  Multivariate Distributions",
    "section": "12.1 Multivariate Distributions",
    "text": "12.1 Multivariate Distributions\n\n12.1.1 Two Random Variables\nIn the last lesson we saw the problem of rolling a single die. Suppose now we roll two dice. What is the probability that any pair of dots will turn up? One way to think about this problem is to imagine two random variables \\(x\\) and \\(y\\), each representing the roll of each die. Now, let’s do just like we did with the single die. Assume you have no other information to go on. What would you guess the probability of rolling any \\((x,y)\\) pair of values should be?\nIf you again employ the principle of indifference, you’d argue as follows: Since \\(x\\) can take on 6 values, and \\(y\\) can also take on 6 values, then the pair \\((x,y)\\) together can take on \\(6 \\cdot 6 = 36\\) possible pairs of values,\n\\[(x,y) = (1,1), (1,2), \\cdots, (1,6), \\cdots, (6,1), (6,2), \\cdots, (6,6).\\]\nThus, each pair of rolls should take an equal probability of \\(p_{x,y} = \\frac{1}{36}\\). Let’s try to unpack the assumptions here. For one thing, we’re assuming each die is fair. That is, the two dice rolled individually should each have equal probabilities \\(p_x = \\frac{1}{6}\\) and \\(p_y = \\frac{1}{6}\\). But, observe that for each pair of values \\(x\\) and \\(y\\) we have\n\\[p_{x,y} = \\frac{1}{36} = \\frac{1}{6} \\cdot \\frac{1}{6} = p_x \\cdot p_y.\\]\nThis is a stronger assumption than assuming the dice are fair. We’re also assuming that they don’t affect each other at all. Rolling \\(x\\) says nothing about what we’d get from rolling \\(y\\), and vice versa. If this fact weren’t true, then we couldn’t get a uniform value for all pairs. Some pair would have a different probability.\nIn probability, this pair of random variables \\((x,y)\\) is called a joint random variable, or a random vector. The probability a random vector takes on any two particular particular values, say \\(x=i\\) and \\(y=j\\), is called the joint probability of \\(x\\) and \\(y\\), written \\(\\mathbb{Pr}(x=i, y=j)\\). The probabilities of each variable acting alone, i.e. \\(\\mathbb{Pr}(x=i)\\) and \\(\\mathbb{Pr}(y=j)\\), are called the marginal probabilities. Instead of saying \\(x\\) and \\(y\\) don’t affect each other, we say the two random variables \\(x\\) and \\(y\\) are statistically independent. Evidently, if they are independent their joint probability factors into a product of marginal probabilities,\n\\[\\mathbb{Pr}\\big(x=i, y=j\\big) = \\mathbb{Pr}\\big(x=i\\big) \\cdot \\mathbb{Pr}\\big(y=j\\big).\\]\nHere’s an experiment mimicking the rolling of two fair, independent dice. What I’ll do is simulate rolling each die 10,000 times by sampling from their “fair die” distributions \\(x \\sim DU(1,7)\\) and \\(y \\sim DU(1,7)\\) each separately 10,000 times. I’ll then use the helper function plot_joint_histogram to do the plotting. You can see from the histogram that things do look more or less uniform if we sample each die fairly and independently. Note since each bin should have an expected count of \\(\\frac{10000}{36} \\approx 278\\), each bin’s count can fluctuate within a range of about \\(278 \\pm 17\\) and still be uniform.\n\n\nCode\nx = np.random.randint(1, 7, size=10000)\ny_indep = np.random.randint(1, 7, size=10000)\n\nplot_joint_histogram([x, y_indep], title='Two Independent Fair Dice', figsize=(8,4))\n\n\n\n\n\nJust as in the univariate case, we can model probabilities of two variables as a ratio of counts per number of trials. If we run \\(N\\) trials (in the above example \\(N=10000\\)), and count the number of times \\(N_{i,j}\\) each pair \\(i,j\\) occurs, then the joint probability \\(\\mathbb{Pr}(x=i,y=j)\\) is given by the ratio of counts per number of trials as \\(N \\rightarrow \\infty\\),\n\\[\\mathbb{Pr}\\big(x=i,y=j\\big) = \\lim_{N \\rightarrow \\infty} \\frac{N_{i,j}}{N}.\\]\nWe can then define a joint distribution by specifying a PMF of two variables \\(p(x,y)\\). If \\(x\\) and \\(y\\) are independent the joint distribution will factor,\n\\[p(x,y) = p(x) \\cdot p(y).\\]\nIn this example of rolling independent fair dice, since \\(x, y \\sim D(1, 7)\\) and \\(x\\) and \\(y\\) are independent, the joint distribution must be of the form\n\\[(x,y) \\sim DU(1,7) \\cdot DU(1,7).\\]\nThis called a product distribution since it’s just the product of two univariate distributions, in this case each \\(D(1,7)\\).\nWhat if the dice are fair but not independent? Then while each individual roll will be uniform, the pair of dice together won’t be. Here’s an example. Suppose that the dice were somehow rigged to where die \\(x\\) could “communicate” information to die \\(y\\). Every time \\(x\\) rolls, it will tell \\(y\\) to roll one number higher (mod 6).\n\n\n\nx\n1\n2\n3\n4\n5\n6\n\n\n\n\ny\n2\n3\n4\n5\n6\n1\n\n\n\nLet’s run a simulation here by again rolling 10,000 times and plotting the histograms, first the marginal histograms, then the joint histogram. To create the given relationship in the table, we can use the formula \\(y = x \\text{ mod } 6 + 1\\), i.e. y = x % 6 + 1.\n\n\nCode\ny_dep = x % 6 + 1\n\nplot_histogram(x, is_discrete=True, title='$x$')\nplot_histogram(y_dep, is_discrete=True, title='$y$')\nplot_joint_histogram([x, y_dep], title='$(x,y)$')\n\n\n\n\n\n\n\n\n\n\n\nFirst, plotting the histograms of \\(x\\) and \\(y\\) individually shows each one still looks quite uniform, hence fair. But when plotted together, we see only 6 of the 36 possible pairs ever show up! Why? Because \\(y\\) depends on \\(x\\). Knowing \\(x\\) gives 100% information to reproduce the values of \\(y\\) (and vice versa). Because of this functional dependence, only 6 unique pairs (the size of \\(x\\)) are possible. In this case, we say that \\(x\\) and \\(y\\) are perfectly correlated.\nWe can see this more starkly doing a 2D histogram plot, which will plot the counts as a heat map with \\(x\\) on the x-axis and \\(y\\) on the y-axis. The cells that are dark blue indicate a lot of counts occurring there, while the white cells indicate few to no counts occurring. You can see only 6 of the 36 cells are being darkened.\n\n\nCode\nplot_hist2d(x, y_indep, title='Two Independent Fair Dice', bins=(6, 6))\n\n\n\n\n\n\n\nCode\nplot_hist2d(x, y_dep, title='Two Perfectly Correlated Fair Dice', bins=(6, 6))\n\n\n\n\n\n\n\n12.1.2 Conditional Probability\nTo understand correlation and independence better, it’s interesting to look at the ratio of the joint PMF \\(p(x,y)\\) with the marginal PMF \\(p(x)\\). This tells us in a sense how much information is contained in the joint distribution \\(p(x,y)\\) if we already knew what information was in the marginal distribution \\(p(x)\\).\nIn the case when both die were independent, \\(p(x,y)=\\frac{1}{36}\\) and \\(p(x)=\\frac{1}{6}\\) for all \\(x\\) and \\(y\\), so\n\\[\\frac{p(x,y)}{p(x)} = \\frac{1/36}{1/6} = \\frac{1}{6}.\\]\nTo make a brief aside, we can measure the information content of a value \\(z \\sim p(z)\\) by defining a function\n\\[I = \\log_2 \\frac{1}{p(z)} = -\\log_2 p(z).\\]\nThis functions tells us how many bits of information are contained in the value \\(z\\) if it’s sampled from \\(p(z)\\). If we average this function over all \\(z\\), it tells us how many bits of information are contained in the entire distribution itself. This is called entropy, denoted \\(H\\). I’ll talk about entropy more generally in a future lesson. But, when the distribution is uniform (as in our case), the entropy is just \\(I\\) itself, i.e. \\(H=I\\). If \\(p(z)=\\frac{1}{n}\\) for all \\(z\\), its entropy is \\(H = \\log_2 n\\). That is, we’d need \\(\\log_2 n\\) bits of information to specify any given value \\(x\\) sampled from \\(p(z)\\).\nWe can apply this idea of information to the ratio given above. In that case, it takes \\(H=\\log_2(6) \\approx 2.6\\) bits of information to specify any given \\((x,y)\\) pair, assuming we knew \\(x\\). It looks like even if we knew what \\(x\\) is doing, we’d still need \\(2.6\\) more bits to figure out what \\(y\\) is doing. In fact, that’s the same number of bits contained in \\(p(y)\\), since \\(I=\\log_2(6) \\approx 2.6\\). It seems like knowing what \\(x\\) is is telling us basically nothing about what \\(y\\) is.\nLet’s compare this now with the perfectly correlated case, where \\(p(x)=\\frac{1}{6}\\) for all \\(x\\), but\n\\[\np(x,y) =\n\\begin{cases}\n\\frac{1}{6}, & (x,y) = (1,2), (2,3), (3,4), (4,5), (5,6), (6,1) \\\\\n0, & \\text{otherwise}.\n\\end{cases}\n\\]\nIf we take the same ratio again, we’d get\n\\[\n\\frac{p(x,y)}{p(x)} =\n\\begin{cases}\n1, & (x,y) = (1,2), (2,3), (3,4), (4,5), (5,6), (6,1) \\\\\n0, & \\text{otherwise}.\n\\end{cases}\n\\]\nLooking at the entropy of this ratio, we’d need \\(H=\\log_2(1) = 0\\) bits of information to specify \\((x,y)\\) if \\(x\\) was known. That is, knowing what \\(x\\) is is completely sufficient to tell us what \\(y\\) is. We don’t need to know anything else. We have all the bits we need already by knowing \\(p(x)\\).\nClearly this ratio is important, so we give it a name. We call it the conditional probability of \\(y\\), given \\(x\\), written\n\\[p(y|x) = \\frac{p(x,y)}{p(x)}.\\]\nIt’s not obvious that this is a valid probability, but it is so long as \\(p(x) \\neq 0\\). It’s always non-negative, and will sum to one over all \\(y\\) values,\n\\[\\sum_j p(y=j|x) = \\sum_j \\frac{p(x,y=j)}{p(x)} = \\frac{p(x)}{p(x)} = 1.\\]\nThe last equality follows from the fact that we can sum over one variable to get the marginal distribution of the other,\n\\[p(x) = \\sum_j p(x,y=j).\\] \\[p(y) = \\sum_i p(x=i,y).\\]\nThis is why they’re called marginal probabilities, because you’re “marginalizing” out the other variables.\nThe random variable \\(y|x\\) (pronounced “y given x”) is different than \\(y\\) or \\(x\\) or \\((x,y)\\). It’s a completely new thing. It’s the variable of \\(y\\) values, assuming we know what \\(x\\) is already. In our perfectly correlated example, if we know that \\(x=1\\), then \\(p(y|x=1) = 1\\) since we then know for certain that \\(y=2\\).\nIn fact, generally \\(y|x\\) won’t even have the same distribution as \\(y\\). The only time \\(y\\) and \\(y|x\\) will have the same distribution is if \\(x\\) and \\(y\\) are independent, since in that case \\(p(x,y)=p(x)p(y)\\), so\n\\[p(y|x) = \\frac{p(x,y)}{p(x)} = \\frac{p(x)p(y)}{p(x)} = p(y).\\]\nWhen \\(x\\) and \\(y\\) aren’t independent, if the come from a joint distribution \\(p(x,y)\\) people often say they’re conditionally independent, since \\(y|x\\) is always independent of \\(x\\) due to the fact that \\(p(x,y)=p(y|x)p(x)\\).\nBack to the dice roll example, here’s a plot of the histogram for \\(y|x=1\\) in each case, first the independent dice rolls, then the perfectly correlated rolls. The first will contain counts in all 6 bins since \\(p(y|x)=p(y)\\) is uniform. The second will only contain counts in bin 2, since \\(y=2\\) with certainty if \\(x=1\\).\n\n\nCode\nplot_histogram(y_indep[x == 1], is_discrete=True, title='$y|x=1$ (independent case)')\n\n\n\n\n\n\n\nCode\nplot_histogram(y_dep[x == 1], is_discrete=True, title='$y|x=1$ (perfectly correlated case)')\n\n\n\n\n\n\n\n12.1.3 Bayes Rule\nSuppose we condition \\(y\\) on the random variable \\(x\\). We just saw the conditional random variable \\(y|x\\) is given by\n\\[p(y|x) = \\frac{p(x,y)}{p(x)}.\\]\nNotice that we could symmetrically condition \\(x\\) on \\(y\\) instead, and ask about the other conditional random variable \\(x|y\\). In that case, its probability would be given symmetrically by\n\\[p(x|y) = \\frac{p(x,y)}{p(y)}.\\]\nLet’s put both of these together. Using the second equation to write the joint probability as \\(p(x,y)=p(x|y)p(y)\\) and plugging this expression into the first equation, we get\n\\[p(y|x) = \\frac{p(x|y)p(y)}{p(x)}.\\]\nThis formula is called Bayes Rule. It looks quaint, but don’t let that fool you. This is perhaps the most important formula in probability and statistics. Entire machine learning algorithms derive practically from this formula alone.\n\n\n12.1.4 \\(n\\) Random Variables\nWhat I just described for two random variables extends to any number of random variables as well. If we have \\(n\\) random variables \\(x_0,x_1,\\cdots,x_{n-1}\\), we can define a random vector out of them, \\(\\mathbf{x} = (x_0,x_1,\\cdots,x_{n-1})\\). As a running example, imagine rolling not just two dice, but \\(n\\) dice. Each dice alone can take on one of 6 values, which means a whole random vector of dice can take on \\(6^n\\) tuples of values. The possible outcomes add up fast!\nThe joint probability of these variables can be defined as the ratio of the number of times \\(n_{i_0,i_1,\\cdots,i_{n-1}}\\) each tuple of values \\((i_0,i_1,\\cdots,i_{n-1})\\) occurs divided by the total number of trials \\(N\\), as \\(N \\rightarrow \\infty\\),\n\\[\\mathbb{Pr}(\\mathbf{x}=\\mathbf{i}) = \\mathbb{Pr}(x_0=i_0,x_1=i_1,\\cdots,x_{n-1}=i_{n-1}) = \\lim_{N \\rightarrow \\infty} \\frac{N_{i_0,i_1,\\cdots,i_{n-1}}}{N}.\\]\nWe can also define a joint distribution by specifying a probability mass function \\(p(\\mathbf{x}) = p(x_0,x_1,\\cdots,x_{n-1})\\). Then each probability is given by\n\\[\\mathbb{Pr}(\\mathbf{x}=\\mathbf{i}) = p(\\mathbf{i}) = p(i_0,i_1,\\cdots,i_{n-1}).\\]\nAny distribution gotten by summing over one or more variables is called a marginal distribution. We can have \\(n\\) marginal distributions \\(p_0(x_0), p_1(x_1), \\cdots, p_{n-1}(x_{n-1})\\), as well as marginals of any pairs of random variables \\(p_{i,j}(x_i,x_j)\\), or triplets \\(p_{i,j,k}(x_i,x_j,x_k)\\), etc.\nIndependence extends as well. Two variables \\(x_i\\) and \\(x_j\\) are independent if their bivariate marginal distribution \\(p_{i,j}(x_i,x_j) = p_i(x_i)p_j(x_j)\\). If all the variables are independent of each other, we can factor the whole joint distribution into products too,\n\\[p(x_0,x_1,\\cdots,x_{n-1}) = \\prod_{i=0}^{n-1} p_i(x_i) = p_0(x_0)p_1(x_1) \\cdots p_{n-1}(x_{n-1}).\\]\nNotation: Trying to keep track of all these notations, where each distribution has its own indices, containing variables with the same indices can be a mess. For this reason it’s common in practice to omit the subscripts when writing down distributions. So instead of writing \\(p_i(x_i)\\) we’d just write \\(p(x_i)\\), where it’s understood by the presence of \\(x_i\\) that we’re referring to the distribution \\(p_i(x_i)\\). This usually works fine, but it can be confusing in some cases, for example when multiple random variables come from the same distribution. We’ll see this exception below. Outside of those exceptions, assume that each distribution need not be the same.\nWhen dealing with \\(n\\) random variables, the most important cases to know about are: - When all variables individually come from the same marginal distribution \\(p(x)\\), i.e. \\(p(x)=p_1(x_1)=\\cdots=p_{n-1}(x_{n-1})\\). We then say the random variables are identically distributed, or ID for short. We’d write \\(x_0,x_1,\\cdots,x_{n-1} \\sim p(x)\\), to make it clear everything is sampled from the same distribution. - When all variables are independent of each other, in which case \\(p(x_0,x_1,\\cdots,x_{n-1}) = p_0(x_0)p_1(x_1) \\cdots p_{n-1}(x_{n-1})\\). A shorthand used sometimes to say random variables are all independent is to write \\(x_0 \\perp x_1 \\perp \\cdots \\perp x_{n-1}\\), where the symbol \\(\\perp\\) means “is independent of”. - When all variables are both identically distributed and independent. Lacking any creativity, we call such variables independent, identically distributed, or IID for short. As a short hand, if we sample \\(n\\) variables IID from some distribution \\(p(x)\\), we’d write\n\\[x_0,x_1,\\cdots,x_{n-1} \\overset{iid}{\\sim} p(x), \\quad \\text{or just} \\quad \\mathbf{x} \\overset{iid}{\\sim} p(x).\\]\nIID functions have the nice property that their joint distribution factors into a product of marginals that all follow the same distribution,\n\\[p(x_0,x_1,\\cdots,x_{n-1}) = \\prod_{i=0}^{n-1} p(x_i) = p(x_0) p(x_1) \\cdots p(x_{n-1}),\\]\nwhere it’s understood each \\(p(x_i)\\) on the right is the same marginal distribution, so \\(p(x)=p_0(x)=\\cdots=p_{n-1}(x)\\). This property is really nice because it means all we need to do to find the joint distribution is to find a single marginal distribution. By studying one, we’re studying them all.\nWe can also condition any random variable on one or more other random variables. For example, if we condition \\(x_{n-1}\\) on \\(x_0,x_1,\\cdots,x_{n-2}\\), then its conditional distribution is given by\n\\[p(x_{n-1}|x_0,x_1,\\cdots,x_{n-2}) = \\frac{p(x_0,x_1,\\cdots,x_{n-2},x_{n-1})}{p(x_0,x_1,\\cdots,x_{n-2})}.\\]\nThis conditional probability is equivalent to asking, how much information is contained in the joint distribution \\(p(x_0,x_1,\\cdots,x_{n-2},x_{n-1})\\) if we already knew the marginal distribution \\(p(x_0,x_1,\\cdots,x_{n-2})\\). Equivalently, how much information do the random variables \\(x_0,x_1,\\cdots,x_{n-2}\\) give us about the variable \\(x_{n-1}\\).\nWe can derive the general form of Bayes Rule in a similar way to the bivariate case. For example, if we know what \\(p(x_0|x_1,\\cdots,x_{n-1})\\), \\(p(x_1,\\cdots,x_{n-1})\\), and \\(p(x_0)\\) are, then we can get \\(p(x_1,\\cdots,x_{n-1}|x_0)\\) by taking\n\\[p(x_1,\\cdots,x_{n-1}|x_0) = \\frac{p(x_0|x_1,\\cdots,x_{n-1}) p(x_1,\\cdots,x_{n-1})}{p(x_0)}.\\]\nI realize this section looks confusing. If you take nothing else away from this section, take away the idea of what IID random variables are. Almost all of machine learning in some way depends on the assumption that data is IID in some way or another. We’ll see enough examples as we go that you should start getting somewhat comfortable with joint distributions of \\(n\\) variables. Most of the time they’ll be IID.\n\n\n12.1.5 Application: The Central Limit Theorem\nAn interesting application of the use of joint random variables is in considering the sum of \\(n\\) IID Bernoulli random variables. Suppose we sample \\(n\\) times IID from a Bernoulli distribution,\n\\[x_0,x_1,\\cdots,x_{n-1} \\overset{iid}{\\sim} \\text{Ber}(\\text{p}).\\]\nConsider a new random variable \\(x\\) gotten by summing up all these IID Bernoulli variables,\n\\[x = \\sum_{j=0}^{n-1} x_j = x_0 + x_1 + \\cdots x_{n-1}.\\]\nLet’s stare at this expression and see what we can already deduce about the sum \\(x\\). Since each \\(x_i=0,1\\), the smallest \\(x\\) can ever be is \\(0\\) (when all \\(x_i=0\\)), and the largest it can be is \\(n\\) (when all \\(x_i=1\\)), so \\(x\\) must have support \\(0,1,2,\\cdots,n\\). Next, let’s use the fact that all the \\(x_i\\) are IID and see what that gives us. In this case, the joint distribution is\n\\[p(x_0,x_1,\\cdots,x_{n-1}) = p(x_0)p(x_1) \\cdots p(x_{n-1}).\\]\nSince each \\(x_i \\sim \\text{Ber}(\\text{p})\\), we must then have\n\\[\n\\begin{align*}\np(x_0,x_1,\\cdots,x_{n-1}) &= p(x_0)p(x_1) \\cdots p(x_{n-1}) \\\\\n&= \\text{p}^{x_0}(1-\\text{p})^{1-x_0} \\text{p}^{x_1}(1-\\text{p})^{1-x_1} \\cdots \\text{p}^{x_{n-1}}(1-\\text{p})^{1-x_{n-1}} \\\\\n&= \\text{p}^{x_0+x_1+\\cdots+x_{n-1}} (1-\\text{p})^{n-(x_0+x_1+\\cdots+x_{n-1})} \\\\\n&= \\text{p}^{x} (1-\\text{p})^{n-x}.\n\\end{align*}\n\\]\nYou may recall that this expression looks kind of like the expression for the binomial distribution,\n\\[p(x; n,\\text{p}) = \\binom{n}{x} \\text{p}^{x} (1-\\text{p})^{n-x}, \\quad \\text{where } x=0,1,\\cdots,n.\\]\nEvidently, the joint distribution \\(p(x_0,x_1,\\cdots,x_{n-1})\\) pretty much is the binomial distribution \\(p(x; n,\\text{p})\\). If we just re-normalize each term by summing over all \\(x=1,2,\\cdots,n\\) and setting the sum equal to one, we get the same normalizing constants from the binomial distribution, namely the binomial coefficients\n\\[\\binom{n}{x} = \\frac{n!}{n!(n-x)!}.\\]\nWe’ve thus shown that the binomial distribution is just the sum of \\(n\\) IID Bernoulli random variables. IID Bernoulli random variables are often called Bernoulli trials. You can think of Bernoulli trials as being equivalent to flipping a (not necessarily fair) coin \\(n\\) times and asking which ones turn up heads. The total number of heads from \\(n\\) coin flips would be given by a binomial distribution \\(\\text{Bin}(n,\\text{p})\\).\nA curious fact is that the binomial distribution looks like a Gaussian distribution when \\(n\\) is large and \\(\\text{p}\\) isn’t too close to 0 or 1. Then,\n\\[\\text{Bin}(n,\\text{p}) \\approx \\mathcal{N}(n\\text{p}, n\\text{p}(1-\\text{p})).\\]\nThat is, the binomial distribution in such cases has an approximate bell-curved shape that’s centered around a mean of \\(n\\text{p}\\) with a variance of \\(n\\text{p}(1-\\text{p})\\).\nThis is a consequence of the Central Limit Theorem. The CLT, which I won’t prove, says that if we have \\(n\\) IID random variables all with the same mean \\(\\mu\\) and variance \\(\\sigma^2\\), then, provided \\(n\\) is large, the distribution of their sum will be approximately a Gaussian,\n\\[x = \\sum_{i=0}^{n-1} x_i \\sim \\mathcal{N}\\bigg(n\\mu, n\\sigma^2\\bigg).\\]\nIn the case of Bernoulli trials, each \\(x_i\\) has mean \\(\\mu=\\text{p}\\) and variance \\(\\sigma^2=\\text{p}(1-\\text{p})\\), so \\(x \\sim \\mathcal{N}(n\\text{p}, n\\text{p}(1-\\text{p}))\\). It’s kind of weird when you think about it. The CLT is saying no matter what distribution we sample from, as long as we sample IID a whole bunch of times, then the sum of those samples will be roughly Gaussian. In fact, the samples need not even be IID, but I won’t go into that.\nI’ll show a quick example of this fact below by sampling from a binomial distribution 10,000 times with parameters \\(n=1000\\) and \\(\\text{p}=0.5\\). Notice first how bell-shaped the histogram appears, similar to a Gaussian. Also notice how almost all of the values seem to lie in the range \\(500 \\pm 32\\), as you’d expect for a Gaussian distribution with mean \\(500\\) and variance \\(250\\).\nEvidently, if you flip a fair coin 10,000 times, the total number of heads would be roughly Gaussian distributed just like this example. Curious, right?\n\n\nCode\nn = 1000\np = 0.5\nx = np.random.binomial(n, p, size=10000)\nplot_histogram(x, title=f'$Bin({n},{p}) \\\\approx \\mathcal{{N}}({int(n*p)}, {int(n*p*(1-p))})$', is_discrete=True)\n\n\n\n\n\n\n\n12.1.6 Continuous Multivariate Distributions\nAll of the common distributions we’ve covered so far can be extended into the multivariate case. The most useful multivariate extensions for machine learning purposes is the multivariate Gaussian distribution.\nProduct Distributions: \\(p(x_0,x_1,\\cdots) = p_1(x_0)p_1(x_1)\\cdots\\)\nMultivariate Gaussian Distribution: \\(\\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\)\nThe multivariate Gaussian distribution, denoted by the symbol \\(\\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\), is defined on all of \\(n\\) dimensional space \\(\\mathbb{R}^n\\) with density function\n\\[p(\\mathbf{x}) = (2 \\pi)^{-n/2} |\\boldsymbol{\\Sigma}|^{-1/2} \\exp\\bigg(-\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu})^\\top  \\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu})\\bigg),\\]\nwhere \\(\\boldsymbol{\\mu} \\in \\mathbb{R}^n\\) is an n-dimensional real vector and \\(\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{n \\times n}\\) is a positive square matrix. The vector \\(\\boldsymbol{\\mu}\\) is the \\(n\\) dimensional generalization of the mean, which is just a vector of means. The mean of variable \\(x_i\\) is \\(\\mu_i\\) for all \\(i=0,\\cdots,n-1\\). The matrix \\(\\boldsymbol{\\Sigma}\\) is called the covariance matrix. It’s a symmetric, positive matrix that generalizes the notion of variance to \\(n\\) dimensions. Why a matrix? Because each \\(x_i,x_j\\) can have a notion of (co)variance with each other. More on this soon.\nNotice the special symbol \\(|\\boldsymbol{\\Sigma}|\\) in the constant term outside the exponential. It’s not important to know what this is. Just consider it a function of \\(\\boldsymbol{\\Sigma}\\) alone. If you must know, it’s the determinant of the matrix. The only thing worth knowing is when \\(\\boldsymbol{\\Sigma}\\) is diagonal, \\(|\\boldsymbol{\\Sigma}|\\) is the product of the diagonal elements, which I’ll use below.\nNote the exponent term is really just a sum of a bunch of scalar terms when expanded out, \\[(\\mathbf{x}-\\boldsymbol{\\mu})^\\top  \\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu}) = \\sum_{i=0}^{n-1} \\sum_{j=0}^{n-1} (x_i - \\mu_i) \\cdot \\Sigma_{i,j}^{-1} \\cdot (x_j - \\mu_j),\\] hence the exponent is really just the sum of a bunch of quadratic terms, reminiscent of the univariate Gaussian.\nIn the special case when the covariance matrix \\(\\boldsymbol{\\Sigma}\\) is diagonal, we say the random variables \\(x_0,x_1,\\cdots,x_{n-1}\\) are uncorrelated. Suppose \\[\\boldsymbol{\\Sigma} = \\text{diag}(\\sigma_0^2, \\sigma_2^2, \\cdots, \\sigma_{n-1}^2).\\] You can pretty easily verify that its inverse is simply the reciprocals of the diagonal terms, \\[\\boldsymbol{\\Sigma}^{-1} = \\text{diag}\\bigg(\\frac{1}{\\sigma_0^2}, \\frac{1}{\\sigma_1^2}, \\cdots, \\frac{1}{\\sigma_{n-1}^2}\\bigg).\\] This makes the complicated looking exponent term reduce to a simple sum of squares. Using the fact \\(|\\boldsymbol{\\Sigma}|=\\sigma_0\\sigma_1\\cdots\\sigma_{n-1}\\) as well, we get\n\\[\n\\begin{align*}\np(\\mathbf{x}) &= \\frac{1}{\\sqrt{2 \\pi \\sigma_0 \\sigma_1 \\cdots \\sigma_{n-1}}} \\exp\\bigg(-\\frac{1}{2} \\sum_{i=0}^{n-1} \\frac{(x_i - \\mu_i)^2}{\\sigma_i^2} \\bigg) \\\\\n&= \\Bigg(\\frac{1}{\\sqrt{2 \\pi \\sigma_0^2}} \\exp{\\bigg(-\\frac{(x_0 - \\mu_0)^2}{2\\sigma_0^2}\\bigg)} \\Bigg) \\cdots \\Bigg(\\frac{1}{\\sqrt{2 \\pi \\sigma_{n-1}^2}} \\exp{\\bigg(-\\frac{(x_{n-1} - \\mu_{n-1})^2}{2\\sigma_{n-1}^2}\\bigg)} \\Bigg) \\\\\n&= p(x_0) p(x_1) \\cdots p(x_{n-1}).\n\\end{align*}\n\\]\nThat is, the joint distribution \\(p(\\mathbf{x})\\) factors into a product of marginal Gaussian distributions \\(p(x_0) p(x_1) \\cdots p(x_{n-1})\\). This means that the variables \\(x_0,x_1,\\cdots,x_{n-1}\\) must be independent if they’re uncorrelated. In the further special case where \\(\\mu_i=\\mu\\) and \\(\\sigma_i^2=\\sigma^2\\) for all \\(i\\), they’re also IID.\nThis distribution is so important to machine learning that it’s worth taking some time to visualize it, as a surface in 3D space, as a set of contours, as 3D histogram, and as a heat map. Let’s start with the first two. Suppose our random variable \\(\\mathbf{x}=(x,y)\\) is 2D. I’m going to plot the surface of the density function \\(p(x,y)\\) as well as the contour plot side-by-side using a helper function plot_multivariate_normal. This function will take in the mean vector \\(\\boldsymbol{\\mu}\\) and covariance matrix \\(\\boldsymbol{\\Sigma}\\) and plot what the distribution looks like for that pair of parameters.\nLet’s define \\(\\boldsymbol{\\mu}=(\\mu_x, \\mu_y)\\) and \\(\\boldsymbol{\\Sigma} = \\begin{pmatrix} \\sigma_x^2 & \\sigma_{x,y} \\\\ \\sigma_{x,y} & \\sigma_y^2 \\end{pmatrix}.\\) A few things to notice: - Changing \\(\\boldsymbol{\\mu}\\) just changes where the density is centered. It doesn’t change the shape. - Changing \\(\\sigma_x^2\\) or \\(\\sigma_y^2\\) changes how much the density spreads in that direction. If they’re equal the density will be perfectly circular. If one is greater than the other the density will become elliptical. In the limit one becomes much much greater than the other, the density will essentially just become a univariate Gaussian. - Making \\(\\sigma_{x,y}\\) non-zero makes the density change its angle in the xy-plane. That’s what introducing correlation essentially does geometrically. - Making \\(\\sigma_{x,y}=0\\) means that \\(x\\) and \\(y\\) become independent as I showed above. Geometrically, independence essentially rotates until the density is aligned along the axes themselves.\nFeel free to play around with different choices of parameters and see how it affects the densities.\n\n\nCode\nmu = np.array([0, 0])\nSigma = np.array([[1, 0.5], [0.5, 2]])\nplot_multivariate_gaussian(mu, Sigma, elev=10, azim=45)\n\n\n\n\n\nWe can also of course plot the histograms. Suppose we sample a bunch of points \\((x,y) \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma})\\) a bunch of times and plot the histogram. We can do this in python by again using np.random.randn, but passing in a shape \\((m,n)\\) instead of a single number.\nWhen \\(n=2\\) we can plot the histogram in 3D space. I’ll plot the standard 2D Gaussian \\(\\mathcal{N}(\\mathbf{0}, \\mathbf{I})\\), which has mean zero and identity covariance. It’s the multivariate generalization of the standard \\(\\mathcal{N}(0,1)\\) Gaussian. I’m using another helper function called plot_3d_hist here to hide a lot of ugly code.\n\n\nCode\nX = np.random.randn(10000, 2)\nx, y = X[:, 0], X[:, 1]\nplot_3d_hist(x, y, bins=30, xlim=(-2, 2), ylim=(-2, 2), elev=20, azim=30, \n             title='3D Histogram: $\\mathcal{N}(\\mathbf{0}, \\mathbf{I})$')\n\n\n\n\n\nFinally, we can view the same histogram contour-style using what’s called a heat map. A heat map is a way of plotting one variable that depends on 2 others, e.g. \\(p(x,y)\\) vs \\(x\\) and \\(y\\). In each grid of the plot the color indicates how “hot” the dependent variable is. The higher the counts in a given bin, the hotter the heat map will look at that bin. We can plot a histogram-style heatmap in matplotlib using plt.hist2d. Heat maps are very useful ways of visualizing data like this, usually more so than janky 3D plots like the one above.\n\n\nCode\nplt.hist2d(x, y, bins=30)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Heat Map: $\\mathcal{N}(\\mathbf{0}, \\mathbf{I})$')\nplt.show()\n\n\n\n\n\n\n\nCode\n## add product distributions, multivariate laplace, one-hot categorical, multinomial"
  },
  {
    "objectID": "notebooks/statistics.html#statistics",
    "href": "notebooks/statistics.html#statistics",
    "title": "13  Statistics",
    "section": "13.1 Statistics",
    "text": "13.1 Statistics\nStatistics is at its root the use of probability to study data. Data can be any real-world measurement, in essentially any form. Statistics treats data as either fixed values or random variables depending on the situation. If the data has already been observed we assume it’s fixed. If not, we assume it’s random and try to model it with a probability distribution. For the purposes of this lesson we’ll assume data comes in the form of some 1D or 2D array and that each data point takes on a numerical value, either an integer or real number.\nLet’s start with a simple 1D array of \\(m\\) samples \\(\\mathbf{x} = (x_0,x_1,\\cdots,x_{m-1})\\). We’d like to know what univariate distribution \\(p(x)\\) would generate the samples \\(x_0,x_1,\\cdots,x_{m-1}\\). If we can get this probability distribution even approximately, we can gain a lot of insight into the nature of the data itself.\nUnfortunately, it’s really hard to figure out what distribution data is coming from with only a finite amount of data in all but the simplest cases. What we often thus settle for instead is to assume the data come from a certain class of distribution, and then try to estimate what the parameters of that distribution would have to be to ensure that distribution fits the data well. In this sense, a whole lot of statistics boils down to how to estimate the parameters of a given distribution from the data.\nSome of the most important parameters to estimate from an array of data are its moments. The hope is to find, without knowing the data’s distribution, the best estimate of that distribution’s moments from the data given. This is where the formulas you’re probably used to come in for things like mean, variance, standard deviation, etc. Traditionally to avoid getting these moment estimates mixed up with the true distribution’s moments, we call them sample moments. I’ll define them below for the univariate case.\nSample Mean: \\[\\overline{x} = \\frac{1}{m}\\sum_{i=0}^{m-1} x_i = \\frac{1}{m}(x_0 + x_1 + \\cdots + x_{m-1})\\]\nSample Variance: \\[s^2 = \\frac{1}{m}\\sum_{i=0}^{m-1} (x_i-\\overline{x})^2 = \\frac{1}{m}\\big((x_0-\\overline{x})^2 + \\cdots + (x_{m-1}-\\overline{x})^2\\big)\\]\nSample Standard Deviation: \\[s = \\sqrt{s^2} = \\sqrt{\\frac{1}{m}\\sum_{i=0}^{m-1} (x_i-\\overline{x})^2}\\]\nOther quantities that might be of interest to estimate aren’t moments at all. One example is the median, which is defined as the midpoint of a distribution, i.e. the \\(x\\) such that \\(p(x)=1/2\\). The median is another way of estimating the center of a distribution, but has slightly different properties than the mean. One of those properties is that the mean depends only on the rank order of values, not on what numbers those values take on. This implies that, unlike the mean, the median is invariant to points “far away from the center”, called outliers.\nWe can estimate the sample median, call it \\(M\\), of an array \\(x_0,x_1,\\cdots,x_{m-1}\\) by sorting them in ascending order and plucking out the midpoint. If \\(m\\) is odd, this is just \\(M = x_{m//2+1}\\). If \\(m\\) is even we by convention take the median as the average of the two midpoints \\(M = \\frac{1}{2}\\big(x_{m//2} + x_{m//2+1}\\big)\\).\nOther quantities we might want to estimate from the data are the sample minimum, the sample maximum, and the sample range, which is defined as the difference between the sample max and min."
  },
  {
    "objectID": "notebooks/statistics.html#moments-of-a-distribution",
    "href": "notebooks/statistics.html#moments-of-a-distribution",
    "title": "13  Statistics",
    "section": "13.2 Moments of a Distribution",
    "text": "13.2 Moments of a Distribution\nProbability distributions have special quantities that are worth keeping track of, called moments. The most important moments for practical purposes are the mean and variance, but there are higher-order moments as well like skewness and kurtosis that sometimes become important.\n\n13.2.1 Univariate Moments\nLet’s start with a boring term that I’ll call the “zeroth moment”. It’s just a restatement of the fact that probabilities sum to one from before,\n\\[\n1 =\n\\begin{cases}\n\\sum_{j=0}^{k-1} p(x_k), & x \\text{ is discrete}, \\\\ \\\\\n\\int_{-\\infty}^\\infty p(x) dx, & x \\text{ is continuous}.\n\\end{cases}\n\\]\nMoving onto the first moment. Define the mean (or expected value) of a distribution \\(p(x)\\), usually denoted by symbols like \\(\\langle x \\rangle\\) or \\(\\mathbb{E}(x)\\) or \\(\\mu\\), by\n\\[\n\\langle x \\rangle = \\mathbb{E}(x) =\n\\begin{cases}\n\\sum_{j=0}^{k-1} x_k p(x_k), & x \\text{ is discrete}, \\\\ \\\\\n\\int_{-\\infty}^\\infty x p(x) dx, & x \\text{ is continuous}.\n\\end{cases}\n\\]\nThe mean of a univariate distribution \\(p(x)\\) is an estimate of the “center of mass” or the “balancing point” of the distribution. It will be a single number.\nMoving onto the second moment. Let \\(\\mu = \\langle x \\rangle\\) for convenience. Define the variance (or mean square) of a distribution \\(p(x)\\), usually denoted \\(\\text{Var}(x)\\) or \\(\\sigma^2\\), by the mean of the squared difference \\((x-\\mu)^2\\), that is\n\\[\n\\text{Var}(x) = \\big\\langle (x-\\mu)^2 \\big\\rangle =\n\\begin{cases}\n\\sum_{j=0}^{k-1} (x_k-\\mu)^2 p(x_k), & x \\text{ is discrete}, \\\\ \\\\\n\\int_{-\\infty}^\\infty (x-\\mu)^2 p(x) dx, & x \\text{ is continuous}.\n\\end{cases}\n\\]\nSimilar to the mean, the variance of a univariate distribution will also be a single number indicating the “spread” of the distribution. More commonly, when talking about the “spread” of a distribution, we like to talk about the square root of the variance, called the standard deviation (or root mean square) and usually denoted \\(\\sigma\\), \\[\\sigma = \\sqrt{\\text{Var}(x)}.\\]\nThe standard deviation has the advantage that it’s on the same scale of \\(x\\), and has the same units.\nI won’t prove it, but this pattern is very general. We can talk about taking the mean of any function of a random variable just as well. Suppose \\(f(x)\\) is some reasonably well-behaved function. Then we can talk about the mean of \\(f(x)\\) by using\n\\[\n\\langle f(x) \\rangle = \\mathbb{E}f(x) =\n\\begin{cases}\n\\sum_{j=0}^{k-1} f(x_j) p(x_j), & x \\text{ is discrete}, \\\\ \\\\\n\\int_{-\\infty}^\\infty f(x) p(x) dx, & x \\text{ is continuous}.\n\\end{cases}\n\\]\nThis trick is sometimes called the Law of the Unconscious Statistician.\nThe variance can be written in a different form by doing some algebraic manipulations, \\[\\text{Var}(x) = \\langle x^2 \\rangle - \\mu^2.\\] You can pretty easily derive this formula if you like, but I won’t do so here. This form is sometimes easier to use to do calculations if we’ve already calculated the mean \\(\\mu\\), since we only need to calculate \\(\\langle x^2 \\rangle\\), which is often easier to do.\nIt’s worth noting that the mean operation is linear, which means two things, - if \\(x\\) and \\(y\\) are two random variables, then \\(\\langle x + y \\rangle = \\langle x \\rangle + \\langle y \\rangle\\), - if \\(x\\) is a random variable and \\(c\\) is some constant, then \\(\\langle cx \\rangle = c\\langle x \\rangle\\).\nThis fact follows immediately from the fact that the mean is just a sum (or integral), and sums are linear. This will always be true, not just for any two random variables, but any number of them. It won’t, however, be true for the variance. That is, \\[\\text{Var}(x+y) \\neq \\text{Var}(x) + \\text{Var}(y), \\qquad \\text{Var}(cx) = c^2\\text{Var}(x).\\]\n\n\n13.2.2 Multivariate Moments\nMoments also extend to multivariate distributions, but it becomes more complicated. The mean for univariate distributions becomes the mean vector for multivariate distributions, while variance becomes the covariance matrix. Other moments can be even more complicated in the multivariate case by becoming higher-rank tensors. I’ll briefly spell out the definition of mean and covariance below and stop there.\nThe mean vector of an \\(n\\) dimensional multivariate distribution \\(p(\\mathbf{x})\\), denoted \\(\\langle \\mathbf{x} \\rangle\\) or \\(\\mathbb{E}(\\mathbf{x})\\) or \\(\\boldsymbol{\\mu}\\), is defined by\n\\[\n\\langle \\mathbf{x} \\rangle = \\mathbb{E}(\\mathbf{x}) =\n\\begin{cases}\n\\sum_{j=0}^{k-1} \\mathbf{x}_j p(\\mathbf{x}_j), & \\mathbf{x} \\text{ is discrete}, \\\\ \\\\\n\\int_{\\mathbb{R}^n} \\mathbf{x} p(\\mathbf{x}) dA_{n-1}, & \\mathbf{x} \\text{ is continuous}.\n\\end{cases}\n\\]\nIf you look carefully, you’ll see the mean vector is just the vector whose components are the ordinary means, \\(\\mu_i = \\langle x_i \\rangle\\). Just as in the univariate case, the mean vector again represents the “center of mass” of the distribution, except now in \\(n\\) dimensional space.\nThe covariance matrix of an \\(n\\) dimensional multivariate distribution \\(p(\\mathbf{x})\\), denoted either \\(\\text{Cov}(\\mathbf{x})\\) or \\(\\boldsymbol{\\Sigma}\\), is defined as the mean square of the outer product of the difference \\(\\mathbf{x} - \\boldsymbol{\\mu}\\), i.e. \\[\\boldsymbol{\\Sigma} = \\text{Cov}(\\mathbf{x}) = \\big\\langle (\\mathbf{x}-\\boldsymbol{\\mu}) (\\mathbf{x}-\\boldsymbol{\\mu})^\\top \\big\\rangle \\qquad \\Rightarrow \\qquad\n\\sigma_{i,j} = \\langle (x_i-\\mu_i)(x_j-\\mu_j) \\rangle.\\]\nThis gives a symmetric, positive \\((n,n)\\) matrix whose elements \\(\\sigma_{i,j}\\) represent how much any two variables “depend on each other”. The diagonal elements \\(\\sigma_{i,i}\\) represent the variances of each \\(x_i\\), while the off-diagonal elements \\(\\sigma_{i,j}\\), \\(i \\neq j\\) represent the correlation of \\(x_i\\) with \\(x_j\\). The higher \\(\\sigma_{i,j}\\), the higher the two variables are said to correlate. If \\(\\sigma_{i,j}=0\\) the two variables are said to be uncorrelated.\n\n\n13.2.3 Example: Bernoulli Distribution\nJust so you can see how this process is done, I’ll work one simple example, the Bernoulli distribution. This one should be easy since it only takes on two values \\(x=0,1\\). The other ones, not so much.\nRecall for Bernoulli we have the probability function\n\\[\np(x) =\n\\begin{cases}\np_0 & x = 1, \\\\\n(1-p_0) & x = 0.\n\\end{cases}\n\\]\nFor the mean, we thus have\n\\[\n\\begin{align}\n\\langle x \\rangle &= \\sum_{x=0}^{1} x p(x) \\\\\n&= (0 \\cdot (1-p_0)) + (1 \\cdot p_0) \\\\\n&= p_0.\n\\end{align}\n\\]\nThat is, \\(\\mu = \\langle x \\rangle = p_0\\). The mean of Bernoulli is just the probability that \\(x=1\\).\nFor the variance, we have\n\\[\n\\begin{align}\n\\text{Var}(x) &= \\langle (x-\\mu)^2 \\rangle \\\\\n&= \\sum_{x=0}^{1} (x - \\mu)^2 p(x) \\\\\n&= ((0-p_0)^2 \\cdot (1-p_0)) + ((1-p_0)^2 \\cdot p_0) \\\\\n&= p_0^2 \\cdot (1-p_0) + (1-p_0)^2 \\cdot p_0 \\\\\n&= p_0 (1 - p_0).\n\\end{align}\n\\]\nThus, the variance of Bernoulli is \\(\\text{Var}(x) = p_0(1-p_0)\\), i.e. the probability \\(x=1\\) times the probability \\(x=0\\). Taking the square root then gives the standard deviation, \\(\\sigma = \\sqrt{p_0(1-p_0)}\\).\n\n\n13.2.4 List of Means and Variances\nRather than put you through the tedium of anymore boring and messy calculations, I’ll just list the mean and variance of several common distributions below for reference. Notice they won’t be a function of the random variable, but they will be a function of the distribution’s parameters.\n\n\n\n\nDistribution\n\n\nSupport\n\n\nMean\n\n\nVariance\n\n\n\n\nDiscrete Uniform \\(\\text{DU}(a,b)\\)\n\n\n\\(\\{a,a+1,\\cdots,b-1\\}\\)\n\n\n\\(\\frac{1}{2}(a+b-1)\\)\n\n\n\\(\\frac{1}{12}((b-a)^2-1)\\)\n\n\n\n\nBernoulli \\(\\text{Ber}(\\text{p})\\)\n\n\n\\(\\{0,1\\}\\)\n\n\n\\(\\text{p}\\)\n\n\n\\(\\text{p}(1-\\text{p})\\)\n\n\n\n\nBinomial \\(\\text{Bin}(n, \\text{p})\\)\n\n\n\\(\\{0,1,\\cdots,n\\}\\)\n\n\n\\(n\\text{p}\\)\n\n\n\\(n\\text{p}(1-\\text{p})\\)\n\n\n\n\nCategorical \\(\\text{Cat}(\\mathbf{p})\\)\n\n\n\\(\\{0,1,\\cdots,k-1\\}\\)\n\n\n\\(p_j\\) for all \\(j\\)\n\n\n\\(p_j (1 - p_j)\\) for all \\(j\\)\n\n\n\n\nUniform \\(U(a,b)\\)\n\n\n\\([a,b]\\)\n\n\n\\(\\frac{1}{2}(a+b)\\)\n\n\n\\(\\frac{1}{12}(b-a)^2\\)\n\n\n\n\nGaussian \\(\\mathcal{N}(\\mu,\\sigma^2)\\)\n\n\n\\(\\mathbb{R}\\)\n\n\n\\(\\mu\\)\n\n\n\\(\\sigma^2\\)\n\n\n\n\nLaplace \\(L(\\mu,\\sigma)\\)\n\n\n\\(\\mathbb{R}\\)\n\n\n\\(\\mu\\)\n\n\n\\(2\\sigma\\)\n\n\n\n\nMultivariate Gaussian \\(\\mathcal{N}(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma})\\)\n\n\n\\(\\mathbb{R}^n\\)\n\n\n\\(\\boldsymbol{\\mu}\\)\n\n\n\\(\\boldsymbol{\\Sigma}\\)"
  }
]