[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning For The 2020s",
    "section": "",
    "text": "Preface\nThis is my book Machine Learning For The 2020s."
  },
  {
    "objectID": "notebooks/basic-math.html",
    "href": "notebooks/basic-math.html",
    "title": "1  Basic Math",
    "section": "",
    "text": "Elementary Arithmetic and Algebra\nIt’s useful in machine learning to be able to read and manipulate basic arithmetic and algebraic equations. I won’t go into depth on the basics of high school arithmetic and algebra. I do have to assume some mathematical maturity of the reader, and this seems like a good place to draw the line. I’ll just mention a few key points.\nRecall that numbers can come in several forms. We can have, - Positive whole numbers \\[0, 1, 2, 3, 4, \\cdots\\] These are sometimes called the natural numbers or the positive integers. Note the inclusion of \\(0\\) in this group. Following the computer science convention I’ll tend to do that. - Whole numbers \\[\\cdots, -3, -2, -1, 0, 1, 2, 3, \\cdots\\] These can be positive or negative. They’re called the integers. - Ratios of whole numbers, for example \\[\\frac{1}{2}, \\frac{5}{4}, -\\frac{3}{4}, \\frac{1000}{999}, \\cdots\\] Any ratio will do, so long as the denominator (the bottom number) is not zero. These are called the rational numbers. - Arbitrary decimal numbers, for example \\[1.00, \\ 5.07956, \\ -0.99999\\dots, \\ \\pi=3.1415\\dots, \\ e=2.718\\dots, \\ \\cdots\\] These are called the real numbers. They include as a special case both the integers and the rational numbers, but also include numbers that can’t be represented as fractions, like \\(\\pi\\) and \\(e\\). - Complex numbers like \\(1 + 2i\\) where \\(i=\\sqrt{-1}\\) is the imaginary number. Complex numbers include the real numbers as a special case. Since they don’t really show up in machine learning we won’t deal with these after this.\nYou should be familiar with the usual arithmetic operations defined on these systems of numbers. Things like addition, subtraction, multiplication, and division. You should also at least vaguely recall the order of operations, which defines the order in which complex arithmetic operations with parenthesis are carried out. For example,\n\\[(5+1) \\cdot \\frac{(7-3)^2}{2} = 6 \\cdot \\frac{4^2}{2} = 6 \\cdot \\frac{16}{2} = 6 \\cdot 8 = 48.\\]\nYou should be able to manipulate and simplify simple fractions by hand. For example,\n\\[\\frac{3}{7} + \\frac{1}{5} = \\frac{3 \\cdot 5 + 1 \\cdot 7}{7 \\cdot 5} = \\frac{22}{35} \\approx 0.62857.\\]\nAs far as basic algebra goes, you should be familiar with algebraic expressions like \\(x+5=7\\) and be able to solve for the unknown variable \\(x\\),\n\\[x=7-5=2.\\]\nYou should be able to take an equation like \\(ax + b = c\\) and solve it for \\(x\\) in terms of coefficients \\(a, b, c\\),\n\\[\\begin{align*}\nax + b &= c \\\\\nax &= c - b \\\\\nx &= \\frac{c - b}{a}.\n\\end{align*}\\]\nYou should also be able to expand simple expressions like this,\n\\[\\begin{align*}\n(ax - b)^2 &= (ax - b)(ax - b) \\\\\n&= (ax)^2 - (ax)b - b(ax) + b^2 \\\\\n&= a^2x^2 - abx - abx + b^2 \\\\\n&= a^2x^2 - 2abx + b^2.\n\\end{align*}\\]\nThat’s pretty much it. Just simple variable manipulation.\nAs I’m sure you’ve seen before, a mathematical function is a way to map inputs \\(x\\) to outputs \\(y\\). That is, a function \\(f(x)\\) is a mapping that takes in a value \\(x\\) and maps it to a unique value \\(y=f(x)\\). These values can be either single numbers (called scalars), or multiple numbers (vectors or tensors). When \\(x\\) and \\(y\\) are both scalars, \\(f(x)\\) is called a univariate function.\nLet’s quickly cover some of the common functions you’d have seen before in a math class, focusing mainly on the ones that show up in machine learning. I’ll also cover a couple machine-learning specific functions you perhaps haven’t seen before.\nWhat we’ve covered thus far only deals with univariate functions, functions where \\(y=f(x)\\), but \\(x\\) and \\(y\\) are just single numbers, i.e. scalars. In machine learning we’re almost always dealing with multivariate functions with lots of variables, sometimes billions of them. It turns out that most of what I’ve covered so far extends straight forwardly to multivariate functions with some small caveats, which I’ll cover below.\nSimply put, a multivariate function is a function of multiple variables. Instead of a single variable \\(x\\), we might have several variables, e.g. \\(x_0, x_1, x_2, x_3, x_4, x_5\\),\n\\[y = f(x_0, x_1, x_2, x_3, x_4, x_5).\\]\nIf you think about mathematical functions analogously to python functions it shouldn’t be surprising functions can have multiple arguments. They usually do, in fact.\nHere’s an example of a function that takes two arguments \\(x\\) and \\(y\\) and produces a single output \\(z\\), more often written as \\(z=f(x,y)\\). The function we’ll look at is \\(z = x^2 + y^2\\). I’ll evaluate the function at three points: - \\(x=0\\), \\(y=0\\), - \\(x=1\\), \\(y=-1\\), - \\(x=0\\), \\(y=1\\).\nThe main thing to notice is the function does exactly what you think it does. If you plug in 2 values, you get out 1 value.\nWe can also have functions that map multiple inputs to multiple outputs. Suppose we have a function that takes in 2 values \\(x_0, x_1\\) and outputs 2 values \\(y_0, y_1\\). We’d write this as \\((y_0, y_1) = f(x_0, x_1)\\).\nConsider the following example,\n\\[(y_0, y_1) = f(x_0, x_1) = (x_0+x_1, x_0-x_1).\\]\nThis is really just two functions, both functions of \\(x_0\\) and \\(x_1\\). We can completely equivalently write this function as\n\\[y_0 = f_1(x_0, x_1) = x_0+x_1,\\] \\[y_1 = f_2(x_0, x_1) = x_0-x_1.\\]\nHere’s this function defined and evaluated at the point \\(x_0=1\\), \\(x_1=1\\).\nFor now I’ll just focus on the case of multiple inputs, single output like the first example. These are usually called scalar-valued functions. We can also have vector-valued functions, which are functions whose outputs can have multiple values as well. I’ll focus on scalar-valued functions here.\nA scalar-valued function of \\(n\\) variables \\(x_0, x_1, \\cdots, x_{n-1}\\) has the form\n\\[y = f(x_0, x_1, \\cdots, x_{n-1}).\\]\nNote \\(n\\) can be as large as we want it to be. When working with deep neural networks (which are just multivariate functions of a certain form) \\(n\\) can be huge. For example, if the input is a \\(256 \\times 256\\) image, the input might be \\(256^2=65536\\) pixels. For a 10 second audio clip that’s sampled at 44 kHz, the input might be \\(10*44k=440k\\) amplitudes. Large numbers indeed.\nCalculating the output of multivariate functions is just as straight-forward as for univariate functions pretty much. Unfortunately, visualizing them is much harder. The human eye can’t see 65536 dimensions, only 3 dimensions. This in some sense means we need to give up on the ability to “graph” a function and instead find other ways to visualize it.\nOne thing that sometimes help to visualize high dimension functions is to pretend they’re functions of two variables, like \\(z=f(x,y)\\). In this special case we can visualize the inputs as an xy-plane, and the output as a third axis sticking out perpendicular to the xy-plane from the origin. Each \\(x,y\\) pair will map to one unique \\(z\\) value. Done this way, we won’t get a graph of a curve as before, but a surface.\nHere’s an example of what this might look like for the simple function \\(z=x^2+y^2\\). I’ll plot the function on the domain \\(-10 \\leq x \\leq 10\\) and \\(-10 \\leq y \\leq 10\\) using the helper function plot_3d. It takes in two lists of values x and y. I’ll use np.linspace to sample 100 points from -10 to 10 for each. Then I’ll define a lambda function that maps x and y to the output z. Passing these three arguments into the helper function gives us our 3D plot.\nNotice how the plot looks like an upward facing bowl. Imagine a bowl lying on a table. The table is the xy-plane. The bowl is the surface \\(z=x^2+y^2\\) we’re plotting. While the plot shows the general idea what’s going on, 3D plots can often be difficult to look at. They’re often slanted at funny angles and hide important details.\nHere’s another way we can visualize the same function: Rather than create a third axis for \\(z\\), we can plot it directly on the xy-plane as a 2D plot. Since we’re dealing with a surface, not a curve, we have to do this for lots of different \\(z\\) values, which will give a family of curves. For example, we might plot all of the following curves corresponding to different values of \\(z\\) in the xy-plane,\n\\[\\begin{align}\n25 &= x^2 + y^2, \\\\\n50 &= x^2 + y^2, \\\\\n75 &= x^2 + y^2, \\\\\n100 &= x^2 + y^2, \\\\\n125 &= x^2 + y^2, \\\\\n150 &= x^2 + y^2.\n\\end{align}\\]\nDoing this will give a family of curves on one 2D plot, with each curve representing some value of \\(z\\). In our example, these curves are all circles of radius \\(z^2\\). Each curve is called a level curve or level set.\nThese kinds of plots are called contour plots. A contour map can be thought of as looking at the surface from the top down, where each level set corresponds to slicing the function \\(z=f(x,y)\\) horizontally for different values of \\(z\\). This trick is often used in topographical maps to visualize 3D terrain on a 2D sheet of paper. Here is a contour plot for \\(z=x^2+y^2\\) using the above level curves.\nNotice how we get a bunch of concentric rings in the contour plot, each labeled by some value (their \\(z\\) values). These rings correspond to the circles I was talking about. You can visually imagine this plot as looking down from the top of the bowl. In the middle you see the bottom. The rings get closer together the further out you go, which indicates that the bowl is sloping steeper the further out we get.\nWe’ll see more examples of multivariate functions in the coming lessons.\nIn machine learning we’ll find ourselves frequently interested not just with single equations, but multiple equations each with many variables. One thing we might seek to do is solve these coupled systems, which means finding a solution that satisfies every equation simultaneously. Consider the following example,\n\\[\\begin{alignat*}{3}\n   x & {}+{} &  y & {}={} & 2  \\\\\n   2x & {}-{} &  3y & {}={} & 7.\n\\end{alignat*}\\]\nThis system consists of two equations, \\(x + y = 2\\), and \\(2x - 3y = 7\\). Each equation contains two unknown variables, \\(x\\) and \\(y\\). We need to find a solution for both \\(x\\) and \\(y\\) that satisfies both of these equations.\nUsually the easiest and most general way to solve simple coupled systems like this is the method of substitution. The idea is to solve one equation for one variable in terms of the other, then plug that solution into the second equation to solve for the other variable. Once the second variable is solved for, we can go back and solve for the first variable explicitly. Let’s start by solving the first equation for \\(x\\) in terms of \\(y\\). This is pretty easy,\n\\[x = 2 - y.\\]\nNow we can take this solution for \\(x\\) and plug it into the second equation to solve for \\(y\\),\n\\[\\begin{align*}\n2x - 3y &= 7 \\\\\n2(2 - y) - 3y &= 7 \\\\\n4 - 5y &= 7 \\\\\n5y &= -3 \\\\\ny &= -\\frac{3}{5}.\n\\end{align*}\\]\nWith \\(y\\) in hand, we can now solve for \\(x\\), \\(x = 2 - y = 2 + \\frac{3}{5} = \\frac{13}{5}\\). Thus, the pair \\(x=\\frac{13}{5}\\), \\(y=-\\frac{3}{5}\\) is the solution that solves both of these coupled equations simultaneously.\nHere’s sympy’s solution to the same system. It should of course agree with what I just got, which it does.\nNotice that both of the equations in this example are linear, since each term only contains terms proportional to \\(x\\) and \\(y\\). There are no terms like \\(x^2\\) or \\(\\sin y\\) or whatever. Linear systems of equations are special because they can always be solved as long as there are enough variables. I’ll spend a lot more time on these when I get to linear algebra.\nWe can also imagine one or more equations being nonlinear. Provided we can solve each equation one-by-one, we can apply the method of substitution to solve these too. Here’s an example. Consider the nonlinear system\n\\[\\begin{align*}\ne^{x + y} &= 10  \\\\\nxy &= 1.\n\\end{align*}\\]\nLet’s solve the second equation first since it’s easier. Solving for \\(y\\) gives \\(y = \\frac{1}{x}\\). Now plug this into the first equation and solve for \\(x\\),\n\\[\\begin{align*}\ne^{x + y} &= 10  \\\\\ne^{x + 1/x} &= 10  \\\\\n\\log \\big(e^{x + 1/x}\\big) &= \\log 10 \\\\\nx + \\frac{1}{x} &= \\log 10 \\\\\nx^2 - \\log 10 \\cdot x + 1 &= 0 \\\\\nx &= \\frac{1}{2} \\bigg(\\log 10 \\pm \\sqrt{(\\log 10)^2 - 4}\\bigg) \\\\\nx &\\approx 0.581, \\ 1.722.\n\\end{align*}\\]\nNote here I had to use the quadratic formula, which I’ll assume you’ve forgotten. If you have a quadratic equation of the form \\(ax^2 + bx + c = 0\\), then it will (usually) have exactly two solutions given by the formula\n\\[x = \\frac{1}{2a} \\bigg(-b \\pm \\sqrt{b^2 - 4ac}\\bigg).\\]\nThis means we have two different possible solutions for \\(x\\), which thus means we’ll also have two possible solutions to \\(y\\) since \\(y=\\frac{1}{x}\\). Thus, this system has two possible solutions,\n\\[\\text{Solution 1: }x \\approx 0.581, \\ y \\approx 1.722,\\] \\[\\text{Solution 2: }x \\approx 1.722, \\ y \\approx 0.581.\\]\nIt’s interesting how symmetric these two solutions are. They’re basically the same with \\(x\\) and \\(y\\) swapped. This is because the system has symmetry. You can swap \\(x\\) and \\(y\\) in the system above and not change the equation, which means the solutions must be the same up to permutation of \\(x\\) and \\(y\\)!\nHere’s sympy’s attempt to solve this system.\nIn general, it’s not even possible to solve a system of nonlinear equations except using numerical methods. The example I gave was rigged so I could solve it by hand. General purpose root-finding algorithms exist that can solve arbitrary systems of equations like this numerically, no matter how nonlinear they are.\nTo solve a nonlinear system like this numerically, you can use the scipy function scipy.optimize.fsolve. Scipy is an extension of numpy that includes a lot of algorithms for working with non-linear functions. To use fsolve, you have to define the system as a function mapping a list of variables to a list of equations. You also have to specify a starting point x0 for the root finder. This tells it where to start looking for the root. Since nonlinear equations have multiple solutions, picking a different x0 can and will often give you a different root. I won’t dwell on all this since we don’t really need to deal with root finding much in machine learning.\nLike many other technical fields, machine learning makes heavy use of the Greek alphabet to represent variable names in mathematical equations. While not all Greek characters are used, certain ones are worth being aware of. Below is a table of the Greek letters upper and lower case, as well as a guide on how to pronounce and write them. You don’t need to memorize all of these letters, but they will frequently show up in future lessons, so you may want to reference this table often until you’re comfortable with Greek letters."
  },
  {
    "objectID": "notebooks/basic-math.html#symbolic-computation",
    "href": "notebooks/basic-math.html#symbolic-computation",
    "title": "1  Basic Math",
    "section": "Symbolic Computation",
    "text": "Symbolic Computation\nThere are two fundamental ways to perform mathematical computations: numerical computation, and symbolic computation. You’re familiar with both even though you may not realize it. Numerical computation involves crunching numbers. You plug in numbers, and get out numbers. When you type something like 10.5 / 12.4 in python, it will return a number, like 0.8467741935483871. This is numerical computation.\n\n10.5 / 12.4\n\n0.8467741935483871\n\n\nThis contrasts with a way of doing computations that you learned in math class, where you manipulate symbols. This is called symbolic computation. Expanding an equation like \\((ax-b)^2\\) to get \\(a^2x^2 - 2abx + b^2\\) is an example of a symbolic computation. You see the presence of abstract variables like \\(x\\) that don’t have a set numeric value.\nUsually in practice we’re interested in numerical computations. We’ll mostly be doing that in this book. But sometimes, when working with equations, we’ll need to do symbolic computations as well. Fortunately, python has a library called SymPy, or sympy, that can do symbolic computation automatically. I won’t use it a whole lot in this book, but it will be convenient in a few places to show you that you don’t need to manipulate mathematical expressions by hand all the time.\nTo use sympy, I’ll import sympy with the alias sp. Before defining a function to operate on, we first have to encode all the symbols in the problem as sympy Symbol objects. Once that’s done, we can create equations out of them and perform mathematical operations.\nHere’s an example of using sympy to expand the equation above, \\((ax-b)^2\\).\n\nimport sympy as sp\n\n\na = sp.Symbol('a')\nb = sp.Symbol('b')\nx = sp.Symbol('x')\na, b, x\n\n(a, b, x)\n\n\n\nequation = (a * x - b) ** 2\nsp.expand(equation, x)\n\n\\(\\displaystyle a^{2} x^{2} - 2 a b x + b^{2}\\)\n\n\nWe can also use sympy to solve equations. Here’s an example of solving the quadratic equation \\(x^2 = 6\\) for its two roots, \\(x = \\pm \\sqrt{6}\\).\n\nequation = x**2 - 6\nsp.solve(equation, x)\n\n[-sqrt(6), sqrt(6)]\n\n\nSympy has a lot of functionality, and it can be a very difficult library to learn due to its often strange syntax for things. Since we won’t really need it all that often I’ll skip the in depth tutorial. See the documentation if you’re interested."
  },
  {
    "objectID": "notebooks/basic-math.html#affine-functions",
    "href": "notebooks/basic-math.html#affine-functions",
    "title": "1  Basic Math",
    "section": "Affine Functions",
    "text": "Affine Functions\nThe most basic functions to be aware of are the straight-line functions: constant functions, linear functions, and affine functions: - Constant functions \\(y=c\\) or \\(x=c\\): For example, \\(y=2\\), \\(x=1\\). - Linear functions \\(y=ax\\): For example, \\(y=-x\\), \\(y=5x\\). - Affine functions \\(y=ax+b\\): For example, \\(y=-x+1\\), \\(y=5x-4\\).\nAll constant functions are affine functions, and all linear functions are affine functions. In the case of affine functions, the value \\(b\\) is called the intercept. It corresponds to the value where the function crosses the y-axis. The value \\(a\\) is called the slope. It corresponds to the steepness of the curve, i.e. its height over its width (or “rise” over “run”). Notice linear functions are the special case where the intercept is always the origin \\(x=0, y=0\\).\n\nPlotting\nWe can plot these and any other univariate function \\(y=f(x)\\) in the usual way you learned about in school. We sample a lot of \\((x,y)\\) pairs from the function, and plot them on a grid with a horizontal x-axis and vertical y-axis.\nBefore plotting some examples I need to mention that plotting in python is usually done with the matplotlib library. Typically what we’d do to get a very simple plot is: 1. Import plt, which is the alias to the submodule matplotlib.pyplot 2. Get a grid of x values we want to plot, e.g. using np.linspace or np.arange 3. Get a grid of y values either directly, or by first defining a python function f(x) 4. Plot x vs y by calling plt.(x, y), followed by plt.show().\nNote step (2) requires another library called numpy to create the grid of points. You don’t have to use numpy for this, but it’s typically easiest. Usually numpy is imported with the alias np. Numpy is python’s main library for working with numerical arrays. We’ll cover it in much more detail in future lessons.\nLet me go ahead and load these libraries. I’ll also show a simple example of a plot. What I’ll do is define a grid x of 100 equally spaced points between -10 and 10, and plot the function \\(y=x-1\\) using the method described above.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nx = np.linspace(-10, 10, 100)\ny = x - 1\nplt.plot(x, y)\nplt.show()\n\n\n\n\nUnfortunately this plot is pretty ugly. It’s too big, arbitrarily scaled, and doesn’t include any information about what’s being plotted against what. In matplotlib if you want to include all these things to make nice plots you have to include a bunch of extra style commands.\nFor this reason, for the rest of the plotting in this lesson I’m going to use a helper function plot_function, which takes in x and y, the range of x values we want to plot, and an optional title. I didn’t think the details of this helper function were worth going into now, so I abstracted it away into the file utils.py in this same directory. It uses matplotlib like I described, but with a good bit of styling to make the plot more readable. If you really want to see the details perhaps the easiest thing to do is create a cell below using ESC-B and type the command ??plot_function, which will print the code inside the function as the output.\nBack to it, let’s plot one example each of a constant function \\(y=2\\), a linear function \\(y=2x\\), and an affine function \\(2x-1\\).\n\nx = np.arange(-10, 10, 0.1)\nf = lambda x: 2 * np.ones(len(x))\nplot_function(x, f, xlim=(-5, 5), ylim=(-5, 5), ticks_every=[1, 1], title='Constant Function: $y=2$')\n\n\n\n\n\nx = np.arange(-10, 10, 0.1)\nf = lambda x: 2 * x\nplot_function(x, f, xlim=(-5, 5), ylim=(-5, 5), ticks_every=[1, 1], title='Linear Function: $y=2x$')\n\n\n\n\n\nx = np.arange(-10, 10, 0.1)\nf = lambda x: 2 * x - 1\nplot_function(x, f, xlim=(-5, 5), ylim=(-5, 5), ticks_every=[1, 1], title='Affine Function: $y=2x-1$')\n\n\n\n\nNote the following observations from each plot. 1. The constant function is horizontal. 2. The linear function is just a rightward-sloping line that passes through the origin \\((0,0)\\) and the point \\((1,2)\\). 3. The affine function is just the same linear function shifted down by one unit of \\(y\\)."
  },
  {
    "objectID": "notebooks/basic-math.html#polynomial-functions",
    "href": "notebooks/basic-math.html#polynomial-functions",
    "title": "1  Basic Math",
    "section": "Polynomial Functions",
    "text": "Polynomial Functions\nPolynomial functions are just sums of positive integer powers of \\(x\\), e.g. something like \\(y=3x^2+5x+1\\) or \\(y=x^{10}-x^{3}+4\\). The highest power that shows up in the function is called the degree of the polynomial. For example, the above examples have degrees 2 and 10 respectively. Polynomial functions tend to look like lines, bowls, or roller coasters that turn up and down some number of times.\nA major example is the quadratic function \\(y=x^2\\), which is just an upward-shaped bowl. Its bowl-shaped curve is called a parabola. We can get a downward-shaped bowl by flipping the sign to \\(y=-x^2\\).\n\nx = np.arange(-10, 10, 0.1)\nf = lambda x: x ** 2\nplot_function(x, f, xlim=(-5, 5), ylim=(0, 10), ticks_every=[1, 1], title='Quadratic Function: $y=x^2$')\n\n\n\n\nThe next one up is the cubic function \\(y=x^3\\). The cubic looks completely different from the bowl-shaped parabola.\n\nx = np.arange(-10, 10, 0.1)\nf = lambda x: x ** 3\nplot_function(x, f, xlim=(-5, 5), ylim=(-5, 5), ticks_every=[1, 1], title='Cubic Function: $y=x^3$')\n\n\n\n\nPolynomials can take on much more interesting shapes than this. Here’s a more interesting polynomial degree 10,\n\\[y = (x^2 - 1)^5 - 5(x^2 - 1)^4 + 10(x^2 - 1)^3 - 10(x^2 - 1)^2 + 5(x^2 - 1) - 1.\\]\n\nx = np.arange(-10, 10, 0.1)\nf = lambda x: (x**2 - 1)**5 - 5 * (x**2 - 1)**4 + 10 * (x**2 - 1)**3 - 10 * (x**2 - 1)**2 + 5 * (x**2 - 1) - 1\nplot_function(x, f, xlim=(-3, 3), ylim=(-40, 40), ticks_every=[1, 10], title='Arbitrary Polynomial')"
  },
  {
    "objectID": "notebooks/basic-math.html#rational-functions",
    "href": "notebooks/basic-math.html#rational-functions",
    "title": "1  Basic Math",
    "section": "Rational Functions",
    "text": "Rational Functions\nRational functions are functions that are ratios of polynomial functions. Examples might be \\(y=\\frac{1}{x}\\), or\n\\[y=\\frac{x^3+x+1}{x^2-1}.\\]\nThese functions typically look kind of like polynomial functions, but have points where the curve “blows up” to positive or negative infinity. The points where the function blows up are called poles or asymptotes.\nHere’s a plot of the function\n\\[y=\\frac{x^3+x+1}{x^2-1}.\\]\nNotice how weird it looks. There are asymptotes (the vertical lines) where the function blows up at \\(\\pm 1\\), which is where the denominator \\(x^2-1=0\\).\n\nx = np.arange(-10, 10, 0.01)\nf = lambda x: (x ** 3 + x + 1) / (x ** 2 - 1)\nplot_function(x, f, xlim=(-5, 5), ylim=(-5, 5), ticks_every=[1, 1], title='Rational Function')\n\n\n\n\nHere’s a plot of \\(y=\\frac{1}{x}\\). There’s an asymptote at \\(x=0\\). When \\(x > 0\\) it starts at \\(+\\infty\\) and tapers down to \\(0\\) as \\(x\\) gets large. When \\(x < 0\\) it does the same thing, except flipped across the origin \\(x=y=0\\). This is an example of an odd function, a function that looks like \\(f(x)=-f(x)\\), which is clear in this case since \\(1/(-x)=-1/x\\). Functions like the linear function \\(y=x\\) and the cubic function \\(y=x^3\\) are also odd functions.\n\nx = np.arange(-10, 10, 0.1)\nf = lambda x: 1 / x\nplot_function(x, f, xlim=(-5, 5), ylim=(-5, 5), ticks_every=[1, 1], title='Odd Function: $y=1/x$')\n\n\n\n\nA related function is \\(y=\\frac{1}{|x|}\\). The difference here is that \\(|x|\\) can never be negative. This means \\(f(x)=f(-x)\\). This is called an even function. Functions like this are symmetric across the y-axis. The quadratic function \\(y=x^2\\) is also an even function.\n\nx = np.arange(-10, 10, 0.1)\nf = lambda x: 1 / np.abs(x)\nplot_function(x, f, xlim=(-5, 5), ylim=(-1, 5), ticks_every=[1, 1], title='Even Function: $y=1/|x|$')"
  },
  {
    "objectID": "notebooks/basic-math.html#power-functions",
    "href": "notebooks/basic-math.html#power-functions",
    "title": "1  Basic Math",
    "section": "Power Functions",
    "text": "Power Functions\nFunctions that look like \\(y=\\frac{1}{x^n}\\) for some \\(n\\) are sometimes called inverse, hyperbolic. These can be represented more easily by using a negative power like \\(y=x^{-n}\\), which means the exact same thing as \\(y=\\frac{1}{x^n}\\).\nWe can extend \\(n\\) to deal with things like square roots or cube roots or any kind of root as well by allowing \\(n\\) to be non-integer. For example, we can represent the square root function \\(y=\\sqrt{x}\\) as \\(y=x^{1/2}\\), and the cube root \\(y=\\sqrt[3]{x}\\) as \\(y=x^{1/3}\\). Roots like these are only defined when \\(x \\geq 0\\).\nThe general class of functions of the form \\(y=x^p\\) for some arbitrary real number \\(p\\) are often called power functions.\nHere’s a plot of what the square root function looks like. Here \\(y\\) grows slower than a linear function, but still grows arbitrarily large with \\(x\\).\n\nx = np.arange(0, 10, 0.1)\nf = lambda x: np.sqrt(x)\nplot_function(x, f, xlim=(0, 5), ylim=(-2, 4), ticks_every=[1, 1], title='Square Root: $y=\\sqrt{x}=x^{1/2}$')\n\n\n\n\nPower functions obey the following rules:\n\n\n\n\n\n\n\nRule\nExample\n\n\n\n\n\\(x^0 = 1\\)\n\\(2^0 = 1\\)\n\n\n\\(x^{m+n} = x^m x^n\\)\n\\(3^{2+5} = 3^2 3^5 = 3^8 = 6561\\)\n\n\n\\(x^{m-n} = \\frac{x^m}{x^n}\\)\n\\(3^{2-5} = \\frac{3^2}{3^5} = 3^{-3} \\approx 0.037\\)\n\n\n\\(x^{mn} = (x^m)^n\\)\n\\(2^{2 \\cdot 5} = (2^2)^5 = 2^{10} = 1024\\)\n\n\n\\((xy)^n = x^n y^n\\)\n\\((2 \\cdot 2)^3 = 2^3 2^3 = 4^3 = 2^6 = 64\\)\n\n\n\\(\\big(\\frac{x}{y}\\big)^n = \\frac{x^n}{y^n}\\)\n\\(\\big(\\frac{2}{4}\\big)^3 = \\frac{2^3}{4^3} = \\frac{1}{8}\\)\n\n\n\\(\\big(\\frac{x}{y}\\big)^{-n} = \\frac{y^n}{x^n}\\)\n\\(\\big(\\frac{2}{4}\\big)^{-3} = \\frac{4^3}{2^3} = 2^3 = 8\\)\n\n\n\\(x^{1/2} = \\sqrt{x} = \\sqrt[2]{x}\\)\n\\(4^{1/2} = \\sqrt{4} = 2\\)\n\n\n\\(x^{1/n} = \\sqrt[n]{x}\\)\n\\(3^{1/4} = \\sqrt[4]{3} \\approx 1.316\\)\n\n\n\\(x^{m/n} = \\sqrt[n]{x^m}\\)\n\\(3^{3/4} = \\sqrt[4]{3^3} = \\sqrt[4]{9} \\approx 1.732\\)\n\n\n\\(\\sqrt[n]{xy} = \\sqrt[n]{x} \\sqrt[n]{y}\\)\n\\(\\sqrt[4]{3 \\cdot 2} = \\sqrt[4]{3} \\sqrt[4]{2} \\approx 1.565\\)\n\n\n\\(\\sqrt[n]{\\frac{x}{y}} = \\frac{\\sqrt[n]{x}}{\\sqrt[n]{y}}\\)\n\\(\\sqrt[4]{\\frac{3}{2}} = \\frac{\\sqrt[4]{3}}{\\sqrt[4]{2}} \\approx 1.107\\)\n\n\n\nIt’s important to remember that power functions do not distribute over addition, i.e.\n\\[(x+y)^n \\neq x^n + y^n,\\]\nand by extension nor do roots,\n\\[\\sqrt[n]{x+y} \\neq \\sqrt[n]{x} + \\sqrt[n]{y}.\\]"
  },
  {
    "objectID": "notebooks/basic-math.html#exponentials-and-logarithms",
    "href": "notebooks/basic-math.html#exponentials-and-logarithms",
    "title": "1  Basic Math",
    "section": "Exponentials and Logarithms",
    "text": "Exponentials and Logarithms\nTwo very important functions are the exponential function \\(y=\\exp(x)\\) and the logarithm function \\(y=\\log(x)\\). They show up surprisingly often in machine learning and the sciences, certainly more than most other special functions do.\nThe exponential function can be written as a power by defining a number \\(e\\) called Euler’s number, given by \\(e = 2.71828\\dots\\) . Like \\(\\pi\\), \\(e\\) is an example of an irrational number, i.e. a number that can’t be represented as a ratio of integers. Using \\(e\\), we can write the exponential function in the more usual form \\(y=e^x\\), where it’s roughly speaking understood that we mean “multiply \\(e\\) by itself \\(x\\) times”. For example, \\(\\exp(2) = e^2 = e \\cdot e\\).\nThe logarithm is defined as the inverse of the exponential function. It’s the unique function satisfying \\(\\log(\\exp(x)) = x\\). The opposite is also true since the exponential must then be the inverse of the logarithm function, \\(\\exp(\\log(x)) = x\\). This gives a way of mapping between the two functions,\n\\[\\log(a) = b \\quad \\Longleftrightarrow \\quad \\exp(b) = a.\\]\nHere are some plots of what the exponential and logarithm functions look like. The exponential function is a function that blows up very, very quickly. The log function grows very, very slowly (much more slowly than the square root does).\nNote the log function is only defined for positive-valued numbers \\(x \\geq 0\\), with \\(\\log(+0)=-\\infty\\). This is dual to the exponential function only taking on \\(y \\geq 0\\).\n\nx = np.arange(-5, 5, 0.1)\nf = lambda x: np.exp(x)\nplot_function(x, f, xlim=(-5, 5), ylim=(-1, 10), ticks_every=[1, 2], title='Exponential Function: $y=\\exp(x)$')\n\n\n\n\n\nx = np.arange(0.01, 5, 0.1)\nf = lambda x: np.log(x)\nplot_function(x, f, xlim=(-1, 5), ylim=(-5, 2), ticks_every=[1, 1], title='Logarithm Function: $y=\\log(x)$')\n\n\n\n\nThe exponential and logarithm functions I defined are the “natural” way to define these functions. We can also have exponential functions in other bases, \\(y=a^x\\) for any positive number \\(a\\). Each \\(a\\) has an equivalent logarithm, written \\(y = \\log_{a}(x)\\). The two functions \\(y=a^x\\) and \\(y=\\log_{a}(x)\\) are inverses of each other. When I leave off the \\(a\\), it’s assumed that all logs are the natural base \\(a=e\\), sometimes also written \\(\\ln(x)\\).\nTwo common examples of other bases that show up sometimes are the base-2 functions \\(2^x\\) and \\(\\log_{2}(x)\\), and the base-10 functions \\(10^x\\) and \\(\\log_{10}(x)\\). Base-2 functions in particular show up often in computer science because of the tendency to think in bits. Base-10 functions show up when we want to think about how many digits a number has.\nHere are some rules that exponentials and logs obey:\n\n\n\n\n\n\n\nRule\nExample\n\n\n\n\n\\(e^0 = 1\\)\n\n\n\n\\(\\log(1) = 0\\)\n\n\n\n\\(\\log(e) = 1\\)\n\n\n\n\\(e^{a+b} = e^a e^b\\)\n\\(e^{2+5} = e^2 e^5 = e^8 \\approx 2980.96\\)\n\n\n\\(e^{a-b} = \\frac{e^a}{e^b}\\)\n\\(e^{2-5} = \\frac{e^2}{e^5} = e^{-3} \\approx 0.0498\\)\n\n\n\\(e^{ab} = (e^a)^b\\)\n\\(e^{2 \\cdot 5} = (e^2)^5 = e^{10} \\approx 22026.47\\)\n\n\n\\(a^b = e^{b \\log(a)}\\)\n\\(2^3 = e^{3 \\log(2)} = 8\\)\n\n\n\\(\\log(ab) = \\log(a) + \\log(b)\\)\n\\(\\log(2 \\cdot 5) = \\log(2) + \\log(5) = \\log(10) \\approx 2.303\\)\n\n\n\\(\\log\\big(\\frac{a}{b}\\big) = \\log(a) - \\log(b)\\)\n\\(\\log\\big(\\frac{2}{5}\\big) = \\log(2) - \\log(5) \\approx -0.916\\)\n\n\n\\(\\log(a^b) = b\\log(a)\\)\n\\(\\log(5^2) = 2\\log(5) \\approx 3.219\\)\n\n\n\\(\\log_a(x) = \\frac{\\log(x)}{\\log(a)}\\)\n\\(\\log_2(5) = \\frac{\\log(5)}{\\log(2)} \\approx 2.322\\)\n\n\n\nHere’s an example of an equation involving exponentials and logs. Suppose you have \\(n\\) bits of numbers (perhaps it’s the precision in some float) and you want to know how many digits this number takes up in decimal form (what you’re used to). This would be equivalent to solving the following equation for \\(x\\),\n\\[\\begin{align}\n2^n &= 10^{x} \\\\\n\\log(2^n) &= \\log(10^{x}) \\\\\nn\\log(2) &= x\\log(10) \\\\\nx &= \\frac{\\log(2)}{\\log(10)} \\cdot n \\\\\nx &\\approx 0.3 \\cdot n.\n\\end{align}\\]\nFor example, you can use this formula to show that 52 bits of floating point precision translates to about 15 to 16 digits of precision. In numpy, the function np.log function calculates the (base-\\(e\\)) log of a number.\n\nn = 52\nx = np.log(2) / np.log(10) * n\nx\n\n15.65355977452702"
  },
  {
    "objectID": "notebooks/basic-math.html#trigonometric-functions",
    "href": "notebooks/basic-math.html#trigonometric-functions",
    "title": "1  Basic Math",
    "section": "Trigonometric Functions",
    "text": "Trigonometric Functions\nOther textbook functions typically covered in math courses are the trig functions: sine, cosine, tangent, cosine, cosecant, and cotangent. Of these functions, the most important to know are the sine function \\(y=\\sin x\\), and the cosine function \\(y = \\cos x\\).\nHere’s what their plots look like. They’re both waves that repeat themselves, in the sense \\(f(x + 2\\pi) = f(x)\\). The length for the function to repeat itself is called the period, in this case \\(2\\pi \\approx 6.28\\). Note that the cosine is just a sine function that’s shifted right by \\(\\frac{\\pi}{2} \\approx 1.57\\).\n\nx = np.arange(-10, 10, 0.1)\nf = lambda x: np.sin(x)\nplot_function(x, f, xlim=(-6, 6), ylim=(-2, 2),  ticks_every=[1, 0.5], title='Sine Function: $y=\\sin(x)$')\n\n\n\n\n\nx = np.arange(-10, 10, 0.1)\nf = lambda x: np.cos(x)\nplot_function(x, f, xlim=(-6, 6), ylim=(-2, 2), ticks_every=[1, 0.5], title='Cosine Function: $y=\\cos(x)$')\n\n\n\n\nTrig functions don’t really show up that much in machine learning, so I won’t remind you of all those obscure trig rules you’ve forgotten. I’ll just mention that we can define all the other trig functions using the sine and cosine as follows,\n\\[\\begin{align}\n&\\tan x = \\frac{\\sin x}{\\cos x}, \\\\\n&\\csc x = \\frac{1}{\\sin x}, \\\\\n&\\sec x = \\frac{1}{\\cos x}, \\\\\n&\\cot x = \\frac{1}{\\tan x} = \\frac{\\cos x}{\\sin x}.\n\\end{align}\\]"
  },
  {
    "objectID": "notebooks/basic-math.html#piecewise-functions",
    "href": "notebooks/basic-math.html#piecewise-functions",
    "title": "1  Basic Math",
    "section": "Piecewise Functions",
    "text": "Piecewise Functions\nThe functions covered so far are examples of continuous functions. Their graphs don’t have jumps or holes in them anywhere. Continuous functions we can often write using a single equation, like \\(y=x^2\\) or \\(y=1 + \\sin(x)\\). We can also have functions that require more than one equation to write. These are called piecewise functions. Piecewise functions usually aren’t continuous, but sometimes can be.\nAn example of a discontinuous piecewise function is the unit step function \\(y=u(x)\\) given by\n\\[\ny =\n\\begin{cases}\n0 & x < 0, \\\\\n1 & x \\geq 0.\n\\end{cases}\n\\]\nThis expression means \\(y=0\\) whenever \\(x < 0\\), but \\(y=1\\) whenever \\(x \\geq 0\\). It breaks up into two pieces, one horizontal line \\(y=0\\) when \\(x\\) is negative, and another horizontal line \\(y=1\\) when \\(x\\) is positive.\nUsing Boolean expressions, we can also write this function in a more economical way by agreeing to identify \\(x=1\\) with \\(\\text{TRUE}\\) and \\(x=0\\) with \\(\\text{FALSE}\\), which python does by default. In this notation, we can write\n\\[u(x) = [x \\geq 0],\\]\nwhich means exactly the same thing as the piecewise definition, since \\(x \\geq 0\\) is only true when (you guessed it), \\(x \\geq 0\\).\nHere’s a plot of this function. Note the discontinuous jump at \\(x=0\\).\n\nx = np.arange(-10, 10, 0.01)\nf = lambda x:  (x >= 0)\nplot_function(x, f, xlim=(-3, 3), ylim=(-1, 2), ticks_every=[1, 0.5], title='Unit Step Function: $y=u(x)$')\n\n\n\n\nAn example of a piecewise function that’s continuous is the ramp function, defined by\n\\[\ny =\n\\begin{cases}\n0 & x < 0, \\\\\nx & x \\geq 0.\n\\end{cases}\n\\]\nThis function gives a horizontal line \\(y=0\\) when \\(x\\) is negative, and a \\(45^\\circ\\) line \\(y=x\\) when \\(x\\) is positive. Both lines connect at \\(x=0\\), but leave a kink in the graph.\nAnother way to write the same thing using Boolean expressions is \\(y = x \\cdot [x \\geq 0]\\), which is of course just \\(y = x \\cdot u(x)\\).\nIn machine learning it’s more common to write the ramp function using the \\(\\max\\) function as \\(y = \\max(0,x)\\). This means, for each \\(x\\), take that value and compare it with \\(0\\), and take the maximum of those two. That is, if \\(x\\) is negative take \\(y=0\\), otherwise take \\(y=x\\). It’s also more common to call this function a rectified linear unit, or ReLU for short. It’s an ugly, unintuitive name, but unfortunately it’s stuck in the field.\nHere’s a plot of the ramp or ReLU function. Notice how it stays at \\(y=0\\) for a while, then suddenly “ramps upward” at \\(x=0\\).\n\nx = np.arange(-10, 10, 0.1)\nf = lambda x:  x * (x >= 0)\nplot_function(x, f, xlim=(-3, 3), ylim=(-3, 3), ticks_every=[1, 1], title='ReLU Function')\n\n\n\n\nLast, I’ll mention here the absolute value function \\(y = |x|\\), defined by the piecewise function\n\\[\ny = \\begin{cases}\nx & \\text{if } x \\ge 0 \\\\\n-x & \\text{if } x < 0.\n\\end{cases}\n\\]\nThe absolute value just ignores negative signs and makes everything positive. The function looks like the usual line \\(y=x\\) when positive, but like the negative-sloped line \\(y=-x\\) when negative. At \\(x=0\\) the two lines meet, creating a distinctive v-shape. To get the absolute value function in python, use abs or np.abs.\n\nx = np.arange(-5, 5, 0.1)\nf = lambda x: abs(x)\nplot_function(x, f, xlim=(-5, 5), ylim=(0, 5), ticks_every=[1, 1], title='Absolute Value Function: $y=|x|$')"
  },
  {
    "objectID": "notebooks/basic-math.html#composite-functions",
    "href": "notebooks/basic-math.html#composite-functions",
    "title": "1  Basic Math",
    "section": "Composite Functions",
    "text": "Composite Functions\nWe can also have any arbitrary hybrid of the above functions. We can apply exponentials to affine functions, logs to sine functions, sines to exponential functions. In essence, this kind of layered composition of functions is what a neural network is as we’ll see later on.\nMath folks often write an abstract compositional function as a function applied to another function, like \\(y=f(g(x))\\) or \\(y=(f \\circ g)(x)\\). These can be chained arbitrarily many times, not just two. Neural networks do just that, often hundreds or thousands of times.\nConsider, for example, the function composition done by applying the following functions in sequence: - an affine function \\(y=wx+b\\) - followed by a linear function \\(y=-x\\) - followed by an exponential function \\(y=e^x\\) - followed by a rational function \\(y=\\frac{1}{x}\\)\nto get the full function \\[y = \\frac{1}{1 + e^{-(wx+b)}}.\\]\nHere’s a plot of what this function looks like for the “standard form” where \\(w=1, b=0\\). Notice that \\(0 \\leq y \\leq 1\\). The values of \\(x\\) get “squashed” to values between 0 and 1 after the function is applied.\n\nx = np.arange(-10, 10, 0.1)\nf = lambda x:  1 / (1 + np.exp(-x))\nplot_function(x, f, xlim=(-6, 6), ylim=(-0.2, 1.2), ticks_every=[2, 0.2], title='Sigmoid Function')\n\n\n\n\nThis function is called the sigmoid function. The sigmoid is very important in machine learning since it in essence creates probabilities. We’ll see it a lot more. The standard form sigmoid function, usually written\n\\(\\sigma(x)\\), is given by \\[\\sigma(x) = \\frac{1}{1 + e^{-x}}.\\]\nArbitrary affine transformations of the standard form would then be written as \\(\\sigma(wx+b)\\).\nA similar looking function shows up sometimes as well called the hyperbolic tangent or tanh function, which has the (standard) form\n\\[\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}.\\]\nThe tanh function looks pretty much the same as the sigmoid except it’s rescaled vertically so that \\(-1 \\leq y \\leq 1\\).\nHere’s a plot of the tanh function. Notice how similar it looks to the sigmoid with the exception of the scale of the y-axis.\n\nx = np.arange(-10, 10, 0.1)\nf = lambda x: (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\nplot_function(x, f, xlim=(-5, 5), ylim=(-2, 2), ticks_every=[1, 0.5], title='Tanh Function')"
  },
  {
    "objectID": "notebooks/basic-math.html#function-transformations",
    "href": "notebooks/basic-math.html#function-transformations",
    "title": "1  Basic Math",
    "section": "Function Transformations",
    "text": "Function Transformations\nSuppose we have some arbitrary function \\(f(x)\\) and we apply a series of compositions to get a new function \\[g(x)=a \\cdot f(b \\cdot (x + c)) + d.\\] We can regard each parameter \\(a,b,c,d\\) as doing some kind of geometric transformation to the graph of the original function \\(f(x)\\). Namely, - \\(a\\) re-scales the function vertically (if \\(a\\) is negative it also flips \\(f(x)\\) upside down) - \\(b\\) re-scales the function horizontally (if \\(b\\) is negative it also flips \\(f(x)\\) left to right) - \\(c\\) shifts the function horizontally (left if \\(c\\) is positive, right if \\(c\\) is negative) - \\(d\\) shifts the function vertically (up if \\(d\\) is positive, down if \\(d\\) is negative)\nHere’s an example of how these work. Consider the function \\(f(x)=x^2\\). We’re going to apply each of these transformations one by one to show what they do to the graph of \\(f(x)\\).\nFirst, let’s look at the transformation \\(g(x) = \\frac{1}{2} f(x) = \\frac{1}{2} x^2\\). Here \\(a=\\frac{1}{2}\\) and the rest are zero. I’ll plot it along side the original graph (the blue curve). Notice the graph gets flattened vertically by a factor of two (the orange curve).\n\nx = np.arange(-10, 10, 0.1)\nf = lambda x: x ** 2\n\n\na = 1 / 2\ng = lambda x: a * x ** 2\nplot_function(x, [f, g], xlim=(-3, 3), ylim=(-2, 10), ticks_every=[1, 2], title=f'$a={a}$')\n\n\n\n\nNow consider at the transformation\n\\[g(x) = f\\big(\\frac{1}{2} x\\big) = \\bigg(\\frac{1}{2} x \\bigg)^2.\\]\nHere \\(b=\\frac{1}{2}\\) and the rest are zero. Notice the graph again gets flattened but in a slightly different way.\n\nb = 1 / 2\ng = lambda x: (b * x) ** 2\nplot_function(x, [f, g], xlim=(-3, 3), ylim=(-2, 10), ticks_every=[1, 2], title=f'$b={b}$')\n\n\n\n\nNext, consider the transformation \\(g(x) = f(x-1) = (x-1)^2.\\) Here \\(c=1\\) and the rest are zero. Notice the graph’s shape doesn’t change. It just gets shifted right by \\(c=1\\) since \\(c\\) is negative.\n\nc = -1\ng = lambda x: (x + c) ** 2\nplot_function(x, [f, g], xlim=(-3, 3), ylim=(-7, 7), ticks_every=[1, 2], title=f'$c={c}$')\n\n\n\n\nFinally, let’s look at the transformation \\(g(x) = f(x) + 2 = x^2 + 2\\). Here \\(d=2\\) and the rest are zero. Notice again the graph’s shape doesn’t change. It just gets shifted up by \\(d=2\\) units.\n\nd = 2\ng = lambda x: x ** 2 + d\nplot_function(x, [f, g], xlim=(-3, 3), ylim=(-1, 8), ticks_every=[1, 2], title=f'$d={d}$')\n\n\n\n\nLet’s now put them all together to see what happens. We should see rescaling in both directions and shifts in both directions. It’s hard to see in the plot, but it’s all there if you zoom in. The vertex of the parabola is at the point \\(x=c=1, y=d=2\\). And the stretching factors due to \\(a=b=1/2\\) are both acting to flatten the parabola.\n\ng = lambda x: a * (b * (x + c)) ** 2 + d\nplot_function(x, [f, g], xlim=(-8, 8), ylim=(-2, 10), ticks_every=[2, 2], title=f'$a={a}, b={b}, c={c}, d={d}$')"
  },
  {
    "objectID": "notebooks/numerical-computing.html",
    "href": "notebooks/numerical-computing.html",
    "title": "2  Numerical Computation",
    "section": "",
    "text": "Integers"
  },
  {
    "objectID": "notebooks/numerical-computing.html#basics",
    "href": "notebooks/numerical-computing.html#basics",
    "title": "2  Numerical Computation",
    "section": "Basics",
    "text": "Basics\nRecall the integers are whole numbers that can be positive, negative, or zero. Examples are 5, 100151, 0, -72, etc. The set of all integers is commonly denoted by the symbol \\(\\mathbb{Z}\\).\nIn python, integers (ints for short) are builtin objects of type int that more or less follow the rules that integers in math follow.\nAmong other things, the following operations can be performed with integers: - Addition: \\(2 + 2 = 4\\). - Subtraction: \\(2 - 5 = -3\\). - Multiplication: \\(3 \\times 3 = 9\\) (in python this is the * operator, e.g. 3 * 3 = 9) - Exponentiation: \\(2^3 = 2 \\times 2 \\times 2 = 8\\) (in python this is the ** operator, e.g. 2 ** 3 = 8). - Remainder (or Modulo): the remainder of 10 when divided by 3 is 1, written \\(10 \\text{ mod } 3 = 1\\) (in python this is the % operator, e.g. 10 % 3 = 1).\nIf any of these operations are applied to two integers, the output will itself always be an integer.\nHere are a few examples.\n\n2 + 2\n2 - 5\n3 * 3\n10 % 3\n2 ** 3\n\n4\n\n\n-3\n\n\n9\n\n\n1\n\n\n8\n\n\nWhat about division? You can’t always divide two integers and get another integer. What you have to do instead is called integer division. Here you divide the two numbers and then round the answer down to the nearest whole number. Since \\(5 \\div 2 = 2.5\\), the nearest rounded down integer is 2.\nIn math, this “nearest rounded down integer” 2 is usually called the floor of 2.5, and represented with the funny symbol \\(\\lfloor 2.5 \\rfloor.\\) Using this notation we can write the above integer division as \\[\\big\\lfloor \\frac{5}{2} \\big\\rfloor = 2.\\]\nIn python, integer division is done using the // operator, e.g. 5 // 2 = 2. I’ll usually write \\(5 \\ // \\ 2\\) instead of \\(\\big\\lfloor \\frac{5}{2} \\big\\rfloor\\) when it makes sense, \\[5 \\ // \\ 2 = \\big\\lfloor \\frac{5}{2} \\big\\rfloor = 2.\\]\n\n5 // 2\n\n2\n\n\nWe can also do regular division / with ints, but the output will not be an integer even if the answer should be, e.g. 4 / 2. Only integer division is guaranteed to return an integer. I’ll get to this shortly.\n\n4 / 2\ntype(4 / 2)\n\n2.0\n\n\nfloat\n\n\n\n4 // 2\ntype (4 // 2)\n\n2\n\n\nint\n\n\nDivision by zero is of course undefined for both division and integer division. In python it will always raise a ZeroDivisionError like so.\n\n4 / 0\n\nZeroDivisionError: division by zero\n\n\n\n4 // 0\n\nZeroDivisionError: integer division or modulo by zero"
  },
  {
    "objectID": "notebooks/numerical-computing.html#representing-integers",
    "href": "notebooks/numerical-computing.html#representing-integers",
    "title": "2  Numerical Computation",
    "section": "Representing Integers",
    "text": "Representing Integers\nJust like every other data type, on a computer integers are actually represented internally as a sequence of bits. A bit is a “binary digit”, 0 or 1. A sequence of bits is just a sequence of zeros and ones, e.g. 0011001010 or 1001001.\nThe number of bits used to represent a piece of data is called its word size. If we use a word size of \\(n\\) bits to represent an integer, then there are \\(2^n\\) possible integer values we can represent.\nIf integers could only be positive or zero, representing them with bits would be easy. We could just convert them to binary and that’s it. To convert a non-negative integer to binary, we just need to keep dividing it by 2 and recording its remainder (0 or 1) at each step. The binary form is then just the sequence of remainders, written right to left. More generally, the binary sequence of some arbitrary number \\(x\\) is the sequence of coefficients \\(b_k=0,1\\) in the sum\n\\[x = \\sum_{k=-\\infty}^\\infty b_k 2^k = \\cdots + b_2 2^2 + b_1 2^1 + b_0 2^0 + b_{-1} 2^{-1} + b_{-2} 2^{-2} + \\cdots.\\]\nHere’s an example. Suppose we wanted to represent the number \\(12\\) in binary. 1. \\(12 \\ // \\ 2 = 6\\) with a remainder of \\(0 = 12 \\text{ mod } 2\\), so the first bit from the right is then \\(0\\). 2. \\(6 \\ // \\ 2 = 3\\) with a remainder of \\(0 = 6 \\text{ mod } 2\\), so the next bit is \\(0\\). 3. \\(3 \\ // \\ 2 = 1\\) with a remainder of \\(1 = 3 \\text{ mod } 2\\), so the next bit is \\(1\\). 4. \\(1 \\ // \\ 2 = 0\\) with a remainder of \\(1 = 1 \\text{ mod } 2\\), so the next bit is \\(1\\).\nSo the binary representation of \\(12\\) is \\(1100\\), which is the sequence of coefficients in the sum\n\\[12 = 1 \\cdot 2^{3} + 1 \\cdot 2^{2} + 0 \\cdot 2^{1} + 0 \\cdot 2^{0}.\\]\nRather than keep doing these by hand, you can quickly convert a number to binary in python by using bin. It’ll return a string representing the binary sequence of that number, prepended with the special prefix 0b. To get back to the integer from, use int, passing in a base of 2.\n\nbin(12)\n\n'0b1100'\n\n\n\nint('0b110', 2)\n\n6\n\n\nThis representation works fine for non-negative integers, also called the unsigned integers in computer science. To represent an unsigned integer with \\(n\\) bits, just get its binary form and prepend it with enough zeros on the left until all \\(n\\) bits are used. For example, if we used 8-bit unsigned integers then \\(n=8\\), hence representing the number \\(12\\) would look like \\(00000110\\). Simple, right?\nUnsigned ints work fine if we never have to worry about negative numbers. But in general we do. These are called the signed integers in computer science. To represent signed ints, we need to use one of the bits to represent the sign. What we can do is reserve the left-most bit for the sign, \\(0\\) if the integers is positive or zero, \\(1\\) if the integer is negative.\nFor example, if we used 8-bit signed integers to represent \\(12\\), we’d again write \\(00000110\\), exactly as before. But this time it’s understood that left-most \\(0\\) is encoding the fact that \\(12\\) is positive. If instead we wanted to represent the number \\(-12\\) we’d need to flip that bit to a \\(1\\), so we’d get \\(10000110\\).\nLet’s now do an example of a simple integer system. Consider the system of 4-bit signed ints. In this simple system, \\(n=4\\) is the word size, and an integer \\(x\\) is represented with the sequence of bits\n\\[x \\equiv sb_1b_2b_3,\\]\nwhere \\(s\\) is the sign bit and \\(b_1b_2b_3\\) are the remaining 3 bits allowed to represent the numerical digits. This system can represent \\(2^4=16\\) possible values in the range \\([-2^3+1,2^3-1] = [-8,7]\\), given in the following table:\n\n\n\nInteger\nRepresentation\nInteger\nRepresentation\n\n\n\n\n-0\n1000\n+0\n0000\n\n\n-1\n1001\n1\n0001\n\n\n-2\n1010\n2\n0010\n\n\n-3\n1011\n3\n0011\n\n\n-4\n1100\n4\n0100\n\n\n-5\n1101\n5\n0101\n\n\n-6\n1110\n6\n0110\n\n\n-7\n1111\n7\n0111\n\n\n\nNote the presence of \\(-0 \\equiv 1110\\) in the upper left. This is because the system as I’ve defined it leaves open the possibility of two zeros, \\(+0\\) and \\(-0\\), since for zero the sign bit is redundant. A way to get around this is to encode the negative numbers slightly differently, by not just setting the sign bit to one, but also inverting the remaining bits and subtracting one from them. This is called the two’s complement representation. It’s how most languages, including python, actually represent integers. I won’t go into this representation in any depth, except to say that it gets rid of the need for \\(-0\\) and replaces it with \\(-2^{n-1}\\).\nHere’s what that table looks like for 4-bit integers. It’s almost the same, except there’s no \\(-0\\), instead a \\(-8\\). Notice the positive integers look exactly the same. It’s only the negative integers that look different. For them, the right three bits get inverted and added with a one.\n\n\n\nInteger\nTwo’s Complement\nInteger\nTwo’s Complement\n\n\n\n\n-1\n1111\n0\n0000\n\n\n-2\n1110\n1\n0001\n\n\n-3\n1101\n2\n0010\n\n\n-4\n1100\n3\n0011\n\n\n-5\n1011\n4\n0100\n\n\n-6\n1010\n5\n0101\n\n\n-7\n1001\n6\n0110\n\n\n-8\n1000\n7\n0111\n\n\n\nIt’s worth visualizing what integers look like on the number line, if for nothing else than to compare it with what floats look like later on. Below I’ll plot what a 6-bit signed integer system would look like. Such a system should go from -32 to 31. I’ll to use the helper function plot_number_dist to do the plotting. As you’d expect, you just see a bunch of equally spaced points from -32 to 31.\n\nn = 6\nsix_bit_ints = range(-2**(n-1), 2**(n-1))\nplot_number_dist(six_bit_ints, title=f'Distribution of {n}-bit Signed Ints')\n\n\n\n\nIn python, integers are represented by default using a much bigger word size of \\(n=64\\) bits, called long integers, or int64 for short. This means (using two’s complement) we can represent \\(2^{64}=18446744073709551616\\) possible integer values in the range \\([-2^{63}, 2^{63}-1]\\).\nYou can see from this that 64-bit integers have a minimum integer allowed and a maximum integer allowed, which are\n\\[\\text{min\\_int}=-2^{63}=-9223372036854775808, \\qquad \\text{max\\_int}=2^{63}-1=9223372036854775807.\\]\nWhat I’ve said is technically only exactly true in older versions of pythons as well as other programming languages like C. It turns out newer versions of python have a few added tricks that allow you to represent essentially arbitrarily large integers. You can see this by comparing it to numpy’s internal int64 representation, which uses the C version. A numpy int64 outside the valid range will throw an overflow error.\n\nmin_int = -2 ** 63\nmax_int = 2 ** 63 - 1\n\n\nmin_int - 1\nnp.int64(min_int - 1)\n\n-9223372036854775809\n\n\nOverflowError: Python int too large to convert to C long\n\n\n\nmax_int + 1\nnp.int64(max_int + 1)\n\n9223372036854775808\n\n\nOverflowError: Python int too large to convert to C long"
  },
  {
    "objectID": "notebooks/numerical-computing.html#basics-1",
    "href": "notebooks/numerical-computing.html#basics-1",
    "title": "2  Numerical Computation",
    "section": "Basics",
    "text": "Basics\nWhat if we want to represent decimal numbers or fractions instead of whole numbers, like \\(1.2\\) or \\(0.99999\\), or even irrational numbers like \\(\\pi=3.1415926\\dots\\)? To do this we need a new system of numbers that I’ll call floating point numbers, or floats, for reasons I’ll explain soon. Floats will be a computer’s best attempt to represent the real numbers \\(\\mathbb{R}\\). They’ll represent real numbers only approximately with some specified precision.\nIn python, floats are builtin objects of type float. Floats obey pretty much the same operations that integers do with some minor exceptions: - Addition: \\(1.2 + 4.3 = 5.5\\). - Subtraction: \\(1.2 - 4.3 = -3.1\\). - Multiplication: \\(1.2 \\times 4.3 = 5.16\\). - Exponentiation: \\(4.3^2 = 18.49\\). - Remainder (or Modulo): \\(4.3 \\text{ mod } 1.2 = 0.7\\). - Integer Division: \\(4.3 \\ // \\ 1.2 = 3.0\\). - Division: \\(4.3 \\div 1.2\\).\nI’ll print these out in python to verify the answers are correct.\n\n1.2 + 4.3\n1.2 - 4.3\n1.2 * 4.3\n4.3 ** 2\n4.3 % 1.2\n4.3 // 1.2\n4.3 / 1.2\n\n5.5\n\n\n-3.0999999999999996\n\n\n5.159999999999999\n\n\n18.49\n\n\n0.7\n\n\n3.0\n\n\n3.5833333333333335\n\n\nMost of them look right. But what the heck is going on with \\(1.2 - 4.3\\) and \\(1.2 \\times 4.3\\)? We’re getting some weird trailing nines that shouldn’t be there. This gets to how floats are actually represented on a computer."
  },
  {
    "objectID": "notebooks/numerical-computing.html#representing-floats",
    "href": "notebooks/numerical-computing.html#representing-floats",
    "title": "2  Numerical Computation",
    "section": "Representing Floats",
    "text": "Representing Floats\nRepresenting real numbers on a computer is a lot more subtle than representing integers. Since a computer can only have a finite number of bits, they can’t represent infinitely many digits, e.g. in irrational numbers like \\(\\pi\\). Using finite word sizes will necessarily have to truncate real numbers to some number of decimal places. This truncation will create an error in the calculation called numerical roundoff.\nSo how should we represent a decimal number using \\(n\\) bits? As an example, let’s imagine we’re trying to represent the number \\(x=157.208\\). Perhaps the first thing you might think of is to use some number of those bits to represent the integer part, and some number to represent the fractional part. Suppose you have \\(n=16\\) bits available to represent \\(x\\). Then maybe you can use 8 bits for the integer part \\(157\\), and 8 bits for the fractional part \\(0.208\\). Converting both halves to binary, you’d get \\[157 \\equiv 10011101, \\quad 0.208 \\equiv 0011010100111111.\\]\nTruncating both sequences to 8 bits (from the left), you could thus adopt a convention that \\(157.208 \\equiv 10011101 \\ 00110101\\).\nThis system is an example of a fixed point representation. This has to do with the fact that we’re always using a fixed number of bits for the integer part, and a fixed number for the fractional part. The decimal point isn’t allowed to float, or move around to allocate more bits to the integer or fractional part depending which needs more precision. The decimal point is fixed.\nAs I’ve suggested, the fixed point representation seems to be limited and not terribly useful. If you need really high precision in the fractional part, your only option is to use a larger word size. If you’re dealing with really big numbers and don’t care much about the fractional part, you also need a larger word size so you don’t run out of numbers. A solution to this problem is to allow the decimal point to float. We won’t allocate a fixed number of bits to represent the integer or fractional parts. We’ll design it in such a way that larger numbers give the integer part more bits, and smaller numbers give the fractional part more bits.\nThe trick to allowing the decimal point to float is to represent not just the digits of a number but also its exponent. Think about scientific notation, where if you have a number like say \\(x=1015.23\\), you can write it as \\(1.01523 \\cdot 10^3\\), or 1.01523e3. That \\(3\\) is the exponent. It says something about how big the number is. What we can do is convert a number to scientific notation. Then use some number of bits to represent the exponent \\(3\\) and some to represent the remaining part \\(1.01523\\). This is essentially the whole idea behind floating point.\nIn floating point representation, instead of using scientific notation with powers of ten, it’s more typical to use powers of two. When using powers of two, the decimal part can always be scaled to be between 1 and 2, so they look like \\(1.567\\) or something like that. Since the \\(1.\\) part is always there, we can agree it’s always there, and only worry about representing the fractional part \\(0.567\\). We’ll call this term the mantissa. Denoting the sign bit as \\(s\\), the exponent as \\(e\\), and the mantissa as \\(m\\), we can thus right any decimal number \\(x\\) in a modified scientific notation of the form \\[x = (-1)^s \\cdot (1+m) \\cdot 2^{e}.\\] Once we’ve converted \\(x\\) to this form, all we need to do is to figure out how to represent \\(s\\), \\(m\\), and \\(e\\) using some number of bits of \\(n\\), called the floating point precision. Assume the \\(n\\) bits of precision allocate \\(1\\) bit for the sign, \\(n_e\\) bits for the exponent, and \\(n_m\\) bits for the mantissa, so \\(n=1+n_e+n_m\\).\nHere are the steps to convert a number \\(x\\) into its \\(n\\)-bit floating point representation. - Given some number \\(x\\), get its modified scientific notation form \\(x = (-1)^s \\cdot (1+m) \\cdot 2^e\\). - Determine the sign of \\(x\\). If negative, set the sign bit to \\(s=1\\), else default to \\(s=0\\). Set \\(x = |x|\\). - Keep performing the operation \\(x = x \\ // \\ 2\\) until \\(1 \\leq x \\leq 2\\). Keep track of the number of times you’re dividing, which is the exponent \\(e\\). - The remaining part will be some \\(1 \\leq x \\leq 2\\). Write it in the form \\(x = 1 + m\\), where \\(m\\) is the mantissa. - Convert the scientific notation form into a sequence of \\(n\\) bits, truncating where necessary. - For reasons I’ll describe in a second, it’s good to add a bias term \\(b\\) to the exponent \\(e\\) before converting the exponent to binary. Let \\(e'=e+b\\) be this modified exponent. - Convert each of \\(e'\\) and \\(m\\) into binary sequences, truncated to sizes \\(n_e\\), and \\(n_m\\) respectively. - Concatenate these binary sequences together to get a sequence of \\(n=1+n_e+n_m\\) total bits. By convention, assume the order of bit concatenation is the sign bit, then exponent bits, then the mantissa bits.\nThere are of course other ways you could do it, for example by storing the sequences in a different order. I’m just stating one common way it’s done.\nSince all of this must seem like Greek, here’s a quick example. Let’s consider the number \\(x=15.25\\). We’ll represent it using \\(n=8\\) bits of precision, where \\(n_e=4\\) is the number of exponent bits, \\(n_m=3\\) is the number of precision bits, and \\(b=10\\) is the bias. - Convert \\(x=15.25\\) to its modified scientific notation. - Since \\(x \\geq 0\\) the sign is positive, so \\(s=0\\). - Keep integer dividing \\(x\\) by \\(2\\) until it’s less than \\(2\\). It takes \\(e=3\\) divisions before \\(x<2\\). - We now have \\(x = 1.90625 \\cdot 2^3\\). The mantissa is then \\(m = (1.90625-1) = 0.90625\\). - In modified scientific notation form we now have \\(x=(-1)^0 \\cdot (1 + 0.90625) \\cdot 2^3\\). - Convert everything to binary. - Adding the bias to the exponent gives \\(e'=3+10=13\\). - Converting each piece to binary we get \\(e' = 13 \\equiv 1101\\), \\(m = 0.90625 \\equiv 11101\\). - Since \\(m\\) requires more than \\(n_m=3\\) bits to represent, truncate off the two right bits to get \\(m \\equiv 111\\). - This truncation will cause numerical roundoff, since \\(0.90625\\) truncates to \\(0.875\\). That’s an error of \\(0.03125\\) that gets permanently lost. - The final representation is thus \\(x \\equiv 0 \\ 1101 \\ 111\\).\nSo you can experiment, I wrote a helper function represent_as_float that lets you visualize this for different values of \\(x\\). Below I show the example I just calculated. I print out both the scientific notation form and its binary representation.\n\nrepresent_as_float(15.25, n=8, n_exp=4, n_man=3, bias=10)\n\nscientific notation: (-1)^0 * (1 + 0.90625) * 2^3\n8-bit floating point representation: 0 1101 111\n\n\nSo what’s going on with the bias term \\(b\\)? Why do we need it? The easiest answer to give is that without it, we can’t have negative exponents without having to use another sign bit for them. Consider a number like \\(x=0.5\\). In modified scientific notation this would look like \\(x=(-1)^0 \\cdot (1+0) \\cdot 2^{-1} = 2^{-1}\\), meaning its exponent would be \\(e=-1\\). Rather than have to keep yet another sign bit for the exponent, it’s easier to just add a bias term \\(b\\) that ensures the exponent \\(e'=e+b\\) is always non-negative. The higher the bias, the more precision we can show in the range \\(-1 < x < 1\\). The trade-off is that we lose precision for large values of \\(x\\).\nOn top of floats defined the way I mentioned, we also have some special numbers that get defined in a floating point system. These are \\(\\pm 0\\), \\(\\pm \\infty\\), and \\(\\text{NaN}\\) or “not a number”. Each of these numbers is allocated its own special sequence of bits, depending on the precision. - \\(+0\\) and \\(-0\\): These numbers are typically represented using a biased exponent \\(e'=0\\) (all zero bits) and a mantissa \\(m=0\\) (all zero bits). The sign bit is used to distinguish between \\(+0\\) and \\(-0\\). In our example, these would be \\(+0 \\equiv 0 \\ 0000 \\ 000\\) and \\(-0 \\equiv 1 \\ 0000 \\ 000\\). - \\(+\\infty\\) and \\(-\\infty\\): These numbers are typically represented using the max allowed exponent (all one bits) and a mantissa \\(m=0\\) (all zero bits). The sign bit is used to distinguish between \\(+\\infty\\) and \\(-\\infty\\). In our example, these would be \\(+\\infty \\equiv 0 \\ 1111 \\ 000\\) and \\(-\\infty \\equiv 1 \\ 1111 \\ 000\\). - \\(\\text{NaN}\\): This value is typically represented using the max allowed exponent (all one bits) and a non-zero \\(m \\neq 0\\). The sign bit is usually not used for \\(\\text{NaN}\\) values. Note this means we can have many different sequences that all represent \\(\\text{NaN}\\). In our example, any number of the form \\(\\text{NaN} \\equiv \\text{x} \\ 1111 \\ \\text{xxx}\\) would work.\nSo I can illustrate some points about how floating point numbers behave, I’m going to generate all possible \\(8\\)-bit floats (excluding the special numbers) and plot them on a number line, similar to what I did above with the \\(8\\)-bit signed integers. I’ll generate the floats using the using the helper function gen_all_floats, passing in the number of mantissa bits n_man=3, the number of exponent bits n_exp=4, and a bias of bias=10.\nFirst, I’ll use these numbers to print out some interesting statistics of this 8-bit floating point system.\n\neight_bit_floats = gen_all_floats(n=8, n_man=3, n_exp=4, bias=10)\nprint(f'Total number of 8-bit floats: {len(eight_bit_floats)}')\nprint(f'Most negative float: {min(eight_bit_floats)}')\nprint(f'Most positive float: {max(eight_bit_floats)}')\nprint(f'Smallest nonzero float: {min([x for x in eight_bit_floats if x > 0])}')\nprint(f'Machine Epsilon: {min([x for x in eight_bit_floats if x > 1]) - 1}')\n\nTotal number of 8-bit floats: 120\nMost negative float: -56.0\nMost positive float: 56.0\nSmallest nonzero float: 0.001953125\nMachine Epsilon: 0.25\n\n\nWe can see that this 8-bit system only contains 120 unique floats. We could practically list them all out. Just like with the integers, we see there’s a most negative float, \\(-56.0\\), and a most positive float, \\(56.0\\). The smallest float, i.e. the one closest to \\(0\\), is \\(0.001953125\\). Notice how much more precision the smallest float has than the largest ones do. The largest ones are basically whole numbers, while the smallest one has nine digits of precision. Evidently, floating point representations give much higher precision to numbers close to zero than to numbers far away from zero.\nWhat happens if you try to input a float larger than the max, in this case \\(56.0\\)? Typically it will overflow. This will result in either the system raising an error, or the number getting set to \\(+\\infty\\), in a sense getting “rounded up”. Similarly, for numbers more negative than the min, in this case \\(-56.0\\), either an overflow error will be raised, or the number will get “rounded down” to \\(-\\infty\\).\nYou have to be careful in overflow situations like this, especially when you don’t know for sure which of these your particular system will do. It’s amusing to note that python will raise an overflow error, but numpy will round to \\(\\pm \\infty\\). Two different conventions to worry about. Just as amusing, when dealing with signed integers, it’s numpy that will raise an error if you overflow, while python won’t care. One of those things…\nWhat happens when you try to input a float smaller than the smallest value, in this case \\(0.001953125\\)? In this case, the number is said to undeflow. Usually underflow won’t raise an error. The number will pretty much always just get set to \\(+0\\) (or \\(-0\\)). This is again something you have to worry about, especially if you’re dealing with small numbers in denominators, where they can lead to division by zero errors which do get raised.\nOverflow and underflow errors are some of the most common numerical bugs that occur in deep learning, and usually result from not handling floats correctly to begin with.\nI also printed out a special value called the machine epsilon. The machine epsilon, denoted \\(\\varepsilon_m\\), is defined as the smallest value in a floating point system that’s larger than \\(1\\). In some sense, \\(\\varepsilon_m\\) is a proxy for how finely you can represent numbers in a given \\(n\\)-bit floating point system. The smaller \\(\\varepsilon_m\\) the more precisely you can represent numbers, i.e. the more decimal places of precision you get access to. In our case, we get \\(\\varepsilon_m=0.25\\). This means numbers in 8-bit floating point tend to be \\(0.25\\) apart from each other on average, which means we can represent numbers in this system only with a measly 2-3 digits of precision.\nWith these numbers in hand let’s now plot their distribution on the number line. I’ll use the helper function plot_number_dist function to do this. Compare with the plot of the signed integers I did above.\n\nplot_number_dist(eight_bit_floats, title='Distribution of 8-bit Floats')\n\n\n\n\nNotice how different this plot is from the ones for the signed integers. With the integers, the points were equally spaced. Now points close to \\(0\\) are getting represented much closer together than points far from \\(0\\). There are \\(74\\) of the \\(120\\) total points showing up just in the range \\([-1,1]\\). That’s over half!. Meanwhile, only \\(22\\) points total show up in the combined ranges of \\([-60,-10]\\) and \\([10,60]\\). Very strange.\nFeel free to play around with different floating point systems by using different choices for n, n_man, n_exp, and bias. Be careful, however, not to make n_exp too large or you may crash the kernel…"
  },
  {
    "objectID": "notebooks/numerical-computing.html#double-precision",
    "href": "notebooks/numerical-computing.html#double-precision",
    "title": "2  Numerical Computation",
    "section": "Double Precision",
    "text": "Double Precision\nSo how does python represent floats? Python by default uses what’s called double precision to represent floats, also called float64. This means \\(n=64\\) total bits of precision are used, with \\(n_e=11\\), \\(n_m=52\\), and bias \\(b=1023=2^{10}-1\\). Double precision allows for a much larger range of numbers than 8-bit precision does: - The max value allowed is \\(2^{2^{n_e}-b} = 2^{1025} \\approx 10^{308}\\). - The min value allowed is \\(-2^{2^{n_e}-b} = -2^{1025} \\approx -10^{308}\\). - Numbers outside the range of about \\([-10^{308}, 10^{308}]\\) will overflow. - The smallest values allowed are (plus or minus) \\(2^{-b+1} = 2^{-1022} \\approx 10^{-308}\\). - Using subordinal numbers, the smallest values are (plus or minus) \\(2^{-b-n_m+1} = 2^{-1074} \\approx 10^{-324}\\). - Numbers inside the range of about \\([-10^{-308}, 10^{-308}]\\) will underflow. - Using subordinal numbers, this range is around \\([-10^{-324}, 10^{-324}]\\). - The machine epsilon is \\(\\varepsilon_m = 2^{-53} \\approx 10^{-16}\\). - Numbers requiring more than about 15-16 digits of precision will get truncated, resulting in numerical roundoff. - The special numbers \\(\\pm \\infty\\), \\(\\pm 0\\), and \\(\\text{NaN}\\) are represented similarly as before, except using 64 bits.\nTo illustrate the point regarding numerical roundoff, here’s what happens if we try to use double precision floating point to define the constant \\(\\pi\\) to its first 100 digits? Notice it just gets truncated to its first 15 digits. Double precision is unable to keep track of the other 85 digits. They just get lost to numerical roundoff.\n\npi = 3.141592653589793238462643383279502884197169399375105820974944592307816406286208998628034825342117068\npi\n\n3.141592653589793\n\n\nAnother thing to worry about is adding small numbers to medium to large sized numbers, e.g. \\(10 + 10^{-16}\\), which will just get rounded down to \\(10.0\\).\n\n10.0 + 1e-16\n\n10.0\n\n\nNumerical roundoff is often an issue when subtracting two floats. Here’s what happens when we try to subtract two numbers that should be equal, \\(x=0.1+0.2\\) and \\(y=0.3\\). Instead of \\(y-x=0\\), we get \\(y-x \\approx -5.55 \\cdot 10^{-17}\\). The problem comes from the calculation \\(x=0.1+0.2\\), which caused a slight loss of precision in \\(x\\).\n\nx = 0.1 + 0.2\ny = 0.3\ny - x\n\n-5.551115123125783e-17\n\n\nA major implication of these calculations is that you should never test floating points for exact equality because numerical roundoff can mess it up. If you’d tried to test something like (y - x) == 0.0, you’d have gotten the wrong answer. Instead, you want to test that y - x is less than some small number tol, called a tolerance, i.e. abs(y - x) < tol.\n\ny - x == 0.0\n\nFalse\n\n\n\ntol = 1e-5\nabs(y - x) < tol\n\nTrue\n\n\nNumerical roundoff explains why we got the weird results above when subtracting \\(1.2 - 4.3\\). The imperfect precision in the two numbers resulted in a numerical roundoff error, leading in the trailing \\(9\\)s that should’ve rounded up to \\(-3.1\\) exactly. In general, subtracting floats is one of the most dangerous operations to do, as it tends to lead to the highest loss of precision in calculations. The closer two numbers are to being equal the worse this loss of precision tends to get.\nI mentioned that double precision has a smallest number of \\(2^{-1022} \\approx 10^{-308}\\), but caveated that by saying that, by using a trick called subordinal numbers, we can get the smallest number down to about \\(10^{-324}\\). What did I mean by this? It turns out that the bits where the biased exponent \\(e'=0\\) (i.e. all exponent bits are zero) go mostly unused in the standard version of double precision. By using this zero exponent and allowing the mantissa \\(m\\) to take on all its possible values, we can get about \\(2^{52}\\) more values (since the mantissa has 52 bits). This lets us get all the way down to \\(2^{-1022} \\cdot 2^{-52} = 2^{-1074} \\approx 10^{-324}\\).\nPython (and numpy) by default implements double precision with subordinal numbers, as we can see.\n\n2 ** (-1074)\n2 ** (-1075)\n\n5e-324\n\n\n0.0\n\n\nThe special numbers \\(\\pm \\infty\\), \\(\\pm 0\\), and \\(\\text{NaN}\\) are also defined in double precision. In python (and numpy) they’re given by the following commands, - \\(\\infty\\): float('inf') or np.inf, - \\(-\\infty\\): float('-inf') or -np.inf, - \\(\\pm 0\\): 0, - \\(\\text{NaN}\\): float('nan') or np.nan.\n\nfloat('inf')\nnp.inf\n\ninf\n\n\ninf\n\n\n\nfloat('-inf')\n-np.inf\n\n-inf\n\n\n-inf\n\n\n\n0\n-0\n\n0\n\n\n0\n\n\n\nfloat('nan')\nnp.nan\n\nnan\n\n\nnan\n\n\nYou may be curious what exactly \\(\\text{NaN}\\) (“not a number”) is and where it might show up. Basically, NaNs are used wherever values are undefined. Anytime an operation doesn’t return a sensible value it risks getting converted to NaN. One example is the operation \\(\\infty - \\infty = \\infty + (-\\infty)\\), which mathematically doesn’t make sense. No, it’s not zero…\n\nfloat('inf') + float('-inf')\nnp.inf - np.inf\n\nnan\n\n\nnan\n\n\nI’ll finish this section by mentioning that there are two other floating point representations worth being aware of: single precision (or float32), and half precision (or float16). Single precision uses 32 bits to represent a floating point number. Half precision uses 16 bits. It may seem strange to even bother having these less-precise precisions lying around, but they do have their uses. For example, half precision shows up in deep learning as a more efficient way to represent the weights of a neural network. Since half precision floats only take up 25% as many bits as default double precision floats do, using them can yield a 4x reduction in model memory sizes. We’ll see more on this later."
  },
  {
    "objectID": "notebooks/numerical-computing.html#common-floating-point-pitfalls",
    "href": "notebooks/numerical-computing.html#common-floating-point-pitfalls",
    "title": "2  Numerical Computation",
    "section": "Common Floating Point Pitfalls",
    "text": "Common Floating Point Pitfalls\nTo cap this long section on floats, here’s a list of common pitfalls people run into when working with floating point numbers, and some ways to avoid each one. This is probably the most important thing to take away from this section. You may find it helpful to reference later on. See this post for more information.\n\nNumerical overflow: Letting a number blow up to infinity (or negative infinity)\n\nClip numbers from above to keep them from being too large\nWork with the log of the number instead\nMake sure you’re not dividing by zero or a really small number\nNormalize numbers so they’re all on the same scale\n\nNumerical underflow: Letting a number spiral down to zero\n\nClip numbers from below to keep them from being too small\nWork with the exp of the number instead\nNormalize numbers so they’re all on the same scale\n\nSubtracting floats: Avoid subtracting two numbers that are approximately equal\n\nReorder operations so approximately equal numbers aren’t nearby to each other\nUse some algebraic manipulation to recast the problem into a different form\nAvoid differencing squares (e.g. when calculating the standard deviation)\n\nTesting for equality: Trying to test exact equality of two floats\n\nInstead of testing x == y, test for approximate equality with something like abs(x - y) <= tol\nUse functions like np.allclose(x, y), which will do this for you\n\nUnstable functions: Defining some functions in the naive way instead of in a stable way\n\nExamples: factorials, softmax, logsumexp\nUse a more stable library implementation of these functions\nLook for the same function but in log form, e.g. log_factorial or log_softmax\n\nBeware of NaNs: Once a number becomes NaN it’ll always be a NaN from then on\n\nPrevent underflow and overflow\nRemove missing values or replace them with finite values"
  }
]