[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Mathematics for Machine Learning",
    "section": "",
    "text": "Preface\nThis is my book Mathematics for Machine Learning."
  },
  {
    "objectID": "notebooks/basic-math.html#elementary-arithmetic-and-algebra",
    "href": "notebooks/basic-math.html#elementary-arithmetic-and-algebra",
    "title": "1  Basic Math",
    "section": "1.1 Elementary Arithmetic and Algebra",
    "text": "1.1 Elementary Arithmetic and Algebra\nIt’s useful in machine learning to be able to read and manipulate basic arithmetic and algebraic equations, particularly when reading research papers, blog posts, or documentation. I won’t go into depth on the basics of high school arithmetic and algebra. I do have to assume some mathematical maturity of the reader, and this seems like a good place to draw the line. I’ll just mention a few key points.\nRecall that numbers can come in several forms. We can have,\n\nNatural Numbers: These are positive whole numbers \\(0, 1, 2, 3, 4, \\cdots\\). Note the inclusion of \\(0\\) in this group. Following the computer science convention I’ll tend to do that. The set of all natural numbers is denoted by the symbol \\(\\mathbb{N}\\).\nIntegers: These are any whole numbers \\(\\cdots, -2, -1, 0, 1, 2, \\cdots\\), positive, negative, and zero. The set of all integers is denoted by the symbol \\(\\mathbb{Z}\\).\nRational Numbers: These are any ratios of integers, for example \\[\\frac{1}{2}, \\frac{5}{4}, -\\frac{3}{4}, \\frac{1000}{999}, \\cdots.\\] Any ratio will do, so long as the denominator (the bottom number) is not zero. The set of all rational numbers is denoted by the symbol \\(\\mathbb{Q}\\).\nReal Numbers: These are any arbitrary decimal numbers on the number line, for example \\[1.00, \\ 5.07956, \\ -0.99999\\dots, \\ \\pi=3.1415\\dots, \\ e=2.718\\dots, \\ \\cdots.\\] They include as a special case both the integers and the rational numbers, but also include numbers that can’t be represented as fractions, like \\(\\pi\\) and \\(e\\). The set of all real numbers is denoted by the symbol \\(\\mathbb{R}\\).\nComplex numbers: These are numbers with both real and imaginary parts, like \\(1 + 2i\\) where \\(i=\\sqrt{-1}\\). Complex numbers include the real numbers as a special case. Since they don’t really show up in machine learning we won’t deal with these after this. The set of all complex numbers is denoted by the symbol \\(\\mathbb{C}\\).\n\nYou should be familiar with the usual arithmetic operations defined on these systems of numbers. Things like addition, subtraction, multiplication, and division. You should also at least vaguely recall the order of operations, which defines the order in which complex arithmetic operations with parenthesis are carried out. For example,\n\\[(5+1) \\cdot \\frac{(7-3)^2}{2} = 6 \\cdot \\frac{4^2}{2} = 6 \\cdot \\frac{16}{2} = 6 \\cdot 8 = 48.\\]\nYou should be able to manipulate and simplify simple fractions by hand. For example,\n\\[\\frac{3}{7} + \\frac{1}{5} = \\frac{3 \\cdot 5 + 1 \\cdot 7}{7 \\cdot 5} = \\frac{22}{35} \\approx 0.62857.\\]\nAs far as basic algebra goes, you should be familiar with algebraic expressions like \\(x+5=7\\) and be able to solve for the unknown variable \\(x\\),\n\\[x=7-5=2.\\]\nYou should be able to take an equation like \\(ax + b = c\\) and solve it for \\(x\\) in terms of coefficients \\(a, b, c\\),\n\\[\\begin{align*}\nax + b &= c \\\\\nax &= c - b \\\\\nx &= \\frac{c - b}{a}.\n\\end{align*}\\]\nYou should also be able to expand simple expressions like this,\n\\[\\begin{align*}\n(ax - b)^2 &= (ax - b)(ax - b) \\\\\n&= (ax)^2 - (ax)b - b(ax) + b^2 \\\\\n&= a^2x^2 - abx - abx + b^2 \\\\\n&= a^2x^2 - 2abx + b^2.\n\\end{align*}\\]\nIt’s also worth recalling what a set is. Briefly, a set is a collection of unique elements. Usually those elements are numbers. To say that an element \\(x\\) is an element of a set \\(S\\), we’d write \\(x \\in S\\), read “\\(x\\) is in \\(S\\)”. If \\(x\\) is not in the set, we’d write \\(x \\notin S\\). For example, the set of elements \\(1, 2, 3\\) can be denoted \\(S = \\{1, 2, 3\\}\\). Then \\(1 \\in S\\), but \\(5 \\notin S\\).\nI’ve already mentioned the most common sets we’ll care about, namely the natural numbers \\(\\mathbb{N}\\), integers \\(\\mathbb{Z}\\), rational numbers \\(\\mathbb{Q}\\), and real numbers \\(\\mathbb{R}\\). Also of interest will be the intervals,\n\nOpen interval: \\((a, b) = \\{x: a < x < b \\}\\).\nHalf-open left interval: \\((a, b] = \\{x: a < x \\leq b \\}\\).\nHalf-open right interval: \\([a, b) = \\{x: a \\leq x < b \\}\\).\nClosed interval: \\([a, b] = \\{x: a \\leq x \\leq b \\}\\).\n\nThink of intervals as representing line segments on the real line, connecting \\(a\\) to \\(b\\). I’ll touch on sets more in coming lessons. I just want you to be familiar with the notation, since I’ll occasionally use it.\n\n1.1.1 Symbolic vs Numerical Computation\nThere are two fundamental ways to perform mathematical computations: numerical computation, and symbolic computation. You’re familiar with both even though you may not realize it. Numerical computation involves crunching numbers. You plug in numbers, and get out numbers. When you type something like 10.5 / 12.4 in python, it will return a number, like 0.8467741935483871. This is numerical computation.\n\n10.5 / 12.4\n\n0.8467741935483871\n\n\nThis contrasts with a way of doing computations that you learned in math class, where you manipulate symbols. This is called symbolic computation. Expanding an equation like \\((ax-b)^2\\) to get \\(a^2x^2 - 2abx + b^2\\) is an example of a symbolic computation. You see the presence of abstract variables like \\(x\\) that don’t have a set numeric value.\nUsually in practice we’re interested in numerical computations. We’ll mostly be doing that in this book. But sometimes, when working with equations, we’ll need to do symbolic computations as well. Fortunately, python has a library called SymPy, or sympy, that can do symbolic computation automatically. I won’t use it a whole lot in this book, but it will be convenient in a few places to show you that you don’t need to manipulate mathematical expressions by hand all the time.\nTo use sympy, I’ll import sympy with the alias sp. Before defining a function to operate on, we first have to encode all the symbols in the problem as sympy Symbol objects. Once that’s done, we can create equations out of them and perform mathematical operations.\nHere’s an example of using sympy to expand the equation above, \\((ax-b)^2\\).\n\nimport sympy as sp\n\n\na = sp.Symbol('a')\nb = sp.Symbol('b')\nx = sp.Symbol('x')\na, b, x\n\n(a, b, x)\n\n\n\nequation = (a * x - b) ** 2\nexpanded = sp.expand(equation, x)\nprint(f'expanded equation: {expanded}')\n\nexpanded equation: a**2*x**2 - 2*a*b*x + b**2\n\n\nWe can also use sympy to solve equations. Here’s an example of solving the quadratic equation \\(x^2 = 6\\) for its two roots, \\(x = \\pm \\sqrt{6}\\).\n\nequation = x**2 - 6\nsolutions = sp.solve(equation, x)\nprint(f'solutions = {solutions}')\n\nsolutions = [-sqrt(6), sqrt(6)]\n\n\nSympy has a lot of functionality, and it can be a very difficult library to learn due to its often strange syntax for things. Since we won’t really need it all that often I’ll skip the in depth tutorial. See the documentation if you’re interested."
  },
  {
    "objectID": "notebooks/basic-math.html#univariate-functions",
    "href": "notebooks/basic-math.html#univariate-functions",
    "title": "1  Basic Math",
    "section": "1.2 Univariate Functions",
    "text": "1.2 Univariate Functions\nAs I’m sure you’ve seen before, a mathematical function is a way to map inputs \\(x\\) to outputs \\(y\\). That is, a function \\(f(x)\\) is a mapping that takes in a value \\(x\\) and maps it to a unique value \\(y=f(x)\\). These values can be either single numbers (called scalars), or multiple numbers (vectors or tensors). When \\(x\\) and \\(y\\) are both scalars, \\(f(x)\\) is called a univariate function.\nLet’s quickly cover some of the common functions you’d have seen before in a math class, focusing mainly on the ones that show up in machine learning. I’ll also cover a couple machine-learning specific functions you perhaps haven’t seen before.\n\n1.2.1 Affine Functions\nThe most basic functions to be aware of are the straight-line functions: constant functions, linear functions, and affine functions:\n\nConstant functions: \\(y=c\\) or \\(x=c\\)\n\nExamples: \\(y=2\\), \\(x=1\\)\n\nLinear functions: \\(y=ax\\)\n\nExamples: \\(y=-x\\), \\(y=5x\\)\n\nAffine functions: \\(y=ax+b\\)\n\nExamples: \\(y=-x+1\\), \\(y=5x-4\\)\n\n\nAll constant functions are linear functions, and all linear functions are affine functions. In the case of affine functions, the value \\(b\\) is called the intercept. It corresponds to the value where the function crosses the y-axis. The value \\(a\\) is called the slope. It corresponds to the steepness of the curve, i.e. its height over its width (or “rise” over “run”). Notice linear functions are the special case where the intercept is always the origin \\(x=0, y=0\\).\n\n1.2.1.1 Plotting\nWe can plot these and any other univariate function \\(y=f(x)\\) in the usual way you learned about in school. We sample a lot of \\((x,y)\\) pairs from the function, and plot them on a grid with a horizontal x-axis and vertical y-axis.\nBefore plotting some examples I need to mention that plotting in python is usually done with the matplotlib library. Typically what we’d do to get a very simple plot is:\n\nImport plt, which is the alias to the submodule matplotlib.pyplot\nGet a grid of x values we want to plot, e.g. using np.linspace or np.arange\nGet a grid of y values either directly, or by first defining a python function f(x)\nPlot x vs y by calling plt.(x, y), followed by plt.show().\n\nNote step (2) requires another library called numpy to create the grid of points. You don’t have to use numpy for this, but it’s typically easiest. Usually numpy is imported with the alias np. Numpy is python’s main library for working with numerical arrays. We’ll cover it in much more detail in future lessons.\nLet me go ahead and load these libraries. I’ll also show a simple example of a plot. What I’ll do is define a grid x of 100 equally spaced points between -10 and 10, and plot the function \\(y=x-1\\) using the method described above.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nx = np.linspace(-10, 10, 100)\ny = x - 1\nplt.plot(x, y)\nplt.show()\n\n\n\n\n\n\n\n\nThis plot is pretty ugly. It’s too big, arbitrarily scaled, and doesn’t include any information about what’s being plotted against what. In matplotlib if you want to include all these things to make nice plots you have to include a bunch of extra style commands.\nFor this reason, for the rest of the plotting in this lesson I’m going to use a helper function plot_function, which takes in x and y, the range of x values we want to plot, and an optional title. I didn’t think the details of this helper function were worth going into now, so I abstracted it away into the file utils.py in this same directory. It uses matplotlib like I described, but with a good bit of styling to make the plot more readable. If you really want to see the details perhaps the easiest thing to do is create a cell below using ESC-B and type the command ??plot_function, which will print the code inside the function as the output.\nBack to it, let’s plot one example each of a constant function \\(y=2\\), a linear function \\(y=2x\\), and an affine function \\(2x-1\\).\n\nx = np.arange(-10, 10, 0.1)\nf = lambda x: 2 * np.ones(len(x))\nplot_function(x, f, xlim=(-5, 5), ylim=(-5, 5), ticks_every=[1, 1], title='Constant Function: $y=2$')\n\n\n\n\n\n\n\n\n\nx = np.arange(-10, 10, 0.1)\nf = lambda x: 2 * x\nplot_function(x, f, xlim=(-5, 5), ylim=(-5, 5), ticks_every=[1, 1], title='Linear Function: $y=2x$')\n\n\n\n\n\n\n\n\n\nx = np.arange(-10, 10, 0.1)\nf = lambda x: 2 * x - 1\nplot_function(x, f, xlim=(-5, 5), ylim=(-5, 5), ticks_every=[1, 1], title='Affine Function: $y=2x-1$')\n\n\n\n\n\n\n\n\n\n\n\n1.2.2 Polynomial Functions\nPolynomial functions are just sums of positive integer powers of \\(x\\), e.g. something like \\(y=3x^2+5x+1\\) or \\(y=x^{10}-x^{3}+4\\). The highest power that shows up in the function is called the degree of the polynomial. For example, the above examples have degrees 2 and 10 respectively. Polynomial functions tend to look like lines, bowls, or roller coasters that turn up and down some number of times.\nA major example is the quadratic function \\(y=x^2\\), which is just an upward-shaped bowl. Its bowl-shaped curve is called a parabola. We can get a downward-shaped bowl by flipping the sign to \\(y=-x^2\\).\n\nx = np.arange(-10, 10, 0.1)\nf = lambda x: x ** 2\nplot_function(x, f, xlim=(-5, 5), ylim=(0, 10), ticks_every=[1, 1], title='Quadratic Function: $y=x^2$')\n\n\n\n\n\n\n\n\nThe next one up is the cubic function \\(y=x^3\\). The cubic looks completely different from the bowl-shaped parabola.\n\nx = np.arange(-10, 10, 0.1)\nf = lambda x: x ** 3\nplot_function(x, f, xlim=(-5, 5), ylim=(-5, 5), ticks_every=[1, 1], title='Cubic Function: $y=x^3$')\n\n\n\n\n\n\n\n\nPolynomials can take on much more interesting shapes than this. Here’s a more interesting polynomial degree 10,\n\\[y = (x^2 - 1)^5 - 5(x^2 - 1)^4 + 10(x^2 - 1)^3 - 10(x^2 - 1)^2 + 5(x^2 - 1) - 1.\\]\n\nx = np.arange(-10, 10, 0.1)\nf = lambda x: (x**2 - 1)**5 - 5 * (x**2 - 1)**4 + 10 * (x**2 - 1)**3 - 10 * (x**2 - 1)**2 + 5 * (x**2 - 1) - 1\nplot_function(x, f, xlim=(-3, 3), ylim=(-40, 40), ticks_every=[1, 10], title='Arbitrary Polynomial')\n\n\n\n\n\n\n\n\n\n\n1.2.3 Rational Functions\nRational functions are functions that are ratios of polynomial functions. Examples might be \\(y=\\frac{1}{x}\\), or\n\\[y=\\frac{x^3+x+1}{x^2-1}.\\]\nThese functions typically look kind of like polynomial functions, but have points where the curve “blows up” to positive or negative infinity. The points where the function blows up are called poles or asymptotes.\nHere’s a plot of the function\n\\[y=\\frac{x^3+x+1}{x^2-1}.\\]\nNotice how weird it looks. There are asymptotes (the vertical lines) where the function blows up at \\(\\pm 1\\), which is where the denominator \\(x^2-1=0\\).\n\nx = np.arange(-10, 10, 0.01)\nf = lambda x: (x ** 3 + x + 1) / (x ** 2 - 1)\nplot_function(x, f, xlim=(-5, 5), ylim=(-5, 5), ticks_every=[1, 1], title='Rational Function')\n\n\n\n\n\n\n\n\nHere’s a plot of \\(y=\\frac{1}{x}\\). There’s an asymptote at \\(x=0\\). When \\(x > 0\\) it starts at \\(+\\infty\\) and tapers down to \\(0\\) as \\(x\\) gets large. When \\(x < 0\\) it does the same thing, except flipped across the origin \\(x=y=0\\). This is an example of an odd function, a function that looks like \\(f(x)=-f(x)\\), which is clear in this case since \\(1/(-x)=-1/x\\). Functions like the linear function \\(y=x\\) and the cubic function \\(y=x^3\\) are also odd functions.\n\nx = np.arange(-10, 10, 0.1)\nf = lambda x: 1 / x\nplot_function(x, f, xlim=(-5, 5), ylim=(-5, 5), ticks_every=[1, 1], title='Odd Function: $y=1/x$')\n\n\n\n\n\n\n\n\nA related function is \\(y=\\frac{1}{|x|}\\). The difference here is that \\(|x|\\) can never be negative. This means \\(f(x)=f(-x)\\). This is called an even function. Functions like this are symmetric across the y-axis. The quadratic function \\(y=x^2\\) is also an even function.\n\nx = np.arange(-10, 10, 0.1)\nf = lambda x: 1 / np.abs(x)\nplot_function(x, f, xlim=(-5, 5), ylim=(-1, 5), ticks_every=[1, 1], title='Even Function: $y=1/|x|$')\n\n\n\n\n\n\n\n\n\n\n1.2.4 Power Functions\nFunctions that look like \\(y=\\frac{1}{x^n}\\) for some \\(n\\) are sometimes called inverse, hyperbolic. These can be represented more easily by using a negative power like \\(y=x^{-n}\\), which means the exact same thing as \\(y=\\frac{1}{x^n}\\).\nWe can extend \\(n\\) to deal with things like square roots or cube roots or any kind of root as well by allowing \\(n\\) to be non-integer. For example, we can represent the square root function \\(y=\\sqrt{x}\\) as \\(y=x^{1/2}\\), and the cube root \\(y=\\sqrt[3]{x}\\) as \\(y=x^{1/3}\\). Roots like these are only defined when \\(x \\geq 0\\).\nThe general class of functions of the form \\(y=x^p\\) for some arbitrary real number \\(p\\) are often called power functions.\nHere’s a plot of what the square root function looks like. Here \\(y\\) grows slower than a linear function, but still grows arbitrarily large with \\(x\\).\n\nx = np.arange(0, 10, 0.1)\nf = lambda x: np.sqrt(x)\nplot_function(x, f, xlim=(0, 5), ylim=(-2, 4), ticks_every=[1, 1], title='Square Root: $y=\\sqrt{x}=x^{1/2}$')\n\n\n\n\n\n\n\n\nPower functions obey the following rules:\n\n\n\n\n\n\n\nRule\nExample\n\n\n\\(x^0 = 1\\)\n\\(2^0 = 1\\)\n\n\n\\(x^{m+n} = x^m x^n\\)\n\\(3^{2+5} = 3^2 3^5 = 3^8 = 6561\\)\n\n\n\\(x^{m-n} = \\frac{x^m}{x^n}\\)\n\\(3^{2-5} = \\frac{3^2}{3^5} = 3^{-3} \\approx 0.037\\)\n\n\n\\(x^{mn} = (x^m)^n\\)\n\\(2^{2 \\cdot 5} = (2^2)^5 = 2^{10} = 1024\\)\n\n\n\\((xy)^n = x^n y^n\\)\n\\((2 \\cdot 2)^3 = 2^3 2^3 = 4^3 = 2^6 = 64\\)\n\n\n\\(\\big(\\frac{x}{y}\\big)^n = \\frac{x^n}{y^n}\\)\n\\(\\big(\\frac{2}{4}\\big)^3 = \\frac{2^3}{4^3} = \\frac{1}{8}\\)\n\n\n\\(\\big(\\frac{x}{y}\\big)^{-n} = \\frac{y^n}{x^n}\\)\n\\(\\big(\\frac{2}{4}\\big)^{-3} = \\frac{4^3}{2^3} = 2^3 = 8\\)\n\n\n\\(x^{1/2} = \\sqrt{x} = \\sqrt[2]{x}\\)\n\\(4^{1/2} = \\sqrt{4} = 2\\)\n\n\n\\(x^{1/n} = \\sqrt[n]{x}\\)\n\\(3^{1/4} = \\sqrt[4]{3} \\approx 1.316\\)\n\n\n\\(x^{m/n} = \\sqrt[n]{x^m}\\)\n\\(3^{3/4} = \\sqrt[4]{3^3} = \\sqrt[4]{9} \\approx 1.732\\)\n\n\n\\(\\sqrt[n]{xy} = \\sqrt[n]{x} \\sqrt[n]{y}\\)\n\\(\\sqrt[4]{3 \\cdot 2} = \\sqrt[4]{3} \\sqrt[4]{2} \\approx 1.565\\)\n\n\n\\(\\sqrt[n]{\\frac{x}{y}} = \\frac{\\sqrt[n]{x}}{\\sqrt[n]{y}}\\)\n\\(\\sqrt[4]{\\frac{3}{2}} = \\frac{\\sqrt[4]{3}}{\\sqrt[4]{2}} \\approx 1.107\\)\n\n\n\nIt’s important to remember that power functions do not distribute over addition, i.e.\n\\[(x+y)^n \\neq x^n + y^n,\\]\nand by extension nor do roots,\n\\[\\sqrt[n]{x+y} \\neq \\sqrt[n]{x} + \\sqrt[n]{y}.\\]\n\n\n1.2.5 Exponentials and Logarithms\nTwo very important functions are the exponential function \\(y=\\exp(x)\\) and the logarithm function \\(y=\\log(x)\\). They show up surprisingly often in machine learning and the sciences, certainly more than most other special functions do.\nThe exponential function can be written as a power by defining a number \\(e\\) called Euler’s number, given by \\(e = 2.71828\\dots\\) . Like \\(\\pi\\), \\(e\\) is an example of an irrational number, i.e. a number that can’t be represented as a ratio of integers. Using \\(e\\), we can write the exponential function in the more usual form \\(y=e^x\\), where it’s roughly speaking understood that we mean “multiply \\(e\\) by itself \\(x\\) times”. For example, \\(\\exp(2) = e^2 = e \\cdot e\\).\nThe logarithm is defined as the inverse of the exponential function. It’s the unique function satisfying \\(\\log(\\exp(x)) = x\\). The opposite is also true since the exponential must then be the inverse of the logarithm function, \\(\\exp(\\log(x)) = x\\). This gives a way of mapping between the two functions,\n\\[\\log(a) = b \\quad \\Longleftrightarrow \\quad \\exp(b) = a.\\]\nHere are some plots of what the exponential and logarithm functions look like. The exponential function is a function that blows up very, very quickly. The log function grows very, very slowly (much more slowly than the square root does).\nNote the log function is only defined for positive-valued numbers \\(x \\geq 0\\), with \\(\\log(+0)=-\\infty\\). This is dual to the exponential function only taking on \\(y \\geq 0\\).\n\nx = np.arange(-5, 5, 0.1)\nf = lambda x: np.exp(x)\nplot_function(x, f, xlim=(-5, 5), ylim=(-1, 10), ticks_every=[1, 2], title='Exponential Function: $y=\\exp(x)$')\n\n\n\n\n\n\n\n\n\nx = np.arange(0.01, 5, 0.1)\nf = lambda x: np.log(x)\nplot_function(x, f, xlim=(-1, 5), ylim=(-5, 2), ticks_every=[1, 1], title='Logarithm Function: $y=\\log(x)$')\n\n\n\n\n\n\n\n\nThe exponential and logarithm functions I defined are the “natural” way to define these functions. We can also have exponential functions in other bases, \\(y=a^x\\) for any positive number \\(a\\). Each \\(a\\) has an equivalent logarithm, written \\(y = \\log_{a}(x)\\). The two functions \\(y=a^x\\) and \\(y=\\log_{a}(x)\\) are inverses of each other. When I leave off the \\(a\\), it’s assumed that all logs are the natural base \\(a=e\\), sometimes also written \\(\\ln(x)\\).\nTwo common examples of other bases that show up sometimes are the base-2 functions \\(2^x\\) and \\(\\log_{2}(x)\\), and the base-10 functions \\(10^x\\) and \\(\\log_{10}(x)\\). Base-2 functions in particular show up often in computer science because of the tendency to think in bits. Base-10 functions show up when we want to think about how many digits a number has.\nHere are some rules that exponentials and logs obey:\n\n\n\n\n\n\n\nRule\nExample\n\n\n\\(e^0 = 1\\)\n\n\n\n\\(\\log(1) = 0\\)\n\n\n\n\\(\\log(e) = 1\\)\n\n\n\n\\(e^{a+b} = e^a e^b\\)\n\\(e^{2+5} = e^2 e^5 = e^8 \\approx 2980.96\\)\n\n\n\\(e^{a-b} = \\frac{e^a}{e^b}\\)\n\\(e^{2-5} = \\frac{e^2}{e^5} = e^{-3} \\approx 0.0498\\)\n\n\n\\(e^{ab} = (e^a)^b\\)\n\\(e^{2 \\cdot 5} = (e^2)^5 = e^{10} \\approx 22026.47\\)\n\n\n\\(a^b = e^{b \\log(a)}\\)\n\\(2^3 = e^{3 \\log(2)} = 8\\)\n\n\n\\(\\log(ab) = \\log(a) + \\log(b)\\)\n\\(\\log(2 \\cdot 5) = \\log(2) + \\log(5) = \\log(10) \\approx 2.303\\)\n\n\n\\(\\log\\big(\\frac{a}{b}\\big) = \\log(a) - \\log(b)\\)\n\\(\\log\\big(\\frac{2}{5}\\big) = \\log(2) - \\log(5) \\approx -0.916\\)\n\n\n\\(\\log(a^b) = b\\log(a)\\)\n\\(\\log(5^2) = 2\\log(5) \\approx 3.219\\)\n\n\n\\(\\log_a(x) = \\frac{\\log(x)}{\\log(a)}\\)\n\\(\\log_2(5) = \\frac{\\log(5)}{\\log(2)} \\approx 2.322\\)\n\n\n\nHere’s an example of an equation involving exponentials and logs. Suppose you have \\(n\\) bits of numbers (perhaps it’s the precision in some float) and you want to know how many digits this number takes up in decimal form (what you’re used to). This would be equivalent to solving the following equation for \\(x\\),\n\\[\\begin{align}\n2^n &= 10^{x} \\\\\n\\log(2^n) &= \\log(10^{x}) \\\\\nn\\log(2) &= x\\log(10) \\\\\nx &= \\frac{\\log(2)}{\\log(10)} \\cdot n \\\\\nx &\\approx 0.3 \\cdot n.\n\\end{align}\\]\nFor example, you can use this formula to show that 52 bits of floating point precision translates to about 15 to 16 digits of precision. In numpy, the function np.log function calculates the (base-\\(e\\)) log of a number.\n\nn = 52\nx = np.log(2) / np.log(10) * n\nprint(f'x = {x}')\n\nx = 15.65355977452702\n\n\n\n\n1.2.6 Trigonometric Functions\nOther textbook functions typically covered in math courses are the trig functions: sine, cosine, tangent, cosine, cosecant, and cotangent. Of these functions, the most important to know are the sine function \\(y=\\sin x\\), the cosine function \\(y = \\cos x\\), and sometimes the tangent function \\(y = \\tan x\\).\nHere’s what their plots look like. They’re both waves that repeat themselves, in the sense \\(f(x + 2\\pi) = f(x)\\). The length for the function to repeat itself is called the period, in this case \\(2\\pi \\approx 6.28\\). Note that the cosine is just a sine function that’s shifted right by \\(\\frac{\\pi}{2} \\approx 1.57\\).\n\nx = np.arange(-10, 10, 0.1)\nf = lambda x: np.sin(x)\nplot_function(x, f, xlim=(-6, 6), ylim=(-2, 2),  ticks_every=[1, 0.5], title='Sine Function: $y=\\sin(x)$')\n\n\n\n\n\n\n\n\n\nx = np.arange(-10, 10, 0.1)\nf = lambda x: np.cos(x)\nplot_function(x, f, xlim=(-6, 6), ylim=(-2, 2), ticks_every=[1, 0.5], title='Cosine Function: $y=\\cos(x)$')\n\n\n\n\n\n\n\n\nTrig functions don’t really show up that much in machine learning, so I won’t remind you of all those obscure trig rules you’ve forgotten. I’ll just mention that we can define all the other trig functions using the sine and cosine as follows,\n\\[\\begin{align*}\n&\\tan x = \\frac{\\sin x}{\\cos x}, \\\\\n&\\csc x = \\frac{1}{\\sin x}, \\\\\n&\\sec x = \\frac{1}{\\cos x}, \\\\\n&\\cot x = \\frac{1}{\\tan x} = \\frac{\\cos x}{\\sin x}.\n\\end{align*}\\]\nWe can talk about the inverse of trig functions as well. These are just the functions that undo the trig operations and give you back the angle (in radians). Since none of the trig functions are monotonic, we can’t invert them on the whole real line, but only on a given range.\nBelow I’ll just list the inverse sine, cosine, and tangent functions and their defined input and output ranges. Note by historical convention, these inverse functions are usually called the arcsine, arccosine, and arctangent respectfully.\n\n\n\n\n\n\n\n\nInverse Function\nInput Range\nOutput Range\n\n\n\\(y = \\arcsin x = \\sin^{-1} x\\)\n\\(-1 \\leq x \\leq 1\\)\n\\(-90^\\circ \\leq y \\leq 90^\\circ\\)\n\n\n\\(y = \\arccos x = \\cos^{-1} x\\)\n\\(-1 \\leq x \\leq 1\\)\n\\(0^\\circ \\leq y \\leq 180^\\circ\\)\n\n\n\\(y = \\arctan x = \\tan^{-1} x\\)\n\\(-\\infty < x < \\infty\\)\n\\(-90^\\circ \\leq y \\leq 90^\\circ\\)\n\n\n\n\n\n1.2.7 Piecewise Functions\nThe functions covered so far are examples of continuous functions. Their graphs don’t have jumps or holes in them anywhere. Continuous functions we can often write using a single equation, like \\(y=x^2\\) or \\(y=1 + \\sin(x)\\). We can also have functions that require more than one equation to write. These are called piecewise functions. Piecewise functions usually aren’t continuous, but sometimes can be.\nAn example of a discontinuous piecewise function is the unit step function \\(y=u(x)\\) given by\n\\[\ny =\n\\begin{cases}\n0 & x < 0, \\\\\n1 & x \\geq 0.\n\\end{cases}\n\\]\nThis expression means \\(y=0\\) whenever \\(x < 0\\), but \\(y=1\\) whenever \\(x \\geq 0\\). It breaks up into two pieces, one horizontal line \\(y=0\\) when \\(x\\) is negative, and another horizontal line \\(y=1\\) when \\(x\\) is positive.\nUsing Boolean expressions, we can also write this function in a more economical way by agreeing to identify \\(x=1\\) with \\(\\text{TRUE}\\) and \\(x=0\\) with \\(\\text{FALSE}\\), which python does by default. In this notation, we can write\n\\[u(x) = [x \\geq 0],\\]\nwhich means exactly the same thing as the piecewise definition, since \\(x \\geq 0\\) is only true when (you guessed it), \\(x \\geq 0\\).\nHere’s a plot of this function. Note the discontinuous jump at \\(x=0\\).\n\nx = np.arange(-10, 10, 0.01)\nf = lambda x:  (x >= 0)\nplot_function(x, f, xlim=(-3, 3), ylim=(-1, 2), ticks_every=[1, 0.5], title='Unit Step Function: $y=u(x)$')\n\n\n\n\n\n\n\n\nAn example of a piecewise function that’s continuous is the ramp function, defined by\n\\[\ny =\n\\begin{cases}\n0 & x < 0, \\\\\nx & x \\geq 0.\n\\end{cases}\n\\]\nThis function gives a horizontal line \\(y=0\\) when \\(x\\) is negative, and a \\(45^\\circ\\) line \\(y=x\\) when \\(x\\) is positive. Both lines connect at \\(x=0\\), but leave a kink in the graph.\nAnother way to write the same thing using Boolean expressions is \\(y = x \\cdot [x \\geq 0]\\), which is of course just \\(y = x \\cdot u(x)\\).\nIn machine learning it’s more common to write the ramp function using the \\(\\max\\) function as \\(y = \\max(0,x)\\). This means, for each \\(x\\), take that value and compare it with \\(0\\), and take the maximum of those two. That is, if \\(x\\) is negative take \\(y=0\\), otherwise take \\(y=x\\). It’s also more common to call this function a rectified linear unit, or ReLU for short. It’s an ugly, unintuitive name, but unfortunately it’s stuck in the field.\nHere’s a plot of the ramp or ReLU function. Notice how it stays at \\(y=0\\) for a while, then suddenly “ramps upward” at \\(x=0\\).\n\nx = np.arange(-10, 10, 0.1)\nf = lambda x:  x * (x >= 0)\nplot_function(x, f, xlim=(-3, 3), ylim=(-3, 3), ticks_every=[1, 1], title='ReLU Function')\n\n\n\n\n\n\n\n\nLast, I’ll mention here the absolute value function \\(y = |x|\\), defined by the piecewise function\n\\[\ny = \\begin{cases}\nx & \\text{if } x \\ge 0 \\\\\n-x & \\text{if } x < 0.\n\\end{cases}\n\\]\nThe absolute value just ignores negative signs and makes everything positive. The function looks like the usual line \\(y=x\\) when positive, but like the negative-sloped line \\(y=-x\\) when negative. At \\(x=0\\) the two lines meet, creating a distinctive v-shape. To get the absolute value function in python, use abs or np.abs.\n\nx = np.arange(-5, 5, 0.1)\nf = lambda x: abs(x)\nplot_function(x, f, xlim=(-5, 5), ylim=(0, 5), ticks_every=[1, 1], title='Absolute Value Function: $y=|x|$')\n\n\n\n\n\n\n\n\n\n\n1.2.8 Composite Functions\nWe can also have any arbitrary hybrid of the above functions. We can apply exponentials to affine functions, logs to sine functions, sines to exponential functions. In essence, this kind of layered composition of functions is what a neural network is as we’ll see later on.\nMath folks often write an abstract compositional function as a function applied to another function, like \\(y=f(g(x))\\) or \\(y=(f \\circ g)(x)\\). These can be chained arbitrarily many times, not just two. Neural networks do just that, often hundreds or thousands of times.\nConsider, for example, the function composition done by applying the following functions in sequence:\n\nan affine function \\(y=wx+b\\)\nfollowed by a linear function \\(y=-x\\)\nfollowed by an exponential function \\(y=e^x\\)\nfollowed by a rational function \\(y=\\frac{1}{x}\\)\n\nto get the full function \\[y = \\frac{1}{1 + e^{-(wx+b)}}.\\]\nHere’s a plot of what this function looks like for the “standard form” where \\(w=1, b=0\\). Notice that \\(0 \\leq y \\leq 1\\). The values of \\(x\\) get “squashed” to values between 0 and 1 after the function is applied.\n\nx = np.arange(-10, 10, 0.1)\nf = lambda x:  1 / (1 + np.exp(-x))\nplot_function(x, f, xlim=(-6, 6), ylim=(-0.2, 1.2), ticks_every=[2, 0.2], title='Sigmoid Function')\n\n\n\n\n\n\n\n\nThis function is called the sigmoid function. The sigmoid is very important in machine learning since it in essence creates probabilities. We’ll see it a lot more. The standard form sigmoid function, usually written\n\\(\\sigma(x)\\), is given by \\[\\sigma(x) = \\frac{1}{1 + e^{-x}}.\\]\nArbitrary affine transformations of the standard form would then be written as \\(\\sigma(wx+b)\\).\nA similar looking function shows up sometimes as well called the hyperbolic tangent or tanh function, which has the (standard) form\n\\[\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}.\\]\nThe tanh function looks pretty much the same as the sigmoid except it’s rescaled vertically so that \\(-1 \\leq y \\leq 1\\).\nHere’s a plot of the tanh function. Notice how similar it looks to the sigmoid with the exception of the scale of the y-axis.\n\nx = np.arange(-10, 10, 0.1)\nf = lambda x: (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\nplot_function(x, f, xlim=(-5, 5), ylim=(-2, 2), ticks_every=[1, 0.5], title='Tanh Function')\n\n\n\n\n\n\n\n\n\n\n1.2.9 Function Transformations\nSuppose we have some arbitrary function \\(f(x)\\) and we apply a series of compositions to get a new function \\[g(x)=a \\cdot f(b \\cdot (x + c)) + d.\\] We can regard each parameter \\(a,b,c,d\\) as doing some kind of geometric transformation to the graph of the original function \\(f(x)\\). Namely,\n\n\\(a\\) re-scales the function vertically (if \\(a\\) is negative it also flips \\(f(x)\\) upside down)\n\\(b\\) re-scales the function horizontally (if \\(b\\) is negative it also flips \\(f(x)\\) left to right)\n\\(c\\) shifts the function horizontally (left if \\(c\\) is positive, right if \\(c\\) is negative)\n\\(d\\) shifts the function vertically (up if \\(d\\) is positive, down if \\(d\\) is negative)\n\nHere’s an example of how these work. Consider the function \\(f(x)=x^2\\). We’re going to apply each of these transformations one by one to show what they do to the graph of \\(f(x)\\).\nFirst, let’s look at the transformation \\(g(x) = \\frac{1}{2} f(x) = \\frac{1}{2} x^2\\). Here \\(a=\\frac{1}{2}\\) and the rest are zero. I’ll plot it along side the original graph (the blue curve). Notice the graph gets flattened vertically by a factor of two (the orange curve).\n\nx = np.arange(-10, 10, 0.1)\nf = lambda x: x ** 2\n\n\na = 1 / 2\ng = lambda x: a * x ** 2\nplot_function(x, [f, g], xlim=(-3, 3), ylim=(-2, 10), ticks_every=[1, 2], title=f'$a={a}$')\n\n\n\n\n\n\n\n\nNow consider at the transformation\n\\[g(x) = f\\big(\\frac{1}{2} x\\big) = \\bigg(\\frac{1}{2} x \\bigg)^2.\\]\nHere \\(b=\\frac{1}{2}\\) and the rest are zero. Notice the graph again gets flattened but in a slightly different way.\n\nb = 1 / 2\ng = lambda x: (b * x) ** 2\nplot_function(x, [f, g], xlim=(-3, 3), ylim=(-2, 10), ticks_every=[1, 2], title=f'$b={b}$')\n\n\n\n\n\n\n\n\nNext, consider the transformation \\(g(x) = f(x-1) = (x-1)^2.\\) Here \\(c=1\\) and the rest are zero. Notice the graph’s shape doesn’t change. It just gets shifted right by \\(c=1\\) since \\(c\\) is negative.\n\nc = -1\ng = lambda x: (x + c) ** 2\nplot_function(x, [f, g], xlim=(-3, 3), ylim=(-7, 7), ticks_every=[1, 2], title=f'$c={c}$')\n\n\n\n\n\n\n\n\nFinally, let’s look at the transformation \\(g(x) = f(x) + 2 = x^2 + 2\\). Here \\(d=2\\) and the rest are zero. Notice again the graph’s shape doesn’t change. It just gets shifted up by \\(d=2\\) units.\n\nd = 2\ng = lambda x: x ** 2 + d\nplot_function(x, [f, g], xlim=(-3, 3), ylim=(-1, 8), ticks_every=[1, 2], title=f'$d={d}$')\n\n\n\n\n\n\n\n\nLet’s now put them all together to see what happens. We should see rescaling in both directions and shifts in both directions. It’s hard to see in the plot, but it’s all there if you zoom in. The vertex of the parabola is at the point \\(x=c=1, y=d=2\\). And the stretching factors due to \\(a=b=1/2\\) are both acting to flatten the parabola.\n\ng = lambda x: a * (b * (x + c)) ** 2 + d\nplot_function(x, [f, g], xlim=(-8, 8), ylim=(-2, 10), ticks_every=[2, 2], title=f'$a={a}, b={b}, c={c}, d={d}$')"
  },
  {
    "objectID": "notebooks/basic-math.html#multivariate-functions",
    "href": "notebooks/basic-math.html#multivariate-functions",
    "title": "1  Basic Math",
    "section": "1.3 Multivariate Functions",
    "text": "1.3 Multivariate Functions\nWhat we’ve covered thus far only deals with univariate functions, functions where \\(y=f(x)\\), but \\(x\\) and \\(y\\) are just single numbers, i.e. scalars. In machine learning we’re almost always dealing with multivariate functions with lots of variables, sometimes billions of them. It turns out that most of what I’ve covered so far extends straight forwardly to multivariate functions with some small caveats, which I’ll cover below.\nSimply put, a multivariate function is a function of multiple variables. Instead of a single variable \\(x\\), we might have several variables, e.g. \\(x_0, x_1, x_2, x_3, x_4, x_5\\),\n\\[y = f(x_0, x_1, x_2, x_3, x_4, x_5).\\]\nIf you think about mathematical functions analogously to python functions it shouldn’t be surprising functions can have multiple arguments. They usually do, in fact.\nHere’s an example of a function that takes two arguments \\(x\\) and \\(y\\) and produces a single output \\(z\\), more often written as \\(z=f(x,y)\\). The function we’ll look at is \\(z = x^2 + y^2\\). I’ll evaluate the function at three points:\n\n\\(x=0\\), \\(y=0\\),\n\\(x=1\\), \\(y=-1\\),\n\\(x=0\\), \\(y=1\\).\n\nThe main thing to notice is the function does exactly what you think it does. If you plug in 2 values, you get out 1 value.\n\nf = lambda x, y: x ** 2 + y ** 2\nprint(f'z = {f(0, 0)}')\nprint(f'z = {f(1, -1)}')\nprint(f'z = {f(0, 1)}')\n\nz = 0\nz = 2\nz = 1\n\n\nWe can also have functions that map multiple inputs to multiple outputs. Suppose we have a function that takes in 2 values \\(x_0, x_1\\) and outputs 2 values \\(y_0, y_1\\). We’d write this as \\((y_0, y_1) = f(x_0, x_1)\\).\nConsider the following example,\n\\[(y_0, y_1) = f(x_0, x_1) = (x_0+x_1, x_0-x_1).\\]\nThis is really just two functions, both functions of \\(x_0\\) and \\(x_1\\). We can completely equivalently write this function as\n\\[y_0 = f_1(x_0, x_1) = x_0+x_1,\\] \\[y_1 = f_2(x_0, x_1) = x_0-x_1.\\]\nHere’s this function defined and evaluated at the point \\(x_0=1\\), \\(x_1=1\\).\n\nf = lambda x0, x1: (x0 + x1, x0 - x1)\nprint(f'(y0, y1) = {f(1, 1)}')\n\n(y0, y1) = (2, 0)\n\n\nFor now I’ll just focus on the case of multiple inputs, single output like the first example. These are usually called scalar-valued functions. We can also have vector-valued functions, which are functions whose outputs can have multiple values as well. I’ll focus on scalar-valued functions here.\nA scalar-valued function of \\(n\\) variables \\(x_0, x_1, \\cdots, x_{n-1}\\) has the form\n\\[y = f(x_0, x_1, \\cdots, x_{n-1}).\\]\nNote \\(n\\) can be as large as we want it to be. When working with deep neural networks (which are just multivariate functions of a certain form) \\(n\\) can be huge. For example, if the input is a \\(256 \\times 256\\) image, the input might be \\(256^2=65536\\) pixels. For a 10 second audio clip that’s sampled at 44 kHz, the input might be \\(10*44k=440k\\) amplitudes. Large numbers indeed.\nCalculating the output of multivariate functions is just as straight-forward as for univariate functions pretty much. Unfortunately, visualizing them is much harder. The human eye can’t see 65536 dimensions, only 3 dimensions. This in some sense means we need to give up on the ability to “graph” a function and instead find other ways to visualize it.\nOne thing that sometimes help to visualize high dimension functions is to pretend they’re functions of two variables, like \\(z=f(x,y)\\). In this special case we can visualize the inputs as an xy-plane, and the output as a third axis sticking out perpendicular to the xy-plane from the origin. Each \\(x,y\\) pair will map to one unique \\(z\\) value. Done this way, we won’t get a graph of a curve as before, but a surface.\nHere’s an example of what this might look like for the simple function \\(z=x^2+y^2\\). I’ll plot the function on the domain \\(-10 \\leq x \\leq 10\\) and \\(-10 \\leq y \\leq 10\\) using the helper function plot_3d. It takes in two lists of values x and y. I’ll use np.linspace to sample 100 points from -10 to 10 for each. Then I’ll define a lambda function that maps x and y to the output z. Passing these three arguments into the helper function gives us our 3D plot.\n\nx = np.linspace(-10, 10, 100)\ny = np.linspace(-10, 10, 100)\nf = lambda x, y: x**2 + y**2\n\n\nplot_function_3d(x, y, f, title='3D Plot: $z=x^2+y^2$', ticks_every=[5, 5, 50], labelpad=5, dist=12)\n\n\n\n\n\n\n\n\nNotice how the plot looks like an upward facing bowl. Imagine a bowl lying on a table. The table is the xy-plane. The bowl is the surface \\(z=x^2+y^2\\) we’re plotting. While the plot shows the general idea what’s going on, 3D plots can often be difficult to look at. They’re often slanted at funny angles and hide important details.\nHere’s another way we can visualize the same function: Rather than create a third axis for \\(z\\), we can plot it directly on the xy-plane as a 2D plot. Since we’re dealing with a surface, not a curve, we have to do this for lots of different \\(z\\) values, which will give a family of curves. For example, we might plot all of the following curves corresponding to different values of \\(z\\) in the xy-plane,\n\\[\\begin{align}\n25 &= x^2 + y^2, \\\\\n50 &= x^2 + y^2, \\\\\n75 &= x^2 + y^2, \\\\\n100 &= x^2 + y^2, \\\\\n125 &= x^2 + y^2, \\\\\n150 &= x^2 + y^2.\n\\end{align}\\]\nDoing this will give a family of curves on one 2D plot, with each curve representing some value of \\(z\\). In our example, these curves are all circles of radius \\(z^2\\). Each curve is called a level curve or level set.\nThese kinds of plots are called contour plots. A contour map can be thought of as looking at the surface from the top down, where each level set corresponds to slicing the function \\(z=f(x,y)\\) horizontally for different values of \\(z\\). This trick is often used in topographical maps to visualize 3D terrain on a 2D sheet of paper. Here is a contour plot for \\(z=x^2+y^2\\) using the above level curves.\n\nplot_countour(x, y, f, title='Countour Plot: $z=x^2+y^2$')\n\n\n\n\n\n\n\n\nNotice how we get a bunch of concentric rings in the contour plot, each labeled by some value (their \\(z\\) values). These rings correspond to the circles I was talking about. You can visually imagine this plot as looking down from the top of the bowl. In the middle you see the bottom. The rings get closer together the further out you go, which indicates that the bowl is sloping steeper the further out we get.\nWe’ll see more examples of multivariate functions in the coming lessons."
  },
  {
    "objectID": "notebooks/basic-math.html#systems-of-equations",
    "href": "notebooks/basic-math.html#systems-of-equations",
    "title": "1  Basic Math",
    "section": "1.4 Systems of Equations",
    "text": "1.4 Systems of Equations\nIn machine learning we’ll find ourselves frequently interested not just with single equations, but multiple equations each with many variables. One thing we might seek to do is solve these coupled systems, which means finding a solution that satisfies every equation simultaneously. Consider the following example,\n\\[\\begin{alignat*}{3}\n   x & {}+{} &  y & {}={} & 2  \\\\\n   2x & {}-{} &  3y & {}={} & 7.\n\\end{alignat*}\\]\nThis system consists of two equations, \\(x + y = 2\\), and \\(2x - 3y = 7\\). Each equation contains two unknown variables, \\(x\\) and \\(y\\). We need to find a solution for both \\(x\\) and \\(y\\) that satisfies both of these equations.\nUsually the easiest and most general way to solve simple coupled systems like this is the method of substitution. The idea is to solve one equation for one variable in terms of the other, then plug that solution into the second equation to solve for the other variable. Once the second variable is solved for, we can go back and solve for the first variable explicitly. Let’s start by solving the first equation for \\(x\\) in terms of \\(y\\). This is pretty easy,\n\\[x = 2 - y.\\]\nNow we can take this solution for \\(x\\) and plug it into the second equation to solve for \\(y\\),\n\\[\\begin{align*}\n2x - 3y &= 7 \\\\\n2(2 - y) - 3y &= 7 \\\\\n4 - 5y &= 7 \\\\\n5y &= -3 \\\\\ny &= -\\frac{3}{5}.\n\\end{align*}\\]\nWith \\(y\\) in hand, we can now solve for \\(x\\), \\(x = 2 - y = 2 + \\frac{3}{5} = \\frac{13}{5}\\). Thus, the pair \\(x=\\frac{13}{5}\\), \\(y=-\\frac{3}{5}\\) is the solution that solves both of these coupled equations simultaneously.\nHere’s sympy’s solution to the same system. It should of course agree with what I just got, which it does.\n\nx, y = sp.symbols('x y')\neq1 = sp.Eq(x + y, 2)\neq2 = sp.Eq(2 * x - 3 * y, 7)\nsol = sp.solve((eq1, eq2), (x, y))\nprint(f'x = {sol[x]}')\nprint(f'y = {sol[y]}')\n\nx = 13/5\ny = -3/5\n\n\nNotice that both of the equations in this example are linear, since each term only contains terms proportional to \\(x\\) and \\(y\\). There are no terms like \\(x^2\\) or \\(\\sin y\\) or whatever. Linear systems of equations are special because they can always be solved as long as there are enough variables. I’ll spend a lot more time on these when I get to linear algebra.\nWe can also imagine one or more equations being nonlinear. Provided we can solve each equation one-by-one, we can apply the method of substitution to solve these too. Here’s an example. Consider the nonlinear system\n\\[\\begin{align*}\ne^{x + y} &= 10  \\\\\nxy &= 1.\n\\end{align*}\\]\nLet’s solve the second equation first since it’s easier. Solving for \\(y\\) gives \\(y = \\frac{1}{x}\\). Now plug this into the first equation and solve for \\(x\\),\n\\[\\begin{align*}\ne^{x + y} &= 10  \\\\\ne^{x + 1/x} &= 10  \\\\\n\\log \\big(e^{x + 1/x}\\big) &= \\log 10 \\\\\nx + \\frac{1}{x} &= \\log 10 \\\\\nx^2 - \\log 10 \\cdot x + 1 &= 0 \\\\\nx &= \\frac{1}{2} \\bigg(\\log 10 \\pm \\sqrt{(\\log 10)^2 - 4}\\bigg) \\\\\nx &\\approx 0.581, \\ 1.722.\n\\end{align*}\\]\nNote here I had to use the quadratic formula, which I’ll assume you’ve forgotten. If you have a quadratic equation of the form \\(ax^2 + bx + c = 0\\), then it will (usually) have exactly two solutions given by the formula\n\\[x = \\frac{1}{2a} \\bigg(-b \\pm \\sqrt{b^2 - 4ac}\\bigg).\\]\nThis means we have two different possible solutions for \\(x\\), which thus means we’ll also have two possible solutions to \\(y\\) since \\(y=\\frac{1}{x}\\). Thus, this system has two possible solutions,\n\\[\\text{Solution 1: }x \\approx 0.581, \\ y \\approx 1.722,\\] \\[\\text{Solution 2: }x \\approx 1.722, \\ y \\approx 0.581.\\]\nIt’s interesting how symmetric these two solutions are. They’re basically the same with \\(x\\) and \\(y\\) swapped. This is because the system has symmetry. You can swap \\(x\\) and \\(y\\) in the system above and not change the equation, which means the solutions must be the same up to permutation of \\(x\\) and \\(y\\)!\nHere’s sympy’s attempt to solve this system.\n\nx, y = sp.symbols('x y')\neq1 = sp.Eq(sp.exp(x + y), 10)\neq2 = sp.Eq(x * y, 1)\nsol = sp.solve((eq1, eq2), (x, y))\nprint(f'x1 = {sol[0][0].round(5)} \\t y1 = {sol[0][1].round(5)}')\nprint(f'x2 = {sol[1][0].round(5)} \\t y2 = {sol[1][1].round(5)}')\n\nx1 = 0.58079     y1 = 1.72180\nx2 = 1.72180     y2 = 0.58079\n\n\nIn general, it’s not even possible to solve a system of nonlinear equations except using numerical methods. The example I gave was rigged so I could solve it by hand. General purpose root-finding algorithms exist that can solve arbitrary systems of equations like this numerically, no matter how nonlinear they are.\nTo solve a nonlinear system like this numerically, you can use the scipy function scipy.optimize.fsolve. Scipy is an extension of numpy that includes a lot of algorithms for working with non-linear functions. To use fsolve, you have to define the system as a function mapping a list of variables to a list of equations. You also have to specify a starting point x0 for the root finder. This tells it where to start looking for the root. Since nonlinear equations have multiple solutions, picking a different x0 can and will often give you a different root. I won’t dwell on all this since we don’t really need to deal with root finding much in machine learning.\n\nfrom scipy.optimize import fsolve\n\nsystem = lambda xy: [np.exp(xy[0] + xy[1]) - 10, xy[0] * xy[1] - 1]\nsolution = fsolve(system, x0=(1, 1))\nprint(f'solution = {solution}')\n\nsolution = [0.5807888 1.7217963]"
  },
  {
    "objectID": "notebooks/basic-math.html#sums-and-products",
    "href": "notebooks/basic-math.html#sums-and-products",
    "title": "1  Basic Math",
    "section": "1.5 Sums and Products",
    "text": "1.5 Sums and Products\nWe typically find ourselves performing operations on large numbers of numbers at a time. By far the most common operation is adding up a bunch of numbers, or summation. Suppose we have some sequence of \\(n\\) numbers \\(x_0,x_1,x_2,\\cdots,x_{n-1}\\). They could be anything, related by a function, or not. If we wanted to sum them together to get a new number \\(x\\) we could write\n\\[x = x_0 + x_1 + x_2 + \\cdots + x_{n-1}.\\]\nBut it’s kind of cumbersome to always write like this. For this reason in math there’s a more compact notation to write sums called summation notation. We introduce the symbol \\(\\sum\\) for “sum”, and write \\[x = \\sum_{i=0}^{n-1} x_i.\\]\nRead this as “the sum of all \\(x_i\\) for \\(i=0,1,\\cdots,n-1\\) is \\(x\\)”. The index \\(i\\) being summed over is called a dummy index. It can be whatever we want since it never appears on the left-hand side. It gets summed over and then disappears. The lower and upper values \\(i=0\\) and \\(i=n-1\\) are the limits of the summation. The limits need not always be \\(i=0\\) and \\(i=n-1\\). We can choose them to be whatever we like as a matter of convenience.\nFrequently summation notation is paired with some kind of generating function \\(f(i) = x_i\\) that generates the sequence. For example, suppose our sequence is generated by the function \\(f(i) = i\\), and we want to sum from \\(i=1\\) to \\(i=n\\). We’d have\n\\[x = \\sum_{i=1}^n x_i = \\sum_{i=1}^n i = 1 + 2 + \\cdots + n = \\frac{1}{2} n(n+1).\\]\nThe right-hand term \\(\\frac{1}{2} n(n-1)\\) is not obvious, and only applies to this particular sum. I just wrote it down since it’s sometimes useful to remember. This is a special kind of sum called an arithmetic series. Here’s a “proof” of this relationship using sympy.\n\ni, n = sp.symbols('i n')\nsummation = sp.Sum(i, (i, 1, n)).doit()\nprint(f'sum i for i=1,...,n = {summation}')\n\nsum i for i=1,...,n = n**2/2 + n/2\n\n\nIn the general case when we don’t have nice rules like this we’d have to loop over the entire sum and do the sum incrementally.\nIn python, the equivalent of summation notation is the sum function, where we pass in the sequence we want to sum as a list. Here’s the arithmetic sum up to \\(n=10\\), which should be \\(\\frac{1}{2} 10 \\cdot (10+1) = 55\\).\n\nsum([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n\n55\n\n\nAnother useful sum to be aware of is the geometric series. A geometric series is a sum over a sequence whose generating function is \\(f(i) = r^i\\) for some real number \\(r \\neq 1\\). Its rule is\n\\[x = \\sum_{i=0}^{n-1} r^i = r^0 + r^1 + \\cdots + r^{n-1} = \\frac{1-r^n}{1-r}.\\]\nFor example, if \\(n=10\\) and \\(r=\\frac{1}{2}\\), we have\n\\[x = \\sum_{i=0}^{9} \\bigg(\\frac{1}{2}\\bigg)^i = \\frac{1-\\big(\\frac{1}{2}\\big)^{10}}{1-\\big(\\frac{1}{2}\\big)} = 2\\bigg(1-\\frac{1}{2^{10}}\\bigg) \\approx 1.998.\\]\n\nr = 1 / 2\nn = 10\nsum([r ** i for i in range(n)])\n\n1.998046875\n\n\nNotice how the term \\(\\big(\\frac{1}{2}\\big)^{10} \\approx 0.00098\\) is really small. We can practically ignore it. In fact, as \\(n \\rightarrow \\infty\\) we can completely ignore it, in which case\n\\[x = \\sum_{i=0}^{\\infty} \\bigg(\\frac{1}{2}\\bigg)^i = \\frac{1}{1-\\big(\\frac{1}{2}\\big)} = 2.\\]\nThis is an example of the infinite version of the geometric series. If \\(0 \\leq r \\leq 1\\), then\n\\[x = \\sum_{i=0}^{\\infty} r^i = r^0 + r^1 + r^2 + \\cdots = \\frac{1}{1-r}.\\]\nWhat happens when \\(r=1\\)? Clearly the rule breaks down at this point, since the denominator becomes infinite. But it’s easy enough to see what it is by writing out the sum,\n\\[x = \\sum_{i=0}^{n-1} 1^i = 1^0 + 1^1 + \\cdots + 1^{n-1} = \\underbrace{1 + 1 + \\cdots + 1}_{\\text{n times}} = n.\\]\nIn this case, if we send \\(n \\rightarrow \\infty\\), then \\(x\\) clearly blows up to \\(\\infty\\) too. You can see this by plotting the function \\(y = \\frac{1}{1-x}\\) and observing it asymptotes at \\(x=1\\).\n\nx = np.arange(0, 1, 0.01)\nf = lambda x: 1 / (1 - x)\nplot_function(x, f, xlim=(0, 1), ylim=(0, 100), ticks_every=[0.2, 20], title='$y=1/(1-x)$')\n\n\n\n\nWe can always factor constants \\(c\\) out of sums. This follows naturally from just expanding the sum out,\n\\[\\sum_{i=0}^{n-1} c x_i = cx_0 + cx_1 + \\cdots + cx_{n-1} = c(x_0 + x_1 + \\cdots + x_{n-1}) = c\\sum_{i=0}^{n-1} x_i.\\]\nSimilarly, we can break sums up into pieces (or join sums together) as long as we’re careful to get the index limits right,\n\\[\\sum_{i=0}^{n-1} x_i = \\sum_{i=0}^{k} x_i + \\sum_{i=k+1}^{n-1} x_i.\\]\nWe can have double sums (sums of sums) as well. If \\(x_{i,j}\\) is some 2-index variable where \\(i=0,\\cdots,n-1\\) and \\(j=0,\\cdots,m-1\\), we can sum over both sets of indices to get \\(n \\cdot m\\) total terms,\n\\[\\sum_{i=0}^{n-1} \\sum_{j=0}^{m-1} x_{i,j} = \\sum_{j=0}^{m-1} \\sum_{i=0}^{n-1} x_{i,j} = x_{0,0} + x_{0,1} + \\cdots x_{0,m-1} + \\cdots + x_{n-1,0} + x_{n-1,1} + \\cdots x_{n-1,m-1}.\\]\nNotice the two sums can swap, or commute, with each other, \\(\\sum_i \\sum_j = \\sum_j \\sum_i\\). This follows by expanding the terms out like on the right-hand side and noting the must be equal in both cases.\nThe notation I’ve covered for sums has an analogue for products, called product notation. Suppose we want to multiply \\(n\\) numbers \\(x_0,x_1,\\cdots,x_{n-1}\\) together to get some number \\(x\\). We could write\n\\[x = x_0 \\cdot x_1 \\cdots x_{n-1},\\]\nbut we have a more compact notation for this as well. Using the symbol \\(\\prod\\) in analogy to \\(\\sum\\), we can write\n\\[x = \\prod_{i=0}^{n-1} x_i.\\]\nRead this as “the product of all \\(x_i\\) for \\(i=0,1,\\cdots,n-1\\) is \\(x\\)”.\nUnlike sums, python doesn’t have a native function to calculate products of elements in a sequence, but numpy has one called np.prod. Here’s an example. I’ll calculate the product of all integers between one and ten.\n\\[x = \\prod_{i=1}^{10} i = 1 \\cdot 2 \\cdot 3 \\cdot 4 \\cdot 5 \\cdot 6 \\cdot 7 \\cdot 8 \\cdot 9 \\cdot 10 = 3628800.\\]\n\nnp.prod([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n\n3628800\n\n\nLuckily, there aren’t any common products to remember. It’s just worth being familiar with the notation, since we’ll occasionally use it.\nProducts don’t obey quite the same properties sums do, so you have to be careful. When in doubt, just write out the product the long way and make sure what you’re doing makes sense. For example, pulling a factor \\(c\\) out of a product gives a factor of \\(c^n\\), not \\(c\\), since there are \\(c\\) total products multiplied together,\n\\[x = \\prod_{i=0}^{n-1} cx_i = cx_0 \\cdot cx_1 \\cdots cx_{n-1} = c^n(x_0 \\cdot x_1 \\cdots x_{n-1}) = c^n \\prod_{i=0}^{n-1} x_i.\\]\nIt’s worth noting (because we’ll use this fact), that we can turn products into sums by taking the log of the product,\n\\[\\log \\bigg(\\prod_{i=0}^{n-1} x_i \\bigg) = \\sum_{i=0}^{n-1} \\log x_i.\\]\nThis follows from the rule \\(\\log(x \\cdot y) = \\log x + \\log y\\), which extends to arbitrarily many products too."
  },
  {
    "objectID": "notebooks/basic-math.html#greek-alphabet",
    "href": "notebooks/basic-math.html#greek-alphabet",
    "title": "1  Basic Math",
    "section": "1.6 Greek Alphabet",
    "text": "1.6 Greek Alphabet\nLike many other technical fields, machine learning makes heavy use of the Greek alphabet to represent variable names in mathematical equations. While not all Greek characters are used, certain ones are worth being aware of. Below is a table of the Greek letters upper and lower case, as well as a guide on how to pronounce and write them. You don’t need to memorize all of these letters, but they will frequently show up in future lessons, so you may want to reference this table often until you’re comfortable with Greek letters."
  },
  {
    "objectID": "notebooks/numerical-computing.html#integers",
    "href": "notebooks/numerical-computing.html#integers",
    "title": "2  Numerical Computation",
    "section": "2.1 Integers",
    "text": "2.1 Integers\n\n2.1.1 Basics\nRecall the integers are whole numbers that can be positive, negative, or zero. Examples are 5, 100151, 0, -72, etc. The set of all integers is commonly denoted by the symbol \\(\\mathbb{Z}\\).\nIn python, integers (ints for short) are builtin objects of type int that more or less follow the rules that integers in math follow.\nAmong other things, the following operations can be performed with integers:\n\nAddition: \\(2 + 2 = 4\\).\nSubtraction: \\(2 - 5 = -3\\).\nMultiplication: \\(3 \\cdot 3 = 9\\)\n\nIn python this is the * operator, e.g. 3 * 3 = 9\n\nExponentiation: \\(2^3 = 2 \\times 2 \\times 2 = 8\\)\n\nIn python this is the ** operator, e.g. 2 ** 3 = 8.\n\nRemainder (or Modulo): the remainder of 10 when divided by 3 is 1, written \\(10 \\text{ mod } 3 = 1\\)\n\nIn python this is the % operator, e.g. 10 % 3 = 1.\n\n\nIf any of these operations are applied to two integers, the output will itself always be an integer.\nHere are a few examples.\n\n2 + 2\n\n4\n\n\n\n2 - 5\n\n-3\n\n\n\n3 * 3\n\n9\n\n\n\n10 % 3\n\n1\n\n\n\n2 ** 3\n\n8\n\n\nWhat about division? You can’t always divide two integers and get another integer. What you have to do instead is called integer division. Here you divide the two numbers and then round the answer down to the nearest whole number. Since \\(5 \\div 2 = 2.5\\), the nearest rounded down integer is 2.\nIn math, this “nearest rounded down integer” 2 is usually called the floor of 2.5, and represented with the funny symbol \\(\\lfloor 2.5 \\rfloor.\\) Using this notation we can write the above integer division as \\[\\big\\lfloor \\frac{5}{2} \\big\\rfloor = 2.\\]\nIn python, integer division is done using the // operator, e.g. 5 // 2 = 2. I’ll usually write \\(5 \\ // \\ 2\\) instead of \\(\\big\\lfloor \\frac{5}{2} \\big\\rfloor\\) when it makes sense, \\[5 \\ // \\ 2 = \\big\\lfloor \\frac{5}{2} \\big\\rfloor = 2.\\]\n\n5 // 2\n\n2\n\n\nWe can also do regular division / with ints, but the output will not be an integer even if the answer should be, e.g. 4 / 2. Only integer division is guaranteed to return an integer. I’ll get to this shortly.\n\n4 / 2\ntype(4 / 2)\n\n2.0\n\n\nfloat\n\n\n\n4 // 2\ntype (4 // 2)\n\n2\n\n\nint\n\n\nDivision by zero is of course undefined for both division and integer division. In python it will always raise a ZeroDivisionError like so.\n\n4 / 0\n\nZeroDivisionError: division by zero\n\n\n\n4 // 0\n\nZeroDivisionError: integer division or modulo by zero\n\n\n\n\n2.1.2 Representing Integers\nJust like every other data type, on a computer integers are actually represented internally as a sequence of bits. A bit is a “binary digit”, 0 or 1. A sequence of bits is just a sequence of zeros and ones, e.g. 0011001010 or 1001001.\nThe number of bits used to represent a piece of data is called its word size. If we use a word size of \\(n\\) bits to represent an integer, then there are \\(2^n\\) possible integer values we can represent.\nIf integers could only be positive or zero, representing them with bits would be easy. We could just convert them to binary and that’s it. To convert a non-negative integer to binary, we just need to keep dividing it by 2 and recording its remainder (0 or 1) at each step. The binary form is then just the sequence of remainders, written right to left. More generally, the binary sequence of some arbitrary number \\(x\\) is the sequence of coefficients \\(b_k=0,1\\) in the sum\n\\[x = \\sum_{k=-\\infty}^\\infty b_k 2^k = \\cdots + b_2 2^2 + b_1 2^1 + b_0 2^0 + b_{-1} 2^{-1} + b_{-2} 2^{-2} + \\cdots.\\]\nHere’s an example. Suppose we wanted to represent the number \\(12\\) in binary.\n\n\\(12 \\ // \\ 2 = 6\\) with a remainder of \\(0 = 12 \\text{ mod } 2\\), so the first bit from the right is then \\(0\\).\n\\(6 \\ // \\ 2 = 3\\) with a remainder of \\(0 = 6 \\text{ mod } 2\\), so the next bit is \\(0\\).\n\\(3 \\ // \\ 2 = 1\\) with a remainder of \\(1 = 3 \\text{ mod } 2\\), so the next bit is \\(1\\).\n\\(1 \\ // \\ 2 = 0\\) with a remainder of \\(1 = 1 \\text{ mod } 2\\), so the next bit is \\(1\\).\n\nSo the binary representation of \\(12\\) is \\(1100\\), which is the sequence of coefficients in the sum\n\\[12 = 1 \\cdot 2^{3} + 1 \\cdot 2^{2} + 0 \\cdot 2^{1} + 0 \\cdot 2^{0}.\\]\nRather than keep doing these by hand, you can quickly convert a number to binary in python by using bin. It’ll return a string representing the binary sequence of that number, prepended with the special prefix 0b. To get back to the integer from, use int, passing in a base of 2.\n\nbin(12)\n\n'0b1100'\n\n\n\nint('0b110', 2)\n\n6\n\n\nThis representation works fine for non-negative integers, also called the unsigned integers in computer science. To represent an unsigned integer with \\(n\\) bits, just get its binary form and prepend it with enough zeros on the left until all \\(n\\) bits are used. For example, if we used 8-bit unsigned integers then \\(n=8\\), hence representing the number \\(12\\) would look like \\(00000110\\). Simple, right?\nUnsigned ints work fine if we never have to worry about negative numbers. But in general we do. These are called the signed integers in computer science. To represent signed ints, we need to use one of the bits to represent the sign. What we can do is reserve the left-most bit for the sign, \\(0\\) if the integers is positive or zero, \\(1\\) if the integer is negative.\nFor example, if we used 8-bit signed integers to represent \\(12\\), we’d again write \\(00000110\\), exactly as before. But this time it’s understood that left-most \\(0\\) is encoding the fact that \\(12\\) is positive. If instead we wanted to represent the number \\(-12\\) we’d need to flip that bit to a \\(1\\), so we’d get \\(10000110\\).\nLet’s now do an example of a simple integer system. Consider the system of 4-bit signed ints. In this simple system, \\(n=4\\) is the word size, and an integer \\(x\\) is represented with the sequence of bits\n\\[x \\equiv sb_1b_2b_3,\\]\nwhere \\(s\\) is the sign bit and \\(b_1b_2b_3\\) are the remaining 3 bits allowed to represent the numerical digits. This system can represent \\(2^4=16\\) possible values in the range \\([-2^3+1,2^3-1] = [-8,7]\\), given in the following table:\n\n\n\nInteger\nRepresentation\nInteger\nRepresentation\n\n\n\n\n-0\n1000\n+0\n0000\n\n\n-1\n1001\n1\n0001\n\n\n-2\n1010\n2\n0010\n\n\n-3\n1011\n3\n0011\n\n\n-4\n1100\n4\n0100\n\n\n-5\n1101\n5\n0101\n\n\n-6\n1110\n6\n0110\n\n\n-7\n1111\n7\n0111\n\n\n\nNote the presence of \\(-0 \\equiv 1110\\) in the upper left. This is because the system as I’ve defined it leaves open the possibility of two zeros, \\(+0\\) and \\(-0\\), since for zero the sign bit is redundant. A way to get around this is to encode the negative numbers slightly differently, by not just setting the sign bit to one, but also inverting the remaining bits and subtracting one from them. This is called the two’s complement representation. It’s how most languages, including python, actually represent integers. I won’t go into this representation in any depth, except to say that it gets rid of the need for \\(-0\\) and replaces it with \\(-2^{n-1}\\).\nHere’s what that table looks like for 4-bit integers. It’s almost the same, except there’s no \\(-0\\), instead a \\(-8\\). Notice the positive integers look exactly the same. It’s only the negative integers that look different. For them, the right three bits get inverted and added with a one.\n\n\n\nInteger\nTwo’s Complement\nInteger\nTwo’s Complement\n\n\n\n\n-1\n1111\n0\n0000\n\n\n-2\n1110\n1\n0001\n\n\n-3\n1101\n2\n0010\n\n\n-4\n1100\n3\n0011\n\n\n-5\n1011\n4\n0100\n\n\n-6\n1010\n5\n0101\n\n\n-7\n1001\n6\n0110\n\n\n-8\n1000\n7\n0111\n\n\n\nIt’s worth visualizing what integers look like on the number line, if for nothing else than to compare it with what floats look like later on. Below I’ll plot what a 6-bit signed integer system would look like. Such a system should go from -32 to 31. I’ll to use the helper function plot_number_dist to do the plotting. As you’d expect, you just see a bunch of equally spaced points from -32 to 31.\n\n\nCode\nn = 6\nsix_bit_ints = range(-2**(n-1), 2**(n-1))\nplot_number_dist(six_bit_ints, title=f'Distribution of {n}-bit Signed Ints')\n\n\n\n\n\n\n\n\n\nIn python, integers are represented by default using a much bigger word size of \\(n=64\\) bits, called long integers, or int64 for short. This means (using two’s complement) we can represent \\(2^{64}=18446744073709551616\\) possible integer values in the range \\([-2^{63}, 2^{63}-1]\\).\nYou can see from this that 64-bit integers have a minimum integer allowed and a maximum integer allowed, which are\n\\[\\text{min\\_int}=-2^{63}=-9223372036854775808, \\qquad \\text{max\\_int}=2^{63}-1=9223372036854775807.\\]\nWhat I’ve said is technically only exactly true in older versions of pythons as well as other programming languages like C. It turns out newer versions of python have a few added tricks that allow you to represent essentially arbitrarily large integers. You can see this by comparing it to numpy’s internal int64 representation, which uses the C version. A numpy int64 outside the valid range will throw an overflow error.\n\nmin_int = -2 ** 63\nmax_int = 2 ** 63 - 1\n\n\nmin_int - 1\nnp.int64(min_int - 1)\n\n-9223372036854775809\n\n\nOverflowError: Python int too large to convert to C long\n\n\n\nmax_int + 1\nnp.int64(max_int + 1)\n\n9223372036854775808\n\n\nOverflowError: Python int too large to convert to C long"
  },
  {
    "objectID": "notebooks/numerical-computing.html#floats",
    "href": "notebooks/numerical-computing.html#floats",
    "title": "2  Numerical Computation",
    "section": "2.2 Floats",
    "text": "2.2 Floats\n\n2.2.1 Basics\nWhat if we want to represent decimal numbers or fractions instead of whole numbers, like \\(1.2\\) or \\(0.99999\\), or even irrational numbers like \\(\\pi=3.1415926\\dots\\)? To do this we need a new system of numbers that I’ll call floating point numbers, or floats, for reasons I’ll explain soon. Floats will be a computer’s best attempt to represent the real numbers \\(\\mathbb{R}\\). They’ll represent real numbers only approximately with some specified precision.\nIn python, floats are builtin objects of type float. Floats obey pretty much the same operations that integers do with some minor exceptions:\n\nAddition: \\(1.2 + 4.3 = 5.5\\).\nSubtraction: \\(1.2 - 4.3 = -3.1\\).\nMultiplication: \\(1.2 \\times 4.3 = 5.16\\).\nExponentiation: \\(4.3^2 = 18.49\\).\nRemainder (or Modulo): \\(4.3 \\text{ mod } 1.2 = 0.7\\).\nInteger Division: \\(4.3 \\ // \\ 1.2 = 3.0\\).\nDivision: \\(4.3 \\div 1.2\\).\n\nLet’s verify the first few of these to see what’s going on.\n\n1.2 + 4.3\n\n5.5\n\n\n\n1.2 - 4.3\n\n-3.0999999999999996\n\n\n\n1.2 * 4.3\n\n5.159999999999999\n\n\nMost of them look right. But what the heck is going on with \\(1.2 - 4.3\\) and \\(1.2 \\times 4.3\\)? We’re getting some weird trailing nines that shouldn’t be there. This gets to how floats are actually represented on a computer.\n\n\n2.2.2 Representing Floats\nRepresenting real numbers on a computer is a lot more subtle than representing integers. Since a computer can only have a finite number of bits, they can’t represent infinitely many digits, e.g. in irrational numbers like \\(\\pi\\). Using finite word sizes will necessarily have to truncate real numbers to some number of decimal places. This truncation will create an error in the calculation called numerical roundoff.\nSo how should we represent a decimal number using \\(n\\) bits? As an example, let’s imagine we’re trying to represent the number \\(x=157.208\\). Perhaps the first thing you might think of is to use some number of those bits to represent the integer part, and some number to represent the fractional part. Suppose you have \\(n=16\\) bits available to represent \\(x\\). Then maybe you can use 8 bits for the integer part \\(157\\), and 8 bits for the fractional part \\(0.208\\). Converting both halves to binary, you’d get \\[157 \\equiv 10011101, \\quad 0.208 \\equiv 0011010100111111.\\]\nTruncating both sequences to 8 bits (from the left), you could thus adopt a convention that \\(157.208 \\equiv 10011101 \\ 00110101\\).\nThis system is an example of a fixed point representation. This has to do with the fact that we’re always using a fixed number of bits for the integer part, and a fixed number for the fractional part. The decimal point isn’t allowed to float, or move around to allocate more bits to the integer or fractional part depending which needs more precision. The decimal point is fixed.\nAs I’ve suggested, the fixed point representation seems to be limited and not terribly useful. If you need really high precision in the fractional part, your only option is to use a larger word size. If you’re dealing with really big numbers and don’t care much about the fractional part, you also need a larger word size so you don’t run out of numbers. A solution to this problem is to allow the decimal point to float. We won’t allocate a fixed number of bits to represent the integer or fractional parts. We’ll design it in such a way that larger numbers give the integer part more bits, and smaller numbers give the fractional part more bits.\nThe trick to allowing the decimal point to float is to represent not just the digits of a number but also its exponent. Think about scientific notation, where if you have a number like say \\(x=1015.23\\), you can write it as \\(1.01523 \\cdot 10^3\\), or 1.01523e3. That \\(3\\) is the exponent. It says something about how big the number is. What we can do is convert a number to scientific notation. Then use some number of bits to represent the exponent \\(3\\) and some to represent the remaining part \\(1.01523\\). This is essentially the whole idea behind floating point.\nIn floating point representation, instead of using scientific notation with powers of ten, it’s more typical to use powers of two. When using powers of two, the decimal part can always be scaled to be between 1 and 2, so they look like \\(1.567\\) or something like that. Since the \\(1.\\) part is always there, we can agree it’s always there, and only worry about representing the fractional part \\(0.567\\). We’ll call this term the mantissa. Denoting the sign bit as \\(s\\), the exponent as \\(e\\), and the mantissa as \\(m\\), we can thus right any decimal number \\(x\\) in a modified scientific notation of the form \\[x = (-1)^s \\cdot (1+m) \\cdot 2^{e}.\\] Once we’ve converted \\(x\\) to this form, all we need to do is to figure out how to represent \\(s\\), \\(m\\), and \\(e\\) using some number of bits of \\(n\\), called the floating point precision. Assume the \\(n\\) bits of precision allocate \\(1\\) bit for the sign, \\(n_e\\) bits for the exponent, and \\(n_m\\) bits for the mantissa, so \\(n=1+n_e+n_m\\).\nHere are the steps to convert a number \\(x\\) into its \\(n\\)-bit floating point representation.\n\nGiven some number \\(x\\), get its modified scientific notation form \\(x = (-1)^s \\cdot (1+m) \\cdot 2^e\\).\n\nDetermine the sign of \\(x\\). If negative, set the sign bit to \\(s=1\\), else default to \\(s=0\\). Set \\(x = |x|\\).\nKeep performing the operation \\(x = x \\ // \\ 2\\) until \\(1 \\leq x \\leq 2\\). Keep track of the number of times you’re dividing, which is the exponent \\(e\\).\nThe remaining part will be some \\(1 \\leq x \\leq 2\\). Write it in the form \\(x = 1 + m\\), where \\(m\\) is the mantissa.\n\nConvert the scientific notation form into a sequence of \\(n\\) bits, truncating where necessary.\n\nFor reasons I’ll describe in a second, it’s good to add a bias term \\(b\\) to the exponent \\(e\\) before converting the exponent to binary. Let \\(e'=e+b\\) be this modified exponent.\nConvert each of \\(e'\\) and \\(m\\) into binary sequences, truncated to sizes \\(n_e\\), and \\(n_m\\) respectively.\nConcatenate these binary sequences together to get a sequence of \\(n=1+n_e+n_m\\) total bits. By convention, assume the order of bit concatenation is the sign bit, then exponent bits, then the mantissa bits.\n\n\nThere are of course other ways you could do it, for example by storing the sequences in a different order. I’m just stating one common way it’s done.\nSince all of this must seem like Greek, here’s a quick example. Let’s consider the number \\(x=15.25\\). We’ll represent it using \\(n=8\\) bits of precision, where \\(n_e=4\\) is the number of exponent bits, \\(n_m=3\\) is the number of precision bits, and \\(b=10\\) is the bias.\n\nConvert \\(x=15.25\\) to its modified scientific notation.\n\nSince \\(x \\geq 0\\) the sign is positive, so \\(s=0\\).\nKeep integer dividing \\(x\\) by \\(2\\) until it’s less than \\(2\\). It takes \\(e=3\\) divisions before \\(x<2\\).\nWe now have \\(x = 1.90625 \\cdot 2^3\\). The mantissa is then \\(m = (1.90625-1) = 0.90625\\).\nIn modified scientific notation form we now have \\(x=(-1)^0 \\cdot (1 + 0.90625) \\cdot 2^3\\).\n\nConvert everything to binary.\n\nAdding the bias to the exponent gives \\(e'=3+10=13\\).\nConverting each piece to binary we get \\(e' = 13 \\equiv 1101\\), \\(m = 0.90625 \\equiv 11101\\).\nSince \\(m\\) requires more than \\(n_m=3\\) bits to represent, truncate off the two right bits to get \\(m \\equiv 111\\).\n\nThis truncation will cause numerical roundoff, since \\(0.90625\\) truncates to \\(0.875\\). That’s an error of \\(0.03125\\) that gets permanently lost.\n\nThe final representation is thus \\(x \\equiv 0 \\ 1101 \\ 111\\).\n\n\nSo you can experiment, I wrote a helper function represent_as_float that lets you visualize this for different values of \\(x\\). Below I show the example I just calculated. I print out both the scientific notation form and its binary representation.\n\nrepresent_as_float(15.25, n=8, n_exp=4, n_man=3, bias=10)\n\nscientific notation: (-1)^0 * (1 + 0.90625) * 2^3\n8-bit floating point representation: 0 1101 111\n\n\nSo what’s going on with the bias term \\(b\\)? Why do we need it? The easiest answer to give is that without it, we can’t have negative exponents without having to use another sign bit for them. Consider a number like \\(x=0.5\\). In modified scientific notation this would look like \\(x=(-1)^0 \\cdot (1+0) \\cdot 2^{-1} = 2^{-1}\\), meaning its exponent would be \\(e=-1\\). Rather than have to keep yet another sign bit for the exponent, it’s easier to just add a bias term \\(b\\) that ensures the exponent \\(e'=e+b\\) is always non-negative. The higher the bias, the more precision we can show in the range \\(-1 < x < 1\\). The trade-off is that we lose precision for large values of \\(x\\).\nOn top of floats defined the way I mentioned, we also have some special numbers that get defined in a floating point system. These are \\(\\pm 0\\), \\(\\pm \\infty\\), and \\(\\text{NaN}\\) or “not a number”. Each of these numbers is allocated its own special sequence of bits, depending on the precision.\n\n\\(+0\\) and \\(-0\\): These numbers are typically represented using a biased exponent \\(e'=0\\) (all zero bits) and a mantissa \\(m=0\\) (all zero bits). The sign bit is used to distinguish between \\(+0\\) and \\(-0\\). In our example, these would be \\(+0 \\equiv 0 \\ 0000 \\ 000\\) and \\(-0 \\equiv 1 \\ 0000 \\ 000\\).\n\\(+\\infty\\) and \\(-\\infty\\): These numbers are typically represented using the max allowed exponent (all one bits) and a mantissa \\(m=0\\) (all zero bits). The sign bit is used to distinguish between \\(+\\infty\\) and \\(-\\infty\\). In our example, these would be \\(+\\infty \\equiv 0 \\ 1111 \\ 000\\) and \\(-\\infty \\equiv 1 \\ 1111 \\ 000\\).\n\\(\\text{NaN}\\): This value is typically represented using the max allowed exponent (all one bits) and a non-zero \\(m \\neq 0\\). The sign bit is usually not used for \\(\\text{NaN}\\) values. Note this means we can have many different sequences that all represent \\(\\text{NaN}\\). In our example, any number of the form \\(\\text{NaN} \\equiv \\text{x} \\ 1111 \\ \\text{xxx}\\) would work.\n\nSo I can illustrate some points about how floating point numbers behave, I’m going to generate all possible \\(8\\)-bit floats (excluding the special numbers) and plot them on a number line, similar to what I did above with the \\(8\\)-bit signed integers. I’ll generate the floats using the using the helper function gen_all_floats, passing in the number of mantissa bits n_man=3, the number of exponent bits n_exp=4, and a bias of bias=10.\nFirst, I’ll use these numbers to print out some interesting statistics of this 8-bit floating point system.\n\neight_bit_floats = gen_all_floats(n=8, n_man=3, n_exp=4, bias=10)\nprint(f'Total number of 8-bit floats: {len(eight_bit_floats)}')\nprint(f'Most negative float: {min(eight_bit_floats)}')\nprint(f'Most positive float: {max(eight_bit_floats)}')\nprint(f'Smallest nonzero float: {min([x for x in eight_bit_floats if x > 0])}')\nprint(f'Machine Epsilon: {min([x for x in eight_bit_floats if x > 1]) - 1}')\n\nTotal number of 8-bit floats: 120\nMost negative float: -56.0\nMost positive float: 56.0\nSmallest nonzero float: 0.001953125\nMachine Epsilon: 0.25\n\n\nWe can see that this 8-bit system only contains 120 unique floats. We could practically list them all out. Just like with the integers, we see there’s a most negative float, \\(-56.0\\), and a most positive float, \\(56.0\\). The smallest float, i.e. the one closest to \\(0\\), is \\(0.001953125\\). Notice how much more precision the smallest float has than the largest ones do. The largest ones are basically whole numbers, while the smallest one has nine digits of precision. Evidently, floating point representations give much higher precision to numbers close to zero than to numbers far away from zero.\nWhat happens if you try to input a float larger than the max, in this case \\(56.0\\)? Typically it will overflow. This will result in either the system raising an error, or the number getting set to \\(+\\infty\\), in a sense getting “rounded up”. Similarly, for numbers more negative than the min, in this case \\(-56.0\\), either an overflow error will be raised, or the number will get “rounded down” to \\(-\\infty\\).\nYou have to be careful in overflow situations like this, especially when you don’t know for sure which of these your particular system will do. It’s amusing to note that python will raise an overflow error, but numpy will round to \\(\\pm \\infty\\). Two different conventions to worry about. Just as amusing, when dealing with signed integers, it’s numpy that will raise an error if you overflow, while python won’t care. One of those things…\nWhat happens when you try to input a float smaller than the smallest value, in this case \\(0.001953125\\)? In this case, the number is said to undeflow. Usually underflow won’t raise an error. The number will pretty much always just get set to \\(+0\\) (or \\(-0\\)). This is again something you have to worry about, especially if you’re dealing with small numbers in denominators, where they can lead to division by zero errors which do get raised.\nOverflow and underflow errors are some of the most common numerical bugs that occur in deep learning, and usually result from not handling floats correctly to begin with.\nI also printed out a special value called the machine epsilon. The machine epsilon, denoted \\(\\varepsilon_m\\), is defined as the smallest value in a floating point system that’s larger than \\(1\\). In some sense, \\(\\varepsilon_m\\) is a proxy for how finely you can represent numbers in a given \\(n\\)-bit floating point system. The smaller \\(\\varepsilon_m\\) the more precisely you can represent numbers, i.e. the more decimal places of precision you get access to. In our case, we get \\(\\varepsilon_m=0.25\\). This means numbers in 8-bit floating point tend to be \\(0.25\\) apart from each other on average, which means we can represent numbers in this system only with a measly 2-3 digits of precision.\nWith these numbers in hand let’s now plot their distribution on the number line. I’ll use the helper function plot_number_dist function to do this. Compare with the plot of the signed integers I did above.\n\n\nCode\nplot_number_dist(eight_bit_floats, title='Distribution of 8-bit Floats')\n\n\n\n\n\n\n\n\n\nNotice how different this plot is from the ones for the signed integers. With the integers, the points were equally spaced. Now points close to \\(0\\) are getting represented much closer together than points far from \\(0\\). There are \\(74\\) of the \\(120\\) total points showing up just in the range \\([-1,1]\\). That’s over half!. Meanwhile, only \\(22\\) points total show up in the combined ranges of \\([-60,-10]\\) and \\([10,60]\\). Very strange.\nFeel free to play around with different floating point systems by using different choices for n, n_man, n_exp, and bias. Be careful, however, not to make n_exp too large or you may crash the kernel…\n\n\n2.2.3 Double Precision\nSo how does python represent floats? Python by default uses what’s called double precision to represent floats, also called float64. This means \\(n=64\\) total bits of precision are used, with \\(n_e=11\\), \\(n_m=52\\), and bias \\(b=1023=2^{10}-1\\). Double precision allows for a much larger range of numbers than 8-bit precision does:\n\nThe max value allowed is \\(2^{2^{n_e}-b} = 2^{1025} \\approx 10^{308}\\).\nThe min value allowed is \\(-2^{2^{n_e}-b} = -2^{1025} \\approx -10^{308}\\).\nNumbers outside the range of about \\([-10^{308}, 10^{308}]\\) will overflow.\nThe smallest values allowed are (plus or minus) \\(2^{-b+1} = 2^{-1022} \\approx 10^{-308}\\).\n\nUsing subordinal numbers, the smallest values are (plus or minus) \\(2^{-b-n_m+1} = 2^{-1074} \\approx 10^{-324}\\).\n\nNumbers inside the range of about \\([-10^{-308}, 10^{-308}]\\) will underflow.\n\nUsing subordinal numbers, this range is around \\([-10^{-324}, 10^{-324}]\\).\n\nThe machine epsilon is \\(\\varepsilon_m = 2^{-53} \\approx 10^{-16}\\).\nNumbers requiring more than about 15-16 digits of precision will get truncated, resulting in numerical roundoff.\nThe special numbers \\(\\pm \\infty\\), \\(\\pm 0\\), and \\(\\text{NaN}\\) are represented similarly as before, except using 64 bits.\n\nTo illustrate the point regarding numerical roundoff, here’s what happens if we try to use double precision floating point to define the constant \\(\\pi\\) to its first 100 digits? Notice it just gets truncated to its first 15 digits. Double precision is unable to keep track of the other 85 digits. They just get lost to numerical roundoff.\n\npi = 3.141592653589793238462643383279502884197169399375105820974944592307816406286208998628034825342117068\npi\n\n3.141592653589793\n\n\nAnother thing to worry about is adding small numbers to medium to large sized numbers, e.g. \\(10 + 10^{-16}\\), which will just get rounded down to \\(10.0\\).\n\n10.0 + 1e-16\n\n10.0\n\n\nNumerical roundoff is often an issue when subtracting two floats. Here’s what happens when we try to subtract two numbers that should be equal, \\(x=0.1+0.2\\) and \\(y=0.3\\). Instead of \\(y-x=0\\), we get \\(y-x \\approx -5.55 \\cdot 10^{-17}\\). The problem comes from the calculation \\(x=0.1+0.2\\), which caused a slight loss of precision in \\(x\\).\n\nx = 0.1 + 0.2\ny = 0.3\ny - x\n\n-5.551115123125783e-17\n\n\nA major implication of these calculations is that you should never test floating points for exact equality because numerical roundoff can mess it up. If you’d tried to test something like (y - x) == 0.0, you’d have gotten the wrong answer. Instead, you want to test that y - x is less than some small number tol, called a tolerance, i.e. abs(y - x) < tol.\n\ny - x == 0.0\n\nFalse\n\n\n\ntol = 1e-5\nabs(y - x) < tol\n\nTrue\n\n\nNumerical roundoff explains why we got the weird results above when subtracting \\(1.2 - 4.3\\). The imperfect precision in the two numbers resulted in a numerical roundoff error, leading in the trailing \\(9\\)s that should’ve rounded up to \\(-3.1\\) exactly. In general, subtracting floats is one of the most dangerous operations to do, as it tends to lead to the highest loss of precision in calculations. The closer two numbers are to being equal the worse this loss of precision tends to get.\nI mentioned that double precision has a smallest number of \\(2^{-1022} \\approx 10^{-308}\\), but caveated that by saying that, by using a trick called subordinal numbers, we can get the smallest number down to about \\(10^{-324}\\). What did I mean by this? It turns out that the bits where the biased exponent \\(e'=0\\) (i.e. all exponent bits are zero) go mostly unused in the standard version of double precision. By using this zero exponent and allowing the mantissa \\(m\\) to take on all its possible values, we can get about \\(2^{52}\\) more values (since the mantissa has 52 bits). This lets us get all the way down to \\(2^{-1022} \\cdot 2^{-52} = 2^{-1074} \\approx 10^{-324}\\).\nPython (and numpy) by default implements double precision with subordinal numbers, as we can see.\n\n2 ** (-1074)\n\n5e-324\n\n\n\n2 ** (-1075)\n\n0.0\n\n\nThe special numbers \\(\\pm \\infty\\), \\(\\pm 0\\), and \\(\\text{NaN}\\) are also defined in double precision. In python (and numpy) they’re given by the following commands,\n\n\\(\\infty\\): float('inf') or np.inf,\n\\(-\\infty\\): float('-inf') or -np.inf,\n\\(\\pm 0\\): 0,\n\\(\\text{NaN}\\): float('nan') or np.nan.\n\n\nfloat('inf')\nnp.inf\n\ninf\n\n\ninf\n\n\n\nfloat('-inf')\n-np.inf\n\n-inf\n\n\n-inf\n\n\n\n0\n-0\n\n0\n\n\n0\n\n\n\nfloat('nan')\nnp.nan\n\nnan\n\n\nnan\n\n\nYou may be curious what exactly \\(\\text{NaN}\\) (“not a number”) is and where it might show up. Basically, NaNs are used wherever values are undefined. Anytime an operation doesn’t return a sensible value it risks getting converted to NaN. One example is the operation \\(\\infty - \\infty = \\infty + (-\\infty)\\), which mathematically doesn’t make sense. No, it’s not zero…\n\nfloat('inf') + float('-inf')\nnp.inf - np.inf\n\nnan\n\n\nnan\n\n\nI’ll finish this section by mentioning that there are two other floating point representations worth being aware of: single precision (or float32), and half precision (or float16). Single precision uses 32 bits to represent a floating point number. Half precision uses 16 bits. It may seem strange to even bother having these less-precise precisions lying around, but they do have their uses. For example, half precision shows up in deep learning as a more efficient way to represent the weights of a neural network. Since half precision floats only take up 25% as many bits as default double precision floats do, using them can yield a 4x reduction in model memory sizes. We’ll see more on this later.\n\n\n2.2.4 Common Floating Point Pitfalls\nTo cap this long section on floats, here’s a list of common pitfalls people run into when working with floating point numbers, and some ways to avoid each one. This is probably the most important thing to take away from this section. You may find it helpful to reference later on. See this post for more information.\n\nNumerical overflow: Letting a number blow up to infinity (or negative infinity)\n\nClip numbers from above to keep them from being too large\nWork with the log of the number instead\nMake sure you’re not dividing by zero or a really small number\nNormalize numbers so they’re all on the same scale\n\nNumerical underflow: Letting a number spiral down to zero\n\nClip numbers from below to keep them from being too small\nWork with the exp of the number instead\nNormalize numbers so they’re all on the same scale\n\nSubtracting floats: Avoid subtracting two numbers that are approximately equal\n\nReorder operations so approximately equal numbers aren’t nearby to each other\nUse some algebraic manipulation to recast the problem into a different form\nAvoid differencing squares (e.g. when calculating the standard deviation)\n\nTesting for equality: Trying to test exact equality of two floats\n\nInstead of testing x == y, test for approximate equality with something like abs(x - y) <= tol\nUse functions like np.allclose(x, y), which will do this for you\n\nUnstable functions: Defining some functions in the naive way instead of in a stable way\n\nExamples: factorials, softmax, logsumexp\nUse a more stable library implementation of these functions\nLook for the same function but in log form, e.g. log_factorial or log_softmax\n\nBeware of NaNs: Once a number becomes NaN it’ll always be a NaN from then on\n\nPrevent underflow and overflow\nRemove missing values or replace them with finite values"
  },
  {
    "objectID": "notebooks/numerical-computing.html#array-computing",
    "href": "notebooks/numerical-computing.html#array-computing",
    "title": "2  Numerical Computation",
    "section": "2.3 Array Computing",
    "text": "2.3 Array Computing\nIn machine learning and most of scientific computing we’re not interested in operating on just single numbers at a time, but many numbers at a time. This is done using array operations. The most popular library in python for doing numerical computation on arrays is numpy.\nWhy not just do numerical computations in base python? After all, if we have large arrays of data we can just put them in a list. Consider the following example. Suppose we have two tables of data, \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\). Each table has \\(m=5\\) rows and \\(n=3\\) columns. The rows represent samples, e.g. measured in a lab, and the columns represent the variables, or features, being measured, call them \\(x\\), \\(y\\), and \\(z\\), if you like. I’ll define these two tables using python lists A and B below.\n\nA = [[3.5, 18.1, 0.3],\n     [-8.7, 3.2, 0.5],\n     [-1.3, 8.4, 0.2],\n     [5.6, 12.9, 0.9],\n     [-6.8, 19.7, 0.7]]\n\nB = [[-9.7, 12.5, 0.1],\n     [-5.1, 14.1, 0.6],\n     [-1.6, 3.7, 0.7],\n     [2.3, 19.3, 0.9],\n     [8.2, 9.7, 0.2]]\n\nSuppose we wanted to add the elements in these two tables together, index by index, like this,\n\\[\n\\begin{bmatrix}\nA[0][0] + B[0][0], & A[0][1] + B[0][1], & A[0][2] + B[0][2] \\\\\nA[1][0] + B[1][0], & A[1][1] + B[1][1], & A[1][2] + B[1][2] \\\\\nA[2][0] + B[2][0], & A[2][1] + B[2][1], & A[2][2] + B[2][2] \\\\\nA[3][0] + B[3][0], & A[3][1] + B[3][1], & A[3][2] + B[3][2] \\\\\nA[4][0] + B[4][0], & A[4][1] + B[4][1], & A[4][2] + B[4][2] \\\\\n\\end{bmatrix}.\n\\]\nIf we wanted to do this in python, we’d have to loop over all rows and columns and place the sums one-by-one inside an array \\(\\mathbf{C}\\), like this.\n\ndef add_arrays(A, B):\n    n_rows, n_cols = len(A), len(A[0])\n    C = []\n    for i in range(n_rows):\n        row = []\n        for j in range(n_cols):\n            x = A[i][j] + B[i][j]\n            row.append(x)\n        C.append(row)\n    return C\n\nC = add_arrays(A, B)\nprint(f'C = {np.array(C).round(2).tolist()}')\n\nC = [[-6.2, 30.6, 0.4], [-13.8, 17.3, 1.1], [-2.9, 12.1, 0.9], [7.9, 32.2, 1.8], [1.4, 29.4, 0.9]]\n\n\nNumpy makes this far easier to do. It implements element-wise array operatations, which allow us to operate on arrays with far fewer lines of code. In numpy, to perform the same adding operation we just did, we’d just add the two arrays together directly, \\(\\mathbf{A}+\\mathbf{B}\\).\nTo use numpy operations we have to convert data into the native numpy data type, the numpy array. Do this by wrapping lists inside the function np.array. Once we’ve done this, we can just add them together in one line. This will simultaneously element-wise add the elements in the array so we don’t have to loop over anything.\n\nA = np.array(A)\nB = np.array(B)\nprint(f'C = \\n{A+B}')\n\nC = \n[[ -6.2  30.6   0.4]\n [-13.8  17.3   1.1]\n [ -2.9  12.1   0.9]\n [  7.9  32.2   1.8]\n [  1.4  29.4   0.9]]\n\n\nThis is really nice. We’ve managed to reduce a double foor loop of 8 lines of code down to just 1 line with no loops at all. Of course, there are loops happening in the background inside the numpy code, we just don’t see them.\nNumpy lets us do this with pretty much any arithmetic operation we can think of. We can element-wise add, subtract, multiply, or divide the two arrays. We can raise them to powers, exponentiate them, take their logarithms, etc. Just like we would do so with single numbers. In numpy, arrays become first class citizens, treated on the same footing as the simpler numerical data types int and float. This is called vectorization.\nHere are a few examples of different vectorized functions we can call on A and B. All of these functions are done element-wise.\n\nA - B\n\narray([[ 13.2,   5.6,   0.2],\n       [ -3.6, -10.9,  -0.1],\n       [  0.3,   4.7,  -0.5],\n       [  3.3,  -6.4,   0. ],\n       [-15. ,  10. ,   0.5]])\n\n\n\nA / B\n\narray([[-0.36082474,  1.448     ,  3.        ],\n       [ 1.70588235,  0.22695035,  0.83333333],\n       [ 0.8125    ,  2.27027027,  0.28571429],\n       [ 2.43478261,  0.66839378,  1.        ],\n       [-0.82926829,  2.03092784,  3.5       ]])\n\n\n\nA ** B\n\narray([[5.27885788e-06, 5.25995690e+15, 8.86568151e-01],\n       [           nan, 1.32621732e+07, 6.59753955e-01],\n       [           nan, 2.62925893e+03, 3.24131319e-01],\n       [5.25814384e+01, 2.71882596e+21, 9.09532576e-01],\n       [           nan, 3.60016490e+12, 9.31149915e-01]])\n\n\n\nnp.sin(A)\n\narray([[-0.35078323, -0.68131377,  0.29552021],\n       [-0.66296923, -0.05837414,  0.47942554],\n       [-0.96355819,  0.85459891,  0.19866933],\n       [-0.63126664,  0.32747444,  0.78332691],\n       [-0.49411335,  0.75157342,  0.64421769]])\n\n\nIf vectorization just made code easier to read it would be a nice to have. But it’s more than this. In fact, vectorization also makes your code run much faster in many cases. Let’s see an example of this. I’ll again run the same operations above to add two arrays, but this time I’m going to profile the code in each case. That is, I’m going to time each operation over several runs and average the times. The ones with the lowest average time is faster than the slower one, obviously. To profile in a notebook, the easiest way is to use the %timeit magic command, which will do all this for you.\n\nA = A.tolist()\nB = B.tolist()\n%timeit C = add_arrays(A, B)\n\n2.61 µs ± 8.21 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n\n\n\nA = np.array(A)\nB = np.array(B)\n%timeit C = A + B\n\n411 ns ± 0.75 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n\n\nEven with these small arrays the numpy vectorized array addition is almost 10 times faster than the python loop array addition. This difference becomes much more pronounced when arrays are larger. The arrays just considered are only of shape \\((10,3)\\). We can easily confirm this in numpy using the methods A.shape and B.shape.\n\nprint(f'A.shape = {A.shape}')\nprint(f'B.shape = {B.shape}')\n\nA.shape = (5, 3)\nB.shape = (5, 3)\n\n\nLet’s try to run the add operations on much larger arrays of shape \\((10000,100)\\). To do this quickly I’ll use np.random.rand(shape), which will sample an array with shape shape whose values are uniformly between 0 and 1. More on sampling in a future lesson. Running the profiling, we’re now running about 100 times faster using numpy vectorization compared to python loops.\n\nD = np.random.rand(10000, 100)\nE = np.random.rand(10000, 100)\n\n\nD = D.tolist()\nE = E.tolist()\n%timeit F = add_arrays(D, E)\n\n119 ms ± 517 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\n\nD = np.array(D)\nE = np.array(E)\n%timeit F = D + E\n\n1.35 ms ± 60.2 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n\n\nSo why is numpy vectorization so much faster than using native python loops? Because it turns out that numpy by and large doesn’t actually perform array operations in python! When array operations are done, numpy compiles them down to low-level C code and runs the operations there, where things are much faster.\nNot only that, numpy takes advantage of very efficient linear algebra functions written over the course of decades by smart people. These functions come from low-level FORTRAN and C libraries like BLAS and LAPACK. They’re hand-designed to take maximum advantage of computational speed-ups where available. These include things like parallelization, caching, and hardware vectorization operations. Native python doesn’t take advantage of any of these nice things. The moral is, if you want to run array operations efficiently, you need to use a numerical library like numpy or modern variants like pytorch.\n\n2.3.1 Higher-Dimensional Arrays\nThe number of different dimensions an array has is called its dimension or rank. Equivalently, the rank or dimension of an array is just the length of its shape tuple. The arrays I showed above are examples of rank-2 or 2-dimensional arrays. We can define arrays with any number of dimensions we like. These arrays of different rank sometimes have special names:\n\nA 0-dimensional (rank-0) array is called a scalar. These are single numbers.\nA 1-dimensional (rank-1) array is called a vector. These are arrays with only one row.\nA 2-dimensional (rank-2) array is called a matrix. These are arrays with multiple rows.\nAn array of dimension or rank 3 or higher is called a tensor. These are arrays with multiple matrices.\n\nMore on these when we get to linear algebra. Here are some examples so you can see what they look like. Note I’m using dtype=np.float64 to explicitly cast the values as float64 when defining the arrays. Numpy’s vectorization operations work for all of these arrays regardless of their shape.\n\nscalar = np.float64(5)\nscalar # 0-dimensional\n\n5.0\n\n\n\nvector = np.array([1, 2, 3], dtype=np.float64)\nprint(f'vector.shape = {vector.shape}')\nprint(f'vector = {vector}')\n\nvector.shape = (3,)\nvector = [1. 2. 3.]\n\n\n\nmatrix = np.array([[1, 2, 3], [4, 5, 6]], dtype=np.float64)\nprint(f'matrix.shape = {matrix.shape}')\nprint(f'matrix = \\n{matrix}')\n\nmatrix.shape = (2, 3)\nmatrix = \n[[1. 2. 3.]\n [4. 5. 6.]]\n\n\n\ntensor = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]], dtype=np.float64)\nprint(f'tensor.shape = {tensor.shape}')\nprint(f'tensor = \\n{tensor}')\n\ntensor.shape = (2, 2, 2)\ntensor = \n[[[1. 2.]\n  [3. 4.]]\n\n [[5. 6.]\n  [7. 8.]]]\n\n\nNumpy also supports array aggregation operations as well. Suppose you have a matrix A and want to get the sum of the values in each row of A. To do this, you could use np.sum(A, axis=1), where axis is the index of the dimension you want to sum over (the columns in this case). This will return a vector where the value at index \\(i\\) is the sum of elements in row \\(i\\). To sum over all elements in the array, don’t pass anything to axis.\n\nA = np.array([[1, 2, 3], [-1, -2, -3], [1, 0, -1]], dtype=np.float64)\nprint(f'A = \\n{A}')\nprint(f'sum over all A = {np.sum(A)}')\nprint(f'row sums of A = {np.sum(A, axis=1)}')\n\nA = \n[[ 1.  2.  3.]\n [-1. -2. -3.]\n [ 1.  0. -1.]]\nsum over all A = 0.0\nrow sums of A = [ 6. -6.  0.]\n\n\nIndexing into numpy arrays like A is more powerful than with python lists. Instead of having to awkwardly index like A[1][0], write A[1, 0]. To get all values in column index 1, write A[:, 1]. To get just the first and last row, we could just pass the index we want in as a list like this, A[[0, -1], :].\n\nprint(f'A[1, 0] = {A[1][0]} = {A[1, 0]}')\n\nA[1, 0] = -1.0 = -1.0\n\n\n\nprint(f'col 1 of A = {A[:, 1]}')\nprint(f'rows 0 and -1 of A = \\n{A[[0, -1], :]}')\n\ncol 1 of A = [ 2. -2.  0.]\nrows 0 and -1 of A = \n[[ 1.  2.  3.]\n [ 1.  0. -1.]]\n\n\nNumpy also supports Boolean masks as indexes. Suppose we want to get all the positive elements x >= 0 in A. We could create a mask A > 0, and pass that into A as an index to pick out the positive elements only.\n\nprint(f'mask of (A >= 0) = \\n{(A >= 0)}')\nprint(f'elements of (A >= 0) = \\n{A[A >= 0]}')\n\nmask of (A >= 0) = \n[[ True  True  True]\n [False False False]\n [ True  True False]]\nelements of (A >= 0) = \n[1. 2. 3. 1. 0.]"
  },
  {
    "objectID": "notebooks/numerical-computing.html#broadcasting",
    "href": "notebooks/numerical-computing.html#broadcasting",
    "title": "2  Numerical Computation",
    "section": "2.4 Broadcasting",
    "text": "2.4 Broadcasting\nBroadcasting is a set of conventions for doing array operations on arrays with incompatible shapes. This may seem like a strange thing to do, but it turns out knowing how and when to broadcast can make your code much shorter, more readable, and efficient. All modern-day numerical libraries in python support broadcasting, including numpy, pytorch, tensorflow, etc. So it’s a useful thing to learn.\n\n2.4.1 Motivation\nLet’s start with a simple example. Suppose we have an array of floats defined below. We’d like to add 1 to every number in the array. How can we do it? One “pythonic” way might be to use a list comprehension like so. This will work just fine, but it requires going back and forth between arrays and lists.\n\nx = np.array([1., 2., 3., 4., 5.])\nprint(f'x = {x}')\n\nx_plus_1 = np.array([val + 1 for val in x])\nprint(f'x + 1 = {x_plus_1}')\n\nx = [1. 2. 3. 4. 5.]\nx + 1 = [2. 3. 4. 5. 6.]\n\n\nWhat if we didn’t want to go back and forth like that? It is slow after all. Anytime numpy has to handoff back to python or vice versa it’s going to slow things down. Another thing we could try is to make a vector of ones of the same size as x, then add it to x. This is also fine, but it requires defining this extra array of ones just to add 1 to the original array.\n\nones = np.ones(len(x))\nx_plus_1 = x + ones\nprint(f'x + 1 = {x_plus_1}')\n\nx + 1 = [2. 3. 4. 5. 6.]\n\n\nWe’d like to be able to just add 1 to the array like we would with numbers. If x were a single number we’d just write x + 1 to add one to it, right? But technically we can’t do this if x is an array, since x has shape (5,) and 1 is just a number with no shape. This is where broadcasting comes in. Broadcasting says let’s define the operation x + 1 so that it means add 1 to every element of x.\n\nx_plus_1 = x + 1\nprint(f'x + 1 = {x_plus_1}')\n\nx + 1 = [2. 3. 4. 5. 6.]\n\n\nThis notation has the advantage of keeping array equations simple, while at the same time keeping all operations in numpy so that they run fast.\n\n\n2.4.2 Broadcasting Rules\nSuppose now that we have two arrays A and B of arbitrary shape and we want to operate on them, e.g. via the operations +, -, *, /, //, **. Here are the general broadcasting rules, quoted directly from the numpy documentation.\n\nNumpy DocumentationWhen operating on two arrays, numpy compares their shapes element-wise. It starts with the trailing (i.e. rightmost) dimensions and works its way left. Two dimensions are compatible when 1. they are equal, or2. one of them is 1 If these conditions are not met, a ValueError: operands could not be broadcast together exception is thrown, indicating that the arrays have incompatible shapes. The size of the resulting array is the size that is not 1 along each axis of the inputs.\n\nLet’s look at an example. First, suppose A has shape (2, 2, 3) and B has shape (3,). Suppose for simplicity that they’re both arrays of all ones. Here’s what this looks like, with B aligned to the right.\n\\[\\begin{align*}\nA &:& 2, & & 2, & & 3 \\\\\nB &:&   & &   & & 3 \\\\\n\\hline\nC &:& 2, & & 2, & & 3 \\\\\n\\end{align*}\\]\nHere are the broadcasting steps that will take place. Note that only B will change in this example. A will stay fixed.\n\nNumpy will start in the rightmost dimension, checking if they’re equal.\nBegin with A of shape (2, 2, 3) and B of shape (3,).\nIn this case, the rightmost dimension is 3 in both arrays, so we have a match.\nMoving left by one, B no longer has anymore dimensions, but A has two, each 2. These arrays are thus compatible.\nNumpy will now copy B to the left in these new dimensions until it has the same shape as A.\n\nCopy values of B twice to get B = [[1, 1, 1], [1, 1, 1]] with shape (2, 3).\nCopy values of B twice again to get B = [[[1, 1, 1], [1, 1, 1]], [[1, 1, 1], [1, 1, 1]]] with shape (2, 2, 3).\n\nThe shapes of A and B are now equal. The output array C will have shape (2, 2, 3).\n\nLet’s verify this is true on two simple arrays of ones. Let’s also print out what C looks like. Since only copying is taking place we should just be adding 2 arrays of ones, hence the output should sum 2 arrays of ones, giving one array C of twos.\n\nA = np.ones((2, 2, 3))\nB = np.ones(3,)\nprint(f'A.shape = {A.shape}')\nprint(f'B.shape = {B.shape}')\n\nC = A + B\nprint(f'C.shape = {C.shape}')\nprint(f'C = \\n{C}')\n\nA.shape = (2, 2, 3)\nB.shape = (3,)\nC.shape = (2, 2, 3)\nC = \n[[[2. 2. 2.]\n  [2. 2. 2.]]\n\n [[2. 2. 2.]\n  [2. 2. 2.]]]\n\n\nLet’s do one more example. Suppose now that A has shape (8, 1, 6, 1) and B has shape (7, 1, 5). Here’s a table of this case, again with B aligned to the right since it has the fewest dimensions.\n\\[\\begin{align*}\nA &:& 8, & & 1, & & 6, & & 1 \\\\\nB &:&    & & 7, & & 1, & & 5 \\\\\n\\hline\nC &:& 8, & & 7, & & 6, & & 5 \\\\\n\\end{align*}\\]\nHere are the broadcasting steps that will take place.\n\nStarting again from the right, dimensions 1 and 5 don’t match. But since A has a 1 rule (2) applies, so A will broadcast itself (i.e. copy its values) 5 times in this dimension to match B.\nMoving left by one we get 6 and 1. Now B will broadcast itself in this dimension 6 times to match A.\nMoving left again we get 1 and 7. Now A will broadcast itself in this dimension 7 times to match B.\nLast, we get 8 in A and B is out of dimensions, so B will broadcast itself 8 times to match A.\nThe shapes of A and B are now equal. The output C thus has shape (8, 7, 6, 5).\n\nHere again is an example on two arrays of ones. Verify that the shapes come out right.\n\nA = np.ones((8, 1, 6, 1))\nB = np.ones((7, 1, 5))\nprint(f'A.shape = {A.shape}')\nprint(f'B.shape = {B.shape}')\n\nC = A / B\nprint(f'C.shape = {C.shape}')\n\nA.shape = (8, 1, 6, 1)\nB.shape = (7, 1, 5)\nC.shape = (8, 7, 6, 5)\n\n\nThat’s pretty much all there is to broadcasting. It’s a systematic way of trying to copy the dimensions in each array until they both have the same shape. All this broadcasting is done under the hood for you when you try to operate on two arrays of different shapes. You don’t need to do anything but understand how the arrays get broadcast together so you can avoid errors in your calculations, sometimes very subtle errors.\nThis can be a bit confusing to understand if you’re not used to it. We’ll practice broadcasting a good bit so you can get the hang of it."
  },
  {
    "objectID": "notebooks/basic-calculus.html#infinitesimals",
    "href": "notebooks/basic-calculus.html#infinitesimals",
    "title": "3  Basic Calculus",
    "section": "3.1 Infinitesimals",
    "text": "3.1 Infinitesimals\nFundamental to the understanding of calculus is the idea of an “infinitely small” number, called an infinitesimal. An infinitesimal is a number that’s not 0 but so close to being 0 that you can’t really tell it isn’t 0. These small numbers are often written in math with letters like \\(\\varepsilon\\) or \\(\\delta\\). Think of them as very very tiny numbers, so tiny their square is basically 0: \\[\\varepsilon > 0, \\quad \\varepsilon^2 \\approx 0.\\]\nBut what does this even mean? Here it might be helpful to recall our discussion of floating point numbers. Recall that we can’t get infinite precision. In python’s double precision floating point we can only get down to about \\(5 \\cdot 10^{-324}\\), or 5e-324. If the square of a small number has a value smaller than about 5e-324 we’d literally get 0.0 as far as python is concerned.\nJust for fun let’s look at the really tiny number \\(10^{-300}\\), or 1e-300. That’s 300 decimal places of zeros before the 1 even shows up. Python thinks 1e-300 is just fine. But what happens if we square it? We should in theory get \\(10^{-600}\\), or 600 decimal places of zeros followed by a 1. But as far as floating point is concerned, the square is zero.\n\nepsilon = np.float64(1e-300)\nprint(f'epsilon = {epsilon}')\nprint(f'epsilon^2 = {epsilon ** 2}')\n\nepsilon = 1e-300\nepsilon^2 = 0.0\n\n\nOf course, you could argue that we could just go to a higher precision then. Use more bits. But eventually, if we keep making \\(\\varepsilon\\) small enough we’ll hit a point where \\(\\varepsilon^2 = 0\\). Thus, if it makes you feel better, when you see an infinitesimal just think “\\(10^{-300}\\) in double precision”.\nAside: If you want to be really pedantic, you might say that it shouldn’t matter what a computer does, since any positive number \\(\\varepsilon\\) squared must still be greater than zero, no matter how small \\(\\varepsilon\\) is. This is true for real numbers \\(\\mathbb{R}\\). But it turns out infinitesimals aren’t real numbers at all. They lie in an extension of the real number line called the hyperreal numbers, denoted \\(\\mathbb{R}^*\\). In my opinion, this isn’t an important distinction to worry about in applied calculus.\nSimilar to infinitesimals being numbers that can be really, really small, we can also talk about numbers being really, really big. These are called infinitely large numbers. In analogy to infinitesimals, infinitely large numbers are positive numbers \\(N\\) whose square is basically infinite,\n\\[N > 0, \\quad N^2 \\approx \\infty.\\]\nWe can get infinitely large numbers by inverting infinitesimals, and vice versa,\n\\[N = \\frac{1}{\\varepsilon}, \\quad \\varepsilon = \\frac{1}{N}.\\]\nIf \\(10^{-300}\\) is a good rule of thumb for an infinitesimal, then \\(10^{300}\\) is a good rule of thumb for an infinitely large number.\n\nN = np.float64(1e300)\nprint(f'N = {N}')\nprint(f'N^2 = {N ** 2}')\n\nN = 1e+300\nN^2 = inf\n\n\nInfinitesimals are especially interesting when added to regular numbers. These are called first order perturbations. For example, consider some finite number \\(x\\). It could be \\(2\\) or \\(-100\\) or whatever you want. Suppose now we add to it an infinitesimal number \\(\\varepsilon\\). Now suppose we have an output \\(y\\) that depends on \\(x\\) through a function \\(y=f(x)=x^2\\). What happens to \\(y\\) if we perturb \\(x\\) to \\(x+\\varepsilon\\)? That is, what is \\(f(x + \\varepsilon)=(x+\\varepsilon)^2\\)? Expanding the square, we have\n\\[f(x + \\varepsilon) = (x + \\varepsilon)^2 = x^2 + 2x\\varepsilon + \\varepsilon^2.\\]\nBut since \\(\\varepsilon^2 \\approx 0\\),\n\\[f(x + \\varepsilon) = (x + \\varepsilon)^2 \\approx x^2 + 2x\\varepsilon.\\]\nOkay, but what does this mean? Well, I can reformulate the question as follows: “If I change \\(x\\) by a little bit, how much does the function \\(y\\) change”? Call this change \\(\\delta\\), the change in \\(y\\) due to \\(x\\) getting changed by \\(\\varepsilon\\). Since \\(\\delta = f(x+\\varepsilon) - f(x)\\) by definition, we’d have\n\\[\\delta = f(x+\\varepsilon) - f(x) = (x+\\varepsilon)^2 - x^2 \\approx 2x\\varepsilon.\\]\nThat is, if we change \\(x\\) by a small amount \\(\\varepsilon\\), then \\(y\\) itself changes by a small amount \\(\\delta=2x\\varepsilon\\). Interestingly, how much \\(y\\) changes actually depends on which \\(x\\) we pick. If \\(x=1\\) then \\(y\\) changes by \\(2\\varepsilon\\), just twice how much \\(x\\) is nudged. If \\(x=1000\\) though, then \\(y\\) changes by \\(2000\\varepsilon\\), a much bigger change, but still infinitesimal. After all, \\(2000 \\cdot 10^{-300} = 2 \\cdot 10^{-297}\\) is still really, really small."
  },
  {
    "objectID": "notebooks/basic-calculus.html#limits",
    "href": "notebooks/basic-calculus.html#limits",
    "title": "3  Basic Calculus",
    "section": "3.2 Limits",
    "text": "3.2 Limits\nOne application of infinitesimals is to look at the nearby behavior of a function around some point. Suppose we have some function \\(y=f(x)\\). We’d like to look at the nearby behavior of the function around some point \\(x=x_0\\). Pick a point \\(x\\) that’s infinitesimally close to \\(x_0\\), so \\(x \\approx x_0\\), yet \\(x \\neq x_0\\) exactly. If \\(f(x) \\approx L\\), we say \\(L\\) is the limit as \\(x\\) approaches \\(x_0\\), and write\n\\[L = \\lim_{x \\rightarrow x_0} f(x).\\]\nMore formally, say \\(L\\) is the limit if the difference \\(|f(x_0+\\varepsilon)-L|\\) is infinitesimal whenever \\(\\varepsilon\\) is infinitesimal. Another notation for the limit is\n\\[y \\rightarrow L \\text{ as } x \\rightarrow x_0.\\]\nThis all seems kind of pedantic if you think about it. It seems like we’re doing a bunch of extra work just to evaluate the function at \\(x_0\\), which of course would imply \\(L=f(x_0)\\). In most practical cases this is true, but not always.\nA classic example is a function with a hole in it. Suppose we have a function \\(y=f(x)\\) like this\n\\[\ny =\n\\begin{cases}\n1 & x = 0, \\\\\nx^2 & x \\neq 0.\n\\end{cases}\n\\]\nHere’s what it looks like. It’s just a parabola \\(y=x^2\\), but with a hole at \\(x=0\\) since \\(f(0)=1\\neq 0^2\\).\n\n\nCode\nx = np.arange(-2, 2, 0.01)\nf_neq_0 = lambda x: x ** 2\n\nplt.plot(x, f_neq_0(x), color='red', zorder=0)\nplt.scatter(0, 1, color='red', marker='o', s=20, zorder=1)\nplt.scatter(0, 0, c='white', edgecolors='red', marker='o', s=20, zorder=2)\nplt.grid(True, alpha=0.5)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('$y = 1$ if $x = 0$ else $x^2$')\nplt.show()\n\n\n\n\n\n\n\n\n\nLet’s try to find the limit of this function at \\(x_0=0\\). Pick an \\(x\\) close to \\(0\\), but not exactly \\(0\\), say \\(x=\\varepsilon\\), where \\(\\varepsilon\\) is infinitesimal. Then we’d have\n\\[L = \\lim_{x \\rightarrow 0} f(x) = f(\\varepsilon) = \\varepsilon^2 \\approx 0.\\]\nBut \\(f(0) = 1\\), which means the limit at \\(x=0\\) is not the value of the function at \\(x=0\\). This is just a long, drawn-out way of saying that the limit is what the function would be if it didn’t have a hole in it at \\(x=0\\).\nA function that is well-behaved in this sense that it has no holes or jumps is called continuous. Continuous functions satisfy the very nice property that the limit can be pulled inside the function,\n\\[\\lim_{x \\rightarrow x_0} f(x) = f\\bigg(\\lim_{x \\rightarrow x_0} x\\bigg) = f(x_0).\\]\nSince almost every function we work with in practice is continuous, we can by and large take for granted that we can do this, keeping in mind that exceptions do occur and you sometimes have to be careful.\nIn my experience, the only real place limits seem to come up in machine learning is the infinite limit case when \\(x \\rightarrow \\infty\\),\n\\[\\lim_{x \\rightarrow \\infty} f(x).\\]\nThe easiest way to figure these limits out is to plug in a large values for \\(x\\) and see what \\(f(x)\\) is tending towards as \\(x\\) gets large. As an example, let’s try to find the limit of \\(y=e^{-x}\\) as \\(x \\rightarrow \\infty\\),\n\\[\\lim_{x \\rightarrow \\infty} e^{-x}.\\]\nHere’s what the function looks like. It seems to rapidly decay to \\(0\\) as \\(x\\) gets large. Not even large. It’s basically at \\(y \\approx 0\\) by the time \\(x=10\\).\n\n\nCode\nx = np.arange(0, 10, 0.1)\nf = lambda x:  np.exp(-x)\nplot_function(x, f, xlim=(-0.5, 10), ylim=(-0.5, 1.5), ticks_every=None, title='$y=e^{-x}$')\n\n\n\n\n\n\n\n\n\nLet’s pick successively large values of \\(x\\) and see if we can identify the limit numerically. I’ll choose \\(x=e^0, e^1, \\cdots, e^5\\). You can see that numerically speaking \\(y=e^{-x}\\) is pretty much \\(0\\) by the time \\(x=20\\). This suggests the limit is \\(L=0\\) as \\(x \\rightarrow \\infty\\). While this isn’t a “proof”, it should be pretty convincing.\n\nfor x in np.exp(range(6)):\n    y = np.exp(-x)\n    print(f'x = {x.round(2)} \\t y = {y.round(8)}')\n\nx = 1.0      y = 0.36787944\nx = 2.72     y = 0.06598804\nx = 7.39     y = 0.00061798\nx = 20.09    y = 0.0\nx = 54.6     y = 0.0\nx = 148.41   y = 0.0\n\n\nIf you like, you can also use sympy to evaluate the infinite limit symbolically.\n\nx = sp.symbols('x')\ny = sp.exp(-x)\nlimit = sp.limit(y, x, sp.oo)\nprint(f'lim e^(-x) as x -> infinity = {limit}')\n\nlim e^(-x) as x -> infinity = 0\n\n\nWhen dealing with infinite limits of functions, the tricks is to identify the term in the function that’ll be the largest when \\(x\\) gets really big. For example, take the continuous function \\(y=x e^{-x} + 1\\). The first term \\(xe^{-x}\\) is dominated by \\(e^{-x}\\) when \\(x\\) is large, since it shrinks to \\(0\\) much faster than \\(x\\) increases. Since the other term just stays at \\(1\\), we get\n\\[\\lim_{x \\rightarrow \\infty} (x e^{-x} + 1) = 0 + 1 = 1.\\]\nWhen \\(x\\) gets large, the following rule of thumb holds for different classes of functions, with functions to the left dominating functions to the right,\n\\[\\text{factorials} >> \\text{exponents} >> \\text{polynomials} >> \\text{logarithms} >> \\text{constants}.\\]\nIf you’ll recall, this exact same chain was shown when I talked about asymptotic notation. In fact, we can use infinite limits to formally define what big-oh notation means. Say a function \\(f(x)\\) is \\(O(g(x))\\) if\n\\[\\lim_{n \\rightarrow \\infty} \\bigg|\\frac{f(n)}{g(n)}\\bigg| \\leq M\\]\nfor some finite constant \\(M > 0\\). For example, the function \\(f(n) = n^3 + 2n^2 - 5n\\) is \\(O(n^3)\\) since\n\\[\\lim_{n \\rightarrow \\infty} \\bigg|\\frac{n^3 + 2n^2 - 5n}{n^3}\\bigg| =  \\lim_{n \\rightarrow \\infty} \\bigg(1 + \\frac{2}{n} - \\frac{5}{n^2}\\bigg) = 1 \\leq 1.\\]"
  },
  {
    "objectID": "notebooks/basic-calculus.html#differentiation",
    "href": "notebooks/basic-calculus.html#differentiation",
    "title": "3  Basic Calculus",
    "section": "3.3 Differentiation",
    "text": "3.3 Differentiation\n\n3.3.1 Derivatives\nLet’s go back to our example before where we looked at first order perturbations to the function \\(y=x^2\\). When we talk about changing \\(x\\) by a little bit and asking how \\(y\\) changes we use a cleaner notation. Instead of writing \\(x + \\varepsilon\\), we’d write \\(x + dx\\). Instead of writing \\(y + \\delta\\), we’d write \\(y + dy\\). These values \\(dx\\) and \\(dy\\) are called differentials. Differentials are just like the infinitesimals I defined before, except the differential notation makes it clear what is a small change of what. Writing \\(dx\\) means “a little bit of \\(x\\)”. Writing \\(dy\\) means “a little bit of \\(y\\)”. This is where the term “differentiation” comes from.\nSuppose \\(x\\) gets nudged a little bit to \\(x+dx\\). Then \\(y=f(x)\\) gets nudged to \\(y+dy=f(x+dx)\\). In our running example, this is\n\\[y + dy = f(x+dx) = (x + dx)^2 = x^2 + 2xdx + (dx)^2 \\approx x^2 + 2xdx.\\]\nThe amount that \\(y\\) gets nudged is just \\(dy\\). Let’s solve for \\(dy\\). Subtracting both sides by \\(y=x^2\\) just gives \\(dy = 2xdx\\). Dividing both sides by \\(dx\\), we get\n\\[\\frac{dy}{dx} = 2x.\\]\nThis ratio of differentials \\(\\frac{dy}{dx}\\) is called the derivative of the function \\(y=x^2\\), usually just pronounced “dydx”. Notice there’s no differential on the right-hand side, hence the derivative is not itself infinitesimal. It’s a finite ratio of infinitesimals.\nWe can talk about any reasonably well-behaved function \\(y=f(x)\\) having a derivative. If we change \\(x\\) by an infinitesimal amount \\(dx\\), then \\(y\\) changes by an amount \\(dy=f(x+dx)-f(x)\\). The derivative is just the ratio of these differential changes,\n\\[\\frac{dy}{dx} = \\frac{f(x+dx)-f(x)}{dx}.\\]\nWhat do I mean when I say the function \\(f(x)\\) needs to be reasonably well behaved? For one thing, the function needs to be continuous. Informally, you can think of a univariate function as being continuous if you can draw its graph on a piece of paper without lifting your pen. There are no jumps or holes anywhere in the functions’ curve. More formally, a function is continuous at \\(x_0\\) if \\(f(x) \\rightarrow f(x_0)\\) as \\(x \\rightarrow x_0\\). That is, all the limits are what they should be. There are no holes or jumps in the graph.\nContinuity is just one condition necessary for \\(f(x)\\) to be differentiable. It also can’t be too jagged in some sense. Derivatives don’t make sense at points where there are kinks in the graph. In practice, however, this isn’t a huge problem. We can just extend the derivative to be what’s called a subderivative. With a subderivative, you can roughly speaking take whatever value for the derivative you want at these kinks and it won’t make a difference. This is how we in practice calculate the derivatives of neural networks. Typically, a neural network does have kinks, and lots of them. At these kinks, we just pick an arbitrary value for the derivative, usually \\(0\\), and go on about our day.\nPractically speaking, you can think of the derivative as a rate or a speed. It’s the rate that \\(y\\) changes per unit \\(x\\). Roughly speaking, if \\(x\\) changes by one unit, then \\(y\\) will change by \\(\\frac{dy}{dx}\\) units. When the derivative is large, \\(y\\) will change a lot in response to small changes in \\(x\\). When the derivative is small, \\(y\\) will barely change at all in response to small changes in \\(x\\). The sign of the derivative indicates whether the change is up or down.\nNotice that the derivative is also itself a function, since it maps inputs \\(x\\) to outputs \\(\\frac{dy}{dx}\\). To indicate this functional relationship people sometimes write\n\\[\\frac{dy}{dx}=\\frac{d}{dx}f(x) \\quad \\text{ or } \\quad \\frac{dy}{dx}=f'(x)\\]\nto make this functional relationship clear.\nNow, how do we actually calculate a derivative? Perhaps the easiest thing to do is this: Suppose you have some function \\(y=f(x)\\), and you want to find its derivative at a point \\(x\\). Choose a small value \\(dx\\). Then just take the ratio\n\\[\\frac{dy}{dx} = \\frac{f(x+dx) - f(x)}{dx}.\\]\nSuppose we wanted to find the derivative of \\(y=x^2\\) at the point \\(x=1\\). This will return a numerical value, not a function, since we’re plugging in a particular value of \\(x\\). Expanding terms exactly, we’d have\n\\[\\frac{dy}{dx}\\bigg|_{x=1} = \\frac{(x+dx)^2 - x^2}{dx}\\bigg|_{x=1} = \\frac{(1+dx)^2 - 1}{dx}.\\]\nNotation: Read the expression \\(|_{x=1}\\) as “evaluated at \\(x=1\\)”. It’s a common shorthand. Another way to write the same thing is \\(f'(1)\\) or \\(\\frac{d}{dx}f(1)\\).\nThe output of this result will depend on what value of \\(dx\\) we choose. If \\(dx\\) were exactly infinitesimal, we already know the right answer should be \\(\\frac{dy}{dx}\\big|_{x=1} = 2 \\cdot 1 = 2\\). Let’s see what happens to our calculation for different choices of \\(dx\\) ranging from \\(dx=1\\) all the way down to \\(dx=10^{-200}\\). I’ll also calculate the error, which is the predicted value \\(2\\) minus the calculated value. Smaller error is better, obviously.\n\nf = lambda x: x ** 2\nx0 = 1\ndydx_exact = 2 * x0\nfor dx in [1, 0.1, 0.01, 0.001, 1e-4, 1e-5, 1e-10, 1e-100, 1e-200]:\n    dydx = (f(x0 + dx) - f(x0)) / dx\n    error = dydx - dydx_exact\n    print(f'dx = {dx:8.16f} \\t dy/dx = {dydx:4f} \\t error = {error:4f}')\n\ndx = 1.0000000000000000      dy/dx = 3.000000    error = 1.000000\ndx = 0.1000000000000000      dy/dx = 2.100000    error = 0.100000\ndx = 0.0100000000000000      dy/dx = 2.010000    error = 0.010000\ndx = 0.0010000000000000      dy/dx = 2.001000    error = 0.001000\ndx = 0.0001000000000000      dy/dx = 2.000100    error = 0.000100\ndx = 0.0000100000000000      dy/dx = 2.000010    error = 0.000010\ndx = 0.0000000001000000      dy/dx = 2.000000    error = 0.000000\ndx = 0.0000000000000000      dy/dx = 0.000000    error = -2.000000\ndx = 0.0000000000000000      dy/dx = 0.000000    error = -2.000000\n\n\nStarting with \\(dx=1\\) is a bad choice, with a huge error of \\(1.0\\). We’re way off. Shrinking to \\(dx=0.1\\) puts us in the ball park with a value \\(\\frac{dy}{dx}=2.1\\). You can see that making \\(dx\\) successively smaller and smaller makes the error successively smaller, in this case by a factor of 10 each time.\nThe error is getting smaller all the way down to about \\(dx=10^{-10}\\) before creeping up again as we make \\(dx\\) even smaller than that. This is due to the numerical roundoff of floating point numbers. We’re subtracting two numbers \\((1+dx)^2 - 1\\) that are very close to each other when \\(dx\\) is really small, which as you’ll recall is one of the pitfalls to avoid when working with floating point numbers.\nThis method we just used to calculate the derivative is, with some minor tweeks, exactly how derivatives are usually calculated on a computer. The process of calculating the derivative this way, directly from its definition essentially, is called numerical differentiation. We choose a small value of \\(dx\\) and just apply the formula above, with some minor tweaks to get better accuracy in fewer steps. It’s common in practice to choose “less small” values for \\(dx\\) when calculating derivatives numerically like this, e.g. 1e-5.\nSince derivatives are themselves functions, we can take the derivative of the derivative too. If \\(\\frac{d}{dx}f(x)\\) is the derivative function, then the derivative of the derivative is just\n\\[\\frac{d^2 y}{dx^2} = \\frac{d}{dx} \\bigg(\\frac{d}{dx} f(x)\\bigg).\\]\nThis is called a second derivative. The left-hand notation \\(\\frac{d^2 y}{dx^2}\\) is the most common way to express the second derivative. Other ways are \\(f''(x)\\) or \\(\\frac{d^2}{dx^2} f(x)\\). If you do some algebra, you can show that the second derivative is given by the following ratio of differentials,\n\\[\\frac{d^2 y}{dx^2} = \\frac{f(x+2dx) - 2f(x+dx) + f(x)}{dx^2}.\\]\nNote \\(dx^2\\) is shorthand for \\((dx)^2\\). Just as you can think of a first derivative as a rate or speed, you can think of the second derivative as a rate of a rate, or an acceleration. When the second derivative is large, the function is accelerating quickly, and the derivative is rapidly changing. When it’s small, the function is barely accelerating at all, and the derivative is roughly constant.\nAs a quick example, the second derivative of our running function \\(y=x^2\\) is the first derivative of \\(2x\\), which is\n\\[\\frac{d^2 y}{dx^2} = \\frac{2(x + dx) - 2x}{dx} = 2.\\]\nEvidently, the second derivative of \\(y=x^2\\) is just the constant function \\(\\frac{d^2 y}{dx^2}=2\\). Higher-order derivatives can be defined as well by continuing to take derivatives of the second derivative, like third derivatives of fourth derivatives. These show up far less often though, so I won’t discuss them.\n\n\n3.3.2 Visualizing Derivatives\nThe first derivative \\(\\frac{dy}{dx}\\) has an interesting and useful geometric interpretation as the slope of the curve \\(y=f(x)\\) at the point \\(x\\). To see this, imagine a right triangle with length \\(dx\\) and height \\(dy\\). The slope, or “steepness”, of its hypotenuse is just the ratio height over width, i.e. \n\\[\\text{slope} = \\frac{\\text{rise}}{\\text{run}} = \\frac{dy}{dx}.\\]\n\n\nCode\nplot_right_triangle()\n\n\n\n\n\n\n\n\n\nNow, imagine placing this triangle on the graph of some function. Suppose the bottom left point is placed at the point \\((x_0,y_0)\\) of some function \\(y=f(x)\\). Then the bottom right point will be \\(x_0+dx\\), and the top right point will be \\(y_0+dy\\). Now imagine extending the hypotenuse in both directions. This will create a tangent line that hugs the graph of the function at the point \\((x_0,y_0)\\). We can solve for what this tangent line has to be. The line should have the form \\(y=mx+b\\), pass through \\((x_0,y_0)\\), and have slope \\(m=\\frac{d}{dx}f(x_0)\\), where \\(y_0=f(x_0)\\). Solving this equation for the intercept and gives the line\n\\[y = y_0 + \\frac{d}{dx}f(x_0)(x - x_0).\\]\nHere’s an example. I’ll plot the function \\(y=x^2\\) and its tangent at a point \\(x_0=2\\) on the x-axis. The corresponding \\(y\\) at \\(x=2\\) is just \\(y_0=x_0^2=4\\). As we derived above, its derivative (and hence slope) at \\(x_0=2\\) is\n\\[\\frac{d}{dx}f(2)=2(2)=4,\\]\nso the equation for the tangent line of \\(y=x^2\\) at \\(x_0=2\\) is\n\\[y \\approx 4 + 4(x - 2) = 4x -4.\\]\nThe code below implements this calculation. I’ll define the function f that gives y = f(x) along with the derivative dydx = dfdx(x), and then use these two define a function f_line to calculate the tangent line, which also depends on a specified point x0.\n\n\nCode\nf = lambda x: x ** 2\ndfdx = lambda x: 2 * x\n\nx0 = 2\nx = np.arange(-3 * x0, 3 * x0, 0.1)\nf_line = lambda x: f(x0) + dfdx(x0) * (x - x0)\n\nplot_tangent_curve(x, x0, f, f_line, xlim=(-5, 5), ylim=(0, 10), \n                   title=f'Tangent Line of $y=x^2$ at ${(x0, f(x0))}$')\n\n\n\n\n\n\n\n\n\nGenerally speaking, if the derivative at a point is positive the tangent line will slant towards the right. If the derivative at that point is negative the tangent line will slant towards the left. If it’s zero, the tangent line there will be horizontal.\nThe tangent line can be used to approximate the function \\(y=f(x)\\) when \\(x\\) is close to \\(x_0\\). When \\(x \\approx x_0\\), we can write\n\\[f(x) \\approx f(x_0) + \\frac{d}{dx}f(x_0)(x - x_0).\\]\nThis is called a first order perturbation, or the best linear approximation to the function at \\(x=x_0\\). It turns out the errors of this approximation are on the order of \\((x-x_0)^2\\), which will usually be small provided \\(x \\approx x_0\\).\nThe second derivative has a geometric interpretation as well. It captures information about the curvature of the function. To see why this is true we need to look at second order perturbations, which extend first order perturbations by adding a quadratic term (shown in red),\n\\[f(x) \\approx f(x_0) + \\frac{d}{dx}f(x_0) (x-x_0) + \\color{red}{\\frac{1}{2}\\frac{d^2}{dx^2}f(x_0) (x-x_0)^2}.\\]\nIt seems weird there’s a \\(\\frac{1}{2}\\) in front of the quadratic term. I won’t go into detail on how this comes out. It relates to the fact that differentiating \\((x-x_0)^2\\) brings a \\(2\\) out front, which kills the \\(\\frac{1}{2}\\). What’s important is that this new term depends on the second derivative at \\(x=x_0\\). This formula gives the best quadratic approximation to \\(y=f(x)\\) at the point \\(x=x_0\\). Note the error to this approximation turns out to be of the order of \\((x-x_0)^3\\).\nA second order perturbation defines a tangent parabola given by a quadratic function \\(y=ax^2+bx+c\\), where \\(a,b,c\\) are coefficients determined by the values of the inputs \\(f(x_0), \\frac{d}{dx}f(x_0), \\frac{d^2}{dx^2}f(x_0)\\). Most importantly, the leading coefficient \\(a\\) is just the second derivative at \\(x=x_0\\), i.e. \\(a = \\frac{d^2}{dx^2}f(x_0)\\).\nSince \\(a\\) determines the shape of the tangent parabola, it also determines the curvature of the function at \\(x=x_0\\). If \\(a\\) is large at \\(x=x_0\\), the function will be sharply curved around that point. If \\(a\\) is small, the function will be very flat around \\(x=x_0\\). The sign of \\(a\\) will indicate whether the function is curved upward or downward. If \\(a>0\\) the function will be curved upward. If \\(a < 0\\) it’ll curve downward.\nIn our example \\(y=x^2\\), since \\(a=\\frac{d^2 y}{dx^2} = 2 > 0\\), meaning the tangent parabola is just the function \\(y=x^2\\) itself. Of course it is. Since \\(a\\) is constant, the curvature for the parabola is the same everywhere.\nSince \\(a=2>0\\), the function \\(y=x^2\\) “bowls upward” everywhere. Functions that “bowl upward” everywhere like this are called convex function. If instead \\(a < 0\\) everywhere, e.g. like it would be for \\(y=-x^2\\), the function would “bowl downward” everywhere. These are called convave functions. Convex (and concave) functions are very important for machine learning and optimization more generally since they’re guaranteed to have a unique global minimum (or maximum). More on this in a future lesson.\nHere’s a more general example where the second derivative isn’t constant, \\(y = \\sin x\\). In this case, \\(\\frac{dy}{dx} = \\cos x\\) and \\(\\frac{d^2y}{dx^2} = -\\sin x\\). Suppose we want to get the tangent parabola at the point \\(x_0=-\\frac{\\pi}{2}\\), so \\(f(x_0)=-1\\), \\(\\frac{d}{dx}f(x_0)=0\\) and \\(\\frac{d^2}{dx^2}f(x_0)=1\\). Then the tangent parabola is given by the equation\n\\[y = -1 + 0 \\cdot \\bigg(x + \\frac{\\pi}{2}\\bigg) + \\bigg(x + \\frac{\\pi}{2}\\bigg)^2 = \\bigg(x + \\frac{\\pi}{2}\\bigg)^2 - 1.\\]\nThis is an upward-sloping parabola with vertex at \\(\\big(\\frac{\\pi}{2}, -1\\big)\\). Here’s a plot of this idea using the same helper function, but passing in f_parabola instead of f_tangent. Notice how the parabola approximates the function pretty well around the point \\(\\big(\\frac{\\pi}{2}, -1\\big)\\), but gets less and less accurate as we get away from that point. The fact that \\(a=1\\) here says that we should expect an upward sloping parabola with “unit curvature”, which is what we see.\n\n\nCode\nf = lambda x: np.sin(x)\ndfdx = lambda x: np.cos(x)\nd2fdx2 = lambda x: -np.sin(x)\n\nx0 = -np.pi / 2\nx = np.arange(-5, 5, 0.1)\nf_parabola = lambda x: f(x0) + dfdx(x0) * (x - x0) + 1/2 * d2fdx2(x0) * (x - x0) ** 2\nplot_tangent_curve(x, x0, f, f_parabola, xlim=(-5, 5), ylim=(-1.5, 1.5), \n                   title=f'Tangent Parabola of $y=\\sin x$ at $(\\pi/2, -1)$')\n\n\n\n\n\n\n\n\n\n\n\n3.3.3 Differentiation Rules\nAs we’ve seen, derivatives are also functions in and of themselves, mapping inputs to outputs via \\(\\frac{dy}{dx} = \\frac{d}{dx}f(x)\\). For this reason, several rules exist relating derivatives to their original functions.\nHere are the derivatives of some common functions that come up:\n\n\n\n\n\n\n\nFunction\nDerivative\n\n\n\\(y = 0\\)\n\\(\\frac{dy}{dx} = 0\\)\n\n\n\\(y = 1\\)\n\\(\\frac{dy}{dx} = 0\\)\n\n\n\\(y = x\\)\n\\(\\frac{dy}{dx} = 1\\)\n\n\n\\(y = x^2\\)\n\\(\\frac{dy}{dx} = 2x\\)\n\n\n\\(y = \\sqrt{x}\\)\n\\(\\frac{dy}{dx} = \\frac{1}{2\\sqrt{x}}\\)\n\n\n\\(y = \\frac{1}{x}\\)\n\\(\\frac{dy}{dx} = -\\frac{1}{x^2}\\)\n\n\n\\(y = e^x\\)\n\\(\\frac{dy}{dx} = e^x\\)\n\n\n\\(y = \\log{x}\\)\n\\(\\frac{dy}{dx} = \\frac{1}{x}\\)\n\n\n\\(y = \\sin{x}\\)\n\\(\\frac{dy}{dx} = \\cos{x}\\)\n\n\n\\(y = \\cos{x}\\)\n\\(\\frac{dy}{dx} = -\\sin{x}\\)\n\n\n\\(y = \\sigma(x)\\)\n\\(\\frac{dy}{dx} = \\sigma(x)\\big(1-\\sigma(x)\\big)\\)\n\n\n\\(y = \\tanh(x)\\)\n\\(\\frac{dy}{dx} = \\big(1 - \\tanh^2(x)\\big)\\)\n\n\n\\(y = \\text{ReLU}(x)\\)\n\\(\\frac{dy}{dx} = u(x) = [x \\geq 0]\\)\n\n\n\nHere are some more general derivative rules you can use to differentiate more arbitrary functions:\n\n\n\n\n\n\n\n\nName\nRule\nExample\n\n\n\n\\(\\frac{d}{dx}(c) = 0\\) for any constant \\(c\\)\n\\(\\frac{d}{dx}(10) = 0\\)\n\n\nPower Rule\n\\(\\frac{d}{dx}x^n = nx^{n-1}\\) for any \\(n \\neq 0\\)\n\\(\\frac{d}{dx}x^3 = 3x^2\\)\n\n\nAddition Rule\n\\(\\frac{d}{dx}(u + v) = \\frac{du}{dx} + \\frac{dv}{dx}\\)\n\\(\\frac{d}{dx}(x^2 + \\log x) = \\frac{d}{dx}x^2 + \\frac{d}{dx}\\log x = 2x + \\frac{1}{x}\\)\n\n\nConstant Rule\n\\(\\frac{d}{dx}(cy) = c \\frac{dy}{dx}\\) for any constant \\(c\\)\n\\(\\frac{d}{dx}2 \\sin x = 2 \\frac{d}{dx}\\sin x = 2 \\cos x\\)\n\n\nProduct Rule\n\\(\\frac{d}{dx}(uv)=u\\frac{dv}{dx} + v\\frac{du}{dx}\\)\n\\(\\frac{d}{dx}(x e^x) = x \\frac{d}{dx}e^x + e^x \\frac{d}{dx} x = xe^x + e^x\\)\n\n\nQuotient Rule\n\\(\\frac{d}{dx}\\big(\\frac{u}{v}\\big) = \\frac{v\\frac{du}{dx}-u\\frac{dv}{dx}}{v^2}\\)\n\\(\\frac{d}{dx} \\frac{\\cos x}{x^2} = \\frac{x^2\\frac{d}{dx}\\cos x-\\cos x\\frac{d}{dx}x^2}{(x^2)^2} = \\frac{-x^2 \\sin x - 2x \\cos x}{x^4}\\)\n\n\nChain Rule\n\\(\\frac{d}{dx}f(g(x)) = \\frac{d}{dy}f(y)\\frac{d}{dx}g(x) = \\frac{dz}{dy}\\frac{dy}{dx}\\)\n\\(\\frac{d}{dx} e^{\\sin x} = \\frac{d}{dy} e^y \\frac{d}{dx}\\sin x = e^{\\sin x} \\cos x\\)\n\n\n\nAside: These rules are simple to derive using infinitesimals. For example, here’s a derivation of the all important chain rule. Suppose we have a composite function of two differentiable functions \\(z=f(y)\\) and \\(y=g(x)\\). That is, \\(z=f(g(x))\\). Perturb \\(x\\) by some infinitesimal \\(dx\\). Then \\(y + dy = g(x + dx)\\), so\n\\[dy = g(x+dx)-g(x) = \\frac{g(x+dx)-g(x)}{dx} dx = \\frac{dy}{dx} dx.\\]\nSince \\(dy\\) is also infinitesimal, perturbing \\(y\\) by \\(dy\\) will do the same thing to \\(z=f(y)\\), since \\(z + dz = f(y + dy)\\), and\n\\[dz = f(y+dy)-f(y) = \\frac{f(y+dy)-f(y)}{dy} dy = \\frac{dz}{dy} dy,\\]\nPutting these two results together, the infinitesimal change \\(dz\\) in \\(z=f(g(x))\\) resulting from the original infinitesimal change \\(dx\\) is given by\n\\[dz = \\frac{dz}{dy} dy = \\frac{dz}{dy} \\frac{dy}{dx} dx.\\]\nDividing both sides by \\(dx\\) gives the derivative of the composite function \\(z=f(g(x))\\),\n\\[\\frac{dz}{dx} = \\frac{dz}{dy}\\frac{dy}{dx}. \\square\\]\nNote that the chain rule extends to arbitrarily many compositions too. For example, if we had a composition of four functions,\n\\[\\begin{align*}\ny &= f(x), \\\\\nz &= g(y), \\\\\nu &= h(z), \\\\\nv &= i(u), \\\\\n\\end{align*}\\]\nthe chain rule would say\n\\[\\frac{dv}{dx} = \\frac{dv}{du} \\frac{du}{dz} \\frac{dz}{dy} \\frac{dy}{dx}.\\]\nThis arbitrary chaining is what allows us to differentiate complex neural networks, where each layer of the network is just a function in this kind of chain.\n\n\n3.3.4 Application: The Sigmoid Function\nTo illustrate how to calculate derivatives I’ll use a relevant example to machine learning, the sigmoid function. Recall the sigmoid function \\(y=\\sigma(x)\\) is defined by\n\\[y = \\frac{1}{1 + e^{-x}}.\\]\nIts shape looks like an S (hence the name), going from \\(y=0\\) at \\(x=-\\infty\\) to \\(y=1\\) at \\(x=\\infty\\).\n\n\nCode\nx = np.arange(-10, 10, 0.1)\nf = lambda x:  1 / (1 + np.exp(-x))\nplot_function(x, f, xlim=(-10, 10), ylim=(-0.5, 1.5), ticks_every=[5, 0.5], title='Sigmoid Function')\n\n\n\n\n\n\n\n\n\nLet’s calculate the derivative of this function. First, though, let’s use some intuition and try to figure out what the derivative should be doing based on the shape of the sigmoid curve. When \\(x\\) is really negative, say \\(x < -5\\), the slope looks basically flat, hence the derivative should be zero there. Similarly, when \\(x\\) is really positive, say \\(x > 5\\), the derivative should be zero there too. Around \\(x=0\\), say \\(-1 < x < 1\\), the sigmoid looks kind of linear with positive slope. There we should expect the derivative to be positive, and roughly constant over that interval.\nTo verify this, let’s calculate the derivative of the sigmoid explicitly. There are a few ways to do this, but I’ll use the chain rule here. We have\n\\[\\begin{align}\n\\frac{d}{dx} \\sigma(x) &= \\frac{d}{dx} \\frac{1}{1 + e^{-x}}\\\\\n&= \\frac{d}{dx} (1 + e^{-x})^{-1} \\\\\n&= (-1) (1 + e^{-x})^{-2} \\frac{d}{dx} e^{-x} \\\\\n&= -(1 + e^{-x})^{-2} (-1) e^{-x} \\\\\n&= \\frac{e^{-x}}{(1 + e^{-x})^{2}} \\\\\n&= \\frac{1}{1 + e^{-x}} \\frac{(1 + e^{-x}) - 1}{1 + e^{-x}} \\\\\n&= \\frac{1}{1 + e^{-x}} \\bigg(1-\\frac{1}{1 + e^{-x}}\\bigg) \\\\\n&= \\sigma(x) \\big(1 - \\sigma(x)\\big).\n\\end{align}\\]\nLet’s look at this answer and verify it matches our intuition as to what the derivative of the sigmoid should be.\n\nWhen \\(x < -5\\), \\(\\sigma(x) \\approx 0\\), hence \\(\\frac{d}{dx} \\sigma(x) \\approx 0\\).\nWhen \\(x > 5\\), \\(\\sigma(x) \\approx 1\\), so \\(1 - \\sigma(x) \\approx 0\\), and again \\(\\frac{d}{dx} \\sigma(x) \\approx 0\\).\nWhen \\(-1 < x < 1\\), \\(\\sigma(x) \\approx \\sigma(0) + \\frac{d}{dx}\\sigma(0) \\cdot (x-0) = \\frac{1}{2} + \\frac{1}{2}\\big(1 - \\frac{1}{2}\\big) \\cdot x = \\frac{1}{2} + \\frac{1}{4}x\\), which is a tangent line with a positive slope of \\(\\frac{1}{4}\\).\n\nTo further verify the things look like a line around \\(x=0\\), we could look at the second derivative and verify it’s approximately zero in the region \\(-1 < x < 1\\). I’ll leave that exercise to you.\nThe sigmoid function shows up in machine learning when doing binary classification. If a problem can be classified into two classes, \\(0\\) or \\(1\\), the sigmoid can be used to model the probability of the input being in class \\(1\\). The closer the sigmoid is to \\(1\\), or equivalently the larger \\(x\\) is, the more likely the input is a \\(1\\). More on this in a future lesson.\nTo calculate the derivative of a function in sympy, use y.diff(x), which means “differentiate \\(y\\) with respect to \\(x\\)”.\nHere is a calculation of the derivative of the sigmoid function. I’ll also calculate the second derivative to show you how easy it is to do relative to the torture of trying to do it by hand.\n\nx = sp.Symbol('x')\ny = 1 / (1 + sp.exp(-x))\ndydx = y.diff(x)\nd2dx2 = y.diff(x).diff(x)\nprint(f'y = {y}')\nprint(f'dydx = {dydx}')\nprint(f'd2dx2 = {d2dx2}')\n\ny = 1/(1 + exp(-x))\ndydx = exp(-x)/(1 + exp(-x))**2\nd2dx2 = -exp(-x)/(1 + exp(-x))**2 + 2*exp(-2*x)/(1 + exp(-x))**3"
  },
  {
    "objectID": "notebooks/basic-calculus.html#integration",
    "href": "notebooks/basic-calculus.html#integration",
    "title": "3  Basic Calculus",
    "section": "3.4 Integration",
    "text": "3.4 Integration\nSince integration is literally half of the subject of calculus, I owe it to at least briefly mention the topic. This section is only really applicable to the other topics in this book if you want to better understand probability distributions, something I’ll cover in detail in the next lesson. If you don’t mind thinking of probability distributions as histograms with infinitely many samples, you’re free to skip this section and move ahead.\n\n3.4.1 Summing Infinitesimals\nThe other half of calculus is essentially about summing up small things to get big things. By small things of course I mean infinitesimals. Suppose we have a bunch of infinitesimals \\(\\varepsilon_0, \\varepsilon_1, \\cdots, \\varepsilon_{n-1}\\). We can add them together to get a new infinitesimal \\(\\varepsilon_0 + \\varepsilon_1 + \\cdots + \\varepsilon_{n-1}\\).\nSuppose we want to add up the same infinitesimal \\(\\varepsilon\\) some number \\(N\\) times, \\[\\underbrace{\\varepsilon + \\varepsilon + \\cdots + \\varepsilon}_{\\text{N times}} = N\\varepsilon.\\]\nIf \\(N\\) is any reasonably sized finite number, say a number like \\(N=1000\\), then the product \\(N\\varepsilon\\) will again be infinitesimal, since \\((N\\varepsilon)^2 \\approx 0\\). But if we make \\(N\\) infinitely large, then \\(N\\varepsilon\\) will be a finite number.\nHere’s how this might look when adopting our informal convention that infinitesimals equal \\(10^{-300}\\) and infinitely large numbers equal \\(10^{300}\\). For \\(N=1000\\) the square \\((N\\varepsilon)^2 \\approx 0\\). But it’s not when we take \\(N=10^{300}\\). It’s finite, with \\((N\\varepsilon)^2=1\\).\n\nepsilon = 1e-300\n\n\nN = 1000\nprint(f'N = {N}')\nprint(f'(N * epsilon)^2 = {(N * epsilon) ** 2}')\n\nN = 1000\n(N * epsilon)^2 = 0.0\n\n\n\nN = 1e300\nprint(f'N = {N}')\nprint(f'(N * epsilon)^2 = {(N * epsilon) ** 2}')\n\nN = 1e+300\n(N * epsilon)^2 = 1.0\n\n\nThus, if we add up only a finite number of infinitesimals we’ll again get an infinitesimal. But, if we add up an infinitely large number of infinitesimals we’ll get something finite. This is the idea behind integration.\n\n\n3.4.2 Area Under The Curve\nLet’s do an example. Suppose we’re interested in calculating the area under the curve \\(y=\\sqrt{x}\\) between two points, say \\(x=0\\) and \\(x=10\\). How would we go about this?\n\n\nCode\nf = lambda x: np.sqrt(x)\nx = np.linspace(0, 10, 100)\na, b = 0, 10\nplot_function_with_area(x, f, a=a, b=b, title='Area Under $y=\\sqrt{x}$ Between $0 \\leq x \\leq 10$')\n\n\n\n\n\n\n\n\n\nPerhaps the easiest idea is to approximate the function by a shape that’s easier to calculate the area of, something you’ve seen in geometry, like a square or a triangle. A better idea is to take a bunch of simple shapes, calculate their areas, and add them together.\nLet’s try to do this using rectangles. Let’s approximate the function \\(f(x)=\\sqrt{x}\\) with \\(N=10\\) equally-spaced rectangles of varying heights \\(f(x)\\), where \\(x\\) is taken at each integer value \\(x=1,2,3,\\cdots,10\\). The width of each rectangle is \\(dx=\\frac{b-a}{N}=1\\). We know for rectangles their area is width times height, which in this case is \\(dx \\cdot f(x) = f(x)dx\\). Then the total area under the curve of \\(y=\\sqrt{x}\\) would roughly be the sum of all these rectangle areas,\n\\[\\begin{align}\nA &\\approx f(1)dx + f(2)dx + f(3)dx + \\cdots + f(10)dx \\\\\n&= \\big(f(1) + f(2) + f(3) + \\cdots + f(10)\\big)\\cdot dx \\\\\n&= \\big(\\sqrt{1} + \\sqrt{2} + \\sqrt{3} + \\cdots + \\sqrt{10}\\big)\\cdot 1 \\\\\n&\\approx 22.468\n\\end{align}\\]\nIt’s helpful to visualize what’s going on. I’ll plot this area approximation scheme by using the helper function plot_approximating_rectangles, which will show the plot of the curve and its approximating rectangles. It also prints out the approximating area calculated above.\n\n\nCode\nplot_approximating_rectangles(x, f, dx=1.0)\n\n\nApproximate Area: 22.468278186204103\n\n\n\n\n\n\n\n\n\nIf you stare at the plot you can see our area estimate is okay but not great. The rectangles are overestimating the area under the curve since they all have segments of area lying above the curve.\nThe problem was that the rectangles we used were too coarse. It’s better to use narrower rectangles, and more of them. What we need to do is make \\(dx\\) smaller by making \\(N\\) bigger. Let’s try using \\(N=50\\) rectangles of width \\(dx=0.2\\) instead and see how much the result improves.\n\n\nCode\nplot_approximating_rectangles(x, f, dx=0.2)\n\n\nApproximate Area: 21.380011968222313\n\n\n\n\n\n\n\n\n\nIt looks better. We’re at \\(21.380\\) now. If you zoom in you’ll see we’re still overestimating the true area, but not by near as much as before. As we make \\(dx\\) smaller and smaller, and \\(N\\) bigger and bigger, this estimate will get better and better.\nThe exact area under this curve turns out to be\n\\[A = \\frac{20}{3}\\sqrt{10} \\approx 21.082.\\]\nThis will be the case when the rectangles are infinitesimally thin, so thin that the errors disappear and the area calculation becomes exact.\nLet’s try to calculate the approximating areas using smaller and smaller rectangles and see how close we can get to the exact answer. To do this, I’ll use a loop to calculate the area for successively smaller values of \\(dx\\).\n\n\nCode\nf = lambda x: np.sqrt(x)\n\nfor dx in [1, 0.1, 0.01, 0.001, 0.0001]:\n    N = int(10 / dx)\n    xs = np.cumsum(dx * np.ones(N))\n    area = np.sum([f(x)*dx for x in xs])\n    print(f'N = {N:6d} \\t dx = {dx:8.4f} \\t A = {area:4f}')\n\n\nN =     10   dx =   1.0000   A = 22.468278\nN =    100   dx =   0.1000   A = 21.233523\nN =   1000   dx =   0.0100   A = 21.097456\nN =  10000   dx =   0.0010   A = 21.083426\nN = 100000   dx =   0.0001   A = 21.082009\n\n\nIt looks like if we want to get the correct answer \\(21.082\\) to 3 decimal places we’d need to use \\(N=100000\\) rectangles of width \\(dx=10^{-4}\\). In practice that’s an awful lot of terms to sum up. There are better ways to actually calculate these things numerically than just using the above definition (e.g. Simpson’s Rule), but I won’t go into those.\nLet’s generalize this. Suppose we want to calculate the area under the curve of some function \\(y=f(x)\\) between two points \\(x=a\\) and \\(x=b\\). We can do this by taking \\(N\\) rectangles each of width \\(dx=\\frac{b-a}{N}\\) and height \\(f(x)dx\\) and summing up their areas. If \\(x_0,x_1,\\cdots,x_{N-1}\\) are the points we’re evaluating the heights at, then \\[A \\approx f(x_0)dx + f(x_1)dx + f(x_2)dx + \\cdots + f(x_{N-1})dx = \\sum_{n=0}^{N-1} f(x_n) dx.\\]\nTo get the exact area, let’s allow \\(N\\) to get infinitely large, which also means \\(dx\\) will become infinitesimal. When we do this, it’s conventional to use a different symbol for the sum, the long-S symbol \\(\\int\\). Instead of writing\n\\[A = \\sum_{n=0}^{N-1} f(x_n) dx,\\]\nwe’d write \\[A = \\int_a^{b} f(x) dx.\\]\nRead this as “the integral from 0 to 10 of \\(f(x)dx\\)”. It’s called the definite integral of the function. In our example, this would be\n\\[A = \\int_0^{10} \\sqrt{x} dx.\\]\nOf course, this fancy notation doesn’t actually tell us anything new. We’re still just summing up the areas of a bunch of rectangles.\n\n\n3.4.3 Integration Rules\nIt’s not at all clear from this definition how we’d get the exact answer \\(A=\\frac{20}{3}\\sqrt{10}\\) shown above. We can get to it approximately by summing rectangle areas, but if we want to get the exact value we’ll need a few integral rules.\nBefore doing so I need to talk about the indefinite integral, sometimes called the antiderivative. If \\(f(x)\\) is some function, then its indefinite integral is some other function \\(F(x)\\) whose derivative is \\(f(x)\\), \\[f(x) = \\frac{d}{dx} F(x).\\]\nTypically the indefinite integral is written as an integral, but without limits of integration shown,\n\\[F(x) = \\int f(x) dx.\\]\nTo evaluate a typical definite integral like \\(\\int_a^b f(x) dx\\) the rule is \\[\\int_a^b f(x) dx = F(x) \\bigg|_{x=a}^{x=b} = F(b) - F(a).\\]\nThat is, we first calculate the indefinite integral \\(F(x)\\), then evaluate it at the points \\(a\\) and \\(b\\), then subtract their difference to get the definite integral, which itself is just the area under the curve of \\(f(x)\\). The fact we can think of areas under curves in terms of another function like this is not obvious. It follows from the Fundamental Theorem of Calculus, which I won’t try to prove here.\nWith this out of the way, here are some common indefinite integrals. Note we can add a constant \\(c\\) to each of these and the answer would still be the same. I’ll state them in the standard form where \\(c=0\\).\n\n\n\n\n\n\n\nFunction\nIntegral\n\n\n\\(y = 0\\)\n\\(\\int y dx = 0\\)\n\n\n\\(y = 1\\)\n\\(\\int y dx = x\\)\n\n\n\\(y = x\\)\n\\(\\int y dx = \\frac{1}{2}x^2\\)\n\n\n\\(y = x^2\\)\n\\(\\int y dx = \\frac{1}{3}x^3\\)\n\n\n\\(y = \\sqrt{x}\\)\n\\(\\int y dx = \\frac{2}{3} x^{3/2}\\)\n\n\n\\(y = \\frac{1}{x}\\)\n\\(\\int y dx = \\log{x}\\)\n\n\n\\(y = e^x\\)\n\\(\\int y dx = e^x\\)\n\n\n\\(y = \\log{x}\\)\n\\(\\int y dx = x \\log{x} - x\\)\n\n\n\\(y = \\sin{x}\\)\n\\(\\int y dx = -\\cos{x}\\)\n\n\n\\(y = \\cos{x}\\)\n\\(\\int y dx = \\sin{x}\\)\n\n\n\nHere are a few more general integral rules:\n\n\n\n\n\n\n\n\nName\nRule\nExample\n\n\nFundamental Theorem of Calculus\n\\(\\int_a^b f(x) dx = F(b) - F(a)\\) where \\(\\frac{d}{dx}F(x) = f(x)\\)\n\\(\\int_0^1 x dx = \\frac{1}{2} x^2 \\big |_0^1 = \\frac{1}{2} 1^2 - \\frac{1}{2} 0^2 = \\frac{1}{2}\\)\n\n\nReversing Limits of Integration\n\\(\\int_b^a y dx = -\\int_a^b y dx\\)\n\\(\\int_2^0 1 dx = -\\int_0^2 1 dx = 2\\)\n\n\nSplitting Up Limits of Integration\n\\(\\int_a^b y dx = \\int_a^c y dx + \\int_c^d y dx\\)\n\\(\\int_0^2 1 dx = \\int_0^1 1 dx + \\int_1^2 1 dx = 1 + (2-1) = 2\\)\n\n\nPower Rule\n\\(\\int x^n dx = \\frac{1}{n+1}x^{n+1}\\) for any \\(n \\neq -1\\)\n\\(\\int x^{-2} dx = \\frac{1}{-2+1}x^{-2+1} = -\\frac{1}{x}\\)\n\n\nAddition Rule\n\\(\\int (u + v) dx = \\int u dx + \\int v dx\\)\n\\(\\int (1 + e^x) dx = \\int 1 dx + \\int e^x dx = x + e^x\\)\n\n\nConstant Rule\n\\(\\int cy dx = c \\int y dx\\)\n\\(\\int 5 \\cos{x} dx = 5 \\int \\cos{x} dx = 5 \\sin{x}\\)\n\n\nIntegration By Parts\n\\(\\int u dv = uv - \\int v du\\)\n\\(\\int x e^x dx = \\int x d(e^x) = x e^x - \\int e^x dx = x e^x - e^x\\)\n\n\nLeibniz Rule\n\\(\\frac{d}{dx} \\int y dx = y\\)\n\\(\\frac{d}{dx} \\int_0^x \\sin t dt = \\sin x\\)\n\n\nChange of Variables\n\\(\\int f(u) du = \\int f(u(x)) \\frac{du}{dx} dx\\)\n\\(\\int x e^{x^2} dx = \\int e^{x^2} d\\big(\\frac{1}{2}x^2\\big) = \\frac{1}{2} \\int e^u du = \\frac{1}{2} e^u = \\frac{1}{2} e^{x^2}\\)\n\n\n\nNote \\(d(f(x))\\) is just the differential form of the derivative \\(\\frac{d}{dx}f(x)\\), so \\(d(f(x)) = \\frac{d}{dx}f(x) dx\\).\nIt’s worth mentioning that integral rules are often much harder to apply than derivative rules. In fact, it’s not even possible to symbolically integrate every function. The Gaussian function \\(y=e^{-x^2}\\) is a well-known example of a function that can’t be integrated symbolically. Of course, we can always calculate the definite integral numerically, even if we can’t symbolically.\nJust as with derivatives, sympy can evaluate integrals for you, both definite and indefinite integrals. Below I’ll use y.integrate((x, 0, 10)) to calculate the area under the curve problem I did before,\n\\[A = \\int_0^{10} \\sqrt{x} dx = \\frac{20}{3} \\sqrt{10}.\\]\nSympy can of course handle indefinite integrals too by leaving off the limits of integration.\n\n\nCode\nx = sp.Symbol('x')\ny = sp.sqrt(x)\nA = y.integrate((x, 0, 10))\ny_int = y.integrate(x)\nprint(f'y = {y}')\nprint(f'A = {A}')\nprint(f'int y dx = {y.integrate(x)}')\n\n\ny = sqrt(x)\nA = 20*sqrt(10)/3\nint y dx = 2*x**(3/2)/3"
  },
  {
    "objectID": "notebooks/linear-systems.html#linear-systems",
    "href": "notebooks/linear-systems.html#linear-systems",
    "title": "4  Systems of Linear Equations",
    "section": "4.1 Linear Systems",
    "text": "4.1 Linear Systems\nYou’re certainly familiar by now with solving a simple linear equation. Suppose you need to solve the linear equation \\(ax=b\\) for \\(x\\). Provided \\(a \\neq 0\\), you can divide both sides by \\(a\\) to get\n\\[x = \\frac{b}{a} = a^{-1} b.\\]\nSuppose now you need to solve not just one, but 2 linear equations with 1 unknown \\(x\\),\n\\[\\begin{alignat*}{2}\n   ax & {}={} & b  \\\\\n   cx & {}={} & d.\n\\end{alignat*}\\]\nThe first equation has the same solution as above, \\(x = a^{-1} b\\). The second equation has solution \\(x = c^{-1} d\\). If we want a solution \\(x\\) that satisifies both of these equations it’s going to be impossible unless \\(a^{-1} b = c^{-1} d\\), that is, if \\(ad = bc\\). If this relationship isn’t satisfied, there’s no single solution that will solve both of these equations.\nStepping up now, suppose you again have 2 linear equations, but this time with two unknowns \\(x\\) and \\(y\\). Here’s an example,\n\\[\\begin{alignat*}{3}\n   x & {}+{} &  y & {}={} & 2  \\\\\n   x & {}-{} &  y & {}={} & 0.\n\\end{alignat*}\\]\nLet’s try to find a pair \\((x,y)\\) that will solve both of these equations simultaneously. This is a pretty simple system to solve. If you stare at equation two, you’ll immediately see it implies \\(x=y\\). Plugging this into equation one then gives \\(x + x = 2\\), or \\(x=1\\). The pair that solves this system is thus \\(x=y=1\\). In fact, this is the only solution to this linear system. No other choice of \\(x\\) and \\(y\\) will work.\nThis is an example of a linear system of equations with 2 linear equations and 2 unknowns. These have the form\n\\[\\begin{alignat*}{3}\n   ax & {}+{} &  by & {}={} & e \\\\\n   cx & {}+{} &  dy & {}={} & f.\n\\end{alignat*}\\]\nSystems with 2 equations and 2 unknowns will always have a unique solution provided the coefficients \\(ad \\neq bc\\), given by\n\\[\\begin{align*}\nx &= \\frac{de-bf}{ad-bc} \\\\\ny &= \\frac{af-ce}{ad-bc}.\n\\end{align*}\\]\nIf you don’t believe me, plug these in to check they satisfy the linear system. Or you can solve the linear system by substitution again. Or just ask sympy.\n\n\nCode\nx, y = sp.symbols('x y')\na, b, c, d, e, f = sp.symbols('a b c d e f')\neq1 = sp.Eq(a * x + b * y, e)\neq2 = sp.Eq(c * x + d * y, f)\nsol = sp.solve((eq1, eq2), (x, y))\nprint(f'x = {sol[x]}')\nprint(f'y = {sol[y]}')\n\n\nx = (-b*f + d*e)/(a*d - b*c)\ny = (a*f - c*e)/(a*d - b*c)\n\n\nIt’s worth plotting what these equations look like to try to visualize what’s going on. In the simple case of 2 variables we can do that. In the above example with solution \\(x=y=1\\), solving each equation for \\(y\\) as a function of \\(x\\) gives\n\\[\\begin{align*}\ny &= 2-x \\\\\ny &= x.\n\\end{align*}\\]\nIf we plot these two lines, the point where they intersect is \\((1,1)\\), i.e. the solution to the system found above. Feel free to play around with different choices of \\(a,b,c,d,e,f\\) to see what happens. What you’re doing is varying the slopes and intercepts of both of the lines. Varying any of these will change the location of the point of intersection, i.e. the solution to the linear system. The special case where both have the same slope is when \\(ad=bc\\). This is when the two lines are parallel. Since parallel lines don’t intersect, such a system would have no solution.\n\n\nCode\na, b, e = 1, 1, 2\nc, d, f = 1, -1, 0\nx = np.linspace(-3, 3, 100)\nf0 = lambda x: -a / b * x + e / b\nf1 = lambda x: -c / d * x + f / d\nplot_function(x, [f0, f1], xlim=(0, 3), ylim=(0, 3), title='2 Linear Equations, 2 Unknowns',\n              labels=[f'$y=2-x$', f'$y=x$'])\n\n\n\n\n\n\n\n\n\nSuppose now you have a system of 2 equations with 3 unknowns \\(x\\), \\(y\\), and \\(z\\). For example,\n\\[\\begin{alignat*}{3}\n   x & {}+{} &  y & {}+{} & z {}={} & 2  \\\\\n   x & {}-{} &  y & {}+{} & z {}={} & 0.\n\\end{alignat*}\\]\nDoes this system have a solution that satisfies both equations? Clearly it does. If we set \\(z=0\\) then the original solution \\(x=y=1\\) still works, so \\(x=1, y=1, z=0\\) is a solution to this system. But is it the only solution? No, any solution of the form\n\\[\\begin{align*}\nx=1-t, \\\\\ny=1, \\\\\nz=t.\n\\end{align*}\\]\nwill work for any real number \\(t\\). If you don’t believe me, pick any choice of \\(t\\) you want and plug it in, and it’ll solve this system. Said differently, this system has infinitely many solutions.\nThis fact will be true for any system of 2 equations with 3 unknowns. The system is underdetermined, meaning it has too many variables to solve for. There will always be one that’s free, in the sense that we can set it to be whatever we want.\nHere’s a plot of what this situation looks like. Since there are 3 variables the space is now 3-dimensional, so I’ll have to use a 3D plot. Notice that now we don’t have 2 intersecting lines, but 2 intersecting planes. Two planes intersect at a line, not a point. Any point on this line is a valid solution to this underdetermined linear system.\n\n\nCode\nx = np.linspace(-3, 3, 100)\ny = np.linspace(-3, 3, 100)\nt = np.linspace(-1.9, 3.9, 100)\nf1 = lambda x, y: 2 - x - y\nf2 = lambda x, y: y - x\nplot_function_3d(x, y, [f1, f2], azim=30, elev=20, ticks_every=[1, 1, 2], figsize=(6, 6), zorders=[0, 1], dist=11,\n        colors=['steelblue', 'limegreen'], alpha=0.6, titlepad=-5, labelpad=2, title='2 Equations, 3 Unknowns',\n        lines=[[1 - t, np.full(len(t), 1), t]])\n\n\n\n\n\n\n\n\n\nLet’s now step up to a system with 3 linear equations and 3 unknowns. For example, take the following,\n\\[\\begin{alignat*}{5}\n   3x & {}+{} &  2y & {}+{} & z {}={} & 0 \\\\\n   x & {}+{} &  y & {}-{} & z {}={} & 1 \\\\\n   x & {}-{} &  3y & {}-{} & z {}={} & -3.\n\\end{alignat*}\\]\nAgain using substitution, you can iteratively solve each equation one by one to check that this system has exactly one solution when \\(x=-\\frac{1}{2}\\), \\(y=1\\), and \\(z=-\\frac{1}{2}\\).\nTo visualize what’s going on here, again realize we’re in 3 dimensions since there are 3 variables. Each equation in the system again defines a plane. Solving each for \\(z=f(x,y)\\), the three planes are given by\n\\[\\begin{align*}\nz &= -3x - 2y + 0, \\\\\nz &= x + y - 1, \\\\\nz &= 4 .\n\\end{align*}\\]\nWe can plot these planes and see if they intersect, and roughly where they intersect. It may be a little hard to visualize, but you should see the planes all intersect at a point, namely the red dot at \\(x=-\\frac{1}{2}\\), \\(y=1\\), and \\(z=-\\frac{1}{2}\\).\n\n\nCode\nx = np.linspace(-2.5, 1.5, 100)\ny = np.linspace(0, 2, 100)\nf1 = lambda x, y: -3 * x - 2 * y + 0\nf2 = lambda x, y: x + y - 1\nf3 = lambda x, y: x - 3 * y + 3\nplot_function_3d(x, y, [f1, f2, f3], azim=65, elev=25, ticks_every=[1, 1, 3], figsize=(6, 6), zorders=[0, 2, 1],\n                 colors=['steelblue', 'salmon', 'limegreen'], points=[[-0.5, 1, -0.5]], alpha=0.6, labelpad=3, \n                 dist=11, title='3 Equations, 3 Unknowns')\n\n\n\n\n\n\n\n\n\nEvery linear system of 3 equations with 3 unknowns will look like this. The only time there won’t be a solution is when any two planes are parallel to each other. Note now it only takes any two being parallel for there to be no solution, not all of them.\nIf we like, we could again outright solve for the solutions of a general \\(3 \\times 3\\) linear system, but the solutions will look far more complex since there are now 12 coefficients \\(a, b, c, \\cdots, k, l\\),\n\\[\\begin{alignat*}{5}\n   ax & {}+{} &  by & {}+{} & cz {}={} & j \\\\\n   dx & {}+{} &  ey & {}+{} & fz {}={} & k \\\\\n   gx & {}+{} &  hy & {}+{} & iz {}={} & l.\n\\end{alignat*}\\]\nHere’s what sympy gives as the solution to this system. Notice again that each term depends on the same denominator. When that denominator is zero the system won’t have a solution.\n\n\nCode\nx, y, z = sp.symbols('x y z')\na, b, c, d, e, f, g, h, i, j, k, l = sp.symbols('a b c d e f g h i j k l')\neq1 = sp.Eq(a * x + b * y + c * z, j)\neq2 = sp.Eq(d * x + e * y + f * z, k)\neq3 = sp.Eq(g * x + h * y + i * z, l)\nsol = sp.solve((eq1, eq2, eq3), (x, y, z))\nprint(f'x = {sol[x]}')\nprint(f'y = {sol[y]}')\nprint(f'z = {sol[z]}')\n\n\nx = (b*f*l - b*i*k - c*e*l + c*h*k + e*i*j - f*h*j)/(a*e*i - a*f*h - b*d*i + b*f*g + c*d*h - c*e*g)\ny = (-a*f*l + a*i*k + c*d*l - c*g*k - d*i*j + f*g*j)/(a*e*i - a*f*h - b*d*i + b*f*g + c*d*h - c*e*g)\nz = (a*e*l - a*h*k - b*d*l + b*g*k + d*h*j - e*g*j)/(a*e*i - a*f*h - b*d*i + b*f*g + c*d*h - c*e*g)\n\n\nThese denominators keep showing up for square-shaped systems like this. They’re called determinants. In the \\(1 \\times 1\\) case \\(x=\\frac{b}{a}\\), so the determinant is just the denominator \\(D=a\\). For the \\(2 \\times 2\\) system the determinant is \\(D = ad-bc\\). For the \\(3 \\times 3\\) system it’s a more complicated expression, but still just a polynomial function of all of the coefficients on the left-hand side,\n\\[D = aei - afh - bdi + bfg + cdh - ceg.\\]\nWhen \\(D \\neq 0\\) these kinds of linear systems evidently have unique solutions. When \\(D=0\\) they have no solution at all since the denominators blow up. This is when two of the lines or planes are parallel to each other. This is a general pattern for any \\(n \\times n\\) linear system.\nI could keep stepping up like this, going to 4 linear equations, 5 linear equations, etc. But you should start to see the idea by now. Suppose we have a system of \\(m\\) linear equations with \\(n\\) unknown variables \\(x_0, x_1, \\cdots, x_{n-1}\\),\n\\[\n\\begin{array}{c<{x_0} c c<{x_1} c c<{\\cdots} c c<{x_{n-1}} c l}\na_{0,0}x_0 & + & a_{0,1}x_1 & + & \\cdots & + & a_{0,n-1}x_{n-1} & = & b_0 \\\\\na_{1,0}x_0 & + & a_{1,1}x_1 & + & \\cdots & + & a_{1,n-1}x_{n-1} & = & b_1 \\\\\n\\vdots    &   & \\vdots    &   &  \\ddots  &   & \\quad \\vdots    &   & \\vdots\\\\\na_{m-1,0}x_0 & + & a_{m-1,1}x_1 & + & \\cdots & + & a_{m-1,n-1}x_{n-1} & = & b_{m-1} \\\\\n\\end{array}.\n\\]\nWe can classify the solutions of an arbitrary \\(m \\times n\\) linear system as follows:\n\nIf the linear system is square, i.e. \\(m=n\\), then the system will have\n\nA unique solution if the determinant is nonzero,\nIf the determinant is zero, the system will have\n\nInfinitely many solutions if all the equations are multiples of each other,\nNo solution otherwise.\n\n\nIf the linear system is underdetermined, i.e. \\(m < n\\), then the system will have infinitely many solutions.\nIf the linear system is overdetermined, i.e. \\(m > n\\), then the system will have no solutions.\n\nGraphically, a unique solution means that the \\(n\\) hyperplanes defined by the \\(n\\) linear equations all intersect at a single point \\((x_0, x_1, \\cdots, x_n)\\) in \\(\\mathbb{R}^n\\). Think of a hyperplane as an \\(n\\)-dimensional generalization of a plane. If any of two hyperplanes are parallel in \\(\\mathbb{R}^n\\), there will be no solution."
  },
  {
    "objectID": "notebooks/linear-systems.html#matrix-vector-notation",
    "href": "notebooks/linear-systems.html#matrix-vector-notation",
    "title": "4  Systems of Linear Equations",
    "section": "4.2 Matrix-Vector Notation",
    "text": "4.2 Matrix-Vector Notation\nThese systems of linear equations are incredibly tedious to write out and analyze as is for all but the simplest cases of like two or three equations. There’s a cleaner notation for working with these things. Here’s what we can do. We have 3 separate types of objects showing up in these equations:\n\nThe \\(m \\cdot n\\) coefficients \\(a_{0,0}, \\ a_{0,1}, \\ \\cdots, \\ a_{m-1,n-1}\\).\nThe \\(n\\) unknown variables \\(x_0, x_1, \\cdots, x_{n-1}\\).\nThe \\(m\\) constant terms \\(b_0, b_1, \\cdots, b_{m-1}\\).\n\nLet’s put each of these three objects inside their own arrays and write the linear system as\n\\[\n\\begin{pmatrix}\na_{0,0} & a_{0,1} & \\cdots & a_{0,n-1} \\\\\na_{1,0} & a_{1,1} & \\cdots & a_{1,n-1} \\\\\n\\vdots  & \\vdots  & \\ddots & \\vdots    \\\\\na_{m-1,0} & a_{m-1,1} & \\cdots & a_{m-1,n-1}\n\\end{pmatrix}\n\\begin{pmatrix}\nx_0 \\\\ x_1 \\\\ \\vdots \\\\ x_{n-1}\n\\end{pmatrix} =\n\\begin{pmatrix}\nb_0 \\\\ b_1 \\\\ \\vdots \\\\ b_{m-1}\n\\end{pmatrix}.\n\\]\nWe’ll define this notation to mean exactly the same thing as writing out the full system of linear equations. The array of coefficients is a rank-2 array of shape \\((m,n)\\). We’ll call this an \\(m \\times n\\) matrix, denoted by a bold-face \\(\\mathbf{A}\\). The array of unknowns is a rank-2 array of shape \\((n,1)\\). Though technically also a matrix, we’ll call an array of this shape a column vector of size \\(n\\), denoted by a bold-face \\(\\mathbf{x}\\). The array of constants is also a rank-2 array of shape \\((m,1)\\). We’ll call this a column vector of size \\(m\\), denoted by a bold-face \\(\\mathbf{b}\\). In this sleek notation, our complicated system of \\(m\\) linear equations with \\(n\\) unknowns can be written\n\\[\\mathbf{A} \\mathbf{x} = \\mathbf{b}.\\]\nFor example, the 3 systems we considered above can be written in matrix-vector notation as\n\\[\\begin{gather*}\n\\begin{alignedat}{3}\n   x & {}+{} &  y & {}={} & 2  \\\\\n   x & {}-{} &  y & {}={} & 0\n\\end{alignedat}\n\\quad \\Longrightarrow \\quad\n\\begin{pmatrix}\n1 & 1 \\\\\n1 & -1\n\\end{pmatrix}\n\\begin{pmatrix}\nx \\\\\ny\n\\end{pmatrix} =\n\\begin{pmatrix}\n2 \\\\\n0\n\\end{pmatrix}.\n\\end{gather*}\\]\n\\[\\begin{gather*}\n\\begin{alignedat}{5}\n   x & {}+{} &  y & {}+{} & z & {}={} & 2  \\\\\n   x & {}-{} &  y & {}+{} & z & {}={} & 0\n\\end{alignedat}\n\\quad \\Longrightarrow \\quad\n\\begin{pmatrix}\n1 & 1 & 1 \\\\\n1 & -1 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\nx \\\\\ny \\\\\nz\n\\end{pmatrix} =\n\\begin{pmatrix}\n2 \\\\\n0 \\\\\n\\end{pmatrix}.\n\\end{gather*}\\]\n\\[\\begin{gather*}\n\\begin{alignedat}{2}\n   x & {}={} & 2  \\\\\n   x & {}={} & 0\n\\end{alignedat}\n\\quad \\Longrightarrow \\quad\n\\begin{pmatrix}\n1 \\\\\n1\n\\end{pmatrix}\n\\begin{pmatrix}\nx\n\\end{pmatrix} =\n\\begin{pmatrix}\n2 \\\\\n0\n\\end{pmatrix}.\n\\end{gather*}\\]\nIt may not be at all obvious, but having written a linear system as a matrix-vector equation, I’ve implicitly defined a new kind of array multiplication. To see this, I’ll define a new column vector that I’ll call \\(\\mathbf{A} \\mathbf{x}\\) whose elements are just the left-hand side of the linear system when written out,\n\\[\n\\mathbf{A} \\mathbf{x} =\n\\begin{pmatrix}\na_{0,0}x_0 & + & a_{0,1}x_1 & + & \\cdots & + & a_{0,n-1}x_{n-1} \\\\\na_{1,0}x_0 & + & a_{1,1}x_1 & + & \\cdots & + & a_{1,n-1}x_{n-1} \\\\\n\\vdots    &   & \\vdots    &   &  \\ddots  &   & \\quad \\vdots     \\\\\na_{m-1,0}x_0 & + & a_{m-1,1}x_1 & + & \\cdots & + & a_{m-1,n-1}x_{n-1} \\\\\n\\end{pmatrix}.\n\\]\nSetting the \\(i\\)th row of \\(\\mathbf{A} \\mathbf{x}\\) equal to the \\(i\\)th row of \\(\\mathbf{b}\\) must imply that each \\(b_i\\) can be written\n\\[b_i = a_{i,0}x_0 + a_{i,1}x_1 + \\cdots + a_{i,n-1}x_{n-1} = \\sum_{k=0}^{n-1} a_{i,k}x_k.\\]\nThat is, each constant term \\(b_i\\) is the sum of the products of the \\(i\\)th row of the matrix \\(\\mathbf{A}\\) with the vector \\(\\mathbf{x}\\). This is matrix-vector multiplication, a special case of matrix multiplication, which I’ll get to shortly.\nHere’s a quick example, where a \\(2 \\times 3\\) matrix \\(\\mathbf{A}\\) is matrix multiplied with a size \\(3\\) vector \\(\\mathbf{x}\\). For each row we’re element-wise multiplying that row of \\(\\mathbf{A}\\) with the vector \\(\\mathbf{x}\\) and then summing up the terms. The output will be the vector \\(\\mathbf{b}\\) of size \\(2\\).\n\\[\n\\mathbf{A} \\mathbf{x} =\n\\begin{pmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6\n\\end{pmatrix}\n\\begin{pmatrix}\n1 \\\\\n1 \\\\\n1\n\\end{pmatrix} =\n\\begin{pmatrix}\n1 \\cdot 1 + 2 \\cdot 1 + 3 \\cdot 1 \\\\\n4 \\cdot 1 + 5 \\cdot 1 + 6 \\cdot 1 \\\\\n\\end{pmatrix} =\n\\begin{pmatrix}\n6 \\\\\n15\n\\end{pmatrix} = \\mathbf{b}.\n\\]\nHere’s a better way of thinking about what matrix-vector multiplication is. Notice that the \\(m \\times n\\) matrix \\(\\mathbf{A}\\) consists of \\(n\\) columns each containing \\(m\\) elements. We can think of each of these \\(n\\) columns as a vector of size \\(m\\). They’re called the column vectors of \\(\\mathbf{A}\\). If \\(\\mathbf{a}_0, \\mathbf{a}_1, \\cdots, \\mathbf{a}_{n-1}\\) are the \\(n\\) column vectors of \\(\\mathbf{A}\\), we can write\n\\[\n\\mathbf{A}=\n\\begin{pmatrix}\n\\mathbf{a}_0 & \\mathbf{a}_1 & \\cdots & \\mathbf{a}_{n-1}\n\\end{pmatrix}.\n\\]\nIn the previous example, the column vectors of \\(\\mathbf{A}\\) are the three size-2 vectors\n\\[\\mathbf{a}_0 = \\binom{1}{4}, \\quad \\mathbf{a}_1 = \\binom{2}{5}, \\quad \\mathbf{a}_2 = \\binom{3}{6}.\\]\nIn this notation, matrix-vector multiplication is just a sum of the column vectors of \\(\\mathbf{A}\\), but with each column \\(\\mathbf{a}_j\\) weighted by some \\(x_j\\),\n\\[\n\\mathbf{A} \\mathbf{x} =\n\\begin{pmatrix}\n\\mathbf{a}_0 & \\mathbf{a}_1 & \\cdots & \\mathbf{a}_{n-1}\n\\end{pmatrix}\n\\begin{pmatrix}\nx_0 \\\\\nx_1 \\\\\n\\cdots \\\\\nx_{n-1}\n\\end{pmatrix} =\nx_0 \\mathbf{a}_0 + x_1 \\mathbf{a}_1 + \\cdots x_{n-1} \\mathbf{a}_{n-1}.\n\\]\nThis weighted sum of vectors on the right-hand side is called a linear combination. A linear combination is a weighted sum of a bunch of vectors. That is, the matrix-vector product \\(\\mathbf{A} \\mathbf{x}\\) is a linear combination of the column vectors of the matrix \\(\\mathbf{A}\\), weighted by the vector \\(\\mathbf{x}\\). In the above example, this would look like\n\\[\n\\begin{pmatrix}\n1 & 2 & 3 \\\\\n4 & 5 & 6\n\\end{pmatrix}\n\\begin{pmatrix}\n1 \\\\\n1 \\\\\n1\n\\end{pmatrix} = 1 \\cdot \\binom{1}{4} + 1 \\cdot \\binom{2}{5} + 1 \\cdot \\binom{3}{6} = \\binom{1+2+3}{4+5+6} = \\binom{6}{15}.\n\\]"
  },
  {
    "objectID": "notebooks/linear-systems.html#matrix-multiplication",
    "href": "notebooks/linear-systems.html#matrix-multiplication",
    "title": "4  Systems of Linear Equations",
    "section": "4.3 Matrix Multiplication",
    "text": "4.3 Matrix Multiplication\nWhile I’m on the topic, I’ll go ahead and define matrix multiplication for two matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) as well. If \\(\\mathbf{A}\\) is \\(m \\times n\\) and \\(\\mathbf{B}\\) is \\(n \\times p\\), define the matrix product \\(\\mathbf{C}=\\mathbf{A}\\mathbf{B}\\) as the \\(m \\times p\\) matrix \\(\\mathbf{C}\\) whose elements are given by\n\\[C_{i,j} = \\sum_{k=0}^{n-1} A_{i,k} B_{k,j} = A_{i,0} B_{0,j} + A_{i,1} B_{1,j} + \\cdots + A_{i,n-1} B_{n-1,j}.\\]\nMatrix multiplication is always expressed symbolically by directly concatenating the two matrix symbols next to each other like \\(\\mathbf{A}\\mathbf{B}\\). We’d never use a multiplication symbol between them since those are often used to represent other kinds of multiplication schemes like element-wise multiplication or convolutions. Further, matrix multiplication is only defined when the numbers of columns in \\(\\mathbf{A}\\) equals the number of rows of \\(\\mathbf{B}\\). We say matrices satisfying this condition are compatible. If they can’t be multiplied, they’re called incompatible.\nIn words, matrix multiplication is the process where you take a row \\(i\\) of the left matrix \\(\\mathbf{A}\\), element-wise multiply it with a column \\(j\\) of the right matrix \\(\\mathbf{B}\\), and then sum up the results to get the entry \\(C_{i,j}\\) of the output matrix \\(\\mathbf{C}\\). Doing this for all pairs of rows and columns will fill in \\(\\mathbf{C}\\).\nHere’s an example where \\(\\mathbf{A}\\) is \\(3 \\times 3\\) and \\(\\mathbf{B}\\) is \\(3 \\times 2\\). The output matrix \\(\\mathbf{C}\\) will be \\(3 \\times 2\\).\n\\[\n\\begin{pmatrix}\n    \\color{red}{1} & \\color{red}{2} & \\color{red}{3} \\\\\n    \\color{blue}{4} & \\color{blue}{5} & \\color{blue}{6} \\\\\n    \\color{green}{7} & \\color{green}{8} & \\color{green}{9} \\\\\n\\end{pmatrix}\n\\begin{pmatrix}\n    \\color{orange}{6} & \\color{purple}{5} \\\\\n    \\color{orange}{4} & \\color{purple}{3} \\\\\n    \\color{orange}{2} & \\color{purple}{1} \\\\\n\\end{pmatrix} =\n\\begin{pmatrix}\n   \\color{red}{1} \\cdot \\color{orange}{6} + \\color{red}{2} \\cdot \\color{orange}{4} + \\color{red}{3} \\cdot \\color{orange}{2} & \\color{red}{1} \\cdot \\color{purple}{5} + \\color{red}{2} \\cdot \\color{purple}{3} + \\color{red}{3} \\cdot \\color{purple}{1} \\\\\n   \\color{blue}{4} \\cdot \\color{orange}{6} + \\color{blue}{5} \\cdot \\color{orange}{4} + \\color{blue}{6} \\cdot \\color{orange}{2} & \\color{blue}{4} \\cdot \\color{purple}{5} + \\color{blue}{5} \\cdot \\color{purple}{3} + \\color{blue}{6} \\cdot \\color{purple}{1} \\\\\n   \\color{green}{7} \\cdot \\color{orange}{6} + \\color{green}{8} \\cdot \\color{orange}{4} + \\color{green}{9} \\cdot \\color{orange}{2} & \\color{green}{7} \\cdot \\color{purple}{5} + \\color{green}{8} \\cdot \\color{purple}{3} + \\color{green}{9} \\cdot \\color{purple}{1} \\\\\n\\end{pmatrix} =\n\\begin{pmatrix}\n   20 & 14 \\\\\n   56 & 41 \\\\\n   92 & 68 \\\\\n\\end{pmatrix}.\n\\]\nAside: If you’re still having a hard time picturing what matrix multiplication is doing, you may find this online visualization tool useful.\nNote that matrix multiplication does not commute. That is, we can’t swap the order of the two matrices being multiplied, \\(\\mathbf{A}\\mathbf{B} \\neq \\mathbf{B}\\mathbf{A}\\). Try to multiply the above example in the opposite order and see what happens. The two matrices won’t even be compatible anymore. However, matrix multiplication is associative, which means you can group parentheses just like you ordinarily would. For example, multiplying three matrices \\(\\mathbf{A}, \\mathbf{B}, \\mathbf{C}\\) could be done by multiplying either the first two, and then the last; or the last two, and then the first. That is,\n\\[\\mathbf{A}\\mathbf{B}\\mathbf{C} = \\mathbf{A}(\\mathbf{B}\\mathbf{C}) = (\\mathbf{A}\\mathbf{B})\\mathbf{C}.\\]\nMatrix multiplication can be thought of as a kind of extension to matrix-vector multiplication, where instead of just trying to solve one linear system, we’re trying to solve a bunch of them in parallel, but all having the same coefficients \\(\\mathbf{A}\\). Suppose we want to simultaneously solve the systems of equations\n\\[\\mathbf{A}\\mathbf{x}_0=\\mathbf{b}_0, \\quad \\mathbf{A}\\mathbf{x}_1=\\mathbf{b}_1, \\quad \\cdots, \\quad \\mathbf{A}\\mathbf{x}_{p-1}=\\mathbf{b}_{p-1}.\\]\nWhat we can do is create two matrices \\(\\mathbf{X}\\) and \\(\\mathbf{B}\\) by making column vectors out of each \\(\\mathbf{x}_j\\) and \\(\\mathbf{b}_j\\),\n\\[\\begin{align*}\n\\mathbf{X} &=\n\\begin{pmatrix}\n\\mathbf{x}_0 & \\mathbf{x}_1 & \\cdots & \\mathbf{x}_{n-1}\n\\end{pmatrix}, \\\\\n\\mathbf{B} &=\n\\begin{pmatrix}\n\\mathbf{b}_0 & \\mathbf{b}_1 & \\cdots & \\mathbf{b}_{m-1}\n\\end{pmatrix}.\n\\end{align*}\\]\nThen the bunch of linear systems we’re trying to solve is just the matrix product \\(\\mathbf{A}\\mathbf{X}=\\mathbf{B}\\). Each column \\(\\mathbf{A}\\mathbf{x}_j=\\mathbf{b}_j\\) can again be thought of as a linear combination of the columns of \\(\\mathbf{A}\\), but with each column weighted by its own vector \\(\\mathbf{x}_j\\).\n\n4.3.1 Matrix Multiplication Algorithm\nMatrix multiplication is perhaps the single most important mathematical operation in machine learning. It’s so important I’m going to write a function to code it from scratch before showing how to do it in numpy. I’ll also analyze the speed of the algorithm in FLOPS and the memory in terms of word size. Algorithmically, all matrix multiplication is doing is looping over every single element of \\(\\mathbf{C}\\) and performing the sum-product calculation above for each \\(C_{i,j}\\). Here’s a function matmul that takes in two numpy arrays A and B and multiplies them, returning the product C if the dimensions are compatible.\n\ndef matmul(A, B):\n    assert A.shape[1] == B.shape[0]\n    m, n, p = A.shape[0], A.shape[1], B.shape[1]\n    C = np.zeros((m, p))\n    for i in range(m):\n        for j in range(p):\n            for k in range(n):\n                C[i, j] += A[i, k] * B[k, j]\n    return C\n\n\nA = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]); print(f'A = \\n{A}')\nB = np.array([[6, 5], [4, 3], [2, 1]]); print(f'B = \\n{B}')\nC = matmul(A, B); print(f'C = AB = \\n{C.astype(A.dtype)}')\n\nA = \n[[1 2 3]\n [4 5 6]\n [7 8 9]]\nB = \n[[6 5]\n [4 3]\n [2 1]]\nC = AB = \n[[20 14]\n [56 41]\n [92 68]]\n\n\nLet’s take a quick look at what this function is doing complexity wise. First off, we’re pre-computing the output matrix \\(\\mathbf{C}\\). That’ll contribute \\(O(mp)\\) memory since \\(\\mathbf{C}\\) is \\(m \\times p\\). All of the FLOPS are happening inside the double loop over \\(m\\) and \\(p\\). For each \\(i,j\\) pair, the function performing \\(n\\) total multiplications and \\(n-1\\) total additions, i.e. \\(2n-1\\) FLOPs per \\(i,j\\) pair. Since we’re doing this \\(m \\times p\\) times, that gives \\(m \\cdot p \\cdot (2n-1)\\) total FLOPS, or to leading order \\(O(mnp)\\) FLOPS. Matrix multiplication is an example of a cubic time algorithm since if \\(n=m=p\\) we’d have a cubic leading order of \\(O(n^3)\\) FLOPS.\nAside: People have found algorithms that can matrix multiply somewhat faster than cubic time. For example, Strassen’s algorithm can matrix multiply in about \\(O(n^{2.8})\\) time. Matrices that have special forms, e.g. banded matrices or sparse matrices, have special algorithms that can multiply them even faster, for example by using the Fast Fourier Transform. These special algorithms have their uses, but it remains the case that practically speaking most matrix multiplication is best done using the naive cubic time algorithm.\nCubic time may seem fast, but it’s really not unless the matrices are relatively small (say \\(n \\leq 1000\\) or so). For this reason, a lot of effort has gone into making matrix multiplication run highly efficiently on hardware, mostly by parallelizing the function above, optimizing blocks to take advantage of meory, and compiling operations down to low-level C or FORTRAN code. In fact, it’s no exaggeration to say that the entire reason the deep learning revolution over the past decade happened because people found ways to multiply matrices much faster by using GPUs.\nAnyway, we’d never want to implement matrix multiplication natively in python like this. It’s far too slow. In practice we’d use np.matmul(A, B) to matrix multiply. A cleaner notation is to use the special @ symbol for matrix multiplication, in which case we can just write A @ B.\n\nA @ B\n\narray([[20, 14],\n       [56, 41],\n       [92, 68]])\n\n\n\n\n4.3.2 Multiplying Multiple Matrices\nWhat about multiplying three or more matrices together. I already said matrix multiplication is associative, so we can multiply any two at a time we like and get the same answer. However, there are often computational advantages to multiplying them together in some particular sequence. For example, suppose we wanted to multiply \\(\\mathbf{D} = \\mathbf{A}\\mathbf{B}\\mathbf{C}\\). Suppose, \\(\\mathbf{A}\\) is \\(m \\times n\\), \\(\\mathbf{B}\\) is \\(n \\times p\\), and \\(\\mathbf{C}\\) is \\(p \\times q\\). No matter which order we do it, the output \\(\\mathbf{D}\\) will have size \\(m \\times q\\). But there are two ways we could do this multiplication.\n\n\\(\\mathbf{D} = \\mathbf{A}(\\mathbf{B}\\mathbf{C})\\): In this case, the \\(\\mathbf{E}=\\mathbf{B}\\mathbf{C}\\) computation requires \\(nq(2p-1)\\) FLOPS, and then the \\(\\mathbf{A}\\mathbf{E}\\) computation requires \\(mq(2n-1)\\) FLOPS. The total is thus the sum of these two, i.e. \\[nq(2p-1) + mq(2n-1) = O(npq+mnq) \\ \\ \\text{FLOPS}.\\]\n\\(\\mathbf{D} = (\\mathbf{A}\\mathbf{B})\\mathbf{C}\\): In this case, the \\(\\mathbf{F}=\\mathbf{A}\\mathbf{B}\\) computation requires \\(mp(2n-1)\\) FLOPS, and then the \\(\\mathbf{F}\\mathbf{C}\\) computation requires \\(mq(2n-1)\\) FLOPS. The total is thus the sum of these two, i.e. \\[mq(2p-1) + mp(2n-1) = O(mpq+mnp) \\ \\ \\text{FLOPS}.\\]\n\nLet’s put some numbers in to make it clear what’s going on. Suppose \\(m=1000\\), \\(n=2\\), \\(p=100\\), and \\(q=100\\). Then the first case takes\n\\[nq(2p-1) + mq(2n-1) = 339800 \\ \\ \\text{FLOPS},\\]\nwhile the second case takes a staggering\n\\[mq(2p-1) + mp(2n-1) = 20200000 \\ \\ \\text{FLOPS}.\\]\nIt would thus behoove us in this case to multiply the matrices in the first order to save on computation, \\(\\mathbf{D} = \\mathbf{A}(\\mathbf{B}\\mathbf{C})\\). Here’s a programmatic way to see this.\n\nm = 1000\nn = 2\np = 100\nq = 100\n\nprint(f'A(BC): {m * q * (2 * n - 1) + n * q * (2 * p - 1)} FLOPS')\nprint(f'(AB)C: {m * p * (2 * n - 1) + m * q * (2 * p - 1)} FLOPS')\n\nA(BC): 339800 FLOPS\n(AB)C: 20200000 FLOPS\n\n\nThe same issues extend to multiplying together arbitrarily many matrices. You can save a lot of compute by first taking time to find the optimal order to multiply them together before doing the computation. Don’t just naively multiply them in order. Numpy has a function np.linalg.multi_dot that can do this for you. If you pass in a list of matrices, it’ll multiply them together in the most efficient order to help save on computation. Here’s an example. I’ll profile the different ways we can do the \\(\\mathbf{A}\\mathbf{B}\\mathbf{C}\\) example above. Notice that indeed \\(\\mathbf{A}(\\mathbf{B}\\mathbf{C})\\) is much faster than \\((\\mathbf{A}\\mathbf{B})\\mathbf{C}\\). The multi_dot solution is roughly as fast as the \\(\\mathbf{A}(\\mathbf{B}\\mathbf{C})\\) solution, but it does take slightly longer because it first calculates the optimal ordering, which adds a little bit of time.\n\nA = np.random.rand(m, n)\nB = np.random.rand(n, p)\nC = np.random.rand(p, q)\n\n\n%timeit A @ (B @ C)\n\n65 µs ± 176 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n\n\n\n%timeit (A @ B) @ C\n\n543 µs ± 31.8 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n\n\n\n%timeit np.linalg.multi_dot([A, B, C])\n\n77.5 µs ± 384 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n\n\n\n\n4.3.3 Matrix Multiplication vs Element-Wise Multiplication\nWe’ve already seen a different way we can multiply two matrices (or any array), namely element-wise multiplication. For matrices, element-wise multiplication is sometimes called the Hadamard product. I’ll denote element-wise multiplication as \\(\\mathbf{A} \\circ \\mathbf{B}\\). It’s only defined when the shapes of \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) are equal (or can be broadcasted to be equal).\nIt’s important to mind the difference between matrix multiplication and element-wise multiplication of matrices. In general \\(\\mathbf{A} \\circ \\mathbf{B} \\neq \\mathbf{A} \\mathbf{B}\\). They’re defined completely differently,\n\\[\n(A \\circ B)_{i,j} = A_{i,j} \\cdot B_{i,j} \\\\\n(AB)_{i,j} = \\sum_k A_{i,k}B_{k,j}.\n\\]\nIn numpy we’ll use A * B for element-wise multiplication and A @ B for matrix multiplication. To make it clear the two kinds of multiplication aren’t the same thing here’s an example.\n\nA = np.array([[1, 2], [3, 4]])\nB = np.array([[1, 0], [0, 1]])\nprint(f'A*B = \\n{A * B}')\nprint(f'AB = \\n{A @ B}')\n\nA*B = \n[[1 0]\n [0 4]]\nAB = \n[[1 2]\n [3 4]]"
  },
  {
    "objectID": "notebooks/linear-systems.html#solving-linear-systems",
    "href": "notebooks/linear-systems.html#solving-linear-systems",
    "title": "4  Systems of Linear Equations",
    "section": "4.4 Solving Linear Systems",
    "text": "4.4 Solving Linear Systems\n\n4.4.1 Square Systems\nWe can use the idea of matrix multiplication to try to solve a linear system of equations. Suppose we have a system of \\(n\\) linear equations with \\(n\\) unknowns (it’s important \\(m=n\\) here),\n\\[\n\\begin{array}{c<{x_0} c c<{x_1} c c<{\\cdots} c c<{x_{n-1}} c l}\na_{0,0}x_0 & + & a_{0,1}x_1 & + & \\cdots & + & a_{0,n-1}x_{n-1} & = & b_0 \\\\\na_{1,0}x_0 & + & a_{1,1}x_1 & + & \\cdots & + & a_{1,n-1}x_{n-1} & = & b_1 \\\\\n\\vdots    &   & \\vdots    &   &  \\ddots  &   & \\quad \\vdots    &   & \\vdots\\\\\na_{m-1,0}x_0 & + & a_{m-1,1}x_1 & + & \\cdots & + & a_{m-1,n-1}x_{n-1} & = & b_{m-1} \\\\\n\\end{array}.\n\\]\nWritten in matrix vector notation, we’d like to solve the equation \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\) for the vector \\(\\mathbf{x}\\). How do we go about this? Let’s look at the simplest cases when \\(n=1,2\\) and see if we can spot a pattern.\nWhen \\(n=1\\), we’re just solving the single linear equation \\(ax=b\\), where \\(a,x,b\\) are all real numbers. In this case it’s easy, as dividing both sides by \\(a\\) gives \\(x = a^{-1}b\\) when \\(a \\neq 0\\).\nWhen \\(n=2\\), we’re solving the \\(2 \\times 2\\) linear system\n\\[\\begin{alignat*}{3}\n   ax & {}+{} &  by & {}={} & e \\\\\n   cx & {}+{} &  dy & {}={} & f,\n\\end{alignat*}\\]\nwhich I showed before was given by\n\\[\\begin{align*}\nx &= \\frac{de-bf}{ad-bc} \\\\\ny &= \\frac{af-ce}{ad-bc},\n\\end{align*}\\]\nprovided \\(ad \\neq bc\\). Now, if I rewrite this equation in matrix-vector notation, I’d get\n\\[\n\\mathbf{A}\\mathbf{x} =\n\\begin{pmatrix}\na & b \\\\\nc & d\n\\end{pmatrix}\n\\begin{pmatrix}\nx \\\\\ny\n\\end{pmatrix} =\n\\begin{pmatrix}\ne \\\\\nf\n\\end{pmatrix}\n= \\mathbf{b},\n\\]\nand the solutions would look like\n\\[\n\\mathbf{x} =\n\\begin{pmatrix}\nx \\\\\ny\n\\end{pmatrix} =\n\\begin{pmatrix}\n\\frac{de-bf}{ad-bc} \\\\\n\\frac{af-ce}{ad-bc}\n\\end{pmatrix} =\n\\begin{pmatrix}\n\\frac{d}{ad-bc} & -\\frac{b}{ad-bc} \\\\\n-\\frac{c}{ad-bc} & \\frac{a}{ad-bc}\n\\end{pmatrix}\n\\begin{pmatrix}\ne \\\\\nf\n\\end{pmatrix} =\n\\mathbf{A}^{-1} \\mathbf{b}.\n\\]\nThe matrix on the right I’m calling \\(\\mathbf{A}^{-1}\\) because it looks something like the \\(x=a^{-1}b\\) equation from the \\(n=1\\) case. You can verify it gives the right result by matrix multiplying \\(\\mathbf{A}^{-1}\\) with \\(\\mathbf{b}\\) and confirming it does indeed give the equations for \\(\\mathbf{x}\\).\nBut what exactly does it mean to talk about “dividing by” a matrix? In the \\(n=1\\) case, dividing by \\(a\\) just means that \\(a a^{-1} = 1\\). That is, the two values undo each other when multiplied together. Let’s see what undoing a matrix would look like by matrix multiplying \\(\\mathbf{A}\\mathbf{A}^{-1}\\) when \\(n=2\\),\n\\[\n\\mathbf{A}\\mathbf{A}^{-1} =\n\\begin{pmatrix}\na & b \\\\\nc & d\n\\end{pmatrix}\n\\begin{pmatrix}\n\\frac{d}{ad-bc} & -\\frac{b}{ad-bc} \\\\\n-\\frac{c}{ad-bc} & \\frac{a}{ad-bc}\n\\end{pmatrix} =\n\\begin{pmatrix}\n\\frac{ad}{ad-bc}-\\frac{bc}{ad-bc} & -\\frac{ab}{ad-bc}+\\frac{ab}{ad-bc} \\\\\n\\frac{cd}{ad-bc}-\\frac{dc}{ad-bc} & -\\frac{cb}{ad-bc}+\\frac{da}{ad-bc}\n\\end{pmatrix} =\n\\begin{pmatrix}\n\\frac{ad-bc}{ad-bc} & \\frac{ab-ab}{ad-bc} \\\\\n\\frac{cd-cd}{ad-bc} & \\frac{ad-bc}{ad-bc}\n\\end{pmatrix} =\n\\begin{pmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{pmatrix} = \\mathbf{I}.\n\\]\nThe matrix on the right \\(\\mathbf{I}\\) seems to behave kind of like the \\(1\\) in the \\(a a^{-1} = 1\\) case. This matrix is called the \\(2 \\times 2\\) identity matrix. The matrix \\(\\mathbf{A}^{-1}\\) is called the inverse of \\(\\mathbf{A}\\). The denominator \\(D=ad-bc\\) is again the determinant of \\(\\mathbf{A}\\), usually denoted \\(\\text{det}(\\mathbf{A})\\) or often more simply just \\(|\\mathbf{A}|\\) when the meaning is clear,\n\\[|\\mathbf{A}| = \\text{det}(\\mathbf{A}) = ad - bc.\\]\nThe inverse \\(\\mathbf{A}^{-1}\\) will only exist when \\(|\\mathbf{A}| = ad-bc \\neq 0\\). If \\(\\mathbf{A}^{-1}\\) doesn’t exist, neither does a solution to the linear system, since we can’t solve for \\(\\mathbf{x}\\).\nThe exact same idea extends to arbitrary \\(n \\times n\\) systems as well. The \\(n \\times n\\) identity matrix \\(\\mathbf{I}\\) is the matrix whose values are \\(1\\) when \\(i=j\\) and \\(0\\) when \\(i \\neq j\\). The terms in a matrix when \\(i=j\\) are called the diagonal of the matrix. The terms when \\(i \\neq j\\) are called the off-diagonals. Thus, the identity matrix is the matrix that takes on the value \\(1\\) on the diagonal and \\(0\\) on the off-diagonals.\nIf we’d like to solve the system \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\), we could find the inverse of \\(\\mathbf{A}\\) somehow, and then get the solution \\(\\mathbf{x}\\) by writing \\(\\mathbf{x} = \\mathbf{A}^{-1} \\mathbf{b}\\), provided its determinant \\(\\text{det}(\\mathbf{A}) \\neq 0\\). A matrix with non-zero determinant is called invertible. Invertible matrices have inverses. A matrix with zero determinant are called singular. Singular matrices can’t be inverted.\nOf course, it’s no longer obvious at all how to even find \\(\\mathbf{A}^{-1}\\) or \\(\\text{det}(\\mathbf{A})\\) when \\(n>2\\). Thankfully we don’t need to cover this for machine learning purposes. I’ll just mention that there are efficient algorithms for solving large linear systems like this. You can use np.linalg.solve(A, b) to do this, provided \\(\\mathbf{A}\\) can be inverted. Here’s an example of solving a \\(3 \\times 3\\) linear system. You can also get the inverse directly by using np.linalg.inv(A), though you’d rarely actually want to do this. It turns out matrix inversion is a very numerically unstable operation. Try to avoid explicitly calculating matrix inverses if you can.\n\nA = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]); print(f'A = \\n{A}')\nb = np.array([[1], [1], [1]]); print(f'b = \\n{b}')\nx = np.linalg.solve(A, b); print(f'x = \\n{x}')\n\nA = \n[[1 2 3]\n [4 5 6]\n [7 8 9]]\nb = \n[[1]\n [1]\n [1]]\nx = \n[[ 0.2]\n [-1.4]\n [ 1.2]]\n\n\n\n\n4.4.2 Rectangular Systems\nWhat about when \\(m \\neq n\\)? Things become more interesting then. If \\(m < n\\) the linear system is under-determined, and will have infinitely many solutions. In this case, there are infinitely many ways to “invert” \\(\\mathbf{A}\\). We just need to find any one of them that will work. If \\(m > n\\) the linear system is over-determined, and will have no exact solution. We can however find an approximate solution by looking for the \\(\\mathbf{x}\\) that most nearly solves the linear system. In both cases, a generalization of the inverse exists, called the pseudoinverse.\nHere’s how to find out what the pseudoinverse should be. Let’s define a matrix similar to \\(\\mathbf{A}\\), but with its rows and columns swapped, called the transpose of \\(\\mathbf{A}\\). The transpose of \\(\\mathbf{A}\\), denoted by the symbol \\(\\mathbf{A}^\\top\\), is defined by the relationship \\(A_{i,j}^\\top = A_{j,i}\\). For example, if \\(\\mathbf{A}\\) is the \\(3 \\times 2\\) matrix\n\\[\n\\mathbf{A} =\n\\begin{pmatrix}\n    1 & 2 & 3 \\\\\n    4 & 5 & 6 \\\\\n\\end{pmatrix}, \\quad \\text{then} \\quad\n\\mathbf{A}^\\top =\n\\begin{pmatrix}\n    1 & 4 \\\\\n    2 & 5 \\\\\n    3 & 6 \\\\\n\\end{pmatrix}\n\\]\nis its transpose. The key thing to notice is that if \\(\\mathbf{A}\\) is \\(m \\times n\\), then \\(\\mathbf{A}^\\top\\) is \\(n \\times m\\). This means that the product of the two is square since \\(\\mathbf{A} \\mathbf{A}^\\top\\) is \\(m \\times m\\). Note a square matrix is a matrix with the same number of rows as columns, i.e. with \\(m=n\\). The product in the opposite order is square too since \\(\\mathbf{A}^\\top \\mathbf{A}\\) is \\(n \\times n\\).\nHere’s an example. In numpy, we can use np.transpose(A), or more simply A.T to get the transpose of a matrix. Notice both of the products \\(\\mathbf{A} \\mathbf{A}^\\top\\) and \\(\\mathbf{A}^\\top \\mathbf{A}^\\top\\) are square, but each\n\nA = np.array([[1, 2, 3], [4, 5, 6]]); print(f'A = \\n{A}')\nAt = A.T; print(f'At = \\n{At}')\nAAt = A @ At; print(f'A At = \\n{AAt}')\nAtA = At @ A; print(f'At A = \\n{AtA}')\n\nA = \n[[1 2 3]\n [4 5 6]]\nAt = \n[[1 4]\n [2 5]\n [3 6]]\nA At = \n[[14 32]\n [32 77]]\nAt A = \n[[17 22 27]\n [22 29 36]\n [27 36 45]]\n\n\nNow, suppose we want to solve an \\(m \\times n\\) linear system \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\), where \\(m \\neq n\\), i.e. \\(\\mathbf{A}\\) isn’t square. A non-square matrix is called rectangular. We can’t invert \\(\\mathbf{A}\\) since it isn’t square. But what we can do is make the linear system square by multiplying both sides on the left by \\(\\mathbf{A}^\\top\\),\n\\[\\mathbf{A}^\\top \\mathbf{A}\\mathbf{x} = \\mathbf{A}^\\top \\mathbf{b}.\\]\nSince \\(\\mathbf{A}^\\top \\mathbf{A}\\) is square, it’s invertible (provided it’s non-singular). Thus, this modified linear system has a solution given by\n\\[\\mathbf{x} = (\\mathbf{A}^\\top \\mathbf{A})^{-1} \\mathbf{A}^\\top \\mathbf{b}.\\]\nThis looks kind of like \\(\\mathbf{x} = \\mathbf{A}^{-1} \\mathbf{b}\\). If we define the pseudoinverse \\(\\mathbf{A}^+\\) by\n\\[\\mathbf{A}^+ = (\\mathbf{A}^\\top \\mathbf{A})^{-1} \\mathbf{A}^\\top,\\]\nthen we could write the solution to this modified linear system as\n\\[\\mathbf{x} = \\mathbf{A}^+ \\mathbf{b}.\\]\nThe solution we get from this is called the least squares solution to \\(\\mathbf{A}\\mathbf{x} = \\mathbf{b}\\). It’s the closest we can get to an exact solution in a certain sense.\nIn numpy, you can use np.linalg.lstsq to solve such a system. It returns a tuple of values, the first of which is the least squares solution itself, i.e. \\(\\mathbf{x}\\). You can also get the pseudoinverse directly by using np.linalg.pinv(A), though you’d rarely actually want to do this.\nIn the example below I’ll take the same \\(2 \\times 3\\) matrix \\(\\mathbf{A}\\) from the transpose example, and \\(\\mathbf{b}=\\binom{1}{1}\\). Since \\(m<n\\), this is an under-determined system, so it’ll have infinitely many solutions. The least squares solution will find one of these possible solutions, which turns out to be\n\\[\n\\mathbf{x} =\n\\begin{pmatrix}\n-\\frac{1}{2} \\\\\n0 \\\\\n\\frac{1}{2} \\\\\n\\end{pmatrix}.\n\\]\nI’ll also confirm that indeed \\(\\mathbf{A}\\mathbf{x}=\\mathbf{b}\\) in this case.\n\nA = np.array([[1, 2, 3], [4, 5, 6]]); print(f'A = \\n{A}')\nb = np.array([1, 1]); print(f'b = \\n{b}')\nx, _, _, _ = np.linalg.lstsq(A, b, rcond=None); print(f'x = {x.round(2)}')\nprint(f'Ax = {A @ x}')\n\nA = \n[[1 2 3]\n [4 5 6]]\nb = \n[1 1]\nx = [-0.5 -0.   0.5]\nAx = [1. 1.]"
  },
  {
    "objectID": "notebooks/vector-spaces.html",
    "href": "notebooks/vector-spaces.html",
    "title": "5  Vector Spaces",
    "section": "",
    "text": "In this lesson I’ll continue on the topic of linear algebra by discussing vector spaces. Vector spaces are essential for abstracting linear algebra away from systems of equations and for visualizing linear algebra objects like vectors and matrices. Let’s get started.\n\nimport numpy as np\nimport sympy as sp\nimport matplotlib.pyplot as plt\nfrom utils.math_ml import *\n\n\n5.0.1 Geometry of Vectors\nI’ve introduced matrices and vectors so far kind of organically as the natural way to write and solve systems of linear equations. They’re good for a lot more than solving linear systems, however. For one thing, they possess important geometric properties. I’m now going to re-define the concepts covered so far, but in more geometric terms.\n\n5.0.1.1 Visualizing Vectors\nLet’s go back to the simple 2-dimensional case. Imagine you have a point in the xy-plane, call it \\((x,y)\\). Now, we can think of this as a single point, but we can also imagine it differently. Suppose there was an arrow pointing from the origin \\((0,0)\\) to the point \\((x,y)\\). For example, if the point was \\((1,1)\\), this arrow might look like this.\n\npoint = np.array([1, 1])\nplot_vectors(point, title=f'Arrow From $(0,0)$ To $(1,1)$', ticks_every=0.5)\n\n\n\n\n\n\n\n\nUnlike the point \\((x,y)\\), the arrow \\((x,y)\\) has both a length and a direction. Its length is given by the Pythagorean Theorem. If the triangle has base \\(x\\) and height \\(y\\), then the length of the arrow is just its hypotenuse, i.e. \\(r = \\sqrt{x^2 + y^2}\\). The direction of the arrow is its angle \\(\\theta\\) with respect to the x-axis. This angle is just given by the inverse tangent of height over base, i.e. \\(\\theta = \\tan^{-1}\\big(\\frac{y}{x}\\big)\\).\nIn the example plotted, the length is \\(r=\\sqrt{1+1}=\\sqrt{2}\\), and the angle is \\(\\theta = \\tan^{-1}(1) = 45^\\circ\\). These two values uniquely specify the arrow, assuming it starts at the origin. If we know the length and direction, we know exactly which arrow we’re talking about.\nWhat I’ve just shown is another way to define a vector. A vector is an arrow in the plane. Said differently, a vector is just a point that’s also been endowed with a length (or magnitude) and a direction. The x and y values are called components of a vector. Usually we’ll write a vector in bold-face and its components in regular type but with subscripts indicating which component. For example, \\(\\mathbf{v}=(v_x,v_y)\\). Here’s the same arrow I plotted above, but explicitly labeled as a vector \\(\\mathbf{v}=(1,1)\\). Its components are \\(v_x=1\\) and \\(v_y=1\\).\n\nv = np.array([1, 1])\nplot_vectors(v, title='$\\mathbf{v}=(1,1)$', labels=['$\\mathbf{v}$'], ticks_every=0.5)\n\n\n\n\n\n\n\n\nNotation: It’s common to represent vectors in a few different ways depending on the situation. One way to represent a vector is as a column vector. This is what I did when doing matrix-vector multiplication. Another way, what I just introduced, is a flat vector, or a 1-dimensional array. This is more common when thinking about a vector geometrically. Yet another way is to think of a vector as a row vector, which is the transpose of a column vector. All of these representations conceptually represent the same object, but their shapes are different. Here’s an example: The size-2 vector \\(\\mathbf{v}=(1,1)\\) can be written in 3 different but all equivalent ways:\n\\[\\begin{align*}\n&\\text{Flat vector of shape } (2,): \\mathbf{v} = (1,1), \\\\\n&\\text{Column vector of shape } (2,1): \\mathbf{v} = \\begin{pmatrix}\n1 \\\\\n1 \\\\\n\\end{pmatrix}, \\\\\n&\\text{Row vector of shape } (1,2): \\mathbf{v}^\\top = \\begin{pmatrix}\n1 & 1 \\\\\n\\end{pmatrix}.\n\\end{align*}\\]\nBe careful when working with vectors in code to make sure you’re using the right shapes for the right situation or you’ll get shape mismatch errors (or worse a silent bug).\n\n\n5.0.1.2 Vector Operations\nThe magnitude, or length, of \\(\\mathbf{v}\\) is typically denoted by the symbol \\(||\\mathbf{v}||\\), called a norm,\n\\[||\\mathbf{v}|| = \\sqrt{v_x^2 + v_y^2}.\\]\nIn the above example with \\(\\mathbf{v}=(1,1)\\), its norm is \\(||\\mathbf{v}||=\\sqrt{1+1}=\\sqrt{2} \\approx 1.414\\).\nNotice that the norm must be non-negative since it’s the square root of a sum of squares, i.e. \\(||\\mathbf{v}|| \\geq 0\\). This should sound right, after all negative lengths don’t make any sense.\nWhat happens if we scale a vector \\(\\mathbf{v}\\) by some scalar \\(c\\)? By the rules of scalar-vector multiplication, the new vector should be \\(c\\mathbf{v}=(cx,cy)\\). Since the new vector has length \\(||c\\mathbf{v}||\\), a little math shows that\n\\[||c\\mathbf{v}|| = \\sqrt{(cv_x)^2 + (cv_y)^2} = \\sqrt{c^2(v_x^2 + v_y^2)} = |c| \\sqrt{v_x^2 + v_y^2} = |c| \\cdot ||\\mathbf{v}||.\\]\nThat is, the re-scaled vector \\(c\\mathbf{v}\\) just gets its length re-scaled by \\(c\\). That’s why \\(c\\) is called a scalar. It rescales vectors. Notice if \\(c\\) is negative, the length stays the same, but the direction gets reversed \\(180^\\circ\\) since in that case \\(c\\mathbf{v} = c(v_x, v_y) = -|c|(v_x,v_y)\\).\nHere’s what vector scaling looks like geometrically. I’ll plot the vector \\(\\mathbf{v}=(1,1)\\) again, but scaled by two numbers, one \\(c=2\\), the other \\(c=-1\\). When \\(c=2\\), the vector just doubles its length. That’s the light blue arrow. When \\(c=-1\\), the vector reverses its direction \\(180^\\circ\\), but maintains its length since \\(|c|=1\\). That’s the light orange arrow.\n\n\nCode\nv = np.array([1, 1])\nplot_vectors([v, -v, 2*v], xlim=(-2,3), ylim=(-2,3), title=f'Scaling Vectors', headwidth=7, ticks_every=1,\n             labels=['$\\mathbf{v}$', '$-\\mathbf{v}$', '$2\\mathbf{v}$'], \n             colors=['black', 'salmon', 'steelblue'],\n             text_offsets=[[-0.2, 0.2], [-0.2, 0.4], [-0.2, 0.2]])\n\n\n\n\n\n\n\n\n\nWhat does adding two vectors do? Let \\(\\mathbf{v}=(v_x,v_y)\\) and \\(\\mathbf{w}=(w_x,w_y)\\) be two vectors in the plane. Then their sum is \\(\\mathbf{v}+\\mathbf{w} = (v_x+w_x,v_y+w_y)\\). I’ll plot an example below with \\(\\mathbf{v}=(1,1)\\) and \\(\\mathbf{w}=(1,3)\\). Their sum should be\n\\[\\mathbf{v}+\\mathbf{w}=(1+1,1+3)=(2,4).\\]\n\n\nCode\nv = np.array([1, 1])\nw = np.array([1, 3])\nplot_vectors([v, w, v + w], xlim=(0, 3), ylim=(0, 5), title=f'Adding Two Vectors', ticks_every=1,\n             labels=['$\\mathbf{v}$', '$\\mathbf{w}$', '$\\mathbf{v}+\\mathbf{w}$'], \n             colors=['salmon', 'steelblue', 'black'])\n\n\n\n\n\n\n\n\n\nIt may not be obvious yet what vector addition is doing geometrically. Let me plot it slightly differently. What I’ll do is plot the vectors “head to tail” by taking the tail of \\(\\mathbf{w}\\) and placing it at the head of \\(\\mathbf{v}\\). Then the head of this translated \\(\\mathbf{w}\\) vector points at the head of the sum \\(\\mathbf{v}+\\mathbf{w}\\). We can do this “head to tail” stuff since the base of a vector is irrelevant. We can place the arrow wherever we want as long as we maintain its length and direction.\nInformally speaking, to add two vectors, just stack them on top of each other head to tail, and draw an arrow from the starting point to the ending point. You can geometrically add arbitrarily many vectors this way, not just two. Just keep stacking them.\n\n\nCode\nplot_vectors([v, w, v + w], xlim=(0, 3), ylim=(0, 5), title=f'Adding Two Vectors (Head to Tail)',\n             colors=['salmon', 'steelblue', 'black'],\n             tails=[[0, 0], [v[0], v[1]], [0, 0]], text_offsets=[[-0.5, -0.85], [0.5, -0.8], [-1.4, -1.6]],\n             labels=['$\\mathbf{v}$', '$\\mathbf{w}$', '$\\mathbf{v}+\\mathbf{w}$'],\n             zorders = [0, 1, 2], ticks_every=1)\n\n\n\n\n\n\n\n\n\nThe norm satisfies what’s known as the triangle inequality: If \\(\\mathbf{v}\\) and \\(\\mathbf{w}\\) are two vectors, then the length of their sum is less than the sum of their individual lengths, i.e.\n\\[||\\mathbf{v}+\\mathbf{w}|| \\leq ||\\mathbf{v}|| + ||\\mathbf{w}||.\\]\nYou can see this by staring at the plot above. The added lengths of \\(\\mathbf{v}\\) and \\(\\mathbf{w}\\) is larger than the length of their sum \\(\\mathbf{v}+\\mathbf{w}\\). In fact, the only time the lengths will be equal is if \\(\\mathbf{v}\\) and \\(\\mathbf{w}\\) are parallel to each other.\nWhat about subtracting two vectors? By combining the rules for scalar multiplication and vector addition, you can convince yourself that the difference of two vectors is also element-wise,\n\\[\\mathbf{v}-\\mathbf{w} = (v_x-w_x,v_y-w_y).\\]\nTo visualize what subtracting two vectors looks like, notice we can write subtraction as a sum like this, \\(\\mathbf{w} + (\\mathbf{v}-\\mathbf{w}) = \\mathbf{v}\\). Now use the same trick for adding vectors, only this time placing \\((\\mathbf{v}-\\mathbf{w})\\) at the head of \\(\\mathbf{w}\\), and noticing that it points to the sum of the two, which is \\(\\mathbf{v}\\).\nAn easy way to remember what subtracting two vectors looks like is to connect the two vectors you’re subtracting with a line segment, and place the head on the first vector. This trick will never fail you.\n\n\nCode\nv = np.array([1, 1])\nw = np.array([1, 3])\nplot_vectors([v, w, v - w], xlim=(-0.5, 1.5), ylim=(-0.5, 3.5), title=f'Subtracting Two Vectors', headwidth=4,\n             ticks_every=1, colors=['salmon', 'steelblue', 'black'],\n             tails=[[0, 0], [0, 0], [w[0], w[1]]], text_offsets=[[-0.5, -0.8], [-0.5, -1], [1.05, 3.8]],\n             labels=['$\\mathbf{v}$', '$\\mathbf{w}$', '$\\mathbf{v}-\\mathbf{w}$'])\n\n\n\n\n\n\n\n\n\n\n\n5.0.1.3 The Dot Product\nIt turns out we can understand both the lengths and angles of vectors in terms of a single operation called the dot product, also called the inner or scalar product. The dot product is a kind of multiplication between two vectors that returns a scalar. If \\(\\mathbf{v}=(v_x,v_y)\\) and \\(\\mathbf{w}=(w_x,w_y)\\) are two vectors in the plane, their dot product is defined as\n\\[\\mathbf{v} \\cdot \\mathbf{w} = v_x w_x + v_y w_y.\\]\nThat is, the dot product is just the sum of the element-wise products of the two vectors.\nIn terms of vectorized numpy code, the dot product is just the operation np.sum(v * w). Numpy also has a convenience function np.dot(v, w) that calculates it directly. Here’s the calculation of the dot product between the two vectors \\(\\mathbf{v}=(5,-1)\\) and \\(\\mathbf{w}=(2,4)\\). The answer should be\n\\[\\mathbf{v} \\cdot \\mathbf{w} = 5 \\cdot 2 + (-1) \\cdot 4 = 10 - 4 = 6.\\]\n\nv = np.array([5, -1])\nw = np.array([2, 4])\nprint(f'v . w = {np.dot(v, w)}')\nnp.sum(v * w) == np.dot(v, w)\n\nv . w = 6\n\n\nTrue\n\n\nAlgorithm Analysis: Evaluating the dot product uses \\(2n-1\\) or \\(O(n)\\) total FLOPS, since for a vector of size \\(n\\) there are \\(n\\) multiplications and \\(n-1\\) additions.\nHere are some fairly trivial properties the dot product satisfies. These follow straight from the definition.\n\nThe dot product of a vector with itself is nonnegative: \\(\\mathbf{v} \\cdot \\mathbf{v} \\geq 0\\).\nIt commutes: \\(\\mathbf{v} \\cdot \\mathbf{w} = \\mathbf{w} \\cdot \\mathbf{v}\\).\nIt distributes over scalar multiplication: \\(c\\mathbf{v} \\cdot \\mathbf{w} = \\mathbf{v} \\cdot c\\mathbf{w} = c(\\mathbf{v} \\cdot \\mathbf{w})\\).\nIt distributes over vector addition: \\((\\mathbf{u} + \\mathbf{v}) \\cdot \\mathbf{w} = \\mathbf{u} \\cdot \\mathbf{w} + \\mathbf{v} \\cdot \\mathbf{w}\\) and \\(\\mathbf{v} \\cdot (\\mathbf{u}+\\mathbf{w}) = \\mathbf{v} \\cdot \\mathbf{u} + \\mathbf{v} \\cdot \\mathbf{w}\\).\n\nNotation: The dot product is often written in several different ways in different fields. Another notation arises by thinking of the dot product as the matrix multiplication of a row vector \\(\\mathbf{v}^\\top = \\begin{pmatrix}v_x & v_y \\end{pmatrix}\\) with a column vector \\(\\mathbf{w} = \\begin{pmatrix} w_x \\\\ w_y \\end{pmatrix}\\). In that case,\n\\[\n\\mathbf{v}^\\top \\mathbf{w} =\n\\begin{pmatrix} v_x & v_y \\end{pmatrix}\n\\begin{pmatrix} w_x \\\\ w_y \\end{pmatrix}\n= v_x w_x + v_y w_y = \\mathbf{v} \\cdot \\mathbf{w}.\n\\]\nThis is the most commonly used notation for the dot product in machine learning. I’ll use it more frequently after this lesson.\nWe can write the norm or length of a vector in terms of the dot product. Observe that by dotting \\(\\mathbf{v}\\) with itself, I get\n\\[\\mathbf{v} \\cdot \\mathbf{v} = v_x^2 + v_y^2 = ||\\mathbf{v}||^2.\\]\nTaking the square root of both sides, you can see that the norm or length of a vector is just the square root of its dot product with itself,\n\\[||\\mathbf{v}|| = \\sqrt{\\mathbf{v} \\cdot \\mathbf{v}}.\\]\nWe can also talk about the distance between any two vectors \\(\\mathbf{v}\\) and \\(\\mathbf{w}\\). Denote the distance between these two vectors as \\(d(\\mathbf{v}, \\mathbf{w})\\). Since the difference vector is \\(\\mathbf{v} - \\mathbf{w}\\), the distance between the two vectors is evidently just the length of the difference vector,\n\\[d(\\mathbf{v}, \\mathbf{w}) = ||\\mathbf{v} - \\mathbf{w}|| = \\sqrt{(\\mathbf{v} - \\mathbf{w}) \\cdot (\\mathbf{v} - \\mathbf{w})} = \\sqrt{(v_x-w_x)^2 - (v_y-w_y)^2}.\\]\nFor example, the distance between the two vectors \\(\\mathbf{v}=(1,1)\\) and \\(\\mathbf{w}=(1, 0)\\) is\n\\[d(\\mathbf{v}, \\mathbf{w}) = ||\\mathbf{v} - \\mathbf{w}|| = \\sqrt{(1-1)^2 + (1-0)^2} = 1.\\]\nIf a vector \\(\\mathbf{e}\\) has norm \\(||\\mathbf{e}||=1\\) it’s called a unit vector. We can convert any non-zero vector \\(\\mathbf{v}\\) into a unit vector by dividing by its norm, which is called normalizing \\(\\mathbf{v}\\). The unit vector gotten from normalizing \\(\\mathbf{v}\\) I’ll call \\(\\mathbf{e_v}\\). It’s given by\n\\[\\mathbf{e_v} = \\frac{\\mathbf{v}}{||\\mathbf{v}||}.\\]\nFor example, if \\(\\mathbf{v}=(1,1)\\), its norm is \\(||\\mathbf{v}||=\\sqrt{2}\\), so if we wanted to normalize it into a new unit vector \\(\\mathbf{e_v}\\), we’d have\n\\[\\mathbf{e_v} = \\frac{\\mathbf{v}}{||\\mathbf{v}||} = \\frac{\\mathbf{v}}{\\sqrt{2}} = \\frac{1}{\\sqrt{2}}(1,1) \\approx (0.707, 0.707).\\]\nUnit vectors will always point in the same direction as the vector used to normalize them. The only difference is they’ll have length one. In the plane, unit vectors will always lie along the unit circle. Here’s a plot of this idea using the previous example.\n\n\nCode\nv = np.array([1, 2])\nev = v / np.sqrt(2)\nplot_vectors([v, ev], title='$\\mathbf{e}_v = ||\\mathbf{v}||^{-1} \\mathbf{v}$', ticks_every=0.5, zorders=[0, 1],\n             text_offsets=[[0.01, 0.05], [-0.2, 0.2]], colors=['steelblue', 'red'],\n             labels=['$\\mathbf{v}$', '$\\mathbf{e}_v$'], headwidth=6)\n\n\n\n\n\n\n\n\n\n\n\n5.0.1.4 Projections\nLet \\(\\mathbf{e}_x=(1,0)\\). It’s the unit vector pointing along the positive x-axis. Notice the dot product between \\(\\mathbf{v}=(v_x, v_y)\\) and \\(\\mathbf{e}_x\\) is just\n\\[\\mathbf{v} \\cdot \\mathbf{e}_x = v_x \\cdot 1 + v_y \\cdot 0 = v_x.\\]\nEvidently the dot product \\(\\mathbf{v} \\cdot \\mathbf{e}_x\\) “picks” out the x-component of \\(\\mathbf{v}\\), namely \\(v_x\\). The vector \\(v_x \\mathbf{e}_x = (v_x,0)\\) gotten by rescaling \\(\\mathbf{e}_x\\) by \\(v_x\\) is called the projection of \\(\\mathbf{v}\\) onto the x-axis. It’s the vector you’d get by dropping \\(\\mathbf{v}\\) perpendicular to the x-axis.\nSimilarly, if \\(\\mathbf{e}_y = (0,1)\\) is the unit vector along the positive y-axis, we can “pick out” the y-component of \\(\\mathbf{v}\\) by taking the dot product of \\(\\mathbf{v}\\) with \\(\\mathbf{e}_y\\), i.e. \\(v_y = \\mathbf{v} \\cdot \\mathbf{e}_y\\). The vector \\(v_y\\mathbf{e}_y\\) is the projection of \\(\\mathbf{v}\\) onto the y-axis.\nEvidently, then, \\(\\mathbf{v}\\) is just the sum of projections of \\(\\mathbf{v}\\) onto all of the axes,\n\\[\\mathbf{v} = v_x \\mathbf{e_x} + v_y \\mathbf{e_y}.\\]\nThis is yet another way to express a vector in terms of its components. Just project down onto the axes and sum up the linear combination.\nHere’s what this looks like when \\(\\mathbf{v}=(0.5,1)\\). In this example, the projection onto the x-axis is just \\(v_x \\mathbf{e}_x=(0.5, 0)\\), and the projection onto the y-axis is just \\(v_y \\mathbf{e_y}=(0,1)\\). Using these projections, we can write \\(\\mathbf{v}=(0.5,1)\\) as \\(\\mathbf{v} = 0.5 \\mathbf{e}_x + \\mathbf{e}_y\\).\n\n\nCode\nv = np.array([1, 2])\nex = np.array([1, 0])\ney = np.array([0, 1])\nplot_vectors([v, v[0] * ex, v[1] * ey], title='Projections Of $\\mathbf{v}$', ticks_every=0.5,\n             text_offsets=[[0.02, 0.1], [-0.1, 0.2], [0.05, 0.05]], colors=['red', 'steelblue', 'steelblue'],\n             labels=['$\\mathbf{v}$', '$v_x \\mathbf{e}_x$', '$v_y \\mathbf{e}_y$'], headwidth=4,\n             xlim=(-0.5, 2.5), ylim=(-0.5, 2.5))\n\n\n\n\n\n\n\n\n\n\n\n5.0.1.5 Linear Independence\nI just showed we can decompose any vector \\(\\mathbf{v} \\in \\mathbb{R}^2\\) into its projections \\(\\mathbf{v} = v_x \\mathbf{e}_x + v_y \\mathbf{e}_y\\). The fact we can do this is because the unit vectors \\(\\mathbf{e}_x\\) and \\(\\mathbf{e}_y\\) are special, for a few reasons.\nThe first reason these vectors are special is that they don’t lie along the same line in the plane. Said differently, we can’t write one vector as a scalar multiple of the other, \\(\\mathbf{e}_x \\neq c \\mathbf{e}_y\\) for any scalar \\(c\\). Vectors with this property are called linearly independent.\nMore generally, a set of \\(k\\) vectors \\(\\mathbf{v}_0, \\mathbf{v}_1, \\cdots, \\mathbf{v}_{k-1}\\) is called linearly independent if no one vector \\(\\mathbf{v}_j\\) in the set can be written as a linear combination of the rest, i.e. for any choice of scalars \\(c_i\\),\n\\[\\mathbf{v}_j \\neq \\sum_{i \\neq j} c_i \\mathbf{v}_i.\\]\nA set of vectors that isn’t linearly independent is called linearly dependent. In a linearly dependent set, you can always express at least one vector as a linear combination of the rest, for example by finding a choice of scalars \\(c_i\\), you could write \\(\\mathbf{v}_0\\) as\n\\[\\mathbf{v}_0 = \\sum_{i=1}^{k-1} c_i \\mathbf{v}_i = c_1 \\mathbf{v}_1 + c_2 \\mathbf{v}_2 + \\cdots + c_{k-1} \\mathbf{v}_{k-1}.\\]\nLinearly dependent sets of vectors are redundant in a sense. We have more than we need. We can always keep dropping vectors from the set until the ones remaining are linearly independent.\nThe vector space spanned by all linear combinations of a set of vectors is called the span of that set. The span of a single vector will always be a line, since a linear combination of any one vector is just the scalar multiples of that vector. The span of any two linearly independent vectors will always be a plane. The span of \\(k\\) linearly independent vectors will form a \\(k\\)-dimensional hyperplane.\nAs a simple example, consider the following set of vectors in the plane,\n\\[\\begin{align*}\n\\mathbf{v}_0 &= (1, 0), \\\\\n\\mathbf{v}_1 &= (0, 1), \\\\\n\\mathbf{v}_2 &= (1, 1).\n\\end{align*}\\]\nIf you stare at these for a second, you’ll see that \\(\\mathbf{v}_2 = \\mathbf{v}_0 + \\mathbf{v}_1\\), so this set can’t be linearly independent. The third vector is redundant. Any two vectors in this set span the exact same plane \\(\\mathbb{R}^2\\). In fact, you’ll never have more than 2 linearly independent vectors of size 2. Why?\n\n\nCode\nv0 = np.array([1, 0])\nv1 = np.array([0, 1])\nv2 = np.array([1, 1])\nplot_vectors(\n    [v2, v0, v1], colors=['salmon', 'steelblue', 'limegreen'], xlim=(-0.5, 1.5), ylim=(-0.5, 1.5),\n    ticks_every=0.5, zorders=[0, 1, 2, 3], headwidth=5, text_offsets=[[0.03, 0.05], [0.03,0.05], [0.03,0.05]],\n    title='$\\mathbf{v}_0$, $\\mathbf{v}_1$, $\\mathbf{v}_2=\\mathbf{v}_0+\\mathbf{v}_1$', \n    labels=['$\\mathbf{v}_2$', '$\\mathbf{v}_0$', '$\\mathbf{v}_1$'])\n\n\n\n\n\n\n\n\n\nFor vectors in \\(\\mathbb{R}^2\\), there are only two possibilities, they either lie on the same line, or they span the whole plane. This follows from the fact that any vector \\(\\mathbf{v}\\) can be decomposed as \\(\\mathbf{v} = v_x \\mathbf{e}_x + v_y \\mathbf{e}_y\\). An implication of this fact is that a set of vectors in \\(\\mathbb{R}^2\\) can only be linearly independent if it contains only one or two vectors. If it contains a third vector, that vector must be a linear combination of the other two. The maximum number of linearly independent vectors in a set is the dimension of the vector space. Since \\(\\mathbb{R}^2\\) is 2-dimensional, it can only sustain 2 linearly independent vectors at a time.\n\n\n5.0.1.6 Basis Vectors\nIn \\(\\mathbb{R}^2\\), if we can find any two vectors \\(\\mathbf{a}\\) and \\(\\mathbf{b}\\) that are linearly independent, then we can write any other vector \\(\\mathbf{v}\\) as a linear combination of those two vectors,\n\\[\\mathbf{v} = v_a \\mathbf{a} + v_b \\mathbf{b}.\\]\nThe set \\(\\{\\mathbf{a}, \\mathbf{b}\\}\\) is called a basis. We can use vectors in this set as a “basis” to write any other vector.\nMore generally, a set of \\(k\\) vectors \\(\\mathbf{v}_0, \\mathbf{v}_1, \\cdots, \\mathbf{v}_{k-1}\\) form a basis for a vector space if the following two conditions hold,\n\nThe vectors are all linearly independent,\nThe vectors span the full vector space.\n\nAnother way of saying the same thing is that a basis is a set of exactly \\(n\\) linearly independent vectors, where \\(n\\) is the dimension of the vector space. A basis contains the minimal number of vectors needed to span the vector space.\nThe special vectors \\(\\mathbf{e}_x\\) and \\(\\mathbf{e}_y\\) form a basis for \\(\\mathbb{R}^2\\), since we can write any other vector as a linear combination of those two. Not only are these two vectors a basis, however. They satisfy two other useful properties,\n\nThey’re both unit vectors, \\(||\\mathbf{e}_x|| = ||\\mathbf{e}_y|| = 1\\).\nThey’re orthogonal to each other, that is, \\(\\mathbf{e}_x \\cdot \\mathbf{e}_y = 0\\).\n\nA basis satisfying these two properties is called an orthonormal basis. An orthonormal basis is special in that it allows us to pick out the components of a vector directly by just taking dot products with the basis vectors. It’s only true in an orthonormal basis that we can write the components of a vector \\(\\mathbf{v}\\) as,\n\\[\\begin{align*}\nv_x &= \\mathbf{v} \\cdot \\mathbf{e}_x, \\\\\nv_y &= \\mathbf{v} \\cdot \\mathbf{e}_y.\n\\end{align*}\\]\nThe set \\(\\{\\mathbf{e}_x, \\mathbf{e}_y\\}\\) is only one example of an orthonormal basis for \\(\\mathbb{R}^2\\). It’s called the standard basis, since it’s the basis whose vectors point along the usual positive x and y axes.\nExpressing any vector in terms of its basis is just projecting the vector down onto each of the basis axes. Let’s do a quick example. Let \\(\\mathbf{v}=(1.25,2)\\) be a vector. Decomposed into the standard basis we just get\n\\[\\mathbf{v} = 1.25 \\mathbf{e}_x + 2 \\mathbf{e}_y.\\]\nGraphically this just looks as follows. We’ve already seen a plot like this, except this time I’m including the basis vectors \\(\\mathbf{e}_x\\) and \\(\\mathbf{e}_y\\) explicitly. Notice that the two basis vectors form a \\(90^\\circ\\) angle, i.e. they’re perpendicular. I’ll show in a moment that this is implied by the fact that \\(\\mathbf{e}_x \\cdot \\mathbf{e}_y = 0\\).\n\n\nCode\nv = np.array([1.25, 2])\nex = np.array([1, 0])\ney = np.array([0, 1])\nplot_vectors(\n    [v, v[0] * ex, v[1] * ey, ex, ey], colors=['red', 'steelblue', 'steelblue', 'black', 'black'], \n    ticks_every=1, zorders=[0, 1, 2, 3, 4, 5], headwidth=5,\n    text_offsets=[[0,0], [0,0.2], [0.05,0], [-0.2,0.2], [0.05,0]],\n    title='$\\mathbf{v}=v_x \\mathbf{e}_x + v_y \\mathbf{e}_y$', \n    labels=['$\\mathbf{v}$', '$v_x \\mathbf{e}_x$', '$v_y \\mathbf{e}_y$', '$\\mathbf{e}_x$', '$\\mathbf{e}_y$'])\n\n\n\n\n\n\n\n\n\nOf course, I already said the standard basis isn’t the only orthonormal basis for \\(\\mathbb{R}^2\\) we could choose. Here’s an example of another one that would work equally well. Let \\(\\mathbf{e}_a=\\frac{1}{\\sqrt{2}} (1,1)\\) and \\(\\mathbf{e}_b=\\frac{1}{\\sqrt{2}} (-1,1)\\). Notice that both vectors have unit length, and they’re orthogonal since \\(\\mathbf{e}_a \\cdot \\mathbf{e}_b = 0\\). Thus, they form an orthonormal basis for \\(\\mathbb{R}^2\\). In this basis, \\(\\mathbf{v}=(1.25, 2)\\) would be written\n\\[\\mathbf{v} = (\\mathbf{v} \\cdot \\mathbf{e}_a) \\mathbf{e}_a + (\\mathbf{v} \\cdot \\mathbf{e}_b) \\mathbf{e}_b \\approx 2.298 \\mathbf{e}_a + 0.530 \\mathbf{e}_b.\\]\nThis is a very different representation for \\(\\mathbf{v}\\). Nevertheless, the two basis vectors are still perpendicular to each other. You can see a plot of this below.\nThere are infinitely many orthonormal bases for \\(\\mathbb{R}^2\\). Just take any two perpendicular vectors in the plane and normalize them to unit length and they’ll form a valid orthonormal basis.\n\n\nCode\nv = np.array([1.25, 2])\nea = np.array([1, 1]) / np.sqrt(2)\neb = np.array([-1, 1]) / np.sqrt(2)\nvectors = [v, np.dot(v, ea) * ea, np.dot(v, eb) * eb, ea, eb]\nplot_vectors(\n    vectors, ticks_every=1, zorders=[0, 1, 5, 3, 4, 2], headwidth=7,\n    colors=['red', 'steelblue', 'steelblue', 'black', 'black'],\n    text_offsets=[[0, 0], [0.03, 0], [-0.3, -0.65], [-0.1, -0.48], [-0.2, 0.15]],\n    title='$\\mathbf{v}=v_a \\mathbf{e}_a + v_b \\mathbf{e}_b$', \n    labels=['$\\mathbf{v}$', '$v_a \\mathbf{e}_a$', '$v_b \\mathbf{e}_b$', '$\\mathbf{e}_a$', '$\\mathbf{e}_b$'])\n\n\n\n\n\n\n\n\n\n\n\n5.0.1.7 Cosine Similarity\nJust like we can express the length of a vector using the dot product, it turns out we can also express the angle between any two vectors in the plane using the dot product. If \\(\\theta\\) is the angle between two vectors \\(\\mathbf{v}\\) and \\(\\mathbf{w}\\), it turns out the dot product is given by\n\\[\\mathbf{v} \\cdot \\mathbf{w} = ||\\mathbf{v}|| \\cdot ||\\mathbf{w}|| \\cos \\theta.\\]\nNote that both sides of this equation are scalars since the dot product is a scalar and the product of norms is a scalar. If you’re good at trigonometry, you can convince yourself this formula must be true by projecting \\(\\mathbf{v}\\) onto \\(\\mathbf{w}\\) similar to the way we did projections onto the x and y axes before. The difference this time is that the component of \\(\\mathbf{v}\\) in the direction of \\(\\mathbf{w}\\) is not \\(v_x\\) or \\(v_y\\) anymore, but instead \\(||\\mathbf{v}|| \\cos \\theta\\).\nYou can see two special cases of this formula by looking at what happens when the two vectors are parallel or perpendicular. If the two vectors are parallel, then \\(\\theta = 0^\\circ, 180^\\circ\\), so \\(\\cos \\theta = \\pm 1\\), so \\(\\mathbf{v} \\cdot \\mathbf{w} = \\pm ||\\mathbf{v}|| \\cdot ||\\mathbf{w}||\\). More importantly, if the two vectors are perpendicular, then \\(\\theta = 90^\\circ, 270^\\circ\\), so \\(\\cos \\theta = 0\\), so \\(\\mathbf{v} \\cdot \\mathbf{w} = 0\\). That is, perpendicular vectors are orthogonal. They mean the same thing.\nIt’s more common to express this formula with \\(\\cos \\theta\\) on one side and the vector terms on the other so you can solve for the angle (or more commonly just the cosine of the angle). In this case, we have \\[\\cos \\theta = \\frac{\\mathbf{v} \\cdot \\mathbf{w}}{||\\mathbf{v}|| \\cdot ||\\mathbf{w}||}.\\]\nWhat matters more than anything is what this formula says and how to use it. Suppose, for example, you want to find the angle between the two vectors \\(\\mathbf{v} = (1,1)\\) and \\(\\mathbf{w} = (0, -1)\\). Then you’d have\n\\[\\begin{align*}\n\\mathbf{v} \\cdot \\mathbf{w} &= 1 \\cdot 0 + 1 \\cdot (-1) = -1, \\\\\n||\\mathbf{v}|| &= \\sqrt{1^2 + 1^2} = \\sqrt{2}, \\\\\n||\\mathbf{w}|| &= \\sqrt{0^2 + (-1)^2} = 1.\n\\end{align*}\\]\nPlugging this into the cosine formula gives,\n\\[\n\\cos \\theta = \\frac{-1}{\\sqrt{2}} \\quad \\Longrightarrow \\quad \\theta = \\cos^{-1}\\bigg(\\frac{-1}{\\sqrt{2}}\\bigg) = 135^\\circ.\n\\]\nYou can verify this is correct by plotting the two vectors and confirming that they’re about \\(135^\\circ\\) from each other, which corresponds to about 1.25 quarter turns around a circle. It’s interesting to note that the dot product will only be negative when the angle between the two vectors is obtuse, i.e. more than \\(90^\\circ\\), which is of course the case here.\n\n\nCode\nv = np.array([1, 1])\nw = np.array([0, -1])\nplot_vectors([v, w], title='$\\mathbf{v} \\cdot \\mathbf{w} = ||\\mathbf{v}||||\\mathbf{w}|| \\cos \\\\theta$', \n             text_offsets=[[0, 0], [0.1, 0]], ticks_every=0.5, xlim=(-1, 2), ylim=(-1.5, 1.5),\n             labels=['$\\mathbf{v}$', '$\\mathbf{w}$'], colors=['red', 'steelblue'], headwidth=7)\n\n\n\n\n\n\n\n\n\nIn machine learning, this formula for \\(\\cos \\theta\\) is called the cosine similarity. The reason for this is that the dot product itself is a measure of how similar two vectors are. To see why, consider two special cases:\n\nThe two vectors are parallel: This is as large as the dot product between two vectors can get in absolute value. The vectors are as similar as they can be in a sense. Up to a scalar multiple, they contain the same information.\nThe two vectors are perpendicular: This is as small as the dot product between two vectors can get in absolute value. The two vectors are as different as they can be in a sense. They share pretty much no information. Information about one vector tells you basically nothing about the other.\n\nThe cosine similarity is a function of two input vectors \\(\\mathbf{v}\\) and \\(\\mathbf{w}\\). Since we don’t actually care about the angle \\(\\theta\\) usually, we’ll more often denote the cosine similarity using a notation like \\(\\cos(\\mathbf{v},\\mathbf{w})\\) to make it clear it’s a function of its two input vectors,\n\\[\\cos(\\mathbf{v},\\mathbf{w}) = \\frac{\\mathbf{v} \\cdot \\mathbf{w}}{||\\mathbf{v}|| \\cdot ||\\mathbf{w}||}.\\]\nNote the cosine similarity is just a normalized dot product, since dividing by the norms forces \\(-1 \\leq \\cos(\\mathbf{v},\\mathbf{w}) \\leq 1\\). It thus captures the same idea of similarity that the dot product does, but it’s more useful when the lengths of vectors get out of control. This is particularly likely to happen in high dimensions, when \\(n >> 2\\). This is the so-called “curse of dimensionality”. We’ll come back to this idea in future lessons.\nHere’s a quick implementation of the cosine similarity function using numpy. There’s no built-in function to do it, but it’s easy enough to implement by making judicious use of the np.dot function. It should give the same answer found above for \\(\\cos \\theta\\), which is \\(-\\frac{1}{\\sqrt{2}} \\approx -0.707\\).\n\ndef cosine_similarity(v, w):\n    return np.dot(v, w) / np.sqrt(np.dot(v, v) * np.dot(w, w))\n\nprint(f'cos(v, w) = {cosine_similarity(v, w)}')\n\ncos(v, w) = -0.7071067811865475\n\n\nAlgorithm Analysis: Like the dot product, this function uses only \\(O(n)\\) FLOPS. There are three independent dot product operations happening here, each adding \\(O(n)\\) FLOPS. Since the outputs of dot products are scalars, the multiply and divide only add one FLOP each. The square root isn’t obvious, but you can assume it takes some constant number of FLOPS as well. The total must therefore be \\(O(n)\\).\n\n\n5.0.1.8 Other Norms\nIt turns out that the norm I defined above is only one way to measure the length of a vector. It’s the most natural way to do so sense it corresponds to your intuitive notions of length, which itself relates to the Pythagorean Theorem. There are other ways to quantify vector length as well that aren’t as intuitive. Because they do sometimes show up in machine learning I’ll briefly mention a couple of these here.\nThe norm I’ve covered is called the 2-norm. It’s called this because it involves squares and square roots. We can write it in the form\n\\[||\\mathbf{v}|| = ||\\mathbf{v}||_2 = \\big(v_x^2 + v_y^2 \\big)^{1/2}.\\]\nIt turns out we can replace the twos with any other positive number \\(p>1\\) to get generalized norms, called p-norms,\n\\[||\\mathbf{v}||_p = \\big(v_x^p + v_y^p \\big)^{1/p}.\\]\nThe p-norms cover a large class of norms, since any \\(1 \\leq p \\leq \\infty\\) can define a valid norm. The 2-norm, as you’d guess, occurs when \\(p=2\\). A couple of other norms that show up in machine learning are the 1-norm when \\(p=1\\), and the infinity norm when \\(p=\\infty\\). For 2-dimensional vectors, these norms are\n\\[\\begin{align*}\n||\\mathbf{v}||_1 &= |v_x| + |v_y|, \\\\\n||\\mathbf{v}||_\\infty &= \\max\\big(|v_x|, |v_y|\\big).\n\\end{align*}\\]\nHere’s an example. I’ll calculate the \\(p=1, 2, \\infty\\) norms for the vector \\(\\mathbf{v}=(1,-2)\\). We have,\n\\[\\begin{align*}\n||\\mathbf{v}||_1 &= |1| + |-2| = 1 + 2 = 3, \\\\\n||\\mathbf{v}||_2 &= \\sqrt{1^2 + (-2)^2} = \\sqrt{1 + 4} = \\sqrt{5} \\approx 2.236, \\\\\n||\\mathbf{v}||_\\infty &= \\max\\big(|1|, |-2|\\big) = \\max(1, 2) = 2.\n\\end{align*}\\]\nNotice that \\(||\\mathbf{v}||_1 \\geq ||\\mathbf{v}||_2 \\geq ||\\mathbf{v}||_\\infty\\). This is a general fact.\nIt’s a little hard right now to describe why these norms are useful in machine learning since we don’t currently have the context. Just know that these norms do come up sometimes. I’ll go into more depth on the uses of these different norms as we apply them. In practice though, we’ll probably work with the regular 2-norm maybe 90% of the time.\nIn numpy, you can calculate any \\(p\\)-norm using the function np.linalg.norm(v, ord=p). Here’s an example.\n\nv = np.array([1, -2])\nprint(f'1-Norm of v: {np.linalg.norm(v, ord=1)}')\nprint(f'2-Norm of v: {np.linalg.norm(v, ord=2)}')\nprint(f'Infinity-Norm of v: {np.linalg.norm(v, ord=np.inf)}')\n\n1-Norm of v: 3.0\n2-Norm of v: 2.23606797749979\nInfinity-Norm of v: 2.0\n\n\n\n\n\n5.0.2 Linear Maps\nSo where do matrices fit into all this vector space stuff? It turns out that matrices correspond to functions between vectors to vectors. These are called linear maps. A linear map is a vector-valued function from one vector space to another that preserves the properties of vectors. In \\(\\mathbb{R}^2\\), a linear map is a function between vectors \\(\\mathbf{v}=(v_x,v_y)\\) and \\(\\mathbf{w}=(w_x,w_y)\\) of the form\n\\[\\mathbf{w} = (w_x, w_y) = (av_x + bv_y, cv_x + dv_y) = \\mathbf{F}(\\mathbf{v}).\\]\nThat is, each component of the output vector \\(\\mathbf{w}\\) is a linear combination of the input vector \\(\\mathbf{v}\\). Now, if you stare at this function for a little bit, you should see that this kind of looks like a \\(2 \\times 2\\) system of linear equations,\n\\[\\begin{alignat*}{3}\n   av_x & {}+{} &  bv_y & {}={} & w_x \\\\\n   cv_x & {}+{} &  dv_y & {}={} & w_y.\n\\end{alignat*}\\]\nThis of course means the linear map is equivalent to a matrix-vector equation. If we identify \\(\\mathbf{v}\\) and \\(\\mathbf{w}\\) with \\(2 \\times 1\\) column vectors, and define a \\(\\mathbf{A}\\) by\n\\[\n\\mathbf{A} =\n\\begin{pmatrix}\na & b \\\\\nc & d\n\\end{pmatrix},\n\\]\nthen the linear map \\(\\mathbf{w} = \\mathbf{F}(\\mathbf{v})\\) is equivalent to the matrix-vector equation \\(\\mathbf{w}=\\mathbf{A}\\mathbf{v}\\), or\n\\[\\mathbf{F}(\\mathbf{v}) = \\mathbf{A}\\mathbf{v}.\\]\nIn fact, every linear map \\(\\mathbf{F}(\\mathbf{v})\\) can be identified with some matrix equation \\(\\mathbf{A}\\mathbf{v}\\). Knowing \\(\\mathbf{A}\\) (in some basis) is equivalent to knowing the linear map itself.\nBut why are linear maps important? The main reason is that they preserve linear structure. Notice that I can define a line through any vector \\(\\mathbf{v}\\) by scaling it with some parameter \\(t\\). If I apply a linear map to this line I’d get \\(\\mathbf{F}(t\\mathbf{v}) = t\\mathbf{F}(\\mathbf{v})\\). Check it yourself from the definition. Said differently, linear maps map lines to lines, thus preserving the linear structure of the vector space. The new line won’t usually be the original line. It may get rotated. But it’s still a line.\nLet’s try to visualize what a linear map does by defining a particular \\(2 \\times 2\\) matrix \\(\\mathbf{A}\\) and seeing how it acts on inputs \\(\\mathbf{v}\\).\n\n\nCode\nA = np.array([[1, -0.1], [0.1, 1]])\nv = np.array([1, 1]).reshape(-1, 1)\nplot_vectors([v.flatten(), (A @ v).flatten()], colors=['black', 'red'],\n             labels=['$\\mathbf{{v}}$', '$\\mathbf{{A}}\\mathbf{{v}}$'], text_offsets=[[0.01, -0.1], [-0.1, 0.05]],\n             title='Linear Map: $\\mathbf{{w}} = \\mathbf{{A}}\\mathbf{{v}}$',  xlim=(0, 1.5), ylim=(0, 1.5))\n\n\n\n\n\n\n\n\n\nWhy stop there? Let’s apply the linear map a whole bunch of times recursively and see what happens to the output vectors \\(\\mathbf{w}\\). I’ll plot each of the \\(k=63\\) vectors in the following sequence, \\[\\mathbf{v}, \\ \\mathbf{A}\\mathbf{v}, \\ \\mathbf{A}^2\\mathbf{v}, \\ \\cdots, \\ \\mathbf{A}^{63}\\mathbf{v}.\\]\nNote each power \\(\\mathbf{A}^j\\) here is a matrix power, defined by applying matrix multiplication over and over \\(k\\) times. It’s not the element-wise power.\nFor the particular values in this plot I’ll choose \\(\\mathbf{v}=\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\\) and \\(\\mathbf{A}=\\begin{pmatrix} 1 & -0.1 \\\\ 0.1 & 1 \\end{pmatrix}\\). The original vector \\(\\mathbf{v}\\) is colored black. Notice that each linear map is slowly rotating the vector counterclockwise and also slightly stretching it. By the time it gets back around it’s already maybe 40% longer than the original vector.\nIn fact, every linear map between two vectors in the plane will do at least one of these two things: rotate the input vector in the plane, or stretch (or shrink) it by some factor. I just chose a particularly nice one to plot.\n\n\nCode\nk = 63\nvectors = [(np.linalg.matrix_power(A, i) @ v).flatten() for i in range(k)]\ntitle = f\"\"\"\n$\\mathbf{{v}}, \\mathbf{{A}}\\mathbf{{v}}, \\mathbf{{A}}^2\\mathbf{{v}}, \\cdots, \\mathbf{{A}}^{{{k}}}\\mathbf{{v}}$\n\"\"\".strip()\nplot_vectors(vectors, colors=['black']+['red']*(k-1), title=title, xlim=(-2.5, 2.5), ylim=(-2.5, 2.5))\n\n\n\n\n\n\n\n\n\nWhat do you suppose the transpose of \\(\\mathbf{A}\\) does in this particular example? That is, suppose you use \\(\\mathbf{A}^\\top\\) instead. Notice what would happen is the minus sign would move from the upper right to the lower left. You can verify that this will just cause the matrix to spin vectors the other way, clockwise instead of counterclockwise.\nIn fact, what I’ve just shown is a special case of what happens when you compose linear maps: Composition of linear maps is equivalent to matrix multiplication. If \\(\\mathbf{F}(\\mathbf{w}) = \\mathbf{A}\\mathbf{w}\\) and \\(\\mathbf{G}(\\mathbf{v}) = \\mathbf{B}\\mathbf{v}\\) are two linear maps, then their composite function \\(\\mathbf{F}(\\mathbf{G}(\\mathbf{v}))\\) is another linear map given by\n\\[\\mathbf{F}(\\mathbf{G}(\\mathbf{v})) = \\mathbf{A}\\mathbf{B}\\mathbf{v}.\\]\nThis is perhaps the real reason matrix multiplication is important. Because linear maps are important, and applying multiple linear maps in sequence is just matrix multiplication.\nTwo other special linear maps worth being aware of are the identity map and the inverse map. The identity map is the map \\(\\mathbf{F}(\\mathbf{v}) = \\mathbf{I}\\mathbf{v}\\). What does it do to \\(\\mathbf{v}\\)? Let’s see. We can get the identity matrix in numpy using np.eye(n), where n is the dimension (in this case 2).\nIt looks like nothing is happening. That is, \\(\\mathbf{I}\\mathbf{v} = \\mathbf{v}\\). You can verify this by writing this out in components and seeing what the matrix-vector product is. In fact, \\(\\mathbf{I}\\mathbf{v} = \\mathbf{v}\\) is always true, for any dimension, and any vector \\(\\mathbf{v}\\).\nAside: Notice that I had to flatten the vectors here to do the plot. That’s because I’ve sneakily defined vectors in two different ways, first as a column vector of shape \\((2,1)\\) and then as a flat vector of shape \\((2,)\\).\n\n\nCode\nI = np.eye(2)\nplot_vectors([v.flatten(), (I @ v).flatten()], zorders=[0, 1],\n             title='Identity Map: $\\mathbf{F}(\\mathbf{v})=\\mathbf{I}\\mathbf{v}$',\n             labels=['$\\mathbf{v}$', '$\\mathbf{I}\\mathbf{v}$'],\n             colors=['black', 'red'], xlim=(-0.5, 1.5), ylim=(-0.5, 1.5), \n             text_offsets=[[0, -0.25], [-0.2, 0.1]])\n\n\n\n\n\n\n\n\n\nThe inverse map is just the linear map that undoes the original linear map \\(\\mathbf{F}(\\mathbf{v}) = \\mathbf{A}\\mathbf{v}\\), i.e.\n\\[\\mathbf{F}^{-1}(\\mathbf{v}) = \\mathbf{A}^{-1}\\mathbf{v}.\\]\nYou can see what this does by applying the two maps in succession. Here’s an example of doing this with the vector \\(\\mathbf{v}=(1,1)\\) and the \\(90^\\circ\\) rotation matrix\n\\[\\mathbf{A}=\\begin{pmatrix} 0 & 1 \\\\ -1 & 0 \\end{pmatrix}.\\]\nApplying \\(\\mathbf{F}(\\mathbf{v})\\) followed by \\(\\mathbf{F}^{-1}(\\mathbf{v})\\) just gives the same vector \\(\\mathbf{v}\\) back. This just follows from the fact that \\(\\mathbf{A}^{-1}\\mathbf{A}=\\mathbf{I}\\), so the composition \\(\\mathbf{F}^{-1}(\\mathbf{F}(\\mathbf{v}))=\\mathbf{v}\\).\n\n\nCode\nA = np.array([[0, 1], [-1, 0]])\nplot_vectors([v.flatten(), (A @ v).flatten(), (np.linalg.inv(A) @ A @ v).flatten()], zorders=[2, 1, 0],\n             title='Inverse Map: $\\mathbf{F}^{-1}(\\mathbf{v})=\\mathbf{A}^{-1}\\mathbf{v}$',\n             labels=['$\\mathbf{v}$', '$\\mathbf{A}\\mathbf{v}$', '$\\mathbf{A}^{-1}\\mathbf{A}\\mathbf{v}$'],\n             colors=['black', 'red', 'blue'], xlim=(-0.5, 2), ylim=(-1.5, 1.5), \n             text_offsets=[[0, -0.3], [0.05, -0.05], [0, 0.1]])\n\n\n\n\n\n\n\n\n\nI’ll close this section by mentioning that we’re often not interested in linear maps in practice, but affine maps. Recall that a simple linear function might have the form \\(y=ax\\). An affine function has the form \\(y=ax+b\\). That is, an affine function is just a linear function shifted upward by \\(b\\). The same idea extends to maps between vectors.\nAn affine map is just a linear map shifted by some constant translation vector \\(\\mathbf{b}\\),\n\\[\\mathbf{F}(\\mathbf{v}) = \\mathbf{A}\\mathbf{v} + \\mathbf{b}.\\]\nThe only difference between an affine map and a linear map is that vectors will get not just scaled and rotated, but also translated by \\(\\mathbf{b}\\). In machine learning, the translation vector \\(\\mathbf{b}\\) often called a bias vector.\nHere’s a plot of what this looks like using the same previous matrix, and a bias vector \\(\\mathbf{b}=(-1, -1)\\). I want to show that \\(\\mathbf{A}\\mathbf{v} + \\mathbf{b}\\) is really just the vector \\(\\mathbf{A}\\mathbf{v}\\), but translated so its tail lies at \\(\\mathbf{b}\\). To do so, I’ll plot the vector \\(\\mathbf{A}\\mathbf{v} + \\mathbf{b}\\) with its tail at the origin, as well as the vector \\(\\mathbf{A}\\mathbf{v}\\) with its tail shifted to the point \\(\\mathbf{b}\\). What matters is that the head of both of these vectors is the same. The main vector \\(\\mathbf{A}\\mathbf{v} + \\mathbf{b}\\) is shown in red.\n\n\nCode\nA = np.array([[0, 1], [-1, 0]])\nv = np.array([1, 1]).reshape(-1, 1)\nb = np.array([-1, -1]).reshape(-1, 1)\nvectors = [x.flatten() for x in [v, A @ v, A @ v + b, b]]\nplot_vectors(\n    vectors, xlim=(-1.5, 1.5), ylim=(-2.5, 1.5), headwidth=5, colors=['black', 'blue', 'red', 'green'],\n    labels=['$\\mathbf{{v}}$', '$\\mathbf{{A}}\\mathbf{{v}}$', \n            '$\\mathbf{{A}}\\mathbf{{v}} + \\mathbf{{b}}$', '$\\mathbf{{b}}$'], \n    text_offsets=[[0, 0], [-1.4, -1.25], [0.07, 0], [-0.2, 0.1]], \n    tails=[[0, 0], [b[0][0], b[1][0]], [0, 0], [0, 0]],\n    title='Affine Map: $\\mathbf{F}(\\mathbf{v}) = \\mathbf{A}\\mathbf{v} + \\mathbf{b}$')\n\n\n\n\n\n\n\n\n\n\n\n5.0.3 \\(n\\)-dimensional Vector Spaces\nIt may seem like everything I’ve said is special for the case of \\(n=2\\) dimensions, but it’s really not. Every single thing I’ve said extends exactly how you’d expect to vectors of arbitrary size \\(n\\). The only difference now is that you can’t visualize the stuff anymore. You just have to trust the math. I’ll restate all of the definitions from above here, but for \\(n\\)-dimensional vector spaces instead.\nA vector of size \\(n\\) can be defined as a 1-dimensional array of real numbers \\(x_0,x_1,x_2,\\cdots,x_{n-1}\\),\n\\[\\mathbf{x} = (x_0,x_1,x_2,\\cdots,x_{n-1}).\\]\nVectors can be added together, and multiplied by scalars. Vector addition is defined element-wise. If \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are two vectors, then\n\\[\\mathbf{x} + \\mathbf{y} = (x_0+y_0, x_1+y_1, \\cdots, x_{n-1}+y_{n-1}).\\]\nTo keep a running example through this section, I’ll use numpy to create two vectors \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) each of size \\(n=10\\). Here’s their vector sum.\n\nx = np.array([1, 2, 3, 4, 5, 5, 4, 3, 2, 1])\ny = np.array([1, 0, -1, 0, 1, 0, -1, 0, 1, 0])\n\nprint(f'x + y = {x + y}')\n\nx + y = [2 2 2 4 6 5 3 3 3 1]\n\n\nScalar multiplication is defined similarly. If \\(c \\in \\mathbb{R}\\) is some scalar and \\(\\mathbf{x}\\) is some vector, then\n\\[c\\mathbf{x} = (cx_0,cx_1,\\cdots,cx_{n-1}).\\]\n\nc = 5\nprint(f'c * x = {c * x}')\n\nc * x = [ 5 10 15 20 25 25 20 15 10  5]\n\n\nVectors of size \\(n\\) live in the \\(n\\)-dimensional vector space \\(\\mathbb{R}^n\\). By definition, any linear combination of two vectors must also live in the same vector space. That is, if \\(\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^n\\) are two vectors and \\(a,b \\in \\mathbb{R}\\) are two scalars, then \\(a \\mathbf{x} + b \\mathbf{y} \\in \\mathbb{R}^n\\).\nThe dot product or inner product between two vectors \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) of size \\(n\\) is defined as their sum product, i.e.\n\\[\\mathbf{x} \\cdot \\mathbf{y} = x_0y_0 + x_1y_1 + \\cdots + x_{n-1}y_{n-1}.\\]\n\nprint(f'x . y = {np.dot(x, y)}')\n\nx . y = 1\n\n\nThe norm (technically the 2-norm) of a vector is defined as the square root of its dot product with itself, i.e.\n\\[||\\mathbf{x}|| = ||\\mathbf{x}||_2 = \\sqrt{\\mathbf{x} \\cdot \\mathbf{x}} = \\sqrt{x_0^2 + x_1^2 + \\cdots + x_{n-1}^2}.\\]\nThis is just the \\(n\\)-dimensional generalization of the Pythagorean Theorem. We can also consider other \\(p\\) norms as well. In particular, the cases when \\(p=1\\) and \\(p=\\infty\\) sometimes show up in applications,\n\\[\\begin{align*}\n||\\mathbf{x}||_1 &= \\sum_{i=0}^{n-1} |x_i| = |x_0| + |x_1| + \\cdots + |x_{n-1}|, \\\\\n||\\mathbf{x}||_\\infty &= \\max_{i=0,\\cdots,n-1} |x_i| = \\max\\big(|x_0|, |x_1|, \\cdots, |x_{n-1}|\\big).\n\\end{align*}\\]\nIt will always be the case that \\(||\\mathbf{x}||_1 \\geq ||\\mathbf{x}||_2 \\geq ||\\mathbf{x}||_\\infty\\).\n\nprint(f'1-Norm of x: {np.linalg.norm(x, ord=1)}')\nprint(f'2-Norm of x: {np.linalg.norm(x, ord=2)}')\nprint(f'Infinity-Norm of x: {np.linalg.norm(x, ord=np.inf)}')\n\n1-Norm of x: 30.0\n2-Norm of x: 10.488088481701515\nInfinity-Norm of x: 5.0\n\n\nThe distance \\(d(\\mathbf{x}, \\mathbf{y})\\) between two vectors \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) is just the norm of their difference vector,\n\\[d(\\mathbf{x}, \\mathbf{y}) = ||\\mathbf{x}-\\mathbf{y}|| = \\sum_{i=0}^{n-1} \\sqrt{(x_i-y_i)^2} = \\sqrt{(x_0-y_0)^2 + (x_1-y_1)^2 + \\cdots + (x_{n-1}-y_{n-1})^2}.\\]\nWe can define the angle between any two vectors \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) of size \\(n\\) by making use of the same identity for the dot product, which still holds in \\(n\\) dimensions,\n\\[\\mathbf{x} \\cdot \\mathbf{y} = ||\\mathbf{x}|| \\cdot ||\\mathbf{y}|| \\cos \\theta.\\]\nUsing this identity, we can define the cosine similarity \\(\\cos(\\mathbf{x}, \\mathbf{y})\\) by solving for \\(\\cos \\theta\\),\n\\[\\cos(\\mathbf{x}, \\mathbf{y}) = \\frac{\\mathbf{x} \\cdot \\mathbf{y}}{||\\mathbf{x}|| \\cdot ||\\mathbf{y}||}.\\]\nThe dot product is a measure of how similar two vectors are, and the cosine similarity is a normalized measure of how similar two vectors are, since dividing by the norms forces \\(-1 \\leq \\cos \\theta \\leq 1\\).\n\nprint(f'cos(x, y) = {cosine_similarity(x, y)}')\n\ncos(x, y) = 0.04264014327112208\n\n\nA set of vectors \\(\\mathbf{x}_0, \\mathbf{x}_1, \\cdots, \\mathbf{x}_{k-1}\\) is linearly independent if no one vector is a linear combination of the rest,\n\\[\\mathbf{x}_j \\neq \\sum_{i \\neq j} c_i \\mathbf{x}_j.\\]\nIf one vector is a linear combination of the rest, they’re linearly dependent. If there are exactly \\(n\\) linear independent vectors in the set, it’s called a basis.\nWe can define the standard basis on \\(\\mathbb{R}^n\\) with the following complete set of size \\(n\\) unit vectors,\n\\[\\begin{align*}\n\\mathbf{e}_0 &= (1, 0, 0, \\cdots, 0), \\\\\n\\mathbf{e}_1 &= (0, 1, 0, \\cdots, 0), \\\\\n\\vdots \\ &= \\qquad \\vdots \\\\\n\\mathbf{e}_{n-1} &= (0, 0, 0, \\cdots, 1).\n\\end{align*}\\]\nThe standard basis is an orthonormal basis since each vector is a unit vector and they’re all mutually orthogonal, i.e.\n\\[\n\\mathbf{e}_i \\cdot \\mathbf{e}_j = \\delta_{ij} =\n\\begin{cases}\n1 & i = j, \\\\\n0 & i \\neq j.\n\\end{cases}\n\\]\nNotation: The symbol \\(\\delta_{ij}\\) is called the Kronecker delta. It’s just a shorthand way of writing something is \\(1\\) if \\(i=j\\) and \\(0\\) if \\(i \\neq j\\).\n\nn = 10\ne = [ei.flatten().astype(int) for ei in np.eye(n)]\nprint(f'e3 = {e[3]}')\nprint(f'e8 = {e[8]}')\nprint(f'e3 . e3 = {np.dot(e[3], e[3])}')\nprint(f'e3 . e8 = {np.dot(e[3], e[8])}')\n\ne3 = [0 0 0 1 0 0 0 0 0 0]\ne8 = [0 0 0 0 0 0 0 0 1 0]\ne3 . e3 = 1\ne3 . e8 = 0\n\n\nIf a basis is orthonormal, any vector \\(\\mathbf{x}\\) can be decomposed into a linear combination of the basis elements by taking the dot product \\(\\mathbf{x} \\cdot \\mathbf{e}_i\\). For the standard basis, these just give the vector components \\(x_i\\),\n\\[\\mathbf{x} = \\sum_{i=0}^{n-1} (\\mathbf{x} \\cdot \\mathbf{e}_i) \\mathbf{e}_i = \\sum_{i=0}^{n-1} x_i \\mathbf{e}_i = x_0 \\mathbf{e}_0 + x_1 \\mathbf{e}_1 + \\cdots x_{n-1} \\mathbf{e}_{n-1}.\\]\nEach term \\(x_i \\mathbf{e}_i\\) in the sum corresponds to the projection of \\(\\mathbf{x}\\) onto the \\(i\\)th axis. Each axis in \\(\\mathbb{R}^n\\) is still a single line, but now there are \\(n\\) of these axis lines, all perpendicular to each other.\nA linear map is a vector-valued function \\(\\mathbf{y}=\\mathbf{F}(\\mathbf{x})\\) between vector spaces that preserves the linear structure of the spaces. In general, \\(\\mathbf{x} \\in \\mathbb{R}^m\\) and \\(\\mathbf{y} \\in \\mathbb{R}^n\\) need not be in the same vector spaces. Either way, a linear map can always be expressed as a matrix-vector equation \\(\\mathbf{y}=\\mathbf{A}\\mathbf{x}\\), where \\(\\mathbf{A}\\) is some \\(m \\times n\\) matrix. More generally, an affine map is a linear map shifted by some bias vector \\(\\mathbf{b} \\in \\mathbb{R}^m\\). Affine maps can always be expressed as a shifted matrix-vector equation, \\(\\mathbf{y}=\\mathbf{A}\\mathbf{x} + \\mathbf{b}\\).\nAside: Roughly speaking a neural network is just a composite function of successive affine maps, except that one adds a nonlinearity function \\(\\boldsymbol{\\sigma}(\\mathbf{x})\\) in between each successive affine map to make it nonlinear. For example, the following nonlinear function could represent a “one hidden layer” neural network,\n\\[\\mathbf{y} = \\boldsymbol{\\sigma}_2\\big(\\mathbf{A}_2\\boldsymbol{\\sigma}_1(\\mathbf{A}_1\\mathbf{x} + \\mathbf{b}_1) + \\mathbf{b}_2\\big).\\]\nThe nonlinearity functions that get chosen are rarely exotic. Most of the time they’re all just the ReLU function \\(\\boldsymbol{\\sigma}(\\mathbf{x})=\\max(\\mathbf{0}, \\mathbf{x})\\), except in the output layer. The coefficients in the matrices and bias vectors become the parameters of the network and get learned from the training data.\nJust as with linear maps in the plane, linear maps in higher dimensions always preserve lines. Not just lines in fact, but planes and hyperplanes as well. These generalizations of lines are called linear subspaces. Linear subspaces will always be hyperplanes in \\(n\\)-dimensional space that pass through the origin. Think of them as planes passing through the origin, but in more dimensions. If the hyperplane spanned by \\(\\mathbf{x}_0, \\mathbf{x}_1, \\cdots, \\mathbf{x}_{k-1}\\) is some \\(k\\)-dimensional linear subspace of \\(\\mathbb{R}^n\\), then its image under the linear map will be a new \\(k\\)-dimensional linear subspace in \\(\\mathbb{R}^m\\) (if \\(k \\leq m\\), otherwise it’ll just be the full vector space \\(\\mathbb{R}^m\\) itself). Any linear combination of vectors in a given subspace will stay inside that subspace. It’s closed under vector space operations. For all practical purposes it’s a new vector space \\(\\mathbb{R}^k\\) unto itself."
  },
  {
    "objectID": "notebooks/matrix-algebra.html#properties-of-matrices",
    "href": "notebooks/matrix-algebra.html#properties-of-matrices",
    "title": "6  Matrix Algebra",
    "section": "6.1 Properties of Matrices",
    "text": "6.1 Properties of Matrices\n\n6.1.1 Matrix Spaces\nJust like vectors, matrices can be thought of as objects in their own matrix space. A matrix space is just a vector space, except it has two dimensions \\(m\\) and \\(n\\). We’ll denote the matrix space of \\(m \\times n\\) matrices with the symbol \\(\\mathbb{R}^{m \\times n}\\). Just like vector spaces, matrix spaces must be closed under linear combinations. If \\(\\mathbf{A}, \\mathbf{B} \\in \\mathbb{R}^{m \\times n}\\) are two matrices, then any matrix linear combination \\(\\mathbf{C} = a\\mathbf{A} + b\\mathbf{B}\\) must also be a valid \\(m \\times n\\) matrix in \\(\\mathbb{R}^{m \\times n}\\). This means matrices behave the same way under addition and scalar multiplication as vectors do.\nWhile this fact should be kind of obvious by now, here’s an example anyway. I’ll choose \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) to both be \\(2 \\times 2\\) here. Adding them together or scalar multiplying them should also obviously give a matrix that’s \\(2 \\times 2\\), since everything is element-wise.\n\na = 5\nA = np.array(\n    [[1, 1], \n     [1, 1]])\nB = np.array(\n    [[1, -1], \n     [-1, 1]])\nprint(f'{a}A = \\n{5 * A}')\nprint(f'A + B = \\n{A + B}')\n\n5A = \n[[5 5]\n [5 5]]\nA + B = \n[[2 0]\n [0 2]]\n\n\nSince every matrix corresponds to a linear map \\(\\mathbf{F}(\\mathbf{x}) = \\mathbf{A}\\mathbf{x}\\), the space of matrices also corresponds to the space of linear maps from vectors \\(\\mathbf{x} \\in \\mathbb{R}^n\\) to vectors \\(\\mathbf{y} \\in \\mathbb{R}^m\\). Recall that the composition of linear maps is equivalent to matrix multiplication. If \\(\\mathbf{F}(\\mathbf{y}) = \\mathbf{A}\\mathbf{y}\\) and \\(\\mathbf{G}(\\mathbf{x}) = \\mathbf{B}\\mathbf{x}\\) are two linear maps, then their composition is equivalent to the matrix product of the two maps,\n\\[\\mathbf{z}=\\mathbf{F}(\\mathbf{G}(\\mathbf{x})) = \\mathbf{A}\\mathbf{B}\\mathbf{x}.\\]\nThe composition, and hence the matrix multiplication operation, only makes sense when the two matrices are compatible, i.e. \\(\\mathbf{A} \\in \\mathbb{R}^{m \\times n}\\) and \\(\\mathbf{B} \\in \\mathbb{R}^{n \\times p}\\). It also follows from this relationship to linear maps (which are of course just functions) that matrix multiplication is associative, i.e. we can put parenthesis wherever we like,\n\\[\\mathbf{A}\\mathbf{B}\\mathbf{C} = (\\mathbf{A}\\mathbf{B})\\mathbf{C} = \\mathbf{A}(\\mathbf{B}\\mathbf{C}).\\]\nDo remember, however, that matrix multiplication (and function composition) doesn’t commute, i.e. \\(\\mathbf{A}\\mathbf{B} \\neq \\mathbf{B}\\mathbf{A}\\), even when the two matrices are compatible.\n\n\n6.1.2 Transposes\nRecall that every matrix \\(\\mathbf{A} \\in \\mathbb{R}^{m \\times n}\\) has a transpose matrix \\(\\mathbf{A}^\\top \\in \\mathbb{R}^{n \\times m}\\) that’s defined as the same matrix, but with the indices swapped,\n\\[(A^\\top)_{i,j} = A_{j,i}.\\]\nHere’s a quick example for a \\(2 \\times 3\\) matrix \\(\\mathbf{A}\\).\n\nA = np.array(\n    [[1, 2, 3], \n     [4, 5, 6]])\nprint(f'A^T = \\n{A.T}')\n\nA^T = \n[[1 4]\n [2 5]\n [3 6]]\n\n\nWhat happens if we multiply two transposed matrices? Suppose \\(\\mathbf{A}\\) is \\(m \\times n\\) and \\(\\mathbf{B}\\) is \\(n \\times p\\). Then \\(\\mathbf{A}\\mathbf{B}\\) is \\(m \\times p\\). That means its transpose \\((\\mathbf{A}\\mathbf{B})^\\top\\) should be \\(p \\times m\\). But \\(\\mathbf{A}^\\top\\) is \\(n \\times m\\) and \\(\\mathbf{B}^\\top\\) is \\(p \\times n\\). This implies that the transpose of the product can only make sense if it’s the product of the transposes, but in opposite order so the shapes match up right,\n\\[(\\mathbf{A}\\mathbf{B})^\\top = \\mathbf{B}^\\top \\mathbf{A}^\\top.\\]\nThis is not really a proof of this fact. If you want a proof, what you’ll want to do is look at the individual elements of each side, and show the equation must be true element-by-element. I won’t bore you with this. I’ll just give you an example with numpy so you can see they have to be equal. I’ll take \\(\\mathbf{A}\\) to be \\(3 \\times 2\\) and \\(\\mathbf{B}\\) to be \\(2 \\times 3\\), which means \\((\\mathbf{A}\\mathbf{B})^\\top\\) should be \\(2 \\times 2\\). Recall you can transpose a matrix in numpy using A.T or np.transpose(A).\n\nA = np.array(\n    [[1, 2, 3], \n     [4, 5, 6]])\nB = np.array(\n    [[-1, -2], \n     [-3, -4], \n     [-5, -6]])\nprint(f'(AB)^T = \\n{(A @ B).T}')\nprint(f'B^T A^T = \\n{B.T @ A.T}')\n\n(AB)^T = \n[[-22 -49]\n [-28 -64]]\nB^T A^T = \n[[-22 -49]\n [-28 -64]]\n\n\n\n\n6.1.3 Inverses\nWhen a matrix is square, i.e. \\(\\mathbf{A}\\) is \\(n \\times n\\), we can think of it as mapping vectors to other vectors in the same vector space \\(\\mathbb{R}^n\\). The identity map (the “do nothing” map) always maps a vector to itself. It corresponds to the \\(n \\times n\\) identity matrix\n\\[\n\\mathbf{I} =\n\\begin{pmatrix}\n1 & 0 & 0 & \\cdots & 0 \\\\\n0 & 1 & 0 & \\cdots & 0 \\\\\n0 & 0 & 1 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & 0 & \\cdots & 1 \\\\\n\\end{pmatrix}.\n\\]\nHere’s an example. I’ll use np.eye(n) to generate the identity matrix for \\(n=5\\).\n\nI = np.eye(5)\nprint(f'I = \\n{I}')\n\nI = \n[[1. 0. 0. 0. 0.]\n [0. 1. 0. 0. 0.]\n [0. 0. 1. 0. 0.]\n [0. 0. 0. 1. 0.]\n [0. 0. 0. 0. 1.]]\n\n\nRecall the inverse of a square matrix \\(\\mathbf{A}\\) is the matrix \\(\\mathbf{A}^{-1}\\) satisfying\n\\[\\mathbf{A}^{-1}\\mathbf{A} = \\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{I}.\\]\nThe inverse matrix \\(\\mathbf{A}^{-1}\\) will exist exactly when the determinant of \\(\\mathbf{A}\\) is nonzero, i.e. \\(\\text{det}(\\mathbf{A}) \\neq 0\\). If the determinant is zero, then the matrix is singular, and no inverse can be found no matter how hard you look for one.\nRecall that in numpy you can invert a square matrix using np.linalg.inv(A). It’s usually not a good idea to do so because inverting a matrix is numerically unstable, but you can in principle. The inverse calculation runs in \\(O(n^3)\\) time just like multiplication.\nHere’s an example where \\(\\mathbf{A}\\) is \\(2 \\times 2\\). You can already see from this example the numerical loss of precision creeping in, since neither \\(\\mathbf{A}^{-1}\\mathbf{A}\\) nor \\(\\mathbf{A}\\mathbf{A}^{-1}\\) exactly yield the identity matrix.\n\nA = np.array(\n    [[1, 2], \n     [3, 4]])\nA_inv = np.linalg.inv(A)\nprint(f'A^(-1) = \\n{A_inv}')\nprint(f'A^(-1) A = \\n{A_inv @ A}')\nprint(f'A A^(-1) = \\n{A @ A_inv}')\n\nA^(-1) = \n[[-2.   1. ]\n [ 1.5 -0.5]]\nA^(-1) A = \n[[1.00000000e+00 0.00000000e+00]\n [1.11022302e-16 1.00000000e+00]]\nA A^(-1) = \n[[1.0000000e+00 0.0000000e+00]\n [8.8817842e-16 1.0000000e+00]]\n\n\nJust like with the transpose, we can ask what happens if we try to invert the product of two matrices. You can convince yourself that the same kind of rule holds: the inverse of a product is the product of the inverses in reverse order,\n\\[(\\mathbf{A}\\mathbf{B})^{-1} = \\mathbf{B}^{-1} \\mathbf{A}^{-1}.\\]\nHere’s a \\(2 \\times 2\\) “proof” of this fact.\n\nA = np.array(\n    [[1, 2], \n     [3, 4]])\nB = np.array(\n    [[1, 0], \n     [1, 1]])\nA_inv = np.linalg.inv(A)\nB_inv = np.linalg.inv(B)\nAB_inv = np.linalg.inv(A @ B)\nprint(f'(AB)^(-1) = \\n{AB_inv}')\nprint(f'B^(-1) A^(-1) = \\n{B_inv @ A_inv}')\n\n(AB)^(-1) = \n[[-2.   1. ]\n [ 3.5 -1.5]]\nB^(-1) A^(-1) = \n[[-2.   1. ]\n [ 3.5 -1.5]]\n\n\nI encourage you to check this result using the fact I derived from the last lesson for \\(2 \\times 2\\) matrices,\n\\[\n\\mathbf{A} =\n\\begin{pmatrix}\na & b \\\\\nc & d \\\\\n\\end{pmatrix} \\quad \\Longrightarrow \\quad\n\\mathbf{A}^{-1} = \\frac{1}{ad-bc}\n\\begin{pmatrix}\nd & -b \\\\\n-c & a \\\\\n\\end{pmatrix}.\n\\]\n\n\n6.1.4 Determinant and Trace\nNotice something from this formula. Since \\(\\text{det}(\\mathbf{A}) = ad - bc\\) in this case, we can evidently write\n\\[\\mathbf{A}^{-1} = \\frac{1}{\\text{det}(\\mathbf{A})} \\mathbf{\\tilde A},\\]\nwhere \\(\\mathbf{\\tilde A}\\) is some kind of matrix related to \\(\\mathbf{A}\\). The properties of \\(\\mathbf{\\tilde A}\\) aren’t important (it’s called the adjugate if you’re curious). But this general fact turns out to be true for any \\(n \\times n\\) matrix, except the formula for the determinant gets a lot more complicated. What’s important is that \\(\\mathbf{A}^{-1}\\) is inversely proportional to the determinant. That’s why we can’t allow \\(\\text{det}(\\mathbf{A}) = 0\\), because then \\(\\mathbf{A}^{-1}\\) blows up due to the division by zero.\nNow, I’ve already said \\((\\mathbf{A}\\mathbf{B})^{-1} = \\mathbf{B}^{-1} \\mathbf{A}^{-1}\\). If then\n\\[\\mathbf{A}^{-1} = \\frac{1}{\\text{det}(\\mathbf{A})}\\mathbf{\\tilde A}, \\quad \\mathbf{B}^{-1} = \\frac{1}{\\text{det}(\\mathbf{B})}\\mathbf{\\tilde B},\\]\nit’s evidently the case that\n\\[(\\mathbf{AB})^{-1} = \\frac{1}{\\text{det}(\\mathbf{AB})}\\mathbf{\\tilde{AB}} = \\frac{1}{\\text{det}(\\mathbf{A}) \\cdot \\text{det}(\\mathbf{B})}\\mathbf{\\tilde B}\\mathbf{\\tilde A} = \\mathbf{B}^{-1} \\mathbf{A}^{-1}.\\]\nProvided that \\(\\mathbf{\\tilde{AB}}=\\mathbf{\\tilde B}\\mathbf{\\tilde A}\\), which is true, it thus follows that\n\\[\\text{det}(\\mathbf{A}\\mathbf{B}) = \\text{det}(\\mathbf{A}) \\cdot \\text{det}(\\mathbf{B}) = \\text{det}(\\mathbf{B}) \\cdot \\text{det}(\\mathbf{A}).\\]\nSaid differently, the determinant of a matrix product is just the product of their individual determinants.\nIn general, the determinant of an \\(n \\times n\\) matrix \\(\\mathbf{A}\\) is a nasty \\(n\\) degree multivariate polynomial of the elements of \\(\\mathbf{A}\\). There’s no reliably easy way to calculate it except for small \\(n\\) matrices. In numpy, you can use np.linalg.det(A) to calculate the determinant, but just as with inverses, this is a numerically unstable operation, and so should be avoided where possible. Moreover, it runs in \\(O(n^3)\\) time, which is just as slow as matrix multiplication.\nHere’s an example. I’ll verify this “product rule” for determinants using two \\(3 \\times 3\\) matrices. The determinant of both matrices turns out to be \\(6\\), which means their product should have determinant \\(36\\).\n\nA = np.array(\n    [[3, 0, 0],\n     [1, 2, 0],\n     [1, 1, 1]])\nB = np.array(\n    [[1, 1, 1],\n     [0, 2, 1],\n     [0, 0, 3]])\ndet_A = np.linalg.det(A)\ndet_B = np.linalg.det(B)\ndet_AB = np.linalg.det(A @ B)\nprint(f'det(A) = {det_A}')\nprint(f'det(B) = {det_B}')\nprint(f'det(AB) = {det_AB}')\n\ndet(A) = 6.0\ndet(B) = 6.0\ndet(AB) = 36.0\n\n\nNotice in both cases the determinant happens to be the product of the diagonal elements\n\\[\\text{det}(\\mathbf{A}) = \\text{det}(\\mathbf{B}) = 1 \\cdot 2 \\cdot 3 = 6.\\]\nI rigged the result to come out this way. It’s not always true. It’s only true when a matrix is either lower triangular (the elements above the diagonal are all zero), upper triangular (the elements below the diagonal are all zero), or diagonal (the elements off the diagonal are all zero). In this example, \\(\\mathbf{A}\\) was lower triangular and \\(\\mathbf{B}\\) was upper triangular. I chose both to have the same diagonal elements (in different order) on purpose.\nMore generally, if \\(\\mathbf{A}\\) is diagonal or upper/lower triangular, then\n\\[\\text{det}(\\mathbf{A}) = \\prod_{i=0}^{n-1} A_{i,i} = A_{0,0} A_{1,1} \\cdots A_{n-1,n-1}.\\]\nIt’s not yet obvious, but we can always “change” a square matrix \\(\\mathbf{A}\\) into one of these three kinds of matrices, and then calculate the determinant of \\(\\mathbf{A}\\) this way. There are a few ways to do this. I’ll cover these when I get to matrix factorizations below.\nSome other properties of the determinant that you can verify are,\n\n\\(\\text{det}(\\mathbf{I}) = 1\\).\n\\(\\text{det}(\\mathbf{A}^\\top) = \\text{det}(\\mathbf{A})\\).\n\\(\\text{det}(\\mathbf{A}^{-1}) = \\frac{1}{\\text{det}(\\mathbf{A})}\\).\n\\(\\text{det}(c\\mathbf{A}) = c^n\\text{det}(\\mathbf{A})\\).\n\nThe determinant is one important way to get a scalar out of a matrix. Another useful scalar is the trace, which is far simpler to calculate. The trace of a matrix \\(\\mathbf{A}\\) is the sum of its diagonal elements, usually written\n\\[\\text{tr}(\\mathbf{A}) = \\sum_{i=0}^{n-1} A_{i,i} = A_{0,0} + A_{1,1} + \\cdots + A_{n-1,n-1}.\\]\nUnlike the determinant, the trace doesn’t split up over products. It instead splits over addition,\n\\[\\text{tr}(\\mathbf{A} + \\mathbf{B}) = \\text{tr}(\\mathbf{A}) + \\text{tr}(\\mathbf{B}).\\]\nThis is very easy to verify from the fact that the sum is element-wise, so \\(\\sum (A+B)_{i,i} = \\sum A_{i,i} + \\sum B_{i,i}\\).\nSome other fairly trivial properties the trace satisfies are,\n\n\\(\\text{tr}(\\mathbf{I}) = n\\).\n\\(\\text{tr}(\\mathbf{A}^\\top) = \\text{tr}(\\mathbf{A})\\).\n\\(\\text{tr}(c\\mathbf{A}) = c\\text{tr}(\\mathbf{A})\\).\n\\(\\text{tr}(\\mathbf{A}\\mathbf{B}) = \\text{tr}(\\mathbf{B}\\mathbf{A})\\).\n\nHere’s a “proof” of the last result on the same \\(3 \\times 3\\) matrices above. In numpy, you can calculate the trace using np.trace. It’s not unstable like the determinant is, and it’s fast to calculate since it’s only summing the \\(n\\) diagonal terms, which is \\(O(n)\\) time.\n\ntr_AB = np.trace(A @ B)\ntr_BA = np.trace(B @ A)\nprint(f'tr(AB) = {tr_AB}')\nprint(f'tr(BA) = {tr_BA}')\n\ntr(AB) = 13\ntr(BA) = 13\n\n\nIt’s kind of obvious what the determinant is good for. It tells you how invertible a matrix is. But what does the trace tell you? It turns out both the trace and the determinant also tell you something important about the scale of the matrix. We’ll see this in more depth below when we talk about eigenvalues.\n\n\n6.1.5 Linear Independence and Rank\nWe can always think of a matrix in terms of its column vectors. If \\(\\mathbf{A}\\) is \\(m \\times n\\), it has \\(n\\) column vectors \\(\\mathbf{a}_0, \\mathbf{a}_1, \\cdots, \\mathbf{a}_{n-1}\\) each of size \\(m\\). Concatenated together in order, the column vectors form the matrix itself,\n\\[\n\\mathbf{A} =\n\\begin{pmatrix}\n\\mathbf{a}_0 & \\mathbf{a}_1 & \\cdots & \\mathbf{a}_{n-1}\n\\end{pmatrix}.\n\\]\nIt turns out these column vectors also tell us how invertible a matrix is, but in a more general and useful way than the determinant does. Roughly speaking, a matrix is invertible if we can’t write any one column vector as a function of the other column vectors. This is just the definition of linear independence.\nRecall a set of vectors \\(\\mathbf{x}_0, \\mathbf{x}_1, \\cdots, \\mathbf{x}_{k-1}\\) is linearly independent if no one vector is a linear combination of the rest,\n\\[\\mathbf{x}_j \\neq \\sum_{i \\neq j} c_i \\mathbf{x}_j.\\]\nIf one vector is a linear combination of the rest, they’re linearly dependent.\nAn \\(n \\times n\\) matrix \\(\\mathbf{A}\\) is invertible if and only if its column vectors are all linearly independent. Equivalently, the column vectors span an \\(n\\)-dimensional vector space. To see why this is true, let’s look at a \\(2 \\times 2\\) matrix \\(\\mathbf{A}\\) with column vectors \\(\\mathbf{a}=\\binom{a}{b}\\) and \\(\\mathbf{b}=\\binom{c}{d}\\),\n\\[\n\\mathbf{A} = \\begin{pmatrix} \\mathbf{a} & \\mathbf{b} \\end{pmatrix} =\n\\begin{pmatrix}\na & b \\\\\nc & d \\\\\n\\end{pmatrix}.\n\\]\nNow, if \\(\\mathbf{a}\\) and \\(\\mathbf{b}\\) are linearly dependent, then \\(\\mathbf{b}\\) must be a scalar multiple of \\(\\mathbf{a}\\), say \\(\\mathbf{b} = \\beta \\mathbf{a}\\). Then \\(\\mathbf{A}\\) would look like\n\\[\n\\mathbf{A} = \\begin{pmatrix} \\mathbf{a} & \\beta \\mathbf{a} \\end{pmatrix} =\n\\begin{pmatrix}\na & \\beta a \\\\\nc & \\beta c \\\\\n\\end{pmatrix}.\n\\]\nThis means its determinant would be \\(\\text{det}(\\mathbf{A}) = \\beta ac - \\beta ac = 0\\), which of course means \\(\\mathbf{A}\\) can’t be invertible.\nGraphically, saying the column vectors are linearly dependent is saying they’ll map any vector onto the same subspace. For the \\(2 \\times 2\\) case, that means any vector \\(\\mathbf{v}\\) hit by \\(\\mathbf{A}\\) will get mapped onto the same line, no matter what \\(\\mathbf{v}\\) you pick. The matrix is collapsing, or projecting, the vector space down to a lower-dimensional subspace.\nHere’s a plot of this idea. I’ll make \\(\\mathbf{A}\\) have two linearly dependent columns, then plot its action on several different vectors, plotted in black. Acting on these by \\(\\mathbf{A}\\) will map them to the red vectors, which all lie on the same line in the plane. They’re all collapsing onto the same subspace, evidently the line \\(y=-x\\).\n\n\nCode\nbeta = 1.5\na0 = np.array([1, -1]).reshape(-1, 1)\na1 = beta * a0\nA = np.hstack([a0, a1])\nv = np.array([1, 1]).reshape(-1, 1)\nw = np.array([-1, 0]).reshape(-1, 1)\nu = np.array([1, -3]).reshape(-1, 1)\nvectors = [x.flatten() for x in [v, A @ v, w, A @ w, u, A @ u]]\n\nplot_vectors(vectors, colors=['black', 'red'] * 3, title='Linearly Dependence',\n             labels=['$\\mathbf{v}$', '$\\mathbf{A}\\mathbf{v}$'] + [''] * 4,\n             text_offsets=[[0, 0]] * 6, headwidth=5)\n\n\n\n\n\n\n\n\n\nThe number of linearly independent column vectors a matrix has is called its rank, written \\(\\text{rank}(\\mathbf{A})\\). Clearly it’ll always be the case that \\(\\text{rank}(\\mathbf{A}) \\leq n\\). When \\(\\text{rank}(\\mathbf{A}) = n\\) exactly the matrix is called full rank. Only full rank square matrices are invertible.\nHere’s an example. I’ll use np.linalg.matrix_rank(A) to calculate the rank of the above \\(2 \\times 2\\) example. Since \\(\\text{rank}(\\mathbf{A})=1<2\\), the matrix \\(\\mathbf{A}\\) must be singular, as I’ve of course already shown.\n\nrank = np.linalg.matrix_rank(A)\nprint(f'rank(A) = {rank}')\n\nrank(A) = 1\n\n\n\n\n6.1.6 Outer Products\nWe’ll frequently be interested in low rank matrices, which are matrices whose rank is much much less than the dimension, i.e. \\(\\text{rank}(\\mathbf{A}) << n\\). As we’ll see, low rank matrices are special because they can efficiently compress the information contained in a matrix, which often allows us to represent data more efficiently, or clean up data by denoising away “unnecessary” dimensions. In fact, approximating a matrix with a lower rank matrix is the whole idea behind dimension reduction, one of the core areas of unsupervised learning.\nThe most useful low-rank matrices are the outer products of two vectors. If \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are size \\(n\\) vectors, define their outer product by\n\\[\n\\mathbf{x} \\mathbf{y}^\\top =\n\\begin{pmatrix}\nx_0 y_0 & x_0 y_1 & \\cdots & x_0 y_{n-1} \\\\\nx_1 y_0 & x_1 y_1 & \\cdots & x_1 y_{n-1} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nx_{n-1} y_0 & x_{n-1} y_1 & \\cdots & x_{n-1} y_{n-1} \\\\\n\\end{pmatrix}.\n\\]\nEach column vector \\(\\mathbf{a}_j\\) of the outer product matrix is linearly proportional to the first column \\(\\mathbf{a}_0\\), since\n\\[\\mathbf{a}_j = \\mathbf{x} y_j = \\mathbf{x} y_0 \\frac{y_j}{y_0} = \\frac{y_j}{y_0} \\mathbf{a}_0.\\]\nThis means that only one column vector is linearly independent, which implies \\(\\text{rank}(\\mathbf{x} \\mathbf{y}^\\top)=1\\). The outer product is evidently rank-1, and hence highly singular. You’d never be able to invert it. But it is useful as we’ll see soon.\nHere’s an example of an outer product calculation. You can either calculate x @ y.T directly or use np.outer(x, y). Since both vectors are size \\(3\\), the outer product should be a \\(3 \\times 3\\) matrix with rank-1.\n\nx = np.array([1, 2, 3])\ny = np.array([3, 2, 1])\nouter = np.outer(x, y)\nprint(f'xy^T = \\n{outer}')\nprint(f'rank(xy^T) = {np.linalg.matrix_rank(outer)}')\n\nxy^T = \n[[3 2 1]\n [6 4 2]\n [9 6 3]]\nrank(xy^T) = 1\n\n\nYou can think of the outer product matrix as a kind of projection matrix. It always projects vectors onto the same one-dimensional line in \\(\\mathbb{R}^n\\). Why? Suppose \\(\\mathbf{v}\\) is some vector. If we hit it with the outer product matrix \\(\\mathbf{x} \\mathbf{y}^\\top\\), using the fact matrix multiplication is associative, we get\n\\[(\\mathbf{x} \\mathbf{y}^\\top) \\mathbf{v} = \\mathbf{x} (\\mathbf{y}^\\top \\mathbf{v}) = (\\mathbf{y} \\cdot \\mathbf{v}) \\mathbf{x}.\\]\nThat is, \\(\\mathbf{v}\\) just gets projected onto the space spanned by the vector \\(\\mathbf{x}\\). Evidently the other outer product vector \\(\\mathbf{y}\\) determines how long the projection vector will be. Here’s a visual representation of this idea for 2-dimensional vectors. Take\n\\[\\begin{align*}\n\\mathbf{x} &= (1, 1) \\\\\n\\mathbf{y} &= (1, -1) \\\\\n\\mathbf{v}_0 &= (-1, 2) \\quad &\\Longrightarrow \\quad (\\mathbf{y} \\cdot \\mathbf{v}_0) \\mathbf{x} &= (-3, -3) \\\\\n\\mathbf{v}_1 &= (2, 0) \\quad &\\Longrightarrow \\quad (\\mathbf{y} \\cdot \\mathbf{v}_1) \\mathbf{x} &= (2, 2) \\\\\n\\mathbf{v}_2 &= (2, -1) \\quad &\\Longrightarrow \\quad (\\mathbf{y} \\cdot \\mathbf{v}_2) \\mathbf{x} &= (3, 3). \\\\\n\\end{align*}\\]\nApplying the outer product \\(\\mathbf{x} \\mathbf{y}^\\top\\) to each \\(\\mathbf{v}_i\\) should project each vector onto the space spanned by \\(\\mathbf{x}=(1, 1)\\), which is just the line \\(y=x\\). Notice the projections are all proportional to \\((1, 1)\\), as they should be. In the plot below, each vector and its projection have the same color. The outer product vectors \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are shown in black.\n\n\nCode\nx = np.array([1, 1]).reshape(-1, 1)\ny = np.array([1, -1]).reshape(-1, 1)\nvs = [np.array([-1, 2]).reshape(-1, 1), \n      np.array([2, 0]).reshape(-1, 1), \n      np.array([2, -1]).reshape(-1, 1)]\nws = [(x @ y.T) @ v for v in vs]\nvectors = [vector.flatten() for vector in vs + ws + [x, y]]\nplot_vectors(\n    vectors, colors=['salmon', 'limegreen', 'steelblue'] * 2 + ['black', 'black'], headwidth=5, width=0.01,\n    labels=['$\\mathbf{v}_0$', '$\\mathbf{v}_1$', '$\\mathbf{v}_2$'] + [''] * 3 + ['$\\mathbf{x}$', '$\\mathbf{y}$'],\n    text_offsets = [[0, 0.2], [0, 0.2], [0.1, -0.3]] + [[0,0]] * 3 + [[-0.4, 0.15], [0, -0.3]], ticks_every=1,\n    title='Outer Product Projections', zorders=[0, 5, 1, 2, 4, 3, 4, 6, 7], xlim=(-3.5, 3.5), ylim=(-3.5, 3.5))"
  },
  {
    "objectID": "notebooks/matrix-algebra.html#special-matrices",
    "href": "notebooks/matrix-algebra.html#special-matrices",
    "title": "6  Matrix Algebra",
    "section": "6.2 Special Matrices",
    "text": "6.2 Special Matrices\nThere are many classes of matrices that have various special properties. I’ll quickly introduce a few that’ll be of interest to us in machine learning.\n\n6.2.1 Diagonal Matrices\nProbably the most basic class of matrices are the diagonal matrices. A diagonal matrix is an \\(m \\times n\\) matrix \\(\\mathbf{D}\\) whose elements are only non-zero on the diagonals, i.e. \\(D_{i,j} = 0\\) if \\(i \\neq j\\). For example, the following \\(3 \\times 3\\) matrix is diagonal since its only non-zero values lie on the diagonal,\n\\[\n\\mathbf{D} =\n\\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 2 & 0 \\\\\n0 & 0 & 3 \\\\\n\\end{pmatrix}.\n\\]\nWe’ve already seen an important diagonal matrix a few times, the identity matrix \\(\\mathbf{I}\\). The identity matrix is the diagonal matrix whose diagonal entries are all ones. It’s common to short-hand a diagonal matrix by just specifying its diagonal entries as a vector. In this notation, we’d use the short-hand\n\\[\\mathbf{D} = \\text{diag}(1,2,3).\\]\nto refer to the matrix in the above example. It means exactly the same thing, we’re just only specifying the diagonal elements. This is also the easiest way to define a diagonal matrix in numpy, by using np.diag. Notice that a diagonal matrix contains \\(n^2\\) elements, but we only need to specify \\(n\\) of them to fully determine what the matrix is (i.e. the diagonal elements themselves).\nIn a sense, a diagonal matrix can only scale a vector it acts on, not rotate it or reflect it. This is because multiplying diagonal matrix with a vector is equivalent to element-wise multiplying the diagonal elements with the vector, which causes each vector component to get stretched by some amount. For example, if \\(\\mathbf{x}=(1,1,1)\\), when the above example \\(\\mathbf{D}\\) acts on it, we’d get\n\\[\n\\mathbf{D}\\mathbf{x} =\n\\begin{pmatrix}\n1 & 0 & 0 \\\\\n0 & 2 & 0 \\\\\n0 & 0 & 3 \\\\\n\\end{pmatrix}\n\\begin{pmatrix}\n1 \\\\\n1 \\\\\n1 \\\\\n\\end{pmatrix} =\n\\begin{pmatrix}\n1 \\\\\n2 \\\\\n3 \\\\\n\\end{pmatrix} =\n\\begin{pmatrix}\n1 \\\\\n2 \\\\\n3 \\\\\n\\end{pmatrix} \\circ\n\\begin{pmatrix}\n1 \\\\\n1 \\\\\n1 \\\\\n\\end{pmatrix}.\n\\]\nHere’s an example of how to define a diagonal matrix in numpy using np.diag. I’ll define the same matrix as the above example, and then act on the same vector to show it just scales the entries.\n\nD = np.diag([1, 2, 3])\nx = np.array([1, 1, 1]).reshape(-1, 1)\nprint(f'D = diag(1,2,3) = \\n{D}')\nprint(f'Dx = {(D @ x).flatten()}')\n\nD = diag(1,2,3) = \n[[1 0 0]\n [0 2 0]\n [0 0 3]]\nDx = [1 2 3]\n\n\n\n\n6.2.2 Symmetric Matrices\nAnother special class of matrices important to machine learning is the symmetric matrix. A symmetric matrix is a square matrix \\(\\mathbf{S}\\) that equals its own transpose, i.e. \\(\\mathbf{S}^\\top = \\mathbf{S}\\). They’re called symmetric matrices because their lower diagonals and upper diagonals are mirror images. Symmetric matrices can be thought of as the matrix equivalent of a real number.\nFor example, consider the matrix \\[\n\\mathbf{S} =\n\\begin{pmatrix}\n1 & -1 & -2 \\\\\n-1 & 2 & 1 \\\\\n-2 & 1 & 3 \\\\\n\\end{pmatrix}.\n\\]\nThis matrix is symmetric since the upper diagonal and lower diagonal are the same, i.e. \\(S_{i,j} = S_{j,i}\\). Symmetric matrices are very important as we’ll see. They’re the matrix generalization of the idea of a real number.\nSince the lower diagonal and upper diagonal of a symmetric matrix always equal, we only need to specify what the diagonal and upper diagonal are to fully determine the matrix. If \\(\\mathbf{S}\\) contains \\(n^2\\) entries, only \\[n + \\frac{1}{2}(n^2 - n) = \\frac{1}{2}n(n+1)\\]\nof those elements are actually unique. This fact can be used to shave a lot of time off of algorithms involving symmetric matrices. In numpy, you can check a matrix \\(\\mathbf{S}\\) is symmetric by checking that it equals its transpose. Due to numerical roundoff, you may want to wrap the condition inside np.allclose.\n\nS = np.array([\n    [1, -1, -2],\n    [-1, 2, 1],\n    [-2, 1, 3]])\nis_symmetric = lambda A: np.allclose(A, A.T)\nis_symmetric(S)\n\nTrue\n\n\n\n\n6.2.3 Upper and Lower Triangular Matrices\nClosely related to diagonal matrices are lower and upper triangular matrices. An \\(m \\times n\\) matrix \\(\\mathbf{L}\\) is lower-triangular if the entries in its upper diagonal are zero, i.e. \\(L_{i,j} = 0\\) when \\(i < j\\). Similarly, an \\(m \\times n\\) matrix \\(\\mathbf{U}\\) is upper-triangular if the entries in its lower diagonal are zero, i.e. \\(U_{i,j} = 0\\) when \\(i > j\\). I’ve already showed an example of these when I covered determinants. Here they are again,\n\\[\n\\mathbf{L} =\n\\begin{pmatrix}\n3 & 0 & 0 \\\\\n1 & 2 & 0 \\\\\n1 & 1 & 1 \\\\\n\\end{pmatrix}, \\qquad \\mathbf{U} =\n\\begin{pmatrix}\n1 & 1 & 1 \\\\\n0 & 2 & 1 \\\\\n0 & 0 & 3 \\\\\n\\end{pmatrix}.\n\\]\nUpper and lower triangular (and diagonal) matrices are useful because it’s easy to invert them and calculate their determinants. Just like symmetric matrices, only \\(\\frac{1}{2}n(n+1)\\) unique elements are needed to fully specify these matrices since an entire off-diagonal is all zeros.\n\n\n6.2.4 Orthogonal Matrices\nThe last class of matrices I’ll introduce are more subtle, but very important geometrically. These are the orthogonal matrices. An orthogonal matrix is an \\(n \\times n\\) matrix \\(\\mathbf{Q}\\) whose transpose is its inverse, i.e.\n\\[\\mathbf{Q}^\\top = \\mathbf{Q}^{-1} \\quad \\text{or} \\quad \\mathbf{Q}^\\top \\mathbf{Q}=\\mathbf{I}.\\]\nAs an example, consider the following matrix,\n\\[\n\\mathbf{Q} = \\frac{1}{\\sqrt{2}}\n\\begin{pmatrix}\n1 & 1 \\\\\n1 & -1 \\\\\n\\end{pmatrix} =\n\\begin{pmatrix}\n\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\\\\n\\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}} \\\\\n\\end{pmatrix}.\n\\]\nWe can check \\(\\mathbf{Q}\\) is orthogonal by checking it satisfies the condition \\(\\mathbf{Q}^\\top \\mathbf{Q}=\\mathbf{I}\\),\n\\[\n\\mathbf{Q}^\\top \\mathbf{Q} =\n\\frac{1}{\\sqrt{2}}\n\\begin{pmatrix}\n1 & 1 \\\\\n1 & -1 \\\\\n\\end{pmatrix}\n\\frac{1}{\\sqrt{2}}\n\\begin{pmatrix}\n1 & 1 \\\\\n1 & -1 \\\\\n\\end{pmatrix} =\n\\frac{1}{2}\n\\begin{pmatrix}\n2 & 0 \\\\\n0 & 2 \\\\\n\\end{pmatrix} =\n\\begin{pmatrix}\n1 & 0 \\\\\n0 & 1 \\\\\n\\end{pmatrix} = \\mathbf{I}.\n\\]\nNotice from this example that the column vectors \\(\\mathbf{q}_0, \\mathbf{q}_1\\) form an orthonormal basis for \\(\\mathbb{R}^2\\), since\n\\[\\mathbf{q}_0 \\cdot \\mathbf{q}_1 = 0, \\quad \\mathbf{q}_0 \\cdot \\mathbf{q}_0 = \\mathbf{q}_1 \\cdot \\mathbf{q}_1 = 1.\\]\nThis is a general fact. The column vectors of an orthogonal matrix \\(\\mathbf{Q}\\) form a complete set of orthonormal basis vectors for \\(\\mathbb{R}^n\\). Conversely, we can always form an orthogonal matrix by first finding an orthonormal basis and then creating column vectors out of the basis vectors. This is usually the way orthogonal matrices are constructed in practice using algorithms like the Gram-Schmidt Algorithm.\nIt’s not at all obvious, but the fact that the column vectors of \\(\\mathbf{Q}\\) form an orthonormal basis constrains the number of unique elements \\(\\mathbf{Q}\\) is allowed to have. Requiring each \\(\\mathbf{q}_i\\) means \\(n\\) total elements are already determined. The further requirement that the column vectors be mutually orthogonal determines another \\(\\frac{1}{2}n(n-1)\\). This means \\(\\mathbf{Q}\\) only has \\(n^2 - n - \\frac{1}{2}n(n-1) = \\frac{1}{2}n(n-1)\\) unique elements. For example, when \\(\\mathbf{Q}\\) is \\(2 \\times 2\\) it only has \\(\\frac{1}{2}2(2-1)=1\\) unique element. The other \\(3\\) are all determined by that one element. This unique element can be thought of as a rotation angle. I’ll come back to this in a minute.\nAn important fact about orthogonal matrices is that they preserve the dot products between vectors. If \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) are two vectors, then\n\\[(\\mathbf{Q} \\mathbf{x}) \\cdot (\\mathbf{Q}\\mathbf{y}) = \\mathbf{x} \\cdot \\mathbf{y}.\\]\nThis follows from the fact that \\((\\mathbf{Q} \\mathbf{x})^\\top (\\mathbf{Q} \\mathbf{y}) = \\mathbf{x}^\\top \\mathbf{Q}^\\top\\mathbf{Q}\\mathbf{y} = \\mathbf{x}^\\top \\mathbf{I} \\mathbf{y} = \\mathbf{x}^\\top \\mathbf{y}\\). Since the dot product encodes the notions of length and angle, this fact implies that orthogonal matrices can’t change the lengths of vectors, nor the angles between vectors. Orthogonal matrices preserve the geometry of the vector space.\nThis fact suggests some deep intuition about what orthogonal matrices do. If they can’t change the lengths of vectors or the angles between them, then all they can do is rotate vectors or reflect them across some line. In fact, it turns out any \\(2 \\times 2\\) orthogonal matrix can be written in the form\n\\[\n\\mathbf{Q} =\n\\begin{pmatrix}\n\\cos \\theta & \\mp \\sin \\theta \\\\\n\\sin \\theta & \\pm \\cos \\theta \\\\\n\\end{pmatrix},\n\\]\nwhere \\(\\theta\\) is some angle (expressed in radians). When the right column vector is \\(\\binom{-\\sin\\theta}{\\cos\\theta}\\), \\(\\mathbf{Q}\\) is a pure rotation matrix. It will rotate any vector in the plane by an angle \\(\\theta\\), counterclockwise if \\(\\theta > 0\\), and clockwise if \\(\\theta < 0\\). When the right column vector is \\(\\binom{\\sin\\theta}{-\\cos\\theta}\\), \\(\\mathbf{Q}\\) becomes a reflection matrix; it’ll reflect vectors about the line at an angle \\(\\frac{\\theta}{2}\\) with the x-axis. The combination of these two together can generate any 2D rotation or reflection.\nHere’s a visual of this idea. I’ll take the unit vector \\(\\mathbf{e}_x=(1,0)\\) and use \\(\\mathbf{Q}\\) to rotate it by some angle, in this case \\(\\theta = 45^\\circ\\). Note the need to convert the angle to radians by multiplying the angle in degrees by \\(\\frac{\\pi}{180}\\). You should be able to confirm that the red vector is indeed the black vector \\(\\mathbf{e}_x\\) rotated counterclockwise by \\(45^\\circ\\) to the new vector \\(\\mathbf{Q}\\mathbf{e}_x = 2^{-1/2}(1,1)\\). The factor of \\(2^{-1/2}\\) appears to keep the vector normalized to unit length.\n\n\nCode\ntheta_degrees = 45\ntheta = theta_degrees * np.pi / 180\nQ = np.array([\n    [np.cos(theta), -np.sin(theta)], \n    [np.sin(theta), np.cos(theta)]])\nex = np.array([1, 0]).reshape(-1, 1)\nQex = Q @ ex\nplot_vectors([ex.flatten(), Qex.flatten()], colors=['black', 'red'], title=f'${theta_degrees}^\\circ$ Rotation',\n             labels=['$\\mathbf{e}_x$', '$\\mathbf{Q}\\mathbf{e}_x$'], text_offsets=[[-0.1, 0.1], [0, 0]],\n             ticks_every=1, xlim=(-0.5, 1.5), ylim=(-0.5, 1.5))\n\n\n\n\n\n\n\n\n\nI’ll finish this section by noting that orthogonal matrices always have determinant \\(\\pm 1\\). You can see this by applying the determinant product formula to \\(\\mathbf{Q}^\\top \\mathbf{Q}=\\mathbf{I}\\),\n\\[1 = \\text{det}(\\mathbf{I}) = \\text{det}(\\mathbf{Q}^\\top \\mathbf{Q}) = \\text{det}(\\mathbf{Q}^\\top) \\cdot \\text{det}(\\mathbf{Q}) = \\big(\\text{det}(\\mathbf{Q})\\big)^2,\\]\nwhich implies \\(\\text{det}(\\mathbf{Q}) = \\pm 1\\). This evidently divides orthogonal matrices into two distinct classes:\n\n\\(\\text{det}(\\mathbf{Q}) = +1\\): These are the orthogonal matrices that correspond to pure rotations.\n\\(\\text{det}(\\mathbf{Q}) = -1\\): These are the orthogonal matrices that correspond to reflections.\n\nI’ll verify that the rotation matrix I just plotted indeed has determinant \\(+1\\).\n\nprint(f'det(Q) = {np.linalg.det(Q)}')\n\ndet(Q) = 1.0"
  },
  {
    "objectID": "notebooks/matrix-algebra.html#matrix-factorizations",
    "href": "notebooks/matrix-algebra.html#matrix-factorizations",
    "title": "6  Matrix Algebra",
    "section": "6.3 Matrix Factorizations",
    "text": "6.3 Matrix Factorizations\nGiven any two compatible matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\), we can get a third matrix \\(\\mathbf{C}\\) by matrix multiplication, \\(\\mathbf{C} = \\mathbf{A}\\mathbf{B}\\). Now suppose we wanted to go the other way. Given a matrix \\(\\mathbf{C}\\), how can we factor it back out into a product \\(\\mathbf{A}\\mathbf{B}\\)? This is the idea behind matrix factorization. In practice, we’re interested in factoring a matrix into a product of special types of matrices that are easier to work with, like symmetric, diagonal, or orthogonal matrices.\n\n6.3.1 LU Factorization\nProbably the most basic matrix factorization is the LU Factorization. LU factorization factors an \\(m \\times n\\) matrix \\(\\mathbf{A}\\) into a product of a lower triangular matrix \\(\\mathbf{L}\\) and an upper triangular matrix \\(\\mathbf{U}\\), \\[\\mathbf{A} = \\mathbf{L}\\mathbf{U}.\\]\nThe LU factorization is most useful for solving a system of linear equations. If \\(\\mathbf{A}\\mathbf{x}=\\mathbf{b}\\), we can do an LU factorization of \\(\\mathbf{A}\\) and write the system as \\(\\mathbf{LUx} = \\mathbf{b}\\). This can then be solved by breaking it into two steps, known as forward substitution and back substitution,\n\nForward substitution: Solve \\(\\mathbf{Ly} = \\mathbf{b}\\) for \\(\\mathbf{y}\\).\nBack Substitution: Solve \\(\\mathbf{Ux} = \\mathbf{y}\\) for \\(\\mathbf{x}\\).\n\nThese two steps are easy to do since each system can be solved by substitution, working from the “tip” of the triangle down. The LU factorization is essentially what matrix solvers like np.linalg.solve do to solve linear systems.\nOf course, the question still remains how to actually factor \\(\\mathbf{A}\\) into \\(\\mathbf{L}\\mathbf{U}\\). I won’t describe the algorithm to do this, or any matrix factorization really, since their inner workings aren’t that relevant to machine learning. If you’re curious, LU factorization is done using some variant of an algorithm known as Gaussian Elimination. Note the LU factorization in general is a cubic time algorithm, i.e. \\(O(n^3)\\) if \\(\\mathbf{A}\\) is \\(n \\times n\\).\nThe LU factorization can also be used to compute the determinant of a square matrix. Since \\(\\mathbf{L}\\) and \\(\\mathbf{U}\\) are triangular, their determinant is just the product of their diagonals. Using the product rule for determinants then gives\n\\[\\text{det}(\\mathbf{A}) = \\text{det}(\\mathbf{LU}) = \\text{det}(\\mathbf{L}) \\cdot \\text{det}(\\mathbf{U}) = \\prod_{i=0}^{n-1} L_{i,i} \\cdot U_{i,i}.\\]\nThe LU factorization can also be used to compute the inverse of a square matrix. The idea is to solve the matrix system of equations\n\\[\\mathbf{A} \\mathbf{X} = \\mathbf{I},\\]\nassuming \\(\\mathbf{X}=\\mathbf{A}^{-1}\\) are the \\(n^2\\) unknown variables you’re solving for. This system can be solved by using the same technique of forward substitution plus back substitution. Note that solving for both the determinant and inverse this way each takes \\(O(n^3)\\) time due to the LU decomposition. This is one reason why you should probably avoid calculating these quantities explicitly unless you really need them.\nStrangely, numpy doesn’t have a built-in LU factorization solver, but scipy does using scipy.linalg.lu. It factors a matrix into not two, but three products, \\(\\mathbf{A}=\\mathbf{PLU}\\). The \\(\\mathbf{P}\\) is a permutation matrix. It just accounts for the fact that sometimes you need to swap the rows before doing the LU factorization. I won’t go into that. Here’s the LU factorization of the above example matrix. I’ll also verify that \\(\\mathbf{A}=\\mathbf{LU}\\).\n\nfrom scipy.linalg import lu\n\nA = np.array([[1, 1], \n              [1, -1]])\nP, L, U = lu(A)\nprint(f'L = \\n{L}')\nprint(f'U = \\n{U}')\nprint(f'LU = \\n{L @ U}')\n\nL = \n[[1. 0.]\n [1. 1.]]\nU = \n[[ 1.  1.]\n [ 0. -2.]]\nLU = \n[[ 1.  1.]\n [ 1. -1.]]\n\n\n\n\n6.3.2 QR Factorization\nAnother useful factorization is to factor a matrix \\(\\mathbf{A}\\) into a product of an orthogonal matrix \\(\\mathbf{Q}\\) and an upper triangular matrix \\(\\mathbf{R}\\),\n\\[\\mathbf{A} = \\mathbf{QR}.\\]\nThe QR factorization is useful if we want to create an orthonormal basis out of the column vectors of \\(\\mathbf{A}\\), since \\(\\mathbf{Q}\\) will give a complete set of basis vectors built from orthogonalizing \\(\\mathbf{A}\\). It’s also useful for calculating other random things of interest. Like LU factorization, it can be used to calculate determinants, since\n\\[\\text{det}(\\mathbf{A}) = \\text{det}(\\mathbf{QR}) = \\text{det}(\\mathbf{Q}) \\cdot \\text{det}(\\mathbf{R}) = 1 \\cdot \\text{det}(\\mathbf{R}) = \\prod_{i=0}^{n-1} R_{i,i}.\\]\nIt can also be used to find the inverse matrix. Use the fact that \\(\\mathbf{A}^{-1} = (\\mathbf{QR})^{-1} = \\mathbf{R}^{-1} \\mathbf{Q}^\\top\\), since \\(\\mathbf{Q}\\) is orthogonal. The matrix \\(\\mathbf{R}^{-1}\\) can be calculated efficiently via back-substitution since \\(\\mathbf{R}\\) just a triangular matrix. Both the determinant and inverse calculation again take \\(O(n^3)\\) time because the QR factorization does.\nQR factorization is also useful for efficiently calculating the eigenvalues and eigenvectors of a symmetric matrix. I’ll cover what those are in a second.\nIn practice, this factorization is done using algorithms like Gram-Schmidt or Householder reflections. Just like LU factorization, QR factorization is in general an \\(O(n^3)\\) algorithm. In numpy, you can get the QR factorization using np.linalg.qr(A). Here’s the QR factorization of the same matrix from before.\n\nA = np.array([[1, 1], \n              [1, -1]])\nQ, R = np.linalg.qr(A)\nprint(f'Q = \\n{Q.round(10)}')\nprint(f'R = \\n{R.round(10)}')\nprint(f'QR = \\n{Q @ R}')\n\nQ = \n[[-0.70710678 -0.70710678]\n [-0.70710678  0.70710678]]\nR = \n[[-1.41421356  0.        ]\n [ 0.         -1.41421356]]\nQR = \n[[ 1.  1.]\n [ 1. -1.]]\n\n\n\n\n6.3.3 Spectral Decomposition\nThe spectral decomposition is a way to factor a symmetric matrix \\(\\mathbf{S}\\) into a product of an orthonormal matrix \\(\\mathbf{X}\\) and a diagonal matrix \\(\\mathbf{\\Lambda}\\),\n\\[\\mathbf{S} = \\mathbf{X \\Lambda X}^\\top.\\]\nThe matrix \\(\\mathbf{\\Lambda}\\) is called the eigenvalue matrix, and \\(\\mathbf{X}\\) is the eigenvector matrix. The diagonal entries of \\(\\mathbf{\\Lambda}\\) are called the eigenvalues of \\(\\mathbf{S}\\), denoted \\(\\lambda_i\\),\n\\[\\mathbf{\\Lambda} = \\text{diag}(\\lambda_0, \\lambda_1, \\cdots, \\lambda_n).\\]\nThe column vectors of \\(\\mathbf{X}\\) are called the eigenvectors of \\(\\mathbf{S}\\), denoted \\(\\mathbf{x}_i\\),\n\\[\\mathbf{X} = \\begin{pmatrix} \\mathbf{x}_0 & \\mathbf{x}_1 & \\cdots & \\mathbf{x}_{n-1} \\end{pmatrix}.\\]\nEigenvalues and eigenvectors arise from trying to find special “characteristic” lines in the vector space \\(\\mathbb{R}^n\\) that stay fixed when acted on by \\(\\mathbf{S}\\). Let \\(\\mathbf{x}\\) be the unit vector along one of these lines. Saying \\(\\mathbf{S}\\) can’t rotate \\(\\mathbf{x}\\) is equivalent to saying it can only scale \\(\\mathbf{x}\\) by some value \\(\\lambda\\). Finding these special characteristic lines is thus equivalent to solving the equation\n\\[\\mathbf{S}\\mathbf{x} = \\lambda \\mathbf{x}\\]\nfor \\(\\lambda\\) and \\(\\mathbf{x}\\). The vector \\(\\mathbf{x}\\) is the eigenvector (German for “characteristic vector”). The scalar \\(\\lambda\\) is its corresponding eigenvalue (German for “characteristic value”). We can rewrite this equation as \\((\\mathbf{S} - \\lambda \\mathbf{I})\\mathbf{x} = \\mathbf{0}\\), where \\(\\mathbf{0}\\) is the zero vector. Taking the determinant of \\(\\mathbf{S} - \\lambda \\mathbf{I}\\) and insisting it must be singular gives a polynomial equation, called the characteristic equation, that can (in principle) be solved for the eigenvalue \\(\\lambda\\),\n\\[\\text{det}(\\mathbf{S} - \\lambda \\mathbf{I}) = 0.\\]\nFor example, if \\(\\mathbf{S}\\) is a symmetric \\(2 \\times 2\\) matrix, we have\n\\[\n\\mathbf{S} =\n\\begin{pmatrix}\na & b \\\\\nb & d \\\\\n\\end{pmatrix} \\quad \\Longrightarrow \\quad\n\\mathbf{S} - \\lambda \\mathbf{I} =\n\\begin{pmatrix}\na-\\lambda & b \\\\\nb & d-\\lambda \\\\\n\\end{pmatrix} \\quad \\Longrightarrow \\quad\n\\text{det}(\\mathbf{S} - \\lambda \\mathbf{I}) = (a - \\lambda)(d - \\lambda) - b^2 = \\lambda^2 - (a + d)\\lambda + (ad-b^2) = 0.\n\\]\nNotice that \\(\\text{tr}(\\mathbf{S}) = a + d\\) and \\(\\text{det}(\\mathbf{S}) = ad-b^2\\), so the characteristic equation in this special \\(2 \\times 2\\) cases reduces to\n\\[\\lambda^2 - \\text{tr}(\\mathbf{S})\\lambda + \\text{det}(\\mathbf{S}) = 0.\\]\nThis is a quadratic equation whose solution is the two eigenvalues \\(\\lambda_0, \\lambda_1\\). Once the eigenvalues are known, they can be plugged back into the linear equation \\((\\mathbf{S} - \\lambda \\mathbf{I})\\mathbf{x} = \\mathbf{0}\\) to solve for the eigenvectors \\(\\mathbf{x}_0, \\mathbf{x}_1\\), e.g. using LU factorization.\nJust to put some numbers in, take the following specific \\(2 \\times 2\\) matrix\n\\[\n\\mathbf{S} =\n\\begin{pmatrix}\n2 & 1 \\\\\n1 & 2 \\\\\n\\end{pmatrix}.\n\\]\nSince \\(\\text{tr}(\\mathbf{S})=2+2=4\\) and \\(\\text{det}(\\mathbf{S})=2 \\cdot 2 - 1 \\cdot 1 = 3\\), the characteristic equation is\n\\[\\lambda^2 - 4\\lambda + 3 = 0 \\quad \\Longrightarrow \\quad (\\lambda-1)(\\lambda - 3) = 0 \\quad \\Longrightarrow \\quad \\lambda=1, 3.\\]\nThe eigenvalues for this matrix are thus \\(\\lambda_0 = 3\\) and \\(\\lambda_1 = 1\\). Note it’s conventional to order the eigenvalues from largest to smallest, though it isn’t required. The eigenvectors are gotten by solving the two systems\n\\[\n(\\mathbf{S} - \\lambda_0 \\mathbf{I})\\mathbf{x}_0 = \\mathbf{0} \\quad \\Longrightarrow \\quad\n\\begin{pmatrix}\n2-3 & 1 \\\\\n1 & 2-3 \\\\\n\\end{pmatrix}\n\\begin{pmatrix}\nx_0 \\\\\ny_0 \\\\\n\\end{pmatrix} =\n\\begin{pmatrix}\n0 \\\\\n0 \\\\\n\\end{pmatrix} \\quad \\Longrightarrow \\quad\n\\mathbf{x}_0 =\n\\frac{1}{\\sqrt{2}}\n\\begin{pmatrix}\n1 \\\\\n1 \\\\\n\\end{pmatrix} \\approx\n\\begin{pmatrix}\n0.707 \\\\\n0.707 \\\\\n\\end{pmatrix},\n\\]\n\\[\n(\\mathbf{S} - \\lambda_1 \\mathbf{I})\\mathbf{x}_1 = \\mathbf{0} \\quad \\Longrightarrow \\quad\n\\begin{pmatrix}\n2-1 & 1 \\\\\n1 & 2-1 \\\\\n\\end{pmatrix}\n\\begin{pmatrix}\nx_1 \\\\\ny_1 \\\\\n\\end{pmatrix} =\n\\begin{pmatrix}\n0 \\\\\n0 \\\\\n\\end{pmatrix} \\quad \\Longrightarrow \\quad\n\\mathbf{x}_1 =\n\\frac{1}{\\sqrt{2}}\n\\begin{pmatrix}\n1 \\\\\n-1 \\\\\n\\end{pmatrix} \\approx\n\\begin{pmatrix}\n0.707 \\\\\n-0.707 \\\\\n\\end{pmatrix}.\n\\]\nYou can easily check that \\(\\mathbf{x}_0\\) and \\(\\mathbf{x}_1\\) are orthogonal. Note the eigenvectors here have been normalized so \\(||\\mathbf{x}_0||=||\\mathbf{x}_1||=1\\). This isn’t required, but it’s the most common convention to ensure the eigenvector matrix \\(\\mathbf{X}\\) is a properly orthogonal.\nHere’s a plot of what this looks like. I’ll show that \\(\\mathbf{v}_0=\\sqrt{2}\\mathbf{x}_0=(1,1)\\) gets scaled by a factor of \\(\\lambda_0=3\\) when acted on by \\(\\mathbf{S}\\). Similarly, I’ll show that \\(\\mathbf{v}_1=\\sqrt{2}\\mathbf{x}_1=(1,-1)\\) gets scaled by a factor of \\(\\lambda_1=1\\) (i.e. not at all) when acted on by \\(\\mathbf{S}\\). Importantly, notice that \\(\\mathbf{S}\\) doesn’t rotate either vector. They stay along their characteristic lines, or eigenspaces, which in this example are the lines \\(y=\\pm x\\).\n\n\nCode\nS = np.array([\n    [2, 1], \n    [1, 2]])\nv0 = np.array([1, 1]).reshape(-1, 1)\nSv0 = S @ v0\nv1 = np.array([1, -1]).reshape(-1, 1)\nSv1 = S @ v1\nvectors = [x.flatten() for x in [v0, Sv0, v1, Sv1]]\nplot_vectors(\n    vectors, colors=['black', 'red', 'black', 'blue'], xlim=(-1, 4), ylim=(-2, 4), zorders=[1, 0, 2, 3], \n    labels=['$\\mathbf{v}_0$', '$\\mathbf{S}\\mathbf{v}_0$', '$\\mathbf{v}_1$', '$\\mathbf{S}\\mathbf{v}_1$'],\n    text_offsets=[[-0.45, 0.25], [0.05, 0.15], [0.1, -0.5], [0.05, 0.3]], \n    title='Eigenspaces of $\\mathbf{S}$')\n\n\n\n\n\n\n\n\n\nA result I won’t prove, called the spectral theorem, guarantees that the eigenvalues of a symmetric matrix will be real-valued, and that the eigenvectors will form an orthonormal basis for \\(\\mathbb{R}^n\\). This is why \\(\\mathbf{X}\\) ends up being an orthogonal matrix. The fact that the eigenvalues have to be real is why we can think of symmetric matrices as the matrix generalization of a real number.\nThe spectral decomposition \\(\\mathbf{S} = \\mathbf{X \\Lambda X}^\\top\\) is just a matrix way of writing the individual equations \\(\\mathbf{S}\\mathbf{x} = \\lambda \\mathbf{x}\\). Grouping the eigenvectors and eigenvalues into matrices, we can write these equations in one go as \\(\\mathbf{S}\\mathbf{X} = \\mathbf{\\Lambda} \\mathbf{X}\\), which is just the spectral decomposition.\nBack to our working example, putting the eigenvalues and eigenvectors into their respective matrices gives\n\\[\n\\mathbf{\\Lambda} =\n\\begin{pmatrix}\n3 & 0 \\\\\n0 & 1 \\\\\n\\end{pmatrix}, \\qquad\n\\mathbf{X} =\n\\frac{1}{\\sqrt{2}}\n\\begin{pmatrix}\n1 & 1 \\\\\n1 & -1 \\\\\n\\end{pmatrix}.\n\\]\nThat is, the symmetric matrix \\(\\mathbf{S}\\) factorizes into the spectral decomposition\n\\[\n\\mathbf{S} = \\mathbf{X \\Lambda X}^\\top =\n\\frac{1}{\\sqrt{2}}\n\\begin{pmatrix}\n1 & 1 \\\\\n1 & -1 \\\\\n\\end{pmatrix}\n\\begin{pmatrix}\n3 & 0 \\\\\n0 & 1 \\\\\n\\end{pmatrix}\n\\frac{1}{\\sqrt{2}}\n\\begin{pmatrix}\n1 & 1 \\\\\n1 & -1 \\\\\n\\end{pmatrix}.\n\\]\nWe can find the spectral decomposition of a symmetric matrix in numpy using np.linalg.eigh(S). Note that np.linalg.eig(S) will also work, but eigh is more efficient for symmetric matrices than eig. In either case, they return a pair of arrays, the first being the diagonals of \\(\\mathbf{\\Lambda}\\), the second being \\(\\mathbf{X}\\). I’ll also verify that the spectral decomposition indeed gives \\(\\mathbf{S}\\).\n\nS = np.array([[2, 1], \n              [1, 2]])\nlambdas, X = np.linalg.eigh(S)\nLambda = np.diag(lambdas)\nprint(f'Lambda = \\n{Lambda}')\nprint(f'X = \\n{X}')\nprint(f'X Lambda X^T = \\n{X @ Lambda @ X.T}')\n\nLambda = \n[[1. 0.]\n [0. 3.]]\nX = \n[[-0.70710678  0.70710678]\n [ 0.70710678  0.70710678]]\nX Lambda X^T = \n[[2. 1.]\n [1. 2.]]\n\n\nNotice something from the example I just worked. It turns out that \\(\\text{tr}(\\mathbf{S}) = 4 = \\lambda_0 + \\lambda_1\\) and \\(\\text{det}(\\mathbf{S}) = 3 = \\lambda_0 \\lambda_1\\). This fact turns out to always be true for \\(n \\times n\\) symmetric matrices, namely if \\(\\mathbf{S}\\) has eigenvalues \\(\\lambda_0, \\lambda_1, \\cdots, \\lambda_{n-1}\\), then\n\\[\\begin{align*}\n\\text{tr}(\\mathbf{S}) &= \\sum_{i=0}^{n-1} \\lambda_i = \\lambda_0 + \\lambda_1 + \\cdots + \\lambda_{n-1}, \\\\\n\\text{det}(\\mathbf{S}) &= \\prod_{i=0}^{n-1} \\lambda_i = \\lambda_0 \\cdot \\lambda_1 \\cdots \\lambda_{n-1}.\n\\end{align*}\\]\nThis fact implies that \\(\\mathbf{S}\\) will be invertible if and only if all the eigenvalues are non-zero, since otherwise we’d have \\(\\text{det}(\\mathbf{S})=0\\).\nGiven how important the spectral decomposition is to many applications, there are a lot of different algorithms for finding it, each with its own trade-offs. One popular algorithm for doing so is the QR algorithm. Roughly speaking, the QR algorithm works as follows:\n\nStart with \\(\\mathbf{S}_0 = \\mathbf{S}\\).\nFor some number of iterations \\(t=0,1,\\cdots, T-1\\) do the following:\n\nCalculate the QR factorization of \\(\\mathbf{S}_t\\): \\(\\mathbf{Q}_{t+1}, \\mathbf{R}_{t+1} = \\text{qr}(\\mathbf{S}_t)\\).\nUpdate \\(\\mathbf{S}_t\\) by reversing the factorization order: \\(\\mathbf{S}_{t+1} = \\mathbf{R}_{t+1} \\mathbf{Q}_{t+1}\\).\n\nTake \\(\\mathbf{\\Lambda} \\approx \\mathbf{S}_{T-1}\\) and \\(\\mathbf{X} \\approx \\mathbf{Q}_{T-1}\\).\n\nDue to the QR factorizations and matrix multiplications, this algorithm will be \\(O(n^3)\\) at each step, which all together gives a time complexity of \\(O(Tn^3)\\). It’s not at all obvious from what I’ve said why the QR algorithm even works. In fact, to work well it requires a few small modifications I won’t go into.\n\n\n6.3.4 Positive Definiteness\nThe eigenvalues of a symmetric matrix \\(\\mathbf{S}\\) are important because they in some sense specify how much \\(\\mathbf{S}\\) tends to stretch vectors in different directions. Most important for machine learning purposes though is the sign of the eigenvalues. The sign of the eigenvalues of a symmetric matrix essentially determine how hard it is to optimize a given function. This is especially relevant in machine learning, since training a model is all about optimizing the loss function of a model’s predictions against the data.\nIf \\(\\mathbf{S}\\) is \\(n \\times n\\), it will have \\(n\\) eigenvalues \\(\\lambda_0, \\lambda_1, \\cdots, \\lambda_{n-1}\\). Ignoring the fact that each eigenvalue can be zero, each one will be either positive or negative. That means the sequence of eigenvalues can have \\(2^n\\) possible arrangements of signs. For example, when \\(n=3\\), we could have any of the \\(2^3=8\\) possible sign arrangements for the eigenvalues \\((\\lambda_0, \\lambda_1, \\lambda_2)\\),\n\\[(+, +, +), \\ (+, +, -), \\ (+, -, +), \\ (-, +, +), \\ (+, -, -), \\ (-, +, -), \\ (-, -, +), \\ (-, -, -).\\]\nMost of these arrangements will have mixed signs, but there will always be exactly two arrangements that don’t, namely when the eigenvalues are all positive, and when the eigenvalues are all negative. These cases turn out to be special, as we’ll see.\nA symmetric matrix whose eigenvalues are all positive is called positive definite. A positive definite matrix is essentially the matrix equivalent of a positive real number. For this reason, we’ll write \\(\\mathbf{S} \\succ 0\\) to make the analogy of a scalar \\(s > 0\\) being positive. Positive definite matrices loosely speaking correspond to what are called convex functions, or “upward bowl shaped” functions.\nSimilarly, a symmetric matrix whose eigenvalues are all negative is called negative definite. A negative definite matrix is essentially the matrix equivalent of a negative real number. For this reason, we’ll write \\(\\mathbf{S} \\prec 0\\) to make the analogy of a scalar \\(s < 0\\) being negative. Negative definite matrices loosely speaking correspond to what are called concave functions, or “downward bowl shaped” functions.\nIf we now allow some of the eigenvalues to also be zero, we get the matrix equivalent of a non-negative and non-positive number, respectively. If the eigenvalues are all non-negative, the matrix is called positive semi-definite, written \\(\\mathbf{S} \\succcurlyeq 0\\). If the eigenvalues are all non-positive, it’s called negative semi-definite, written \\(\\mathbf{S} \\preccurlyeq 0\\).\nBy taking the spectral decomposition of \\(\\mathbf{S}\\) and expanding everything out, it’s possible to show that the following facts hold for any non-zero vector \\(\\mathbf{x} \\in \\mathbb{R}^n\\), - Positive definite: If \\(\\mathbf{S} \\succ 0\\), then \\(\\mathbf{x}^\\top \\mathbf{S} \\mathbf{x} > 0\\). - Negative definite: If \\(\\mathbf{S} \\prec 0\\), then \\(\\mathbf{x}^\\top \\mathbf{S} \\mathbf{x} < 0\\). - Positive semi-definite: If \\(\\mathbf{S} \\succcurlyeq 0\\), then \\(\\mathbf{x}^\\top \\mathbf{S} \\mathbf{x} \\geq 0\\). - Negative semi-definite: If \\(\\mathbf{S} \\preccurlyeq 0\\), then \\(\\mathbf{x}^\\top \\mathbf{S} \\mathbf{x} \\leq 0\\).\nExpressions of the form \\(\\mathbf{x}^\\top \\mathbf{S} \\mathbf{x}\\) are called quadratic forms. They’ll always be scalars, since all they’re doing is taking the dot product \\(\\mathbf{x} \\cdot \\mathbf{S} \\mathbf{x}\\). This is why these types of matrices are the matrix generalization of positive or negative numbers. If these dot products are positive for any vector, that’s about as good as we can do to say that the matrix itself is positive. Etc.\nAs you’d probably guess, the easiest way to determine if a symmetric matrix is any of these types of definite is to just calculate the eigenvalues and check their signs. For example, I showed before that the matrix\n\\[\n\\mathbf{S} =\n\\begin{pmatrix}\n2 & 1 \\\\\n1 & 2 \\\\\n\\end{pmatrix}\n\\]\nhas eigenvalues \\(\\lambda_0 = 3\\) and \\(\\lambda_1 = 1\\). Since both of these are positive, \\(\\mathbf{S}\\) is positive definite. It’s also positive semi-definite since they’re both non-negative. To check if a matrix is positive definite, for example, in numpy, you can do something like the following. Modify the inequality accordingly for the other types.\n\ndef is_positive_definite(S):\n    eigvals = np.linalg.eigvals(S)\n    return np.all(eigvals > 0)\n\nS = np.array([[2, 1], \n              [1, 2]])\nis_positive_definite(S)\n\nTrue\n\n\n\n\n6.3.5 Singular Value Decomposition\nThe spectral decomposition is mostly useful for square symmetric matrices. Yet, the properties of eigenvalues and eigenvectors seem to be incredibly useful for understanding how a matrix behaves. They say something useful about the characteristic scales and directions of a matrix and its underlying linear operator. It turns out we can generalize the spectral decomposition to arbitrary matrices, but with some slight modifications. This modified factorization is called the singular value decomposition, or SVD for short.\nSuppose \\(\\mathbf{A}\\) is some arbitrary \\(m \\times n\\) matrix. It turns out we can always factor \\(\\mathbf{A}\\) into a product of the form\n\\[\\mathbf{A} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^\\top,\\]\nwhere \\(\\mathbf{U}\\) is an \\(m \\times m\\) orthogonal matrix called the left singular matrix, \\(\\mathbf{V}\\) is a different \\(n \\times n\\) orthogonal matrix called the left singular matrix, and \\(\\mathbf{\\Sigma}\\) is an \\(m \\times n\\) diagonal matrix called the singular value matrix.\nThe singular value matrix \\(\\mathbf{\\Sigma}\\) is a rectangular diagonal matrix. This means the diagonal will only have \\(k=\\min(m, n)\\) entries. The diagonal entries are called the singular values of \\(\\mathbf{A}\\), usually denoted \\(\\sigma_0, \\sigma_1, \\cdots, \\sigma_{k-1}\\). Unlike eigenvalues, singular values are required to be non-negative.\nThe column vectors of \\(\\mathbf{U}\\) and \\(\\mathbf{V}\\) are called the left and right singular vectors respectively. Since both matrices are orthogonal, their singular vectors will form an orthonormal basis for \\(\\mathbb{R}^m\\) and \\(\\mathbb{R}^n\\) respectively.\nNotice that whereas with the spectral composition \\(\\mathbf{S} = \\mathbf{X} \\mathbf{\\Lambda} \\mathbf{X}^\\top\\) has only a single orthogonal matrix \\(\\mathbf{X}\\), the SVD has two different orthogonal matrices \\(\\mathbf{U}\\) and \\(\\mathbf{V}\\) to worry about, and each one is a different size. Also, while \\(\\mathbf{\\Lambda}\\) can contain eigenvalues of any sign, \\(\\mathbf{\\Sigma}\\) can only contain singular values that are nonnegative.\nNonetheless, the two factorizations are related by the following fact: The singular values of \\(\\mathbf{A}\\) are the eigenvalues of the symmetric matrix \\(\\mathbf{S} = \\mathbf{A}^\\top \\mathbf{A}\\). Not only that, they’re also the eigenvalues of the transposed symmetric matrix \\(\\mathbf{S}^\\top = \\mathbf{A} \\mathbf{A}^\\top\\). This fact gives one way you could actually calculate the SVD. The singular value matrix \\(\\mathbf{\\Sigma}\\) will just be the eigenvalue matrix of \\(\\mathbf{S}\\) (and \\(\\mathbf{S}^\\top\\)). The left singular matrix \\(\\mathbf{U}\\) will be the eigenvector matrix of \\(\\mathbf{S}\\). The right singular matrix \\(\\mathbf{V}\\) will be the eigenvector matrix of \\(\\mathbf{S}^\\top\\). Very roughly speaking, this is what many SVD algorithms use, e.g. by applying the QR algorithm on both \\(\\mathbf{S}\\) and \\(\\mathbf{S}^\\top\\).\nCalculating the SVD by hand is much more of a pain than the spectral decomposition is because you have to do it twice, once on \\(\\mathbf{S}\\) and once on \\(\\mathbf{S}^\\top\\). I’ll spare you the agony of this calculation, and just use numpy to calculate the SVD of the following matrix,\n\\[\n\\mathbf{A} =\n\\begin{pmatrix}\n1 & 1 \\\\\n1 & 0 \\\\\n1 & -1 \\\\\n\\end{pmatrix}.\n\\]\nWe can use np.linalg.svd(A) to calculate the SVD of \\(\\mathbf{A}\\). It’ll return a triplet of arrays, in order \\(\\mathbf{U}\\), the diagonal of \\(\\mathbf{\\Sigma}\\), and \\(\\mathbf{V}^T\\). Note to get the full \\(\\mathbf{\\Sigma}\\) you can’t just use np.diag since \\(\\mathbf{\\Sigma}\\) won’t be square here. You have to add a row of zeros after to make the calculation work out. I’ll do this just using a loop and filling in the diagonals manually.\nNotice that the two singular values are positive, \\(\\sigma_0 = \\sqrt{3} \\approx 1.732\\) and \\(\\sigma_1 = \\sqrt{2} \\approx 1.414\\). In this example, the right singular matrix \\(\\mathbf{V}\\) is just \\(\\text{diag}(-1, 1)\\), which is clearly orthogonal. The left singular matrix \\(\\mathbf{U}\\) is a little harder to see, but it’s also orthogonal. Finally, the product \\(\\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V}^\\top\\) indeed gives \\(\\mathbf{A}\\).\n\nA = np.array([\n    [1, 1],\n    [1, 0],\n    [1, -1]])\nm, n = A.shape\nk = min(m, n)\nU, sigma, Vt = np.linalg.svd(A)\nSigma = np.zeros((m, n))\nfor i in range(k):\n    Sigma[i, i] = sigma[i]\nUSVt = U @ Sigma @ Vt\nprint(f'U = \\n{U.round(10)}')\nprint(f'Sigma = \\n{Sigma.round(10)}')\nprint(f'V = \\n{Vt.T.round(10)}')\nprint(f'U Sigma V^T = \\n{USVt.round(10)}')\n\nU = \n[[-0.57735027  0.70710678  0.40824829]\n [-0.57735027  0.         -0.81649658]\n [-0.57735027 -0.70710678  0.40824829]]\nSigma = \n[[1.73205081 0.        ]\n [0.         1.41421356]\n [0.         0.        ]]\nV = \n[[-1.  0.]\n [-0.  1.]]\nU Sigma V^T = \n[[ 1.  1.]\n [ 1.  0.]\n [ 1. -1.]]\n\n\nTo give you an intuition is to what the SVD is doing, suppose \\(\\mathbf{x} \\in \\mathbb{R}^n\\) is some size-\\(n\\) vector. Suppose we want to operate on \\(\\mathbf{x}\\) with \\(\\mathbf{A}\\) to get a new vector \\(\\mathbf{v} = \\mathbf{A}\\mathbf{x}\\). Writing \\(\\mathbf{A} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^\\top\\), we can do this operation in a sequence of three successive steps:\n\nCalculate \\(\\mathbf{y} = \\mathbf{V}^\\top \\mathbf{x}\\): The output is also a size-\\(n\\) vector \\(\\mathbf{y} \\in \\mathbb{R}^n\\). Since \\(\\mathbf{V}\\) is orthogonal, this action can only rotate (or reflect) \\(\\mathbf{x}\\) by some angle in space.\nCalculate \\(\\mathbf{z} = \\mathbf{\\Sigma}\\mathbf{y}\\): The output is now a size-\\(k\\) vector \\(\\mathbf{z} \\in \\mathbb{R}^k\\). Since \\(\\mathbf{\\Sigma}\\) is diagonal, it can only stretch \\(\\mathbf{y}\\) along the singular directions of \\(\\mathbf{V}\\), not rotate it.\nCalculate \\(\\mathbf{v} = \\mathbf{U}\\mathbf{z}\\): The output is now a size-\\(m\\) vector \\(\\mathbf{v} \\in \\mathbb{R}^m\\). Since \\(\\mathbf{U}\\) is orthogonal, this action can only rotate (or reflect) \\(\\mathbf{z}\\) by some angle in space.\n\nThe final output is thus a vector \\(\\mathbf{v} = \\mathbf{A}\\mathbf{x}\\) that first got rotated in \\(\\mathbb{R}^n\\), then scaled in \\(\\mathbb{R}^k\\), then rotated again in \\(\\mathbb{R}^m\\). So you can visualize this better let’s take a specific example. To make everything show up on one plot I’ll choose a \\(2 \\times 2\\) matrix, so \\(m=n=k=2\\), for example\n\\[\n\\mathbf{A} =\n\\begin{pmatrix}\n1 & 2 \\\\\n1 & 1 \\\\\n\\end{pmatrix}.\n\\]\nThe singular values to this matrix turn out to be \\(\\sigma_0 \\approx 2.618\\) and \\(\\sigma_1 \\approx 0.382\\). What I’m going to do is randomly sample a bunch of unit vectors \\(\\mathbf{x}\\), then apply the successive operations above to each vector. The original vectors \\(\\mathbf{x}\\) are shown in red, the vectors \\(\\mathbf{y} = \\mathbf{V}^\\top \\mathbf{x}\\) in blue, the vectors \\(\\mathbf{z} = \\mathbf{\\Sigma}\\mathbf{y}\\) in green, and finally the vectors \\(\\mathbf{v} = \\mathbf{U}\\mathbf{z}\\) in black. Notice that the red vectors just kind of fill in the unit circle, since they’re all unit vectors of length one. The blue vectors also fill in the unit circle, since \\(\\mathbf{V}^\\top\\) can only rotate vectors, not stretch them. The green vectors then get stretched out into an elliptical shape due to \\(\\mathbf{\\Sigma}\\). The distortion of the ellipse depends on the “distortion ratio” \\(\\frac{\\sigma_0}{\\sigma_1} \\approx 6.85\\). This means one axis gets stretched about \\(6.85\\) times as much as the other. Finally, since \\(\\mathbf{U}\\) can only rotate vectors, the black vectors then rotate these stretched vectors into their final position.\n\nA = np.array([\n    [1, 2],\n    [1, 1]])\nm, n = A.shape\nk = min(m, n)\nU, sigma, Vt = np.linalg.svd(A)\nSigma = np.diag(sigma)\nprint(f'Sigma = \\n{Sigma.round(10)}')\n\nSigma = \n[[2.61803399 0.        ]\n [0.         0.38196601]]\n\n\n\nplot_svd(A)\n\n\n\n\n\n\n\n\nThe “distortion ratio” \\(\\frac{\\sigma_0}{\\sigma_1}\\) mentioned above can actually be used as a measure of how invertible a matrix is. It’s called the condition number, denoted \\(\\kappa\\). For a general \\(n \\times n\\) matrix, the condition number is defined as the ratio of the largest to the smallest singular value,\n\\[\\kappa = \\frac{\\sigma_0}{\\sigma_{k-1}}.\\]\nThe higher the condition number is, the harder it is to invert \\(\\mathbf{A}\\). A condition number of \\(\\kappa=1\\) is when the singular values are the same. These are easiest to invert. Matrices with low \\(\\kappa\\) are called called well-conditioned matrices. The identity matrix has \\(\\kappa=1\\), for example. If one of the singular values is \\(0\\) then \\(\\kappa\\) will be infinite, meaning the matrix isn’t invertible at all. Matrices with high \\(\\kappa\\) are called ill-conditioned matrices. For this reason, the condition number is very often used in calculations when it’s important to make sure that \\(\\mathbf{A}\\) isn’t singular or close to singular. In numpy, you can calculate the condition number of a matrix directly by using np.linalg.cond(A).\n\n\n6.3.6 Low-Rank Approximations\nThe SVD is useful for many reasons. In fact, it’s probably the single most useful factorization in all of applied linear algebra. One reason this is true is because every matrix has one. When in doubt, if you can’t figure out how to do something with a matrix, you can take its SVD and try to work with those three matrices one-by-one. While that’s nice, the more useful application of the SVD to machine learning is that it’s a good way to compress or denoise data. To see why we need to look at the SVD in a slightly different way.\nSuppose \\(\\mathbf{A}\\) is some \\(m \\times n\\) matrix. Suppose \\(\\mathbf{u}_0, \\mathbf{u}_1, \\cdots, \\mathbf{u}_{m-1}\\) are the column vectors of \\(\\mathbf{U}\\), and \\(\\mathbf{v}_0, \\mathbf{v}_1, \\cdots, \\mathbf{v}_{n-1}\\) are the column vectors of \\(\\mathbf{V}\\). Suppose \\(\\sigma_0, \\sigma_1, \\cdots, \\sigma_{k-1}\\) are the singular values of \\(\\mathbf{A}\\), by convention ordered from largest to smallest. Then writing out the SVD in terms of the column vectors, and multiplying everything out matrix multiplication style, we have\n\\[\n\\mathbf{A} = \\mathbf{U} \\mathbf{\\Sigma} \\mathbf{V}^\\top =\n\\begin{pmatrix}\n\\mathbf{u}_0 & \\mathbf{u}_1 & \\cdots & \\mathbf{u}_{m-1}\n\\end{pmatrix}\n\\text{diag}\\big(\\sigma_0, \\sigma_1, \\cdots, \\sigma_{k-1}\\big)\n\\begin{pmatrix}\n\\mathbf{v}_0^\\top \\\\ \\mathbf{v}_1^\\top \\\\ \\cdots \\\\ \\mathbf{v}_{n-1}^\\top\n\\end{pmatrix} =\n\\sum_{i=0}^{k-1} \\sigma_i \\mathbf{u}_i \\mathbf{v}_i^\\top =\n\\sigma_0 \\mathbf{u}_0 \\mathbf{v}_0^\\top + \\sigma_1 \\mathbf{u}_1 \\mathbf{v}_1^\\top + \\cdots + \\sigma_{k-1} \\mathbf{u}_{k-1} \\mathbf{v}_{k-1}^\\top.\n\\]\nThat is, we can write \\(\\mathbf{A}\\) as a sum of outer products over the singular vectors, each weighted by its singular value. That’s fine. But why is it useful? All I did was re-write the SVD in a different form, after all. The gist of it is that we can use this formula to approximate \\(\\mathbf{A}\\) by a lower-dimensional matrix. Supposing we only kept the first \\(d < k\\) terms of the right-hand side and dropped the rest, we’d have\n\\[\\mathbf{A} \\approx \\mathbf{U}_d \\mathbf{\\Sigma}_d \\mathbf{V}_d^\\top = \\sigma_0 \\mathbf{u}_0 \\mathbf{v}_0^\\top + \\sigma_1 \\mathbf{u}_1 \\mathbf{v}_1^\\top + \\cdots + \\sigma_{d-1} \\mathbf{u}_{d-1} \\mathbf{v}_{d-1}^\\top.\\]\nThis approximation will be a rank-\\(d\\) matrix again of size \\(m \\times n\\). It’s rank \\(d\\) because it’s a sum of \\(d\\) “independent” rank-1 matrices. When \\(d<<k\\), this is called the low-rank approximation. While this approximation is low rank it still has size \\(m \\times n\\). It’s the inner dimensions that got cut from \\(k\\) to \\(d\\), not the outer dimensions. To get a true low-dimensional approximation, we need to multiply both sides by \\(\\mathbf{V}_d\\),\n\\[\\mathbf{A}_d =  \\mathbf{A} \\mathbf{V}_d = \\mathbf{U}_d \\mathbf{\\Sigma}_d.\\]\nWe’re now approximating the \\(m \\times n\\) matrix \\(\\mathbf{A}\\) with an \\(m \\times d\\) matrix I’ll call \\(\\mathbf{A}_d\\). Said differently, we’re compressing the \\(n\\) columns of \\(\\mathbf{A}\\) down to just \\(d<<n\\) columns. Note that we’re not dropping the last \\(n-d\\) columns, we’re building new columns that best approximate all of the old columns.\nLet’s try to understand why low rank approximations are useful, and that they indeed do give good approximations to large matrices. To do so, consider the following example. I’m going to load some data from a well-known dataset in machine learning called MNIST. It’s a dataset of images of handwritten digits. When the low-rank approximation is applied to data, it’s called principle components analysis, or PCA. PCA is probably the most fundamental dimension reduction algorithm, a way of compressing high-dimensional data into lower-dimensional data.\nEach image is size \\(28 \\times 28\\), which flatten out into \\(n = 28 \\cdot 28 = 784\\) dimensions. I’ll load \\(m=1000\\) random samples from the MNIST dataset. This will create a matrix \\(\\mathbf{A}\\) of shape \\(1000 \\times 784\\). I’ll go ahead and calculate the SVD to get \\(\\mathbf{U}\\), \\(\\mathbf{\\Sigma}\\), and \\(\\mathbf{V}^\\top\\). In this case, \\(k=\\min(m,n)=784\\), so these matrices will have sizes \\(1000 \\times 1000\\), \\(1000 \\times 784\\), and \\(784 \\times 784\\) respectively. As I mentioned before, numpy only returns the non-zero diagonals of \\(\\mathbf{\\Sigma}\\), which is a size \\(k=784\\) vector of the singular values. Thankfully, that’s all we’ll need here.\n\nm = 1000\nA = sample_mnist(size=m)\nU, sigma, Vt = np.linalg.svd(A)\nA.shape, U.shape, sigma.shape, Vt.shape\nprint(f'A.shape = {A.shape}')\nprint(f'U.shape = {U.shape}')\nprint(f'sigma.shape = {sigma.shape}')\nprint(f'Vt.shape = {Vt.shape}')\n\nA.shape = (1000, 784)\nU.shape = (1000, 1000)\nsigma.shape = (784,)\nVt.shape = (784, 784)\n\n\nThink of each row of \\(\\mathbf{A}\\) as representing a single image in the dataset, and each column of \\(\\mathbf{A}\\) as representing a single pixel of the image.\nSince these are images, I might as well show you what they look like. To do that, just pick a random row from the matrix. Each row will be a flattened image. To turn it into an image, we can just reshape the row to have shape \\(28 \\times 28\\), then plot it using plt.imshow. Below, I’m picking off the first row, which turns out to be an image of a handwritten \\(0\\).\n\n\nCode\nimg = A[0, :].reshape(28, 28)\nplt.imshow(img, cmap='Greys')\nplt.axis('off')\nplt.show()\n\n\n\n\n\n\n\n\n\nLet’s start by taking \\(d=2\\). Why? Because when \\(d=2\\) we can plot each image as a point in the xy-plane! This suggests a powerful application of the low-rank approximation, to visualize high-dimensional data. To calculate \\(\\mathbf{A}_d\\), we’ll need to truncate \\(\\mathbf{U}\\), \\(\\mathbf{\\Sigma}\\), and \\(\\mathbf{V}^\\top\\). To make the shapes come out right, we’ll want to drop the first \\(d\\) columns of \\(\\mathbf{U}\\) and the first \\(d\\) rows of \\(\\mathbf{V}^\\top\\). Once we’ve got these, we can calculate \\(\\mathbf{A}_d\\), which in this case will be size \\(1000 \\times 2\\).\n\nd = 2\nU_d, sigma_d, Vt_d = U[:, :d], sigma[:d], Vt[:d, :]\nA_d = A @ Vt_d.T\nprint(f'U_d.shape = {U_d.shape}')\nprint(f'sigma_d = {sigma_d}')\nprint(f'Vt_d.shape = {Vt_d.shape}')\nprint(f'A_d.shape = {A_d.shape}')\n\nU_d.shape = (1000, 2)\nsigma_d = [197.89062659  66.60026657]\nVt_d.shape = (2, 784)\nA_d.shape = (1000, 2)\n\n\nNow we have \\(m=1000\\) “images”, each with \\(d=2\\) “variables”. This means we can plot them in the xy-plane, taking \\(x\\) to be the first column A_d[:, 0], and \\(y\\) to be the second column A_d[:, 1]. Here’s a scatter plot of all images projected down to 2 dimensions. I can’t make out any patterns in the plot, and you probably can’t either. But at least we’ve found an interesting and sometimes useful way to visualize high-dimensional data.\n\n\nCode\nplt.scatter(A_d[:, 0], A_d[:, 1], s=1, alpha=0.8)\nplt.xticks([])\nplt.yticks([])\nplt.title(f'{m} MNIST Images')\nplt.show()\n\n\n\n\n\n\n\n\n\nHow good is our approximation? We can use the singular values to figure this out. In the low rank approximation, we’re keeping \\(d\\) singular values and dropping the remaining \\(k-d\\). Throwing away those remaining singular values is throwing away information about our original matrix \\(\\mathbf{A}\\). To figure out how much information we’re keeping in our approximation, we can just look at the ratio of the sum of singular values kept to the total sum of all singular values,\n\\[R_d = \\frac{\\sigma_0 + \\sigma_1 + \\cdots + \\sigma_{d-1}}{\\sigma_0 + \\sigma_1 + \\cdots + \\sigma_{k-1}}.\\]\nThis ratio is sometimes called the explained variance for reasons I’ll get into in a future lesson.\nIn the rank-2 case I just worked out, this ratio turns out to be \\(R_2 = \\frac{\\sigma_0 + \\sigma_1}{\\sum \\sigma_i} \\approx 0.087\\). That is, this rank-2 approximation is preserving about 8.7% of the information in the original data.\n\nR_d = np.sum(sigma_d) / np.sum(sigma)\nprint(f'R_d = {R_d}')\n\nR_d = 0.08740669535517863\n\n\nThat’s pretty bad. We can do better. Let’s take \\(d=100\\) and see how well that does. Of course, we won’t be able to plot the data in the xy-plane anymore, but it’ll better represent the original data. We’re now at \\(R_d \\approx 0.643\\), which means we’re preserving about 64.3% of the information in the original data, and we’re doing it using only \\(\\frac{100}{784} \\approx 0.127\\), or 12.7% of the total columns of \\(\\mathbf{A}\\).\n\nd = 100\nU_d, sigma_d, Vt_d = U[:, :d], sigma[:d], Vt[:d, :]\nA_d = A @ Vt_d.T\nprint(f'A_d.shape = {A_d.shape}')\n\nA_d.shape = (1000, 100)\n\n\n\nR_d = np.sum(sigma_d) / np.sum(sigma)\nprint(f'R_d = {R_d}')\n\nR_d = 0.6433751746962163\n\n\nAnother way to see how good our compression is is to “unproject” the compressed images and plot them. To unproject \\(\\mathbf{A}_d\\), just multiply on the right again by \\(\\mathbf{V}^\\top\\) to get the original \\(m \\times n\\) matrix approximation again,\n\\[\\mathbf{A} \\approx \\mathbf{A}_d \\mathbf{V}^\\top.\\]\nOnce I’ve done that, I can just pluck a random row from the approximation, resize it, and plot it using plt.imshow, just like before. Notice this time we can still clearly see the handwritten \\(0\\), but it’s a bit grainer than it was before. The edges aren’t as sharp. Nevertheless, we can still make out the digit pretty solidly.\n\n\nCode\nimg = (A_d @ Vt_d)[0, :].reshape(28, 28)\nplt.imshow(img, cmap='Greys')\nplt.axis('off')\nplt.show()\n\n\n\n\n\n\n\n\n\nBut why is this approach good for compression anyway? After all, we still have to unproject the rows back into the original \\(m \\times n\\) space. Maybe think about it this way. If you just stored the full matrix \\(\\mathbf{A}\\), you’d have tot store \\(m \\cdot n\\) total numbers. In this example, that’s \\(1000 \\cdot 784 = 784000\\) numbers you’d have to store in memory.\nBut suppose now we do the low rank approximation. What we can then do is just store \\(\\mathbf{A}_d\\) and \\(\\mathbf{V}\\) instead. That means we’d instead store \\(m \\cdot d + d \\cdot n\\) total numbers. In our example, that comes out to \\(1000 \\cdot 100 + 100 \\cdot 784 = 100000 + 78400 = 178400\\), which is only \\(\\frac{178400}{784000} \\approx 0.227\\) or 22.7% of the numbers we’d have to store otherwise. We’ve thus compressed our data by a factor of about \\(\\frac{1}{0.227} \\approx 4.4\\). That’s a 4.4x compression of the original images.\nNow, this kind of PCA compression isn’t perfect, or lossless, since we can’t recover the original images exactly. But we can still recover the most fundamental features of the image, which in this case are the handwritten digits. This kind of compression is lossy, since it irreversibly throws away some information in the original data. Yet, it still maintains enough information to be useful in many settings."
  },
  {
    "objectID": "notebooks/multivariate-calculus.html#multivariate-differentiation",
    "href": "notebooks/multivariate-calculus.html#multivariate-differentiation",
    "title": "7  Multivariate Calculus",
    "section": "7.1 Multivariate Differentiation",
    "text": "7.1 Multivariate Differentiation\nJust as we can differentiate univariate functions like \\(y=f(x)\\), we can also differentiate multivariate functions like \\(z=f(x,y)\\). The main difference is that we can take derivatives of many inputs variables, not just one.\n\n7.1.1 The Gradient\nSuppose \\(z=f(x,y)\\) and we want to ask the question, how does \\(z\\) change if we change \\(x\\) by an infinitesimal amount \\(dx\\), holding \\(y\\) constant? Evidently it would be \\(z + dz = f(x+dx, y)\\). If we pretend \\(y\\) is constant, this would mean\n\\[dz = f(x+dx, y) - f(x, y).\\]\nDividing both sides by \\(dx\\) we’d get something like a derivative. But it’s not the derivative since we’re only changing \\(x\\) and fixing \\(y\\). For this reason it’s called the partial derivative of \\(z\\) with respect to \\(x\\), and typically written with funny \\(\\partial\\) symbols instead of \\(d\\) symbols,\n\\[\\frac{\\partial z}{\\partial x} = \\frac{f(x+dx, y) - f(x, y)}{dx}.\\]\nSimilarly, we can ask the dual question, how does \\(z\\) change if we change \\(y\\) by an infinitesimal amount \\(dy\\), holding \\(x\\) constant? By the same logic, we’d get\n\\[dz = f(x, y + dy) - f(x, y),\\]\nand dividing by \\(dy\\) would give the partial derivative of \\(z\\) with respect to \\(y\\),\n\\[\\frac{\\partial z}{\\partial y} = \\frac{f(x, y + dy) - f(x, y)}{dy}.\\]\nBut these don’t tell us everything. We want to know how \\(z\\) changes if we change \\(x\\) and \\(y\\) arbitrarily, not if we hold one of them constant. That is, we want the full \\(dz\\). In the case when \\(y=f(x)\\), we saw that \\(dy=\\frac{dy}{dx}dx\\). If we only change \\(x\\), evidently \\(dz = \\frac{\\partial z}{\\partial x} dx\\). Similarly if we only change \\(y\\), then \\(dz = \\frac{\\partial z}{\\partial y} dy\\). It seems like if we want to change both, we should add these two effects together,\n\\[dz = \\frac{\\partial z}{\\partial x} dx + \\frac{\\partial z}{\\partial y} dy.\\]\nThis equation is called the bivariate chain rule. Since it depends on changes in both \\(x\\) and \\(y\\), \\(dz\\) is called the total differential. The chain rule tells us everything we need to know about how \\(z\\) changes when either \\(x\\) or \\(y\\) are perturbed by some small amount. The amount that \\(z\\) gets perturbed is \\(dz\\).\nIf we have a composite function like, say, \\(z=f(x,y)\\), \\(x=g(u, v)\\), \\(y=h(u, v)\\), we can do just like in the univariate chain rule and divide the total differential by \\(du\\) or \\(dv\\) to get the chain rule in partial derivative form,\n\\[\\frac{\\partial z}{\\partial u} = \\frac{\\partial z}{\\partial x}\\frac{\\partial x}{\\partial u} + \\frac{\\partial z}{\\partial y}\\frac{\\partial y}{\\partial u},\\]\n\\[\\frac{\\partial z}{\\partial v} = \\frac{\\partial z}{\\partial x}\\frac{\\partial x}{\\partial v} + \\frac{\\partial z}{\\partial y}\\frac{\\partial y}{\\partial v}.\\]\nThis is the form in which the bivariate chain rule usually appears in deep learning, but with many more variables.\nIt’s interesting to write this formula as a dot product of two vectors. If we define two vectors as follows,\n\\[\\frac{dz}{d\\mathbf{x}}=\\big(\\frac{\\partial z}{\\partial x}, \\frac{\\partial z}{\\partial y}\\big),\\]\n\\[d\\mathbf{x} = (dx, dy),\\]\nthen the chain rule would say\n\\[dz = \\frac{dz}{d\\mathbf{x}} \\cdot d\\mathbf{x}.\\]\nThis looks just like the equation for the ordinary derivative, \\(dy=\\frac{dy}{dx}dx\\),except there’s a dot product of vectors here.\nThe vector \\(\\frac{dz}{d\\mathbf{x}}\\) looks like the ordinary derivative, but it’s now a vector of partial derivatives. It’s called the gradient of \\(z=f(x,y)\\).\nIn many texts, the gradient is often written with the funny symbol \\(\\nabla f(x,y)\\). Other notations used are \\(\\frac{d}{d\\mathbf{x}}f(\\mathbf{x})\\), or \\(\\mathbf{f}'(\\mathbf{x})\\). I’ll often instead use the simpler notation of \\(\\mathbf{g}\\) or \\(\\mathbf{g}(x,y)\\) for the gradient when it’s clear what the function is we’re differentiating. All of these notations can represent the same gradient vector,\n\\[\\mathbf{g} = \\nabla f(\\mathbf{x}) = \\mathbf{f}'(\\mathbf{x}) = \\frac{d}{d\\mathbf{x}}f(\\mathbf{x}) = \\frac{dz}{d\\mathbf{x}}.\\]\nNote it’s very common in calculus and applications to abuse the difference between points and vectors. We might write \\(f(x, y)\\), or just \\(f(\\mathbf{x})\\), where it’s understood \\(\\mathbf{x}\\) is the vector \\(\\mathbf{x}=(x,y)\\). We’ll do this a lot. There’s no real difference between them.\nLet’s do an example. Consider the function \\(z=x^2+y^2\\). This function has a surface that looks like a bowl.\n\n\nCode\nx = np.linspace(-10, 10, 100)\ny = np.linspace(-10, 10, 100)\nf = lambda x, y: x**2 + y**2\n\nplot_function_3d(x, y, f, title='$z=x^2+y^2$', titlepad=10, labelpad=5, ticks_every=[4, 4, 50], dist=12)\n\n\n\n\n\n\n\n\n\nSuppose we treat \\(y\\) as constant, say \\(y=2\\). If we nudge \\(x\\) to \\(x+dx\\), then \\(z\\) would get nudged to\n\\[z+dz = f(x+dx,y) = (x+dx)^2 + y^2 = (x^2 + 2xdx + dx^2) + y^2 \\approx z + 2xdx.\\]\nThat is, \\[\\frac{\\partial z}{\\partial x} = 2x.\\]\nThis is exactly what we got before in the univariate case with \\(f(x)=x^2\\). This makes since. By treating \\(y\\) as constant we’re effectively pretending it’s not there in the calculation, which makes it act like we’re taking the 1D derivative \\(z=x^2\\).\nSince \\(z=x^2+y^2\\) is symmetric in \\(x\\) and \\(y\\), the exact same argument above would show\n\\[\\frac{\\partial z}{\\partial y} = 2y.\\]\nThe gradient vector would thus be \\[\\frac{dz}{d\\mathbf{x}} = (2x, 2y) = 2\\mathbf{x}, \\quad \\text{where }\\mathbf{x} = (x,y).\\]\nThe gradient looks exactly like the 1D version where \\(y=x^2\\) and \\(\\frac{dy}{dx}=2x\\), except there’s a vector \\(\\mathbf{x}\\) instead.\nJust as with the ordinary derivative, we can see that the gradient is a function of its inputs. The difference though is the gradient is a vector-valued function. Its output is a vector, not a scalar.\nNumerical differentiation extends naturally to the bivariate case as well. We can calculate partial derivatives numerically straight from their definitions, using reasonably small values like \\(dx=dy=10^{-5}\\). To get the gradient, just calculate the partials numerically and put them into an array.\nHere’s an example. I’ll calculate the partials \\(\\frac{\\partial z}{\\partial x}, \\frac{\\partial z}{\\partial y}\\) at the point \\(x_0=1, y_0=1\\). The partials are given by dzdx and dzdy respectively, and the gradient vector by grad. Notice the error is again on the order of \\(dx\\) and \\(dy\\), hence we get good agreement with the above equation when \\(x_0=1, y_0=1\\).\n\n\nCode\nx0 = y0 = 1\ndx = dy = 1e-5\n\ndzdx = (f(x0 + dx, y0) - f(x0, y0)) / dx\ndzdy = (f(x0, y0 + dy) - f(x0, y0)) / dy\n\ngrad = [dzdx, dzdy]\nprint(f'grad = {grad}')\n\n\ngrad = [2.00001000001393, 2.00001000001393]\n\n\n\n\n7.1.2 Visualizing Gradients\nIn the case of the ordinary univariate derivative \\(\\frac{dy}{dx}\\), we could think of it geometrically as the slope of the tangent line to \\(y=f(x)\\) at a point \\((x_0,y_0)\\). We can do something similar for the gradient \\(\\frac{dz}{d\\mathbf{x}}\\) by thinking of it as the vector of slopes defining a tangent plane to \\(z=f(\\mathbf{x})\\) at a point \\((\\mathbf{x}_0, z_0)\\).\nSuppose \\(z=f(x,y)\\). Let \\((x_0,y_0,z_0) \\in \\mathbb{R}^3\\) be a point in 3D space, with \\(z_0=f(x_0,y_0)\\). This is just a point on the 2D surface of \\(z=f(x,y)\\). Now, it doesn’t make much sense to talk about a single line that hugs this point, since there can now be infinitely many lines that hug that point. What we instead want to do is think about a plane that hugs the surface. This will be called the tangent plane. It’s given by the first order perturbation\n\\[z = z_0 + \\frac{\\partial}{\\partial x}f(x_0,y_0) (x - x_0) + \\frac{\\partial}{\\partial y}f(x_0,y_0) (y - y_0),\\]\nor in vector notation just, \\[z = z_0 + \\frac{d}{d\\mathbf{x}} f(\\mathbf{x}_0) \\cdot (\\mathbf{x} - \\mathbf{x}_0), \\quad \\text{or} \\quad z = z_0 + \\mathbf{g}(\\mathbf{x}_0) \\cdot (\\mathbf{x} - \\mathbf{x}_0).\\]\nThis tangent plane will hug the surface of the function at the point \\((x_0,y_0,z_0)\\).\nHere’s an example, where I’ll calculate the tangent plane to \\(z=x^2+y^2\\) at the point \\((1,1)\\). Since I showed above that the gradient in this case is \\((2x, 2y)\\), the tangent line becomes \\(z=2 + 2(x-1) + 2(y-1)\\). Everything is done in an analogous way to the tangent line calculation from before.\n\n\nCode\nf = lambda x, y: x**2 + y**2\ndfdx = lambda x, y: (2 * x, 2 * y)\n\nx0 = y0 = 1\nz0 = f(x0, y0)\n\nx = np.linspace(-2 * x0, 2 * x0, 100)\ny = np.linspace(-2 * y0, 2 * y0, 100)\n\nf_tangent = lambda x, y: 2 * (x - x0) + 2 * (y - y0) + 2\n\nplot_tangent_plane(x, y, x0, y0, f, f_tangent, dfdx, plot_grad=True, grad_scale=2,\n                   title=f'Tangent Plane to $z=x^2+y^2$ at ${(x0, y0, z0)}$')\n\n\n\n\n\n\n\n\n\nIf you look at the plane, the partial of \\(z\\) with respect to \\(x\\) turns out to represent the slope of the line running along the plane parallel to the x-axis at the point \\((1,1)\\). Similarly, the partial of \\(z\\) with respect to \\(y\\) represents the slope of the line running along the plane parallel to the y-axis at the point \\((1,1)\\).\nThe gradient vector (shown in red) is both of these together, which gives a vector \\((2, 2)\\) that points in the steepest direction up the surface from the point \\((1,1)\\). Said differently, the gradient vector is the direction of steepest ascent.\nThis fact can be visualized easier by looking at the contour plot. In the contour plot, the tangent plane will appear as a line hugging tangent to the contour at the point \\((1,1)\\). The gradient vector will always point outward perpendicular to this line in the direction of steepest ascent of the function.\n\n\nCode\nplot_tangent_contour(x, y, x0, y0, f, f_tangent, dfdx, title=f'Tangent to $z=x^2+y^2$ at ${(x0, y0, z0)}$')\n\n\n\n\n\n\n\n\n\nHere’s an argument for why this is true. A contour is by definition a curve where \\(z\\) is constant. Imagine taking the surface of \\(z=f(x,y)\\) and at each \\(z\\) value slicing the surface parallel to the xy-plane. That’s all a contour is. This means that along any given contour we must have \\(dz=0\\), since \\(z\\) can’t change. But by the chain rule we already know\n\\[dz = \\frac{dz}{d\\mathbf{x}} \\cdot d\\mathbf{x}.\\]\nBut since \\(dz=0\\), this means \\[\\frac{dz}{d\\mathbf{x}} \\cdot d\\mathbf{x} = 0.\\]\nNow, recall two vectors \\(\\mathbf{x}\\) and \\(\\mathbf{x}\\) are orthogonal (i.e. perpendicular) if \\(\\mathbf{x} \\cdot \\mathbf{y} = 0\\). I’ve thus shown that the gradient vector \\(\\mathbf{g}\\) must be perpendicular to the differential vector \\(d\\mathbf{x}\\) along contours where \\(z\\) is constant.\nSince we’re confined to a contour of constant \\(z\\), any small changes \\(d\\mathbf{x}\\) as we move around the contour must be parallel to the contour, otherwise \\(dz\\) wouldn’t be zero. This means \\(\\mathbf{g}\\) must be perpendicular to the line tangent to the contour at \\((1,1)\\). That is, the gradient at \\((1,1)\\) is a vector pointing outward in the direction of steep ascent from the point \\((1,1)\\).\n\n\n7.1.3 The Hessian\nIn the univariate case, we had not just first derivatives \\(\\frac{dy}{dx}\\), but second derivatives \\(\\frac{d^2y}{dx^2}\\) too. In the multivariate case we can take second partial derivatives as well in the usual way, but there are now \\(2^2=4\\) different ways to calculate second derivatives,\n\\[\\frac{\\partial^2 z}{\\partial x^2}, \\frac{\\partial^2 z}{\\partial x \\partial y}, \\frac{\\partial^2 z}{\\partial y \\partial x}, \\frac{\\partial^2 z}{\\partial y^2}.\\]\nNote the partials are by convention applied from right to left. Thankfully this doesn’t matter, since for well-behaved functions the mixed partials commute with each other, i.e. \\[\\frac{\\partial^2 z}{\\partial x \\partial y} = \\frac{\\partial^2 z}{\\partial y \\partial x}.\\]\nJust as we could group first partial derivatives into a vector to get the gradient, we can group second partial derivatives into a matrix to get what’s called the Hessian matrix, \\[\n\\frac{d^2 z}{d\\mathbf{x}^2} =\n\\begin{pmatrix}\n\\frac{\\partial^2 z}{\\partial x^2} & \\frac{\\partial^2 z}{\\partial x \\partial y} \\\\\n\\frac{\\partial^2 z}{\\partial y \\partial x} & \\frac{\\partial^2 z}{\\partial y^2}\n\\end{pmatrix}.\n\\]\nThe Hessian is the multivariate generalization of the full second derivative, just as the gradient vector is the generalization of the full first derivative. I’ll often write the Hessian matrix with the symbol \\(\\mathbf{H}\\) or \\(\\mathbf{H}(\\mathbf{x})\\) for brevity.\nJust as the second derivative of a univariate function can be interpreted geometrically as representing the curvature of the curve \\(y=f(x)\\), the Hessian of a multivariate function represents the curvature of the surface \\(z=f(x,y)\\). This comes from looking at the multivariate second-order perturbation\n\\[z = z_0 + \\mathbf{g}(\\mathbf{x}_0) \\cdot (\\mathbf{x} - \\mathbf{x}_0) + \\frac{1}{2}(\\mathbf{x} - \\mathbf{x}_0)^\\top \\mathbf{H}(\\mathbf{x}_0) (\\mathbf{x} - \\mathbf{x}_0).\\]\nThe curvature of the function can be obtained by looking at the eigenvalues of the Hessian at \\(\\mathbf{x}= \\mathbf{x}_0\\). Large eigenvalues represent steep curvature, while small eigenvalues represent shallow curvature. The sign of the eigenvalues indicate whether the function\n\nBowls upward: both eigenvalues are non-negative,\nBowls downward: both eigenvalues are non-positive,\nSaddles: one eigenvalue is positive, one is eigenvalue negative.\n\nCase (3) creates what’s called a saddlepoint, a point where the function slopes upwards in one direction, but downward in the other, creating the shape of something that resembles a horse’s saddle.\nFor the same working example \\(z=x^2+y^2\\), we’d have\n\\[\n\\mathbf{H} = \\frac{d^2 z}{d\\mathbf{x}^2} =\n\\begin{pmatrix}\n2 & 2 \\\\\n2 & 2\n\\end{pmatrix},\n\\]\nthat is, the Hessian of this function is constant, since no elements depend on \\(x\\) or \\(y\\).\nThe eigenvalues of this Hessian are \\(\\lambda=4,0\\), both of which are non-negative. Since the Hessian is constant, this means the function bowls upward at all points \\((x,y)\\). This also means this Hessian matrix is positive semi-definite.\n\nH = sp.Matrix([[2, 2], [2, 2]])\neigs = H.eigenvals()\nprint(f'eigenvalues = {list(eigs.keys())}')\n\neigenvalues = [4, 0]\n\n\nWhen a function’s Hessian is positive semi-definite, i.e. it bowls upward, it’s called a convex function. Convex functions are very important in optimization since convex functions always have a unique global minimum. Classical machine learning algorithms often take advantage of this fact.\nWhat about higher derivatives of multivariate functions? It turns out the \\(k\\)th derivative of a multivariate function is a rank-\\(k\\) tensor. This makes higher derivatives especially nasty, so we rarely see them.\n\n\n7.1.4 Differentiation in \\(n\\) Dimensions\nSimilarly, we can define all of these quantities for any n-dimensional multivariate function \\(y=f(\\mathbf{x})=f(x_0,x_1,\\cdots,x_{n-1}).\\) The partial derivative of \\(y\\) with respect to some \\(x_i\\) is the one whose only first order perturbation is \\(x_i+dx_i\\), with the rest staying fixed,\n\\[\\frac{\\partial y}{\\partial x_i} = \\frac{f(x_0,x_1,\\cdots,x_i+dx_i,\\cdots,x_{n-1}) - f(x_0,x_1,\\cdots,x_i,\\cdots,x_{n-1})}{dx_i}.\\]\nThat is, it’s the derivative of \\(y\\) with respect to \\(x_i\\) where all other inputs \\(x_j \\neq x_i\\) are held constant. The chain rule extends by adding a term for each \\(dx_i\\),\n\\[dy = \\sum_{i=0}^{n-1} \\frac{\\partial y}{\\partial x_i} dx_i = \\frac{\\partial y}{\\partial x_0} dx_0 + \\frac{\\partial y}{\\partial x_1} dx_1 + \\cdots + \\frac{\\partial y}{\\partial x_{n-1}} dx_{n-1},\\]\nOr, written as a dot product of \\(n\\) dimensional vectors,\n\\[\\begin{align*}\n\\frac{dy}{d\\mathbf{x}} &= \\bigg(\\frac{\\partial y}{\\partial x_0}, \\frac{\\partial y}{\\partial x_1},\\cdots,\\frac{\\partial y}{\\partial x_{n-1}} \\bigg), \\\\\nd\\mathbf{x} &= (dx_0, dx_1,\\cdots,dx_{n-1}), \\\\\ndy &= \\frac{dy}{d\\mathbf{x}} \\cdot d\\mathbf{x}.\n\\end{align*}\\]\nI’ll calculate a quick example with the \\(n\\) dimensional generalization of our running quadratic function,\n\\[y = x_0^2 + x_1^2 + \\cdots + x_{n-1}^2 = \\sum_{i=0}^{n-1} x_i^2.\\]\nSince each partial derivative gives \\(\\frac{\\partial y}{\\partial x_i} = 2x_i\\), the gradient for this function should be the n-dimensional vector\n\\[\\mathbf{g} = \\frac{dy}{d\\mathbf{x}} = (2x_0, 2x_1, \\cdots, 2x_{n-1}) = 2\\mathbf{x}.\\]\nUsing numpy we can efficiently calculate this function with the vectorized command np.sum(x ** 2). I’ll choose our point of interest to be the vector \\(\\mathbf{x}_0\\) of all ones. I’ll define a helper function dfdxi to calculate the ith partial derivative at \\(\\mathbf{x}_0\\). Note dx will be a vector of all zeros except at dx[i] = dxi. This will then be used in the function dfdx to calculate the gradient. It will loop over every index, calculate each partial, and put them in a vector grad. Observe that yet again we have a gradient vector of all twos to within an error of around 1e-5, except instead of 1 or 2 elements we have 100 of them.\n\n\nCode\ndef dfdxi(f, x0, i, dxi=1e-5):\n    dx = np.zeros(len(x0))\n    dx[i] = dxi\n    dydxi = (f(x0 + dx) - f(x0)) / dxi\n    return dydxi\n\ndef dfdx(f, x0, dxi=1e-5):\n    return np.array([dfdxi(f, x0, i, dxi=dxi) for i in range(len(x0))])\n\nf = lambda x: np.sum(x ** 2)\nx0 = np.ones(100)\ngrad = dfdx(f, x0)\nprint(f'grad.shape = {grad.shape}')\nprint(f'grad = \\n{grad}')\n\n\ngrad.shape = (100,)\ngrad = \n[2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001\n 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001\n 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001\n 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001\n 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001\n 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001\n 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001\n 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001\n 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001\n 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001\n 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001 2.00001\n 2.00001]\n\n\nThe Hessian matrix of second partial derivatives also extends to \\(n\\) dimensional scalar-valued functions \\(y = f(\\mathbf{x})\\). The difference is that instead of just \\(2^2=4\\) second partials, we now have \\(n^2\\) possible second partials. These can be organized into an \\(n \\times n\\) matrix\n\\[\n\\mathbf{H} = \\frac{d^2 y}{d\\mathbf{x}^2} =\n\\begin{pmatrix}\n\\frac{\\partial^2 y}{\\partial x_0^2} & \\frac{\\partial^2 y}{\\partial x_0 \\partial x_1} & \\cdots & \\frac{\\partial^2 y}{\\partial x_0 \\partial x_{n-1}} \\\\\n\\frac{\\partial^2 y}{\\partial x_1 \\partial x_0} & \\frac{\\partial^2 y}{\\partial x_1^2} & \\cdots & \\frac{\\partial^2 y}{\\partial x_1 \\partial x_{n-1}} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial^2 y}{\\partial x_{n-1} \\partial x_0} & \\frac{\\partial^2 y}{\\partial x_{n-1} \\partial x_1} & \\cdots & \\frac{\\partial^2 y}{\\partial x_{n-1}^2}\n\\end{pmatrix}.\n\\]\nThe mixed partials again typically all commute, which means \\(\\mathbf{H}\\) is a symmetric matrix, i.e. \\(\\mathbf{H}^\\top = \\mathbf{H}\\). The eigenvalues of \\(\\mathbf{H}\\) again determine the curvature of the function at any point \\(\\mathbf{x}=\\mathbf{x}_0\\). If the eigenvalues of the Hessian are all non-negative, \\(\\mathbf{H}\\) will be positive semi-definite, i.e. \\(\\mathbf{H} \\succcurlyeq 0\\). In the case, the function \\(f(\\mathbf{x})\\) will be a convex function and hence bowl upwards. If the Hessian isn’t positive semidefinite it’ll usually have saddlepoints, usually far more saddlepoints than minima or maxima in fact.\n\n\n7.1.5 The Jacobian\nThus far we’ve seen the following two types of functions:\n\nscalar-valued functions of a scalar variable: \\(y=f(x)\\),\nscalar-valued functions of a vector variable: \\(y = f(\\mathbf{x})\\).\n\nAs you might expect, we can also have the equivalent vector-valued functions:\n\nvector-valued functions of a scalar variable: \\(\\mathbf{y} = f(x)\\),\nvector-valued functions of a vector variable: \\(\\mathbf{y} = f(\\mathbf{x})\\).\n\nThe most relevant of these two for machine learning purposes is the vector-valued function of a vector variable \\(\\mathbf{y} = f(\\mathbf{x})\\). These functions are just extensions of the scalar-valued vector variable functions \\(y = f(\\mathbf{x})\\) we’ve been working with so far, except now we can have \\(m\\) scalar-valued functions \\(y_i = f_i(\\mathbf{x})\\), which when put together make up a vector output\n\\[\\mathbf{y} = (y_0,y_1,\\cdots,y_{m-1}) = (f_0(\\mathbf{x}),f_1(\\mathbf{x}),\\cdots,f_{m-1}(\\mathbf{x})).\\]\nTo define the gradient of a vector-valued function, we just take the gradient of each output element \\(y_i=f_i(\\mathbf{x})\\). Doing this over all \\(m\\) output elements will give \\(m\\) gradients each of size \\(n\\),\n\\[\\frac{dy_0}{d\\mathbf{x}}, \\ \\frac{dy_1}{d\\mathbf{x}}, \\ \\cdots, \\ \\frac{dy_{m-1}}{d\\mathbf{x}}.\\]\nBy treating all these gradients as row vectors, we can assemble them into a single \\(m \\times n\\) matrix to get the derivative of the vector-valued function \\(\\mathbf{y} = f(\\mathbf{x})\\). This matrix is usually called the Jacobian matrix, sometimes denoted in short-hand by the symbol \\(\\mathbf{J}\\). It’s defined as the \\(m \\times n\\) of all possible first partial derivatives,\n\\[\n\\mathbf{J} = \\frac{d\\mathbf{y}}{d\\mathbf{x}} =\n\\begin{pmatrix}\n\\frac{\\partial y_0}{\\partial x_0} & \\frac{\\partial y_0}{\\partial x_1} & \\cdots & \\frac{\\partial y_0}{\\partial x_{n-1}} \\\\\n\\frac{\\partial y_1}{\\partial x_0} & \\frac{\\partial y_1}{\\partial x_1} & \\cdots & \\frac{\\partial y_1}{\\partial x_{n-1}} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\frac{\\partial y_{m-1}}{\\partial x_0} & \\frac{\\partial y_{m-1}}{\\partial x_1} & \\cdots & \\frac{\\partial y_{m-1}}{\\partial x_{n-1}}\n\\end{pmatrix}.\n\\]\nTo see an example of a vector-valued function, consider the function \\(\\mathbf{y} = f(\\mathbf{x})\\) given by\n\\[\n\\mathbf{y} =\n\\begin{pmatrix}\ny_0 \\\\\ny_1\n\\end{pmatrix} =\n\\begin{pmatrix}\nx_0^3 + x_1^2 \\\\\n2 x_0 - x_1^4 \\\\\n\\end{pmatrix}.\n\\]\nThis is really two functions \\(y_0 = x_0^3 + x_1^2\\) and \\(y_1 = 2 x_0 - x_1^4\\). Here’s what its Jacobian would look like,\n\\[\n\\mathbf{J} =\n\\begin{pmatrix}\n\\frac{\\partial y_0}{\\partial x_0} & \\frac{\\partial y_0}{\\partial x_1} \\\\\n\\frac{\\partial y_1}{\\partial x_0} & \\frac{\\partial y_1}{\\partial x_1}\n\\end{pmatrix} =\n\\begin{pmatrix}\n3 x_0^2 & 2 x_1  \\\\\n2 & -4 x_1^3\n\\end{pmatrix}.\n\\]\nNotice each row of the Jacobian is the gradient of the elements of \\(\\mathbf{y}\\), as you’d expect,\n\\[\\frac{dy_0}{d\\mathbf{x}} = (3x_0^2, 2x_1), \\qquad \\frac{dy_1}{d\\mathbf{x}} = (2, -4x_1^3).\\]\n\n\n7.1.6 Application: The Softmax Function\nA more interesting example of vector-valued functions and Jacobians that’s very relevant to machine learning is the softmax function, defined by\n\\[\n\\mathbf{y} = \\text{softmax}(\\mathbf{x}) =\n\\begin{pmatrix}\ny_0 \\\\\ny_1 \\\\\n\\dots \\\\\ny_{n-1}\n\\end{pmatrix} =\n\\begin{pmatrix}\n\\frac{1}{Z} e^{x_0} \\\\\n\\frac{1}{Z} e^{x_1} \\\\\n\\dots \\\\\n\\frac{1}{Z} e^{x_{n-1}}\n\\end{pmatrix},\n\\]\nwhere \\(Z = \\sum_k e^{x_k}\\) is a normalizing constant, often called the partition function. This function shows up in machine learning as a way to create probabilities out of \\(n\\) categories. It takes inputs \\(x_i\\) of any real value and scales them so that \\(0 \\leq y_i \\leq 1\\) and \\(\\sum_i y_i = 1\\), so that the output is a valid probability vector.\nThe softmax is useful in defining models for multi-class classification problems, since it can be used to classify things into one of \\(n\\) classes. To classify an object as type \\(k\\), choose the index \\(k\\) such that \\(y_k\\) is the largest probability in the probability vector \\(\\mathbf{y}\\). More on this in future lessons.\nHere’s an example illustrating what the softmax function does. I’ll define a vector \\(x\\) of size \\(n=5\\) by randomly sampling from the interval \\([-1,1]\\). I’ll use a quick lambda function to implement the softmax. Observe what the softmax seems to do is take the elements of \\(x\\) and re-scale them so they’re all in the interval \\([0,1]\\). The outputs also indeed sum to one by construction.\n\n\nCode\nx = np.random.randn(5)\nprint(f'x = {x.round(3)}')\n\nsoftmax = lambda x: np.exp(-x) / np.sum(np.exp(-x))\ny = softmax(x)\nprint(f'y = {y.round(3)}')\nprint(f'sum(y) = {y.sum().round(10)}')\n\n\nx = [-0.542 -0.126 -0.854  1.209  0.322]\ny = [0.276 0.182 0.377 0.048 0.116]\nsum(y) = 1.0\n\n\nNote you would not want to implement the softmax this way at scale due to numerical instability. We’ll get back to this stuff in much more depth in a later lesson.\nSince we’ll need it later on anyway, let’s go ahead and calculate the Jacobian of the softmax function. Let’s work term by term, focusing on the \\(j\\)th partial derivative of \\(y_i=\\frac{1}{Z} e^{x_i}\\). First, notice that the derivative of the partition function is\n\\[\\frac{\\partial Z}{\\partial x_j} = \\frac{\\partial}{\\partial x_j} \\sum_k e^{x_k} = \\sum_k \\frac{\\partial}{\\partial x_j} e^{x_k} = e^{x_j}\\]\nsince the only term in the sum containing \\(x_j\\) is \\(e^{x_j}\\). Using this along with the quotient rule, we thus have\n\\[\nJ_{i,j} = \\frac{\\partial y_i}{\\partial x_j} = \\frac{\\partial}{\\partial x_j} \\frac{e^{x_i}}{Z} =\n\\frac{1}{Z^2}\\bigg(Z \\frac{\\partial e^{x_i}}{\\partial x_j} - e^{x_i} \\frac{\\partial Z}{\\partial x_j} \\bigg) =\n\\begin{cases}\n\\frac{e^{x_i}}{Z} \\big(1 - \\frac{e^{x_i}}{Z}\\big), & i = j \\\\\n-\\frac{e^{x_i}}{Z} \\frac{e^{x_j}}{Z}, & i \\neq j\n\\end{cases} \\ = \\\n\\begin{cases}\ny_i (1 - y_i), & i = k \\\\\n-y_i y_j & i, \\neq k.\n\\end{cases}\n\\]\nPutting all this into the Jacobian matrix, the \\(i=j\\) terms go in the diagonal, and the \\(i \\neq j\\) terms go in the off-diagonals, hence\n\\[\n\\mathbf{J} =\n\\begin{pmatrix}\ny_0 (1 - y_0) & -y_0 y_1 & \\cdots & -y_0 y_{n-1} \\\\\n-y_1 y_0 & y_1 (1 - y_1) & \\cdots & -y_1 y_{n-1} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n-y_{n-1} y_0 & -y_{n-1} y_1 & \\cdots & y_{n-1} (1 - y_{n-1})\n\\end{pmatrix}\n\\]\nIf you play with this expression a little bit, you’ll see we can write this softmax Jacobian efficiently as \\(\\mathbf{J} = \\text{diag}(\\mathbf{y}) - \\mathbf{y} \\mathbf{y}^\\top\\).\nHere’s the Jacobian for the above example where \\(n=5\\), which I’ll call grad. It’s common in machine learning to blur the distinction between Jacobians and gradients and just call everything a gradient. Notice grad is a \\(5 \\times 5\\) matrix.\n\ngrad = np.diag(y) - y @ y.T\nprint(f'grad = \\n{grad}')\n\ngrad = \n[[ 0.00841128 -0.26767389 -0.26767389 -0.26767389 -0.26767389]\n [-0.26767389 -0.08553925 -0.26767389 -0.26767389 -0.26767389]\n [-0.26767389 -0.26767389  0.10971353 -0.26767389 -0.26767389]\n [-0.26767389 -0.26767389 -0.26767389 -0.21971262 -0.26767389]\n [-0.26767389 -0.26767389 -0.26767389 -0.26767389 -0.15124236]]\n\n\nAside: We’ve talked about functions with scalar and vector inputs or outputs. What about functions with matrix or tensor inputs or outputs? We could just as well define scalar-valued functions of a matrix variable \\(y = f(\\mathbf{X})\\), matrix-valued function of a matrix variable \\(\\mathbf{Y} = f(\\mathbf{X})\\), etc. The derivative rules extend into these cases as well, but things get a lot more complicated. Taking any kind of derivative of these kinds of functions can cause the rank of the derivative to blow up. For example, the derivative of a (rank-2) matrix with respect to another (rank-2) matrix is now a rank-4 tensor. For at least partly this reason, derivatives of such functions are less commonly used. See this comprehensive paper if you’re interested in how to take derivatives of matrices.\n\n\n7.1.7 Gradient and Jacobian Rules\nHere are a few common gradient and Jacobian rules. I’ll state them assuming a vector-valued function with a vector input unless the scalar-valued form looks different, in which case I’ll state it explicitly. Don’t worry too much about how to derive these. Don’t even try to memorize them. This is just a reference.\n\n\n\n\n\n\n\n\nName\nGradient or Jacobian\nScalar Equivalent\n\n\nConstant Rule\n\\(\\frac{d}{d\\mathbf{x}} (c\\mathbf{y}) = c\\frac{d\\mathbf{y}}{d\\mathbf{x}}\\)\n\\(\\frac{d}{dx} (cy) = c\\frac{dy}{dx}\\)\n\n\nAddition Rule\n\\(\\frac{d}{d\\mathbf{x}}(\\mathbf{u} + \\mathbf{v}) = \\frac{d\\mathbf{u}}{d\\mathbf{x}} + \\frac{d\\mathbf{v}}{d\\mathbf{x}}\\)\n\\(\\frac{d}{dx}(u + v) = \\frac{du}{dx} + \\frac{dv}{dx}\\)\n\n\nProduct Rule (scalar-valued)\n\\(\\frac{d}{d\\mathbf{x}}(uv) = u\\frac{dv}{d\\mathbf{x}} + v\\frac{du}{d\\mathbf{x}}\\)\n\\(\\frac{d}{dx}(uv) = u\\frac{dv}{dx} + v\\frac{du}{dx}\\)\n\n\nProduct Rule (dot products)\n\\(\\frac{d}{d\\mathbf{x}}(\\mathbf{u}^\\top \\mathbf{v}) = \\mathbf{u}^\\top \\frac{d\\mathbf{v}}{d\\mathbf{x}} + \\mathbf{v}^\\top \\frac{d\\mathbf{u}}{d\\mathbf{x}}\\)\n\\(\\frac{d}{dx}(uv) = u\\frac{dv}{dx} + v\\frac{du}{dx}\\)\n\n\nChain Rule (scalar-valued, vector-valued)\n\\(\\frac{dz}{d\\mathbf{x}} = \\big(\\frac{dz}{d\\mathbf{y}}\\big)^\\top \\frac{d\\mathbf{y}}{d\\mathbf{x}}\\)\n\\(\\frac{dz}{dx} = \\frac{dz}{dy} \\frac{dy}{dx}\\)\n\n\nChain Rule (both vector-valued)\n\\(\\frac{d\\mathbf{z}}{d\\mathbf{x}} = \\frac{d\\mathbf{z}}{d\\mathbf{y}} \\frac{d\\mathbf{y}}{d\\mathbf{x}}\\)\n\\(\\frac{dz}{dx} = \\frac{dz}{dy} \\frac{dy}{dx}\\)\n\n\nConstant Function\n\\(\\frac{d}{d\\mathbf{x}} \\mathbf{c} = \\mathbf{0}\\)\n\\(\\frac{d}{dx} c = 0\\)\n\n\nSquared Two-Norm\n\\(\\frac{d}{d\\mathbf{x}} ||\\mathbf{x}||^2 = \\frac{d}{d\\mathbf{x}} \\mathbf{x}^\\top \\mathbf{x} = 2 \\mathbf{x}\\)\n\\(\\frac{d}{dx} x^2 = 2x\\)\n\n\nLinear Combination\n\\(\\frac{d}{d\\mathbf{x}} \\mathbf{c}^\\top \\mathbf{x} = \\mathbf{c}\\)\n\\(\\frac{d}{dx} cx = c\\)\n\n\nSymmetric Quadratic Form\n\\(\\frac{d}{d\\mathbf{x}} \\mathbf{x}^\\top \\mathbf{S} \\mathbf{x} = 2 \\mathbf{S} \\mathbf{x}\\)\n\\(\\frac{d}{dx} sx^2 = 2sx\\)\n\n\nAffine Function\n\\(\\frac{d}{d\\mathbf{x}} (\\mathbf{A}\\mathbf{x} + \\mathbf{b}) = \\mathbf{A}^\\top\\) or \\(\\frac{d}{d\\mathbf{x}} (\\mathbf{x}^\\top \\mathbf{A} + \\mathbf{b}) = \\mathbf{A}\\)\n\\(\\frac{d}{dx} (ax+b) = a\\)\n\n\nSquared Error Function\n\\(\\frac{d}{d\\mathbf{x}} ||\\mathbf{A}\\mathbf{x}-\\mathbf{b}||^2 = 2\\mathbf{A}^\\top (\\mathbf{A}\\mathbf{x}-\\mathbf{b})\\)\n\\(\\frac{d}{dx} (ax-b)^2 = 2a(ax-b)\\)\n\n\nCross Entropy Function\n\\(\\frac{d}{d\\mathbf{x}} (-\\mathbf{c}^\\top \\log \\mathbf{x}) = -\\frac{\\mathbf{c}}{\\mathbf{x}}\\) (element-wise division)\n\\(\\frac{d}{dx} (-c \\log x) = -\\frac{c}{x}\\)\n\n\nReLU Function\n\\(\\frac{d}{d\\mathbf{x}} \\max(\\mathbf{0}, \\mathbf{x}) = \\text{diag}(\\mathbf{x} \\geq \\mathbf{0})\\) (element-wise \\(\\geq\\))\n\\(\\frac{d}{dx} \\max(0, x) = \\text{$1$ if $x \\geq 0$ else $0$}\\)\n\n\nSoftmax Function\n\\(\\frac{d}{d\\mathbf{x}} \\text{softmax}(\\mathbf{x}) = \\text{diag}(\\mathbf{y}) - \\mathbf{y} \\mathbf{y}^\\top\\) where \\(\\mathbf{y} = \\text{softmax}(\\mathbf{x})\\)\n\n\n\n\nYou can calculate gradients and Jacobians in sympy, though in my opinion it can be kind of painful except in the simplest cases. Here’s an example, where I’ll calculate the Jacobian of the squared error function \\(||\\mathbf{A}\\mathbf{x}-\\mathbf{b}||^2\\).\nAside: There is also a nice online tool that lets you do this somewhat more easily.\n\n\nCode\nm = sp.Symbol('m')\nn = sp.Symbol('n')\nA = sp.MatrixSymbol('A', m, n)\nx = sp.MatrixSymbol('x', n, 1)\nb = sp.MatrixSymbol('b', m, 1)\n\ny = (A * x - b).T * (A * x - b)\ndydx = y.diff(x)\nprint(f'y = {y}')\nprint(f'dydx = {dydx}')\n\n\ny = (-b.T + x.T*A.T)*(A*x - b)\ndydx = 2*A.T*(A*x - b)"
  },
  {
    "objectID": "notebooks/multivariate-calculus.html#multivariate-integration",
    "href": "notebooks/multivariate-calculus.html#multivariate-integration",
    "title": "7  Multivariate Calculus",
    "section": "7.2 Multivariate Integration",
    "text": "7.2 Multivariate Integration\n\n7.2.1 Integration in 2 Dimensions\nWe can also integrate multivariate functions like \\(z=f(x,y)\\). Geometrically these integrals translate into calculating the volume under the surface of \\(z=f(x,y)\\). I’ll very briefly touch on this.\nThe idea here is to approximate the volume \\(V\\) under a surface not with \\(N\\) rectangles of width \\(dx\\) and height \\(f(x)\\), but instead with \\(N \\cdot M\\) rectangular prisms of base area \\(dA = dx \\cdot dy\\) and height \\(z=f(x,y)\\),\n\\[V = \\int_R f(x,y) dA = \\sum_{n=0}^{N-1} \\sum_{m=0}^{M-1} f(x_n,y_m) dxdy = f(x_0,y_0) dxdy + f(x_0,y_1) dxdy + \\cdots + f(x_1,y_0) dxdy + \\cdots + f(x_{N-1},y_{M-1}) dxdy.\\]\nRather than integrate from one endpoint \\(a\\) to another endpoint \\(b\\), we now have to integrate over a 2D region in the xy-plane that I’ll call \\(R\\).\nIf \\(R\\) is just a rectangle in the xy-plane, say \\(R = [a,b] \\times [c,d]\\) we can break the integral \\(\\int_R dA\\) into two integrals \\(\\int_a^b dx\\) and \\(\\int_c^d dy\\). If we can also factor \\(f(x,y) = g(x)h(y)\\), we can further break the integral into a product of two univariate integrals,\n\\[\\int_R f(x,y) dA = \\int_a^b \\int_c^d f(x,y) dxdy = \\bigg(\\int_a^b g(x) dx \\bigg) \\bigg( \\int_c^d h(y) dy \\bigg).\\]\nAs an example, suppose we wanted to integrate the function \\(f(x,y) = x^2 \\sqrt{y}\\) over the rectangle \\(R = [0,1] \\times [0,1]\\). This function factors into a product of two functions \\(g(x) = x^2\\) and \\(h(y) = \\sqrt{y}\\). We can thus integrate each individually to get\n\\[\\int_0^1 \\int_0^1 x^2 \\sqrt{y} dxdy = \\bigg(\\int_0^1 x^2 dx\\bigg) \\bigg(\\int_0^1 \\sqrt{y} dy\\bigg) = \\frac{1}{3} x^3 \\bigg |_{x=0}^1 \\cdot \\frac{2}{3} y^{3/2} \\bigg |_{y=0}^1 = \\frac{1}{3} \\cdot \\frac{2}{3} = \\frac{2}{9}.\\]\nIn general \\(R\\) won’t be a rectangle, but some arbitrary shape. And \\(f(x,y)\\) won’t usually factor. When this is the case we usually have to fall back to numerical integration methods.\n\n\n7.2.2 Application: Integrating the Gaussian\nOne of the most important functions in machine learning, if not all of science, is the Gaussian function\n\\[y = e^{-\\frac{1}{2} x^2}.\\]\nThe Gaussian is the function that gives the well-known bell-curve shape.\n\n\nCode\nx = np.arange(-10, 10, 0.1)\nf = lambda x:  np.exp(-1 / 2 * x ** 2)\nplot_function(x, f, xlim=(-3, 3), ylim=(-0, 1.5), title='Gaussian Function')\n\n\n\n\n\n\n\n\n\nIt’s very important in many applications of probability and statistics to be able to integrate the Gaussian function between two points \\(a\\) and \\(b\\),\n\\[\\int_a^b e^{-\\frac{1}{2} x^2} dx.\\]\nUnfortunately, this turns out to be impossible to do analytically, because the Gaussian function has no indefinite integral. No matter how hard you try, you’ll never find an elementary function \\(F(x)\\) whose derivative is \\(f(x)=e^{-\\frac{1}{2} x^2}\\).\nOne special case, however, where we can integrate the Gaussian analytically is when the region is the whole real line,\n\\[\\int_{-\\infty}^\\infty e^{-\\frac{1}{2} x^2} dx.\\]\nIt’s surprising we can even do this. We can do it using a trick. The trick is to square the Gaussian. Consider instead the function\n\\[f(x,y) = e^{-\\frac{1}{2} x^2} e^{-\\frac{1}{2} y^2} = e^{-\\frac{1}{2} (x^2+y^2)}.\\]\nConsider now the bivariate integral\n\\[\\int_{\\mathbb{R}^2} f(x,y) dxdy = \\int_{-\\infty}^\\infty \\int_{-\\infty}^\\infty e^{-\\frac{1}{2} (x^2+y^2)} dxdy.\\]\nThis integral doesn’t on the face of it look any easier, but we can do something with it that we can’t with the univariate integral: change variables. I won’t go into detail here, but if we define 2 new variables \\(r\\) and \\(\\theta\\) (which turn out to be polar coordinates)\n\\[r^2 = x^2+y^2, \\quad \\tan \\theta = -\\frac{y}{x},\\]\nthen \\(dxdy = rdrd\\theta\\), and we can re-write the bivariate integral as\n\\[\\int_{\\mathbb{R}^2} f(x,y) dxdy = \\int_0^\\infty \\int_0^{2\\pi} e^{-\\frac{1}{2}r^2} rdrd\\theta = \\bigg(\\int_0^{2\\pi} d\\theta \\bigg) \\bigg( \\int_0^\\infty re^{-\\frac{1}{2}r^2} dr \\bigg).\\]\nThis is just a product of two univariate integrals that we can evaluate. The first integral is easy,\n\\[\\int_0^{2\\pi} d\\theta = \\theta \\bigg |_{\\theta=0}^{2\\pi} = 2\\pi.\\]\nThe second integral is a little harder, but we can solve it by using another change of variables \\(u=r^2\\), so \\(du = 2rdr\\), to get\n\\[\\int_0^\\infty re^{-\\frac{1}{2}r^2} dr = \\int_0^\\infty e^{-\\frac{1}{2}u} \\bigg(\\frac{1}{2} du\\bigg) = \\frac{1}{2} \\int_0^\\infty e^{-\\frac{1}{2}u} du = -e^{-\\frac{1}{2}u} \\bigg |_{u=0}^\\infty = -(e^{-\\infty} - 1) = 1,\\]\nsince \\(e^{-\\infty} = \\frac{1}{e^{\\infty}} = \\frac{1}{\\infty} = 0\\). Putting these together, the bivariate integral is thus\n\\[\\int_{-\\infty}^\\infty \\int_{-\\infty}^\\infty e^{-\\frac{1}{2} (x^2+y^2)} dxdy = 2\\pi.\\]\nSince \\(e^{-\\frac{1}{2} (x^2+y^2)} = e^{-\\frac{1}{2} x^2} e^{-\\frac{1}{2} y^2}\\), we can factor this integral into a product to get\n\\[2\\pi = \\int_{-\\infty}^\\infty \\int_{-\\infty}^\\infty e^{-\\frac{1}{2} x^2} dxdy = \\bigg(\\int_{-\\infty}^\\infty e^{-\\frac{1}{2} x^2} dx \\bigg) \\bigg(\\int_{-\\infty}^\\infty e^{-\\frac{1}{2} y^2} dy\\bigg).\\]\nBoth of the integrals on the right are the same, so they must equal the same number, call it \\(A\\). We thus have an equation \\(A^2 = 2\\pi\\), which we can solve to get the area under each integral, which is \\(A=\\sqrt{2\\pi}\\). Thus, we’ve arrived at the final result for the univariate integral of the Gaussian,\n\\[\\int_{-\\infty}^\\infty e^{-\\frac{1}{2} x^2} dx = \\sqrt{2\\pi} \\approx 2.507.\\]\nAny time from now on you see the factors of \\(\\sqrt{2\\pi}\\) in a Gaussian function, this is where they come from.\nIt’s interesting that we can integrate a function all the way from \\(-\\infty\\) to \\(\\infty\\) and still get a finite number. This is because Gaussian functions rapidly decay, so most of their area ends up being around \\(x=0\\). In fact, the interval \\([-3,3]\\) alone contains 99.7% of the area of the under the bell curve!\nHere’s the same integral verified using sympy. Note the unusual notation sympy uses for \\(\\infty\\), which is sp.oo.\n\n\nCode\nx = sp.Symbol('x') \ny = sp.exp(-sp.Rational(1, 2) * x ** 2)\nintegral = y.integrate((x, -sp.oo, sp.oo))\nprint(f'y = {y}')\nprint(f'integral = {integral}')\n\n\ny = exp(-x**2/2)\nintegral = sqrt(2)*sqrt(pi)\n\n\n\n\n7.2.3 Integration in \\(n\\) Dimensions\nThe same idea extends to \\(n\\) dimensional functions \\(y=f(x_0,\\cdots,x_{n-1})\\). In this case we’re calculating the \\(n+1\\) dimensional hypervolume \\(V_{n+1}\\) under the \\(n\\) dimensional manifold \\(y=f(x_0,\\cdots,x_{n-1})\\). The hyperrectangles would now have base hyperarea \\(dA_n = dx_0dx_1\\cdots dx_{n-1}\\) and height \\(y\\), so\n\\[V_{n+1} = \\int_{R_n} f(x_0,\\cdots,x_{n-1}) dA_n = \\sum_{\\text{all hyperrectangles}} f(x_0,\\cdots,x_{n-1}) dx_0dx_1\\cdots dx_{n-1}.\\]\nIf we’re sufficiently lucky, we can factor a multivariate integral into a product of univariate integrals. We can do this as long as\n\nthe multivariate function \\(f(x_0,\\cdots,x_{n-1})\\) factors into a product of univariate functions \\[f(x_0,x_1,\\cdots,x_{n-1}) = f_0(x_0) f_1(x_1) \\cdots f_{n-1}(x_{n-1}),\\]\nthe integration region \\(R_n\\) is a product of rectangles, \\[R_n = [a_0,b_0] \\times [a_1,b_1] \\times \\cdots \\times [a_{n-1},b_{n-1}].\\]\n\nWhen this is the case, we can simplify the integral to\n\\[\\int_{R_n} f(x_0,x_1,\\cdots,x_{n-1}) dA_n = \\bigg(\\int_{a_0}^{b_0} f_0(x_0) dx_0\\bigg) \\bigg(\\int_{a_1}^{b_1} f_1(x_1) dx_1\\bigg) \\cdots \\bigg(\\int_{a_{n-1}}^{b_{n-1}} f_{n-1}(x_{n-1}) dx_{n-1}\\bigg).\\]\nWe can then evaluate each univariate integral one-by-one and put the results together to get the full multivariate integral.\nAs a quick example, suppose we wanted to integrate the following multivariate Gaussian function over all space,\n\\[f(x_0,x_1,\\cdots,x_{n-1}) = \\exp\\bigg(-\\frac{1}{2}||\\mathbf{x}||^2\\bigg) = \\exp\\bigg(-\\frac{1}{2}\\sum_{i=0}^{n-1}x_i^2\\bigg) = \\prod_{i=0}^{n-1} \\exp\\bigg(-\\frac{1}{2}x_i^2\\bigg).\\]\nSince each product on the right is independent, the integral splits up into a product itself, so we have\n\\[\\int_{\\mathbb{R}^n} f(x_0,x_1,\\cdots,x_{n-1}) dx_0dx_1\\cdots dx_{n-1} = \\prod_{i=0}^{n-1} \\int_{-\\infty}^\\infty \\exp\\bigg(-\\frac{1}{2}x_i^2\\bigg) dx_i = \\big(\\sqrt{2\\pi}\\big)^n = (2\\pi)^{n/2}.\\]\nIf you don’t understand what’s going on here, that’s fine. When you see multivariate integrals come up in future lessons, just think of them as a way to calculate the volumes under surfaces. That’s the most important thing to take away."
  },
  {
    "objectID": "notebooks/probability.html#randomness",
    "href": "notebooks/probability.html#randomness",
    "title": "8  Probability Distributions",
    "section": "8.1 Randomness",
    "text": "8.1 Randomness\nProbability is a calculus for modeling random processes. There are things we just can’t predict with certainty given the information we have available. Stuff that we can’t predict with certainty we call random, or noise, or non-deterministic. Stuff we can predict with certainty we call deterministic or certain. Here are some examples of these two kinds of processes. The questions in the deterministic column have exact answers, while those in the random column do not.\n\n\n\n\n\n\n\nDeterministic Process\nRandom Process\n\n\n\n\nDoes \\(2+2=4\\)?\nWill it rain today?\n\n\nWhat is the capital of France?\nWhat is the result of rolling a pair of dice?\n\n\nHow many sides does a square have?\nWhat is the next card in a shuffled deck?\n\n\nWhat is the value of pi?\nWhat is the stock price of Apple tomorrow?\n\n\nWhat is the boiling point of water at sea level?\nWhat is the winning number for next week’s lottery?\n\n\n\nDeterministic processes aren’t terribly interesting. They either will occur with certainty, or they won’t. Random processes might occur. To quantify what we mean by might we’ll introduce the notion of probability. You can think of probability as a function mapping questions like “Will it rain today?” to a number between \\(0\\) and \\(1\\) that indicates our “degree of belief” in whether that question is true,\n\\[0 \\leq \\mathbb{Pr}(\\text{Will it rain today?}) \\leq 1.\\]\nThe question inside this probability function is called an event. An event is anything that might occur. Mathematically speaking, an event is a set that lives in some abstract sample space of all possible outcomes.\nWhen we’re certain an event will occur we say it has probability one, or a 100% chance of happening. When we’re certain an event will not occur we say it has probability zero, or a 0% chance of happening. These extremes are deterministic processes. Random processes are anything in between. For the question “Will it rain today?”, we might say there is a 20% chance of rain, in which case we believe \\(\\mathbb{Pr}(\\text{Will it rain today?}) = 0.2\\).\nA common theme we’ll see in machine learning is that we’re interested in mapping arbitrary data structures like strings to numerical data structures that we can do mathematical calculations with, like floats or arrays. In this particular example, it’s convenient to map the question “Will it rain today?” to a binary variable I’ll call \\(x\\), \\[\nx =\n\\begin{cases}\n1, & \\text{It will rain today} \\\\\n0, & \\text{It will not rain today}.\n\\end{cases}\n\\]\nThen asking for \\(\\mathbb{Pr}(\\text{Will it rain today?})\\) is the same thing as asking “what is the probability that \\(x=1\\)”, or equivalently, what is \\(\\mathbb{Pr}(x=1)\\)? Saying we believe there’s a 20% chance of rain today is equivalent to saying we believe there is a 20% chance that \\(x=1\\), i.e. \\(\\mathbb{Pr}(x=1)=0.2\\).\nVariables like \\(x\\) are called random variables. They’re a way of encoding random events numerically via some kind of encoding convention like I just used. It’s much more convenient to work with random variables than events or questions since we can now use all our usual mathematical tools like calculus and linear algebra to understand random processes.\nTo understand how random variables work, it’s often helpful to think of them as the outputs of random number generators. These are algorithms that generate, or sample, random numbers from some given distribution. Unlike regular functions, where a given input will always produce a definite output, a random number generator can (and usually will) produce different outputs every single time the same input is passed in.\nThe canonical example of a random number generator is called rand. It’s an algorithm for uniformly generating (pseudo) random real numbers \\(0 \\leq x \\leq 1\\). Every time we call rand we’ll get a different number with no clear pattern.\nHere’s an example. I’ll call rand via the numpy function np.random.rand a bunch of times and print the first 10 outputs. Notice how all over the place they seem to be. The only thing we know is they’re between zero and one.\n\nx = np.random.rand(100)\nx[:12]\n\narray([0.5488135 , 0.71518937, 0.60276338, 0.54488318, 0.4236548 ,\n       0.64589411, 0.43758721, 0.891773  , 0.96366276, 0.38344152,\n       0.79172504, 0.52889492])\n\n\nThink of a random variable informally as being some variable \\(x\\) whose values are determined by a function \\(x=f(n)\\), except the function can’t make up its mind or follow a pattern. On one sampling we might get \\(x=f(0)=0.548\\). Next, \\(x=f(1)=0.715\\). Next, \\(x=f(2)=0.603\\). Etc. We can’t force \\(x\\) to take on a definite value. It jumps around with no clear pattern.\n\nplt.scatter(range(len(x)), x)\nplt.xlabel('n')\nplt.ylabel('x')\nplt.title('$x = f(n)$')\nplt.show();\n\n\n\n\nSince random variable outputs jump around like this we need a different way to visualize them than just thinking of them as points on the number line. The most useful way to visualize random variables is using a histogram. To create a histogram, we sample a random variable a whole bunch of times, and plot a count of how many times the variable takes on each given value. We then show these counts in a bar chart with the heights indicating the counts for each value.\nIn matplotlib we can plot histograms of an array of samples x using the function plt.hist(x). Here’s an example. I’ll sample 100 values from rand and put them in an array x, then plot the histogram.\n\nx = np.random.rand(100)\nplt.hist(x)\nplt.show();\n\n\n\n\nNotice that we just sampled \\(100\\) different values, but we don’t see \\(100\\) different bars. That’s because histograms don’t plot bars for all values. First, the values get binned into some number of equally spaced subintervals, called bins, then the counts that get plotted are the counts of values inside each bin. In this case, the histogram divides the samples into \\(10\\) equally spaced bins. If you look carefully you should see \\(10\\) bars in the plot. We can change the number of bins by passing in a keyword bins specifying how many bints to take.\nSince I’ll be using histograms a lot in this lesson I’m going to write a helper function plot_histogram to bundle up the code to plot them nicely. Instead of using plt.hist, however, I’ll use the seaborn library’s sns.histplot, which creates much nicer looking histograms. Seaborn is an extension library of matplotlib made specifically for making nicer plots of data. Ignore the is_discrete argument for now. I’ll use it in the next section.\n\ndef plot_histogram(x, is_discrete=False, title='', **kwargs):\n    if is_discrete:\n        sns.histplot(x, discrete=True, shrink=0.8, **kwargs)\n        unique = np.unique(x)\n        if len(unique) < 15:\n            plt.xticks(unique)\n    else:\n        sns.histplot(x, **kwargs)\n    plt.title(title)\n    plt.show()\n\nIt’s still kind of hard to see if the \\(100\\) rand samples have any kind of pattern in the above histogram plot. Let’s now sample 10,000 numbers from rand and see if we can find one.\n\nx = np.random.rand(10000)\nplot_histogram(x, bins=10, title=f'rand({10000})')\n\n\n\n\nIt should be increasingly clear now that what’s going on is that rand is sampling numbers between 0 and 1 with equal probability. Each bin should contain roughly \\(\\frac{10000}{10}=1000\\) counts, since there are \\(10000\\) samples and \\(10\\) bins. Said differently, the values in each bin should have a \\(\\frac{1}{10}=0.1\\) probability of being sampled. For example, the values in the left-most bin, call it \\(I_0 = [0, 0.1]\\) should have\n\\[\\mathbb{Pr}(x \\in I_0) = \\mathbb{Pr}(0 \\leq x \\leq 0.1) = 0.1.\\]\nThis type of “flat”, equal probability sampling is called uniform random sampling.\nYou may be questioning that it’s indeed the case that each bin is truly getting sampled as much as the other bins. After all, the plot still clearly shows their heights vary a bit. Some bins have slightly more values than others do. We can look at how many counts are in the bin using np.histogram, which also defaults to \\(10\\) bins. You can see some bins have as many as \\(1037\\) values, some as few as \\(960\\) values.\n\nbin_counts, _ = np.histogram(x)\nbin_counts\n\narray([1025, 1036,  999,  981, 1037,  989,  956,  996,  976, 1005])\n\n\n\n8.1.0.1 Aside: Estimating the Fluctuation in Bin Counts\nThis variation in the bin counts is really due to the fact that we’re only sampling a finite number of values. To get true uniform sampling, where all bins have the same counts, we’d have to sample an infinitely large number of times.\nHere’s a rule of thumb for how much the bin counts should be expected to fluctuate as a function of the sample size. If \\(N\\) is the number of samples, and each bin \\(k\\) contains \\(N_k\\) counts (i.e. its bar height is \\(N_k\\)), then you can expect the counts to fluctuate above and below \\(N_k\\) by about\n\\[\\sigma_k = \\sqrt{N_k\\bigg(1 - \\frac{N_k}{N}\\bigg)}.\\]\nSaid differently, the counts should be expected to roughly lie in a range \\(N_k \\pm \\sigma_k\\). This notation means the same thing as saying the counts should roughly speaking lie in the range \\([N_k - \\sigma_k, N_k + \\sigma_k]\\). By “roughly”, I mean sometimes bins can have counts outside this range, but it’s uncommon.\nIn the above example, there are \\(N=10000\\) samples, and each bin has about \\(N_k=1000\\) counts, so you should expect the counts to fluctuate by about\n\\[\\sigma_k = \\sqrt{1000\\bigg(1 - \\frac{1000}{10000}\\bigg)} = 30,\\]\nwhich means the counts should rougly lie in the range \\(1000 \\pm 30\\). This seems to be in line with what we’re seeing experimentally. Notice as the sample size \\(N \\rightarrow \\infty\\), the fluctuations \\(\\sigma_k \\rightarrow 0\\). We’ll see where this rule comes from later (hint: the binomial distribution).\nBack to random variables. Broadly speaking we can divide random variables into two classes of distributions:\n\ndiscrete distributions: random variables that can only take on a discrete set of values.\ncontinuous distributions: random variables that can take on any continuum of real values.\n\nI’ll start by talking about the discrete case since it’s easier to understand."
  },
  {
    "objectID": "notebooks/probability.html#discrete-probability",
    "href": "notebooks/probability.html#discrete-probability",
    "title": "8  Probability Distributions",
    "section": "8.2 Discrete Probability",
    "text": "8.2 Discrete Probability\nDiscrete random variables are variables that can only take on a discrete range of values. Usually this range is a finite set like \\(\\{0,1\\}\\) or \\(\\{1,2,3,4,5,6\\}\\) or something like that. But they could have an infinite range too, for example the set \\(\\mathbb{N}\\) of all non-negative integers. Rand is not an example of a discrete random variable, since there the range is all of the interval \\([0,1]\\).\nHere are some examples of real life things that can be modeled by a discrete random variable:\n\nModeling the rolls of a die with faces \\(1,2,3,4,5,6\\).\nModeling values from flipping a coin taking on a value of heads or tails.\nModeling a hand of poker, where there are 5 cards each drawn from the same deck of 52 cards.\nModeling the outputs of data used to train a machine learning classification model.\nModeling the number of heads gotten from flipping a coin a whole bunch of times.\nModeling the number of people entering a building per hour.\n\n\n8.2.1 Motivation: Rolling a Die\nConsider a very simple toy problem: rolling a die (singular of dice). If you’ve never seen dice before, they’re white cubes with black dots on each face of the cube. Each face gets some number of black dots on it between 1 and 6. People like to “roll” these dice in games by shaking and tossing them onto the ground. The person with the highest score, i.e. the most number of dots facing upward, wins that round.\n\n\n🎲\n\n\nLet’s think a little bit about a single die. Suppose I want to roll a single die. Having not rolled the die yet, what should I “expect” the value to be when I roll the die? Call this score \\(x\\). The possible values I can have are just the number of dots on each face of the die, i.e. \\(1,2,3,4,5,6\\). This alone doesn’t tell me what the chance is that any given \\(x\\) turns up in a roll. We need some other information.\nPerhaps your common sense kicks in and you think, “Well clearly each number has an equal chance of showing up if you roll the die”. This is called the principle of indifference. In practice you’d usually be right. You’re saying that, since we don’t have any other information to go on, each number should have an equal chance of showing up on each roll. That is, on any given roll, the random variable \\(x\\) should take on each value \\(k=1,2,\\cdots,6\\) with probability,\n\\[p_k = \\mathbb{Pr}(x=k) = \\frac{1}{6}.\\]\nThis just says that the probability of rolling \\(x=1\\) is \\(p_1 = \\frac{1}{6}\\), the probability of rolling \\(x=2\\) is also \\(p_2 = \\frac{1}{6}\\), etc. Notice that these probabilities satisfy two properties that all probabilities must satisfy: 1. Each probability is non-negative: \\(p_k = \\frac{1}{6} \\geq 0\\), 2. The sum of all the possible probabilities is one: \\(\\sum_{k=1}^6 p_k = p_1 + p_2 + p_3 + p_4 + p_5 + p_6 = 6 \\cdot \\frac{1}{6} = 1\\).\nThese two properties are the defining characteristics of a probability. The second condition is just a mathematical way of saying that rolling the die must return some value \\(x \\in \\{1,2,3,4,5,6\\}\\). It can’t just make up some new value, or refuse to answer.\nAnyway, suppose I rolled the die \\(N=36\\) times and got the following values:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRoll\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n\n\n\n\nValue\n3\n4\n5\n4\n3\n1\n3\n6\n5\n2\n1\n5\n4\n2\n1\n1\n1\n6\n5\n6\n3\n5\n5\n3\n3\n6\n6\n1\n5\n4\n2\n2\n4\n6\n2\n4\n\n\n\nWe can make a histogram out of these and check the principle of indifference by verifying the bins are all of about the same height (at least as close to the same as only 30 rolls will allow). Note that I’m now using is_discrete=True here, which tells the helper function to give each unique \\(k\\) its own bin.\n\nx = [3, 4, 5, 4, 3, 1, 3, 6, 5, 2, 1, 5, 4, 2, 1, 1, 1, 6, 5, 6, 3, 5, 5, 3, 3, 6, 6, 1, 5, 4, 2, 2, 4, 6, 2, 4]\nplot_histogram(x, is_discrete=True, title='36 Die Rolls')\n\n\n\n\nGiven the fact that I only rolled \\(36\\) times, this histogram looks very uniform, giving a pretty strong hint that each value has an equal probability of being rolled. Since most bars have height \\(6\\), they correspond to probabilities of \\(\\frac{6}{36}=\\frac{1}{6}\\), which is what our common sense expected. Note the counts can fluctuate in this case in a range of about \\(6 \\pm 2\\). This is an example of a fair die.\nWhat if our common sense was incorrect? What if I rolled the die a bunch of times and found out some numbers occurred a lot more often than others? This would happen if the die were weighted unevenly, or loaded. In this case we’re left to assign some weight \\(N\\) to each number \\(k\\).\nTo determine what the right weights should be empirically, probably the easiest way would again be to roll the die a bunch of times and count how many times each value \\(k\\) occurs. Those counts will be your weights \\(N_k\\). These are just the heights of each bin in the histogram. To turn them into probabilities \\(p_k\\), divide by the total number of rolls, call it \\(N\\). The probabilities would then be given approximately by\n\\[p_k = \\mathbb{Pr}(x=k) \\approx \\frac{N_k}{N}.\\]\nThat is, the probability \\(p_k\\) is just a ratio of counts, the fraction of times \\(x=k\\) occurred in \\(N\\) counts. As \\(N \\rightarrow \\infty\\) this equality goes from approximate to exact. In fact, we could define the probability \\(p_k = \\mathbb{Pr}(x=k)\\) as the limit\n\\[p_k = \\mathbb{Pr}(x=k) = \\lim_{N \\rightarrow \\infty} \\frac{N_k}{N}.\\]\nThis is an alternate way of defining a probability, different from the “degree of belief” approach I used above. This is usually called the frequentist or objective approach. In this approach, probability is the frequency of the number of times an outcome occurs in an experiment, i.e. \\(\\frac{N_k}{N}\\). In contrast, the “degree of belief” perspective is called the Bayesian or subjective approach. Both approaches have their uses, so we’ll go back and forth between the two as it suits us.\nTo test if your die is loaded, what you can do is roll the die \\(N\\) trials and calculate the probabilities. If they’re all roughly equal to \\(1/6\\) like the example above then the die is fair. Otherwise it’s loaded. Suppose when I’d rolled the die I’d instead gotten the following outcomes:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRoll\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n\n\n\n\nValue\n4\n4\n5\n4\n3\n5\n3\n6\n5\n6\n1\n5\n4\n5\n6\n5\n1\n6\n5\n6\n3\n5\n5\n4\n3\n6\n6\n4\n5\n4\n2\n5\n4\n6\n2\n4\n\n\n\nLet’s plot the histogram of these outcomes and compare to the fair die case.\n\nx = [4, 4, 5, 4, 3, 5, 3, 6, 5, 6, 1, 5, 4, 5, 6, 5, 1, 6, 5, 6, 3, 5, 5, 4, 3, 6, 6, 4, 5, 4, 2, 5, 4, 6, 2, 4]\nplot_histogram(x, is_discrete=True, title='36 Die Rolls (Round 2)')\n\n\n\n\nNotice how now the outcomes are skewed towards higher values. This clearly doesn’t look uniform anymore since most of the counts aren’t in the expected range of \\(6 \\pm 2\\). The die has been “loaded to roll high”.\nUsing the experimental approach we can estimate what the probability of rolling each value is. To do that, we can just take each value \\(k\\) and sum up the number of times \\(x=k\\) and divide it by the total counts \\(N\\). This will return an array of probabilities, where each index \\(k\\) contains the entry \\(p_{k+1} = \\frac{N_k}{N}\\).\n\nsupport = np.unique(x)\nN = len(x)\nNk = [sum([x == k]) for k in support]\np = Nk / N\n[f\"Pr(x={i+1}) = {round(p[i], 3)}\" for i in range(len(p))]\n\n['Pr(x=1) = 0.056',\n 'Pr(x=2) = 0.056',\n 'Pr(x=3) = 0.111',\n 'Pr(x=4) = 0.25',\n 'Pr(x=5) = 0.306',\n 'Pr(x=6) = 0.222']\n\n\n\n\n8.2.2 General Case\nOf course, there’s nothing special about a die. We can define probabilities in exactly the same way for any discrete random variable. A random variable \\(x\\) is called discrete if it can take on one of \\(n\\) countable values \\(x_0,x_1,\\cdots,x_{n-1}\\). Suppose we run an experiment \\(n\\) times and observe the outcomes of \\(x\\) at each trial. If \\(x=x_k\\) for some number of counts \\(n_j\\), then the probability \\(x=x_k\\) is given by the limit of running the experiment infinitely many times,\n\\[p_k = \\mathbb{Pr}(x=k) = \\lim_{N \\rightarrow \\infty} \\frac{N_k}{N}.\\]\nThe set of values that \\(x\\) can take on are called the support of the random variable. For values outside the support, it’s assumed the probability is zero. As will always be true with probabilities, it’s still the case that each probability must be non-negative, and they must all sum to one,\n\\[p_k \\geq 0, \\quad \\sum_{k=0}^{n-1} p_k = 1.\\]\nWhile we have an experimental way to calculate probabilities now, it would be useful to define probabilities as functions of random variables so we can study them mathematically. These functions are called probability distributions. Suppose the probabilities \\(p_k\\) are given by some function \\(p(x)\\) mapping outcomes to probabilities. When this is true, we say \\(x\\) is distributed as \\(p(x)\\), written in short-hand as \\(x \\sim p(x)\\). If \\(x\\) is discrete, we call the function \\(p(x)\\) a probability mass function, or PMF for short.\nIn the simple case of the fair die, since each \\(p_k = \\frac{1}{6}\\), its PMF is just the simple constant function \\(p(x) = \\frac{1}{6}\\). This distribution is an example of the discrete uniform distribution. If \\(x\\) is a discrete random variable taking on one of \\(k\\) outcomes, and \\(x\\) is distributed as discrete uniform, then its probabilities are given by \\(p_k = \\frac{1}{n}\\) for all \\(k\\). In histogram language, all bins have approximately the same number of counts.\nIn the less simple case of the loaded die we had to estimate each probability empirically. Supposing we could calculate those probabilities exactly, the PMF for that particular loaded die would look like\n\\[\np(x) =\n\\begin{cases}\n0.056, & x = 1, \\\\\n0.056, & x = 2, \\\\\n0.111, & x = 3, \\\\\n0.250, & x = 4, \\\\\n0.306, & x = 5, \\\\\n0.220, & x = 6.\n\\end{cases}\n\\]\nThis is an example of a categorical distribution. Their histograms can look completely arbitrary. Each bin can contain as many counts as it likes. All that matters is that \\(k\\) is finite and all the probabilities sum to one. Any time you take a discrete uniform random variable and weigh the outcomes (e.g. by loading a die) you’ll create a categorical distribution.\nTypically each distribution will have one or more parameters \\(\\theta\\) that can be adjusted to change the shape or support of the distribution. Instead of writing \\(p(x)\\) for the PMF, when we want to be explicit about the parameters we’ll sometimes write \\(p(x; \\theta)\\). The semi-colon is used to say that any arguments listed after it are understood to be parameters, not function inputs. In this notation, parameters of a distribution are assumed to be known, non-random values. We’ll relax this requirement below, but assume parameters are non-random for now.\nFor example, the discrete uniform distribution has two parameters indicating the lowest and highest values in the support, called \\(a\\) and \\(b\\). We could thus express its PMF as \\(p(x;a,b)\\), which means “the probability of \\(x\\) given known parameters \\(a\\) and \\(b\\)”.\nUsing these parameters, it’s also common to use special symbols as a short-hand for common distributions. For example, the discrete uniform distribution with parameters \\(a\\) and \\(b\\) is often shortened to something like \\(DU(a,b)\\). If we want to say \\(x\\) is a discrete uniform random variable, we’d write \\(x \\sim DU(a,b)\\). You’ll also sometimes see people use the symbol to write the PMF as well, for example \\(DU(x;a,b)\\).\n\n\n8.2.3 Discrete Distributions\nSome discrete probability distributions occur so frequently that they get a special name. Each one tends to occur when modeling certain kinds of phenomena. Here are a few of the most common discrete distributions. I’ll just state them and summarize their properties for future reference.\n\n8.2.3.1 Discrete Uniform Distribution\n\nSymbol: \\(DU(a,b)\\)\nParameters: Integers \\(a, b\\), where \\(a\\) is the minimum and \\(b-1\\) is the maximum value in the support\nSupport: \\(x=a,a+1,\\cdots,b-1\\)\nProbability mass function: \\[p(x; a,b) = \\frac{1}{b-a}, \\ \\text{ for } x = a, a+1, \\cdots, b-1.\\]\nCumulative distribution function: \\[\nP(x; a,b) =\n\\begin{cases}\n0 & x < a, \\\\\n\\frac{\\text{int}(x) - a}{b-a}, & a \\leq x \\leq b, \\\\\n1 & x \\geq 1.\n\\end{cases}\n\\]\nRandom number generator: np.random.randint(a, b)\nNotes:\n\nUsed to model discrete processes that occur with equal weight, or are suspected to (the principle of indifference)\nExample: The fair die, taking \\(a=1, b=7\\) gives \\(x \\sim D(1,7)\\) with \\(p(x) = \\frac{1}{7-1} = \\frac{1}{6}\\)\n\n\n\na = 1\nb = 7\nx = np.random.randint(a, b, size=100000)\nplot_histogram(x, is_discrete=True, stat='probability', title=f'$DU({a},{b})$ PMF')\n\n\n\n\n\n\n8.2.3.2 Bernoulli Distribution\n\nSymbol: \\(\\text{Ber}(\\text{p})\\)\nParameters: The probability of success \\(0 \\leq \\text{p} \\leq 1\\)\nSupport: \\(x=0,1\\)\nProbability mass function: \\[\np(x; \\text{p}) = \\text{p}^x (1-\\text{p})^{1-x} =\n\\begin{cases}\n1-\\text{p} & x = 0, \\\\\n\\text{p} & x = 1.\n\\end{cases}\n\\]\nCumulative distribution function: \\[\nP(x; \\text{p}) =\n\\begin{cases}\n0 & \\text{if } x < 0 \\\\\n1-p & \\text{if } 0 \\leq x < 1 \\\\\n1 & \\text{if } x \\geq 1.\n\\end{cases}.\n\\]\nRandom number generator: np.random.choice([0, 1], p=[1 - p, p])\nNotes:\n\nUsed to model binary processes where the probability of success can be estimated\nExample: Flipping a fair coin, where \\(\\text{tails} = 0\\), \\(\\text{heads} = 1\\), and \\(\\text{p}=\\frac{1}{2}\\)\nUsed for binary classification. Given an input \\(\\mathbf{x}\\) with some binary output \\(y=0,1\\). If \\(\\text{p}=\\hat y\\), then \\(y \\sim \\text{Ber}(\\hat y)\\).\nSpecial case of the binomial distribution where \\(n=1\\): \\(\\text{Ber}(\\text{p}) = \\text{Bin}(1, \\text{p})\\).\n\n\n\np = 0.7\nx = np.random.choice([0, 1], p=[1 - p, p], size=1000)\nplot_histogram(x, is_discrete=True, stat='probability', title=f'$Ber({p})$ PMF')\n\n\n\n\n\n\n8.2.3.3 Categorical Distribution\n\nSymbol: \\(\\text{Cat}(p_0,p_1,\\cdots,p_{k-1})\\) or \\(\\text{Cat}(\\mathbf{p})\\)\nParameters: \\(k\\) non-negative real numbers \\(p_j\\) that sum to one, each representing the probability of getting \\(x_j\\)\n\nCommonly written as a vector \\(\\mathbf{p} = (p_0,p_1,\\cdots,p_{k-1})\\)\n\nSupport: \\(x = 0, 1, \\cdots, k-1\\)\nProbability mass function: \\[\np(x; \\mathbf{p}) = \\begin{cases}\np_0 & x = 0, \\\\\np_1 & x = 1, \\\\\n\\vdots & \\vdots \\\\\np_{k-1} & x = k-1.\n\\end{cases}\n\\]\nCumulative distribution function: \\[\nP(x; \\mathbf{p}) =\n\\begin{cases}\n0 & \\text{if } x \\leq x_0 \\\\\np_0 & \\text{if } x_0 \\leq x \\leq x_1 \\\\\np_0 + p_1 & \\text{if } x_1 \\leq x \\leq x_2 \\\\\np_0 + p_1 + p_2 & \\text{if } x_2 \\leq x \\leq x_3 \\\\\n\\vdots & \\vdots \\\\\n1 & \\text{if } x \\geq x_{n-1}.\n\\end{cases}\n\\]\nRandom number generator: np.random.choice(np.arange(k), p=p)\nNotes:\n\nUsed to model categorical processes where a finite number of classes can occur with arbitrary probabilities\nUsed for multiclass classification. Given an input \\(\\mathbf{x}\\) with outputs in one of \\(k\\) classes \\(y=0,1,\\cdots,k-1\\). If \\(\\mathbf{p}=\\mathbf{\\hat y}\\), then \\(\\mathbf{y} \\sim \\text{Cat}(\\mathbf{\\hat y})\\).\nGeneralization of the Bernoulli distribution, allowing for \\(k\\) distinct outcomes instead of just \\(2\\).\nModels the values rolled from a die when \\(k=6\\).\n\n\n\np = [0.2, 0.5, 0.3]\nx = np.random.choice(np.arange(len(p)), p=p, size=1000)\nplot_histogram(x, is_discrete=True, stat='probability', title=f'$Cat{tuple(p)}$ PMF')\n\n\n\n\n\n\n8.2.3.4 Binomial Distribution\n\nSymbol: \\(\\text{Bin}(n, \\text{p})\\)\nParameters: The number of trials \\(n=1,2,3,\\cdots\\) and probability \\(0 \\leq \\text{p} \\leq 1\\) of success of each trial\nSupport: \\(x = 0, 1, \\cdots, n\\)\nProbability mass function: \\[p(x; n,\\text{p}) = \\binom{n}{x} \\text{p}^{x} (1-\\text{p})^{n-x}, \\ \\text{for} \\ x=0,1,\\cdots,n, \\ \\text{where} \\ \\binom{n}{x} = \\frac{n!}{x!(n-x)!}.\\]\nCumulative distribution function: \\[P(x; n,\\text{p}) = \\sum_{k=0}^{\\text{int}(x)} {n \\choose k} p^k(1-p)^{n-k}.\\]\nRandom number generator: np.random.binomial(n, p)\nNotes:\n\nUsed to model the number of successes from \\(n\\) independent binary processes (analogous to coin flips)\nExample: Flipping a fair coin \\(n\\) times and counting the number of heads\nGeneralization of the Bernoulli distribution. The sum of \\(n\\) independent Bernoulli variables is \\(\\text{Bin}(n, \\text{p})\\).\nThe number of counts in each bin of a histogram of independent samples can be modeled as a binomial random variable\n\n\n\nn = 10\np = 0.7\nx = np.random.binomial(n, p, size=1000)\nplot_histogram(x, is_discrete=True, stat='probability', title=f'$Bin{(n,p)}$ PMF')\n\n\n\n\n\n\n8.2.3.5 Poisson Distribution\n\nSymbol: \\(\\text{Poisson}(\\lambda)\\)\nParameters: A rate parameter \\(\\lambda \\geq 0\\)\nSupport: \\(x = 0, 1, 2, 3, \\cdots\\)\nProbability mass function: \\[p(x; \\lambda) = \\frac{\\lambda^x e^{-\\lambda}}{x!}, \\quad \\text{for} \\ x=0,1,2,3,\\cdots.\\]\nCumulative distribution function: \\[P(x; \\lambda) = e^{-\\lambda}\\sum_{k=0}^{\\text{int}(x)}\\frac{\\lambda^k}{k!}.\\]\nRandom number generator: np.random.poisson(lambda)\nNotes:\n\nUsed to model counting processes, like the number of calls coming into a call center, or the number of times a Geiger counter registers a click\nExample: The number of people walking through the door of a coffee shop per hour can be modeled as a Poisson distribution\n\n\n\nlambda_ = 4\nx = np.random.poisson(lambda_, size=1000)\nplot_histogram(x, is_discrete=True, stat='probability', title=f'$Poisson({lambda_})$ PMF')\n\n\n\n\n\n\n\n8.2.4 Probabilities of Multiple Outcomes\nWe’ve seen how to calculate the probabilities of any one outcome. The probability that \\(x=k\\) is given by \\(\\text{Pr}(x=k) = p(k)\\), where \\(p(k)\\) is the PMF. It’s natural to then ask how we can think about probabilities of multiple outcomes. For example, consider again the situation of rolling a fair die. Suppose we were interested in knowing what the probability was of rolling an even number, i.e. \\(x=2,4,6\\). How would we approach this? Your intuition suggests the right idea. We can just sum the probabilities of each outcome together,\n\\[\\mathbb{Pr}(x\\text{ is even}) = \\mathbb{Pr}(x=2,4,6) = p(2) + p(4) + p(6) = \\frac{1}{6} + \\frac{1}{6} + \\frac{1}{6} = \\frac{1}{2}.\\]\nThis same idea extends to any discrete set. Suppose we’re interested in the probability that some discrete random variable \\(x\\) takes on values in some set \\(E = \\{x_0, x_1, \\cdots, x_{m-1}\\}\\). Then all we need to do is some over the probabilities of all the outcomes in \\(E\\), i.e.\n\\[\\mathbb{Pr}(x \\in E) = \\sum_{k \\in E} p(k) = \\sum_{i=0}^{m-1} p(x_i) = p(x_0) + p(x_1) + \\cdots p(x_{m-1}).\\]\nWhen the set of interest is the entire support of \\(x\\), the right-hand side is just the sum the probability of all possible outcome, which is just one. Thus, we’ll always have \\(0 \\leq \\mathbb{Pr}(x \\in E) \\leq 1\\) for any set \\(E\\).\nThough we don’t really have to for discrete variables, it’s conventional to define another function \\(P(x)\\) called the cumulative distribution function, or CDF. It’s the probability \\(x \\in (-\\infty, x_0]\\) for some fixed value \\(x_0 \\in \\mathbb{R}\\),\n\\[P(x_0) = \\mathbb{Pr}(x \\leq x_0) = \\sum_{k \\leq x_0} p(k) = \\sum_{k=-\\infty}^{\\text{int}(x_0)} p(k),\\]\nwhere it’s understood that \\(p(k)=0\\) whenever \\(k\\) isn’t in the support of \\(x\\). Note the CDF is a real-valued function. We can ask about \\(P(x_0)\\) for any \\(x_0 \\in \\mathbb{R}\\), not just discrete values of \\(x_0\\).\nBut why should we care? It turns out if we know the CDF in some simple form, we can use it to calculate the probability \\(x\\) is in any other interval by differencing the CDF at the endpoints. Suppose we’re interested in the probability \\(a \\leq x \\leq b\\). If we know the CDF for a particular distribution in some simple form, we can just difference it to get the probability of being in the interval, i.e.\n\\[\\mathbb{Pr}(a \\leq x \\leq b) = \\mathbb{Pr}(x \\leq b) - \\mathbb{Pr}(x \\leq a) = P(b) - P(a).\\]\nThis fact is more useful for continuous distributions than discrete ones, since in the discrete case we can always just sum over the values, which is usually pretty quick to do.\n\n8.2.4.1 Application: Getting a Job\nHere’s a useful application where probabilities of multiple outcomes can sometimes come in handy. Suppose you’re applying to a bunch of jobs, and you want to know what is the probability that you’ll get at least one offer. Suppose you’ve applied to \\(n\\) jobs. For simplicity, assume each job has roughly the same probability \\(\\text{p}\\) of giving you an offer. Then each job application looks kind of like the situation of flipping a coin. If \\(x_i=1\\) you get an offer, if \\(x_i=0\\) you get rejected. We can thus think of each job application as a Bernoulli random variable \\(x_i \\sim \\text{Ber}(\\text{p})\\).\nNow, assume that the job applications are all independent of each other, so one company’s decision whether to give you an offer doesn’t affect another company’s decision to give you an offer. This isn’t perfectly true, but it’s reasonably true. In this scenario, the total number of offers \\(x\\) you get out of \\(n\\) job applications will then be binomially distributed, \\(x \\sim \\text{Bin}(n, \\text{p})\\).\nWe can use this fact to answer the question we started out with: What is the probability that you receive at least one offer? It’s equivalent to asking, if \\(x\\) is binomial, what is the probability that \\(x \\geq 1\\)? Now, since \\(x\\) is only supported on non-negative values, we have\n\\[\\begin{align*}\n\\mathbb{Pr}(x \\geq 1) &= \\mathbb{Pr}(x \\geq 0) - \\mathbb{Pr}(x=0) \\\\\n&= 1 - \\mathbb{Pr}(x=0) \\\\\n&= 1 - p(0;n,\\text{p}) \\\\\n&= 1 - \\binom{n}{0} \\text{p}^0 (1-\\text{p})^{n-0} \\\\\n&= 1 - \\frac{n!}{0!(n-0)!} (1-\\text{p})^n \\\\\n&= 1 - (1-\\text{p})^n.\n\\end{align*}\\]\nWe thus have a formula. The probability of receiving at least one job offer from applying to \\(n\\) jobs, assuming each gives an offer with probability \\(\\text{p}\\), and applications are independent of each other, is\n\\[\\mathbb{Pr}(\\text{at least one offer}) = 1 - (1-\\text{p})^n.\\]\nHere’s an example of how this formula can be useful. Suppose you believe you have a 10% chance of getting an offer from any one company you apply to, so \\(\\text{p}=0.1\\). If you apply to \\(n=10\\) jobs, you’ll have about a 34.86% chance of receiving at least one offer.\n\np = 0.1\nn = 10\nprob_offer = 1 - (1 - p) ** n\nprob_offer\n\n0.6513215599\n\n\nLet’s now ask how many jobs you’d have to apply to to give yourself at least a 90% chance of getting at least one job offer? Here’s what you can do. Let \\(O = \\mathbb{Pr}(\\text{at least one offer})\\), so \\(O = (1-p)^n\\). Set \\(O=0.9\\) and solve for \\(n\\). Then you’d have\n\\[\\begin{align*}\nO &= 1 - (1-p)^n \\\\\n(1-p)^n &= 1 - O \\\\\nn \\log(1-p) &= \\log(1 - O) \\\\\nn &= \\frac{\\log(1 - O)}{\\log(1 - p)}.\n\\end{align*}\\]\nPlugging in \\(p=0.1\\) and \\(O=0.9\\) gives \\(n \\approx 21.85\\). Thus, you’d need to apply to at least \\(n=22\\) jobs to have a decent chance of getting at least one offer. Here’s a plot of this idea. Each curve is a plot of \\(n=n(p)\\) for different choices of \\(O\\), in this case, 50%, 75%, 90%, and 99%.\n\np = np.linspace(0.01, 0.999, 100)\nO = [0.5, 0.75, 0.9, 0.99]\nfor o in O:\n    n = np.log(1 - o) / np.log(1 - p)\n    plt.plot(p, n, label=f'$O={round(o*100)}$%')\nplt.xticks(0.1 * np.arange(11))\nplt.ylim(0, 70)\nplt.title(\n    \"\"\"How many jobs would you have to \n    apply to to get at least one job offer\n    with confidence $O$?\"\"\".title(), fontsize=11)\nplt.xlabel('$p$')\nplt.ylabel('$n$')\nplt.grid(True, alpha=0.5)\nplt.legend()\nplt.show()\n\n\n\n\nThe moral of this story is that you have two ways to up your chances of getting a job offer: Up your chances of getting any one job (i.e. increase \\(p\\)), or apply to a lot more jobs (i.e. increase \\(n\\)). The more confident you want to be of getting an offer (i.e. \\(O\\)), the more jobs you’ll need to apply to. This same idea can be used to model the probability of at least one occurrence for any binary event similar to this."
  },
  {
    "objectID": "notebooks/probability.html#continuous-probability",
    "href": "notebooks/probability.html#continuous-probability",
    "title": "8  Probability Distributions",
    "section": "8.3 Continuous Probability",
    "text": "8.3 Continuous Probability\nSo far we’ve covered discrete random variables, ones that take on a finite (or countably infinite) set of values. We can also consider random variables that take on a continuous range of values. For example, a continuous random variable \\(x\\) can take on values in the entire interval \\([0,1]\\), or the whole real line \\(\\mathbb{R} = (-\\infty, \\infty)\\). The key difference between continuous variables and discrete variables is that we have to think in terms of calculus now. Instead of points we’ll have infinitesimal areas. Instead of sums we’ll have integrals.\nIt may not be obvious to you that there are practical examples where continuous random variables would be useful. Here are some examples:\n\nModeling the behavior of random number generators like rand.\nModeling the total sales a business will do next quarter.\nModeling the time it takes for a customer to complete a purchase in an online store.\nModeling the amount of fuel consumed by a vehicle on a given day.\nModeling the height of waves in the ocean at a given time.\nModeling the length of a stay in a hospital by a typical patient.\nModeling the amount of rainfall in a specific region over a period of time.\nModeling the measured voltage of a car battery at any point in time.\n\nIn fact, any continuous variable you can think of could be treated as random depending on the situation. Even if a variable is completely deterministic, there may be situations where it’s helpful to think of it as random. The whole idea of Monte Carlo methods is based on this idea, in fact.\n\n8.3.1 Motivation: Rand Again\nI showed example of a continuous random variable already at the beginning of this lesson, when I introduced the idea of random number generators like rand. Rand is an example of a function that can (approximately) generate samples of a continuous random variable. In particular, it samples uniformly from the interval \\([0,1]\\). I already showed what its histogram looks like for a large number of samples. Here it is again.\n\nx = np.random.rand(10000)\nplot_histogram(x, bins=10, title=f'rand({10000})')\n\n\n\n\nLet’s now try to figure out how we should define the probability of values sampled from rand. In the discrete case, we were able to define probabilities by running an experiment (e.g. rolling a die a bunch of times). We could look at the ratio of the number of times \\(N_k\\) an outcome \\(k\\) occurred over the number of total trials \\(N\\). This made sense in the discrete case since we could reasonably well rely on each outcome \\(x=k\\) occurring enough times to get a meaningful count.\nThis approach doesn’t work well for continuous random variables. Suppose \\(x\\) is the random variable resulting from rand, uniform on the interval \\([0,1]\\). If I sample a single value from rand, there’s no reason to assume I’ll ever see that exact value again. There are uncountably infinitely many values to choose from in \\([0,1]\\), so I’m pretty much guaranteed to never see the same value twice. Instead of counting how many times each value occurs, what I can do is use the binning trick we saw with histograms. For example, I can divide \\([0,1]\\) up into ten subintervals (or bins)\n\\[I_0=[0,0.1], \\quad I_1=[0.1,0.2], \\quad I_3=[0.2,0.3], \\quad \\cdots, \\quad I_9=[0.9,1].\\]\nIf I sample one value from rand it’s guaranteed to be in one of these subintervals \\(I_k\\). If I sample a whole bunch of values from rand, say \\(N=1000\\), I should expect each \\(I_k\\) to contain about \\(N_k=100\\) counts (10% of the total since there are 10 bins). It thus seems to make perfect sense to define a probability on each \\(I_k\\),\n\\[\\mathbb{Pr}(x \\in I_k) = \\frac{N_k}{N} = \\frac{100}{1000} = \\frac{1}{10} = 0.1.\\]\n\nN = 10000\nM = 10\ndx = 1 / M\nx = np.random.rand(N)\nplot_histogram(x, bins=M, title=f'M=${M}$ subintervals of length $dx={dx}$')\n\n\n\n\nWe still want to approximate the discrete idea of having a probability \\(\\mathbb{Pr}(x=k)\\). How can we do it using this idea of subintervals? Enter calculus. What we can imagine doing is allowing each subinterval \\(I_k\\) to become infinitesimally small. Suppose we subdivide \\([0,1]\\) into \\(M\\) total subintervals each of infinitesimal length \\(dx\\), satisfying \\(M=\\frac{1}{dx}\\), i.e.\n\\[I_0=[0,dx], \\quad I_1=[dx, 2dx], \\quad I_2=[2dx, 3dx], \\quad \\cdots, \\quad I_{M-1}=[(M-1)dx, 1].\\]\nSuppose \\(x_0\\) is some point in one of these tiny intervals \\(I_k=[kdx, (k+1)dx]\\). Since each \\(I_k\\) is a very tiny interval, the probability that \\(x \\approx x_0\\) is pretty much exactly the same thing as the probability that \\(x \\in I_k\\). Let’s thus define the probability that \\(x \\approx x_0\\) as the probability that \\(x \\in I_k\\),\n\\[\\mathbb{Pr}(x \\approx x_0) = \\mathbb{Pr}(x \\in I_k) = \\lim_{N \\rightarrow \\infty} \\frac{N_k}{N}.\\]\nHere’s an approximate representation of this idea. I won’t be able to make \\(M=10^{300}\\) bins like I’d like, but I can at least make bins so you can see the point. I’ll need to generate a huge number of samples \\(N\\) so the histogram will populate. Notice each \\(N_k \\approx \\frac{N}{M} = 1000\\). That is,\n\\[\\mathbb{Pr}(x \\approx x_0) \\approx \\frac{N_k}{N} \\approx \\frac{N/M}{N} = \\frac{1}{M} = dx.\\]\nEvidently, the probability \\(x \\approx x_0\\) is infinitesimal, so very very tiny. This is why you’ll basically never sample the same value twice.\n\nN = 1000000\nM = N // 1000\ndx = 1 / M\nx = np.random.rand(N)\nplot_histogram(x, bins=M, title=f'M=${M}$ subintervals of length $dx={dx}$')\n\n\n\n\n\n\n8.3.2 General Case\nThe facts I’ve shown about rand extend to more general continuous random variables as well. Suppose \\(x\\) is supported on some interval \\([a,b]\\). It could even be infinite. Let’s divide this interval up into \\(M\\) tiny sub-intervals of length \\(dx\\), where \\(M\\) must satisfy \\(M = \\frac{b-a}{dx}\\),\n\\[I_0=[a,a+dx], \\quad I_1=[a+dx, a+2dx], \\quad I_2=[a+2dx, a+3dx], \\quad \\cdots, \\quad I_{M-1}=[a+(M-1)dx, b].\\]\nNow, run an experiment \\(N\\) times and count how many times outcomes occur, not for each \\(x\\), but for each subinterval \\(I_k=[a+kdx, a+(k+1)dx]\\). If \\(x_0 \\in I_k\\), that is, if \\(a+kdx \\leq x_0 \\leq a+(k+1)dx\\), then the probability that \\(x \\approx x_0\\) is defined by,\n\\[\\mathbb{Pr}(x \\approx x_0) = \\mathbb{Pr}(x \\in I_k) = \\lim_{N \\rightarrow \\infty} \\frac{N_k}{N}.\\]\nJust as with the uniform case before, it’s useful to think of the probability \\(\\mathbb{Pr}(x \\approx x_0)\\) as explicitly being proportional to the subinterval length \\(dx\\). In the uniform case it was just \\(\\mathbb{Pr}(x \\approx x_0)=dx\\) exactly. In the more general case, \\(\\mathbb{Pr}(x \\approx x_0)\\) may depend on the value of \\(x_0\\), so we need to weight the right-hand side by some non-negative weighting function \\(p(x) \\geq 0\\), so\n\\[\\mathbb{Pr}(x \\approx  x_0) = \\mathbb{Pr}(x \\in I_k) = p(x_0)dx.\\]\nThis weighting function \\(p(x)\\) is called the probability density function, or PDF for short. It’s the continuous analogue of the probability mass function from the discrete case (hence why I use the same notation). Unlike the discrete PMF, the PDF is not a probability all by itself. It’s a probability per infinitesimal unit \\(dx\\). That is, it’s a density. For this reason, the PDF need not sum to one. It only needs to be non-negative, i.e. all outputs \\(p(x_0)\\) should lie on or above the x-axis, never below it. But any one output \\(p(x_0)\\) can be arbitrarily large, even \\(\\infty\\)!\nWhat must be true is that all probabilities sum to one. Since each \\(\\mathbb{Pr}(x \\approx x_0)\\) is infinitesimal now, this means all probablities must integrate to one over the support of \\(x\\). If \\(x\\) is supported on \\([a,b]\\), then\n\\[\\mathbb{Pr}(a \\leq x \\leq b) = \\sum_{k=0}^{M-1} \\mathbb{Pr}(x \\in I_k) = \\int_a^b p(x)dx = 1.\\]\nThis means we can think of a PDF as being any non-negative function that integrates to one. In fact, any function that satisfies this property is a valid PDF for some continuous random variable.\nSpecifying the functional form of the PDF \\(p(x)\\) creates a continuous probability distribution. By specifying \\(p(x)\\), we’ve uniquely specified what the probabilities have to be for the variable \\(x\\). In the next section I’ll define some of the most common continuous distributions.\nJust as with discrete probabilities, we can get the probability that \\(x\\) is in any set by summing over all the values in that set. The only difference is we replace the sum with an integral over the set. For example, the probability that \\(c \\leq x \\leq d\\) is given by\n\\[\\mathbb{Pr}(c \\leq x \\leq d) = \\int_c^d p(x)dx.\\]\nWe can also define a cumulative distribution function \\(P(x)\\) for continuous probabilities in exactly the same way, except again replacing sums with integrals,\n\\[P(x_0) = \\mathbb{Pr}(x \\leq x_0) = \\int_{-\\infty}^{x_0} p(x')dx',\\]\nwhere it’s understood that \\(p(x')=0\\) whenever \\(x'\\) is outside the support of \\(x\\).\nIf we can obtain the CDF for a distribution, we can calculate the probability \\(x\\) is in any set without having to evaluate an integral. For example, if the set is again the interval \\([c,d]\\), then\n\\[\\mathbb{Pr}(c \\leq x \\leq d) = P(d) - P(a).\\]\nThis is just a restatement of the rule for definite integrals from the calculus lesson, if \\(f(x)=\\frac{d}{dx}F(x)\\), then\n\\[\\int_c^d f(x) dx = F(d) - F(c).\\]\nTo show a brief example, I’ll calculate the CDF of the rand distribution shown already, where \\(x\\) is uniform on \\([0,1]\\). I already showed that its PDF is just \\(p(x)=1\\) for all \\(0 \\leq x \\leq 1\\). Outside this interval \\(p(x)=0\\) everywhere. Using the PDF I can calculate the CDF by integrating. There are three cases to consider. If \\(x < 0\\), the CDF will just be \\(P(x)=0\\) since \\(p(x)=0\\). If \\(x > 1\\), \\(P(x) = 1\\) since we’re integrating over the whole support \\([0,1]\\). Otherwise, we’re integrating over some subinterval \\([0,x]\\), in which case \\(P(x)=x\\). That is,\n\\[\nP(x) = \\int_{-\\infty}^x p(x') dx' =\n\\begin{cases}\n0, & x < 0 \\\\\nx, & 0 \\leq x \\leq 1 \\\\\n1, & x > 1.\n\\end{cases}\\]\nHere’s a plot of both the PDF and CDF of rand. Notice the PDF is just the constant \\(p(x)=1\\) on \\([0,1]\\), whose area under the curve is just one, since the total probability must integrate to one. Also, notice how this same area is the exact same thing that the histogram tries to approximate. In fact, a histogram is just a discrete approximation to the area under a continuous PDF.\nFor the CDF, notice how the function starts at \\(P(x)=0\\) on the far left, and ramps up monotonically to \\(P(x)=1\\) as \\(x\\) increases. Every CDF will have this property. The only difference is what the ramp looks like. It’ll always be the case that \\(P(-\\infty)=0\\), \\(P(\\infty)=1\\), and some monotonic increasing curve connects these two extremes.\n\nx = np.linspace(0, 1, 100)\np = lambda x: np.ones(len(x))\nplot_function(x, p, xlim=(-0.5, 1.5), ylim=(-0.5, 1.5), set_ticks=True, title='Rand PDF')\n\n\n\n\n\nx = np.linspace(-1, 2, 100)\nP = lambda x: np.clip(x, 0, 1) ## quick way to define the piecewise CDF shown above\nplot_function(x, P, xlim=(-1, 2), ylim=(-0.5, 1.5), title='Rand CDF')\n\n\n\n\n\n\n8.3.3 Continuous Distributions\nAs with discrete distributions, some continuous distributions occur so frequently that they get a special name. Here are a few of the most common continuous distributions. I’ll just state them and summarize their properties for future reference.\n\n8.3.3.1 Uniform Distribution\n\nSymbol: \\(U(a,b)\\)\nParameters: The minimum \\(a\\) and maximum \\(b\\) values in the support\nSupport: \\(x \\in [a,b]\\)\nProbability density function: \\[p(x; a,b) = \\frac{1}{b-a}, \\ \\text{ for } a \\leq x \\leq b.\\]\nCumulative distribution function: \\[\nP(x; a, b) =\n\\begin{cases}\n0 & x < a, \\\\\n\\frac{x - a}{b-a}, & a \\leq x \\leq b, \\\\\n1 & x \\geq 1.\n\\end{cases}\n\\]\nRandom number generator: np.random.uniform(a, b)\nNotes:\n\nUsed to model continuous processes that occur with equal weight, or are suspected to (the principle of indifference)\nExample: The values sampled from rand, where \\(a=0\\) and \\(b=1\\), so \\(x \\sim U(0,1)\\).\nThe rand example \\(U(0,1)\\) is called the standard uniform distribution.\n\n\n\na, b = -2, 5\nx = np.linspace(a, b, 1000)\np = lambda x: 1 / (b - a) * np.ones(len(x))\nplot_function(x, p, xlim=(a - 0.5, b + 0.5), ylim=(-0.5 / (b - a), 1.5 / (b - a)), set_ticks=True,\n              title=f'$U({a},{b})$ PDF')\n\n\n\n\n\n\n8.3.3.2 Gaussian Distribution (Normal Distribution)\n\nSymbol: \\(\\mathcal{N}(\\mu, \\sigma^2)\\)\nParameters: The mean \\(\\mu \\in \\mathbb{R}\\) and variance \\(\\sigma^2 \\geq 0\\) of the distribution\nSupport: \\(x \\in \\mathbb{R}\\)\nProbability density function: \\[p(x; \\mu , \\sigma^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp{\\bigg(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\bigg)}.\\]\nCumulative distribution function: \\[P(x; \\mu , \\sigma^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\int_{-\\infty}^x \\exp{\\bigg(-\\frac{(x' - \\mu)^2}{2\\sigma^2}\\bigg)} dx'.\\]\nRandom number generator: np.random.normal(mu, sigma) (note it sigma is the square root of the variance \\(\\sigma^2\\))\nNotes:\n\nUsed to model the sum or mean of many continuous random variables, e.g. the distribution of unbiased measurements of some continuous quantity\nExample: The distribution of heights in a given population of people.\nUsed in machine learning to model the outputs of an L2 regression model. Given a input \\(\\mathbf{x}\\) with a continuous output \\(y\\), model \\(y = f(\\mathbf{x}) + \\varepsilon\\), where \\(\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)\\) is some random error term and \\(f(\\mathbf{x})\\) is some deterministic function to be learned. Then \\(y \\sim \\mathcal{N}(f(\\mathbf{x}),\\sigma^2)\\).\nThe special case when \\(\\mu=0, \\sigma^2=1\\) is called the standard Gaussian distribution, written \\(\\mathcal{N}(0,1)\\). Values sampled from a standard Gaussian are commonly denoted by \\(z\\). By convention, its PDF is denoted by \\(\\phi(z)\\) and its CDF by \\(\\Phi(z)\\).\nCan turn any Gaussian random variable \\(x\\) into a standard Gaussian or vice versa via the transformations \\[z = \\frac{x-\\mu}{\\sigma}, \\qquad x = \\sigma z + \\mu.\\]\nThe CDF of a Gaussian can’t be written in closed form since the Gaussian integral can’t be written in terms of elementary functions. Since the standard Gaussian CDF \\(\\Phi(z)\\) has a library implementation it’s most common to transform other Gaussian CDFs into standard form and then calculate that way. Use the function norm.cdf from scipy.stats to get the standard CDF function \\(\\Phi(z)\\).\n\n\n\nx = np.linspace(-10, 10, 1000)\np_gaussian = lambda x: 1 / np.sqrt(2 * np.pi) * np.exp(-1/2 * x**2)\nplot_function(x, p_gaussian, xlim=(-3, 3), ylim=(0, 0.5), set_ticks=False,\n              title=f'Standard Gaussian PDF')\n\n\n\n\n\nfrom scipy.stats import norm\n\nx = np.linspace(-3, 3, num=100)\nPhi = lambda x: norm.cdf(x)\nplot_function(x, Phi, xlim=(-3, 3), ylim=(0, 1), set_ticks=False, title='Standard Gaussian CDF')\n\n\n\n\n\n\n8.3.3.3 Laplace Distribution\n\nSymbol: \\(\\text{Laplace}(\\mu, s)\\)\nParameters: The mean \\(\\mu \\in \\mathbb{R}\\) and scale \\(s \\geq 0\\) of the distribution\nSupport: \\(x \\in \\mathbb{R}\\)\nProbability density function: \\[p(x; \\mu , s) = \\frac{1}{2s} \\exp\\bigg(-\\frac{|x-\\mu|}{s}\\bigg).\\]\nCumulative distribution function: \\[\nP(x; \\mu , s) =\n\\begin{cases}\n\\frac{1}{2} \\exp\\bigg(-\\frac{|x-\\mu|}{s}\\bigg), & x \\leq \\mu \\\\\n1 - \\frac{1}{2} \\exp\\bigg(-\\frac{|x-\\mu|}{s}\\bigg), & x > \\mu.\n\\end{cases}\n\\]\nRandom number generator: np.random.laplace(mu, s)\nNotes:\n\nUsed to model Gaussian-like situations where extreme values are somewhat more likely to occur than in a Gaussian. These are called outliers.\nExample: The distribution of finanical stock returns, where extreme returns are more likely than expected under a Gaussian distribution.\nUsed in machine learning to model the outputs of an L1 regression model. Given an input \\(\\mathbf{x}\\) with a continuous output \\(y\\), model \\(y = f(\\mathbf{x}) + \\varepsilon\\), where \\(\\varepsilon \\sim \\text{Laplace}(0, s)\\) is some random error term (that can be extreme-valued) and \\(f(\\mathbf{x})\\) is some deterministic function to be learned. Then the outputs are also Laplace distributed, with \\(y \\sim \\text{Laplace}(f(\\mathbf{x}), s)\\).\nThe special case when \\(\\mu=0, s=1\\) is called the standard Laplace distribution, written \\(\\text{Laplace}(0, 1)\\).\n\n\n\nx = np.linspace(-10, 10, 1000)\np_laplace = lambda x: 1 / (2 * np.pi) * np.exp(-np.abs(x))\nps = [p_gaussian, p_laplace]\nplot_function(x, ps, xlim=(-4, 4), ylim=(0, 0.5), set_ticks=False, labels=['Gaussian', 'Laplace'],\n             title='Gaussian vs Laplace PDFs')\n\n\n\n\n\n\n\n8.3.4 Cauchy Distribution\n\nSymbol: \\(\\text{Cauchy}(m, s)\\)\nParameters: The median \\(m \\in \\mathbb{R}\\) and scale \\(s > 0\\) of the distribution.\nSupport: \\(x \\in \\mathbb{R}\\)\nProbability density function: \\[p(x; m, s) = \\frac{1}{\\pi s} \\frac{1}{1 + \\big(\\frac{x-m}{s}\\big)^2}.\\]\nCumulative distribution function: \\[P(x; m, s) = \\frac{1}{\\pi} \\arctan \\bigg(\\frac{x-m}{s}\\bigg) + \\frac{1}{2}.\\]\nRandom number generator: s * np.random.standard_cauchy() + m\nNotes:\n\nUsed to model Gaussian-like situations where extreme values are highly likely to occur frequently.\nSuch a distribution is said to exhibit heavy-tailed behavior, since there’s a “heavy” amount of probability in the tails of the distribution, making extreme values likely to occur.\nExample: The distribution of computer program runtimes often exhibits heavy-tailed behavior.\nThe case when \\(m=0\\) and \\(s=1\\) is called the standard Cauchy distribution, denoted \\(\\text{Cauchy}(0, 1)\\).\nTechnically speaking, the mean of the Cauchy distribution doesn’t exist, so you have to use the median instead.\n\n\n\nx = np.linspace(-10, 10, 1000)\np_cauchy = lambda x: 1 / np.pi * 1 / (1 + x ** 2)\nps = [p_gaussian, p_laplace, p_cauchy]\nplot_function(x, ps, xlim=(-10, 10), ylim=(0, 0.5), set_ticks=False, labels=['Gaussian', 'Laplace', 'Cauchy'],\n             title='Gaussian vs Laplace vs Caucy PDFs')\n\n\n\n\n\n8.3.4.1 Exponential Distribution\n\nSymbol: \\(\\text{Exp}(\\lambda)\\)\nParameters: A rate parameter \\(\\lambda > 0\\)\nSupport: \\(x \\in [0,\\infty)\\)\nProbability density function: \\[p(x; \\lambda) = \\lambda e^{-\\lambda x}.\\]\nCumulative distribution function: \\[P(x; \\lambda) = 1 - e^{-\\lambda x}.\\]\nRandom number generator: np.random.exponential(lambda)\nNotes:\n\nUsed to model the time between two independent discrete events, assuming those events occur at a roughly constant rate.\nExample: The time between earthquakes in a given region, assuming earthquakes are rare and independent events.\nFrequently used to model the time between two Poisson distributed events. If the events are Poisson distributed and independent, then the time between any two events will be exponentially distributed.\n\n\n\nlambda_ = 1\nx = np.linspace(0, 20, 100)\np = lambda x: lambda_ * np.exp(-lambda_ * x)\nplot_function(x, p, xlim=(0, 20), ylim=(0, 1), set_ticks=False, title=f'$Exp({lambda_})$ PDF')"
  }
]